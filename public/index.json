[{"categories":["折腾日记"],"content":"使用github action构建自动交付与持续集成，algolia构建全文搜索引擎 原先博客是使用的lunr作为搜索实现，但是只能搜索英文，对于中文不能很好的检索，于是便使用了algolia全文搜索引擎。 Algolia 是一个托管搜索引擎，提供全文、数字和分面搜索，能够通过第一次击键提供实时结果。Algolia 强大的 API 可让您在网站和移动应用程序中快速、无缝地实施搜索。我们的搜索 API 每月为数千家公司提供数十亿次查询，在世界任何地方在 100 毫秒内提供相关结果。 在本站点所使用主题LoveIt已经集成了algolia，原作者认为配置algolia的索引文件比较麻烦，于是便自己想实现一个，最初是使用的官方的algolia cli，但是会出现bufio.Reader: token is too long的源码错误。于是乎，在网上冲浪看到了使用github action自动上传索引文件的项目，便采用之。 最后，workflows依然会出现一点问题，主要是我的索引文件比较大，但不影响使用，能传送成功。 ","date":"2023-12-28","objectID":"/%E6%8A%98%E8%85%BE%E5%B0%8F%E8%AE%B0-2023-12-28/:0:0","tags":[],"title":"折腾小记 2023 12 28","uri":"/%E6%8A%98%E8%85%BE%E5%B0%8F%E8%AE%B0-2023-12-28/"},{"categories":["golang"],"content":"这里介绍go的栈内存管理","date":"2023-10-30","objectID":"/go%E8%AF%AD%E8%A8%80%E8%AE%BE%E8%AE%A1%E4%B8%8E%E5%AE%9E%E7%8E%B0%E4%B9%8B%E6%A0%88%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86/","tags":["go语言底层原理"],"title":"Go语言设计与实现之栈内存管理","uri":"/go%E8%AF%AD%E8%A8%80%E8%AE%BE%E8%AE%A1%E4%B8%8E%E5%AE%9E%E7%8E%B0%E4%B9%8B%E6%A0%88%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86/"},{"categories":["golang"],"content":"go的栈内存 栈内存与堆内存不一样，一般GC扫描的对象属于堆区，局部变量、函数参数等都分配到栈内存，而全局变量等会分配到堆区。 那么栈内存并不由GC来释放没有使用的内存，而是由编译器自动分配和释放：随着函数生命周期创建和销毁 局部变量不一定分配在栈内存，也可能会逃逸到堆区。 func NewObj() *int { a:=1 return \u0026a } ","date":"2023-10-30","objectID":"/go%E8%AF%AD%E8%A8%80%E8%AE%BE%E8%AE%A1%E4%B8%8E%E5%AE%9E%E7%8E%B0%E4%B9%8B%E6%A0%88%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86/:0:0","tags":["go语言底层原理"],"title":"Go语言设计与实现之栈内存管理","uri":"/go%E8%AF%AD%E8%A8%80%E8%AE%BE%E8%AE%A1%E4%B8%8E%E5%AE%9E%E7%8E%B0%E4%B9%8B%E6%A0%88%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86/"},{"categories":["golang"],"content":"设计原理 ","date":"2023-10-30","objectID":"/go%E8%AF%AD%E8%A8%80%E8%AE%BE%E8%AE%A1%E4%B8%8E%E5%AE%9E%E7%8E%B0%E4%B9%8B%E6%A0%88%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86/:1:0","tags":["go语言底层原理"],"title":"Go语言设计与实现之栈内存管理","uri":"/go%E8%AF%AD%E8%A8%80%E8%AE%BE%E8%AE%A1%E4%B8%8E%E5%AE%9E%E7%8E%B0%E4%B9%8B%E6%A0%88%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86/"},{"categories":["golang"],"content":"线程栈 大多数OS创建线程的大小默认都在2MB到4MB左右，而且后续不会扩缩容，大小固定。 这对于瞬时创建大量并发任务，但是所需栈空间较小的场景，固定栈不太合适。 go是在用户态自己实现了一个可以自扩缩容的栈内存。 ","date":"2023-10-30","objectID":"/go%E8%AF%AD%E8%A8%80%E8%AE%BE%E8%AE%A1%E4%B8%8E%E5%AE%9E%E7%8E%B0%E4%B9%8B%E6%A0%88%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86/:1:1","tags":["go语言底层原理"],"title":"Go语言设计与实现之栈内存管理","uri":"/go%E8%AF%AD%E8%A8%80%E8%AE%BE%E8%AE%A1%E4%B8%8E%E5%AE%9E%E7%8E%B0%E4%B9%8B%E6%A0%88%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86/"},{"categories":["golang"],"content":"逃逸分析 逃逸分析指的是确定哪些变量应该分配到栈上，哪些变量应该分配到堆上。逃逸分析两个不变性： 指向栈对象的指针不能存在于堆中； 指向栈对象的指针不能在栈对象回收后存活； ","date":"2023-10-30","objectID":"/go%E8%AF%AD%E8%A8%80%E8%AE%BE%E8%AE%A1%E4%B8%8E%E5%AE%9E%E7%8E%B0%E4%B9%8B%E6%A0%88%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86/:1:2","tags":["go语言底层原理"],"title":"Go语言设计与实现之栈内存管理","uri":"/go%E8%AF%AD%E8%A8%80%E8%AE%BE%E8%AE%A1%E4%B8%8E%E5%AE%9E%E7%8E%B0%E4%B9%8B%E6%A0%88%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86/"},{"categories":["golang"],"content":"栈内存空间 分段栈 go1.3之前，采用分段栈的方式划分栈内存。协程初始化时，会分配固定大小的栈空间，之后随着函数调用越来越多，会创建新的栈空间与被调用函数的栈空间链表相连 缺点： 如果当前 Goroutine 的栈几乎充满，那么任意的函数调用都会触发栈扩容，当函数返回后又会触发栈的收缩，如果在一个循环中调用函数，栈的分配和释放就会造成巨大的额外开销，这被称为热分裂问题（Hot split）； 一旦 Goroutine 使用的内存越过了分段栈的扩缩容阈值，运行时会触发栈的扩容和缩容，带来额外的工作量； 连续栈 连续栈可以解决分段栈中存在的两个问题。连续栈扩容时，会创建新的一个大栈，再将以前旧栈的数据拷贝过来，销毁旧栈 ","date":"2023-10-30","objectID":"/go%E8%AF%AD%E8%A8%80%E8%AE%BE%E8%AE%A1%E4%B8%8E%E5%AE%9E%E7%8E%B0%E4%B9%8B%E6%A0%88%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86/:1:3","tags":["go语言底层原理"],"title":"Go语言设计与实现之栈内存管理","uri":"/go%E8%AF%AD%E8%A8%80%E8%AE%BE%E8%AE%A1%E4%B8%8E%E5%AE%9E%E7%8E%B0%E4%B9%8B%E6%A0%88%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86/"},{"categories":["golang"],"content":"栈操作 ","date":"2023-10-30","objectID":"/go%E8%AF%AD%E8%A8%80%E8%AE%BE%E8%AE%A1%E4%B8%8E%E5%AE%9E%E7%8E%B0%E4%B9%8B%E6%A0%88%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86/:2:0","tags":["go语言底层原理"],"title":"Go语言设计与实现之栈内存管理","uri":"/go%E8%AF%AD%E8%A8%80%E8%AE%BE%E8%AE%A1%E4%B8%8E%E5%AE%9E%E7%8E%B0%E4%B9%8B%E6%A0%88%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86/"},{"categories":["golang"],"content":"栈初始化 运行时使用全局的 runtime.stackpool 和线程缓存中的空闲链表分配 32KB 以下的栈内存，使用全局的 runtime.stackLarge 和堆内存分配 32KB 以上的栈内存，提高本地分配栈内存的性能。 ","date":"2023-10-30","objectID":"/go%E8%AF%AD%E8%A8%80%E8%AE%BE%E8%AE%A1%E4%B8%8E%E5%AE%9E%E7%8E%B0%E4%B9%8B%E6%A0%88%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86/:2:1","tags":["go语言底层原理"],"title":"Go语言设计与实现之栈内存管理","uri":"/go%E8%AF%AD%E8%A8%80%E8%AE%BE%E8%AE%A1%E4%B8%8E%E5%AE%9E%E7%8E%B0%E4%B9%8B%E6%A0%88%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86/"},{"categories":["golang"],"content":"栈分配 如果栈空间较小（小于32KB），使用全局栈缓存或者线程缓存上固定大小的空闲链表分配内存 如果栈空间较大（大于32KB），从全局的大栈缓存 runtime.stackLarge 中获取内存空间 如果栈空间较大并且 runtime.stackLarge 空间不足，在堆上申请一片大小足够内存空间 ","date":"2023-10-30","objectID":"/go%E8%AF%AD%E8%A8%80%E8%AE%BE%E8%AE%A1%E4%B8%8E%E5%AE%9E%E7%8E%B0%E4%B9%8B%E6%A0%88%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86/:2:2","tags":["go语言底层原理"],"title":"Go语言设计与实现之栈内存管理","uri":"/go%E8%AF%AD%E8%A8%80%E8%AE%BE%E8%AE%A1%E4%B8%8E%E5%AE%9E%E7%8E%B0%E4%B9%8B%E6%A0%88%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86/"},{"categories":["golang"],"content":"栈扩容 主要指连续栈： 在内存空间中分配更大的栈内存空间； 将旧栈中的所有内容复制到新栈中； 将指向旧栈对应变量的指针重新指向新栈； 销毁并回收旧栈的内存空间； ","date":"2023-10-30","objectID":"/go%E8%AF%AD%E8%A8%80%E8%AE%BE%E8%AE%A1%E4%B8%8E%E5%AE%9E%E7%8E%B0%E4%B9%8B%E6%A0%88%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86/:2:3","tags":["go语言底层原理"],"title":"Go语言设计与实现之栈内存管理","uri":"/go%E8%AF%AD%E8%A8%80%E8%AE%BE%E8%AE%A1%E4%B8%8E%E5%AE%9E%E7%8E%B0%E4%B9%8B%E6%A0%88%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86/"},{"categories":["golang"],"content":"栈缩容 运行时只会在栈内存使用不足 1/4 时进行缩容，缩容也会调用扩容时使用的 runtime.copystack 开辟新的栈空间。触发栈的缩容时，新栈的大小会是原始栈的一半，不过如果新栈的大小低于程序的最低限制 2KB，那么缩容的过程就会停止。 ","date":"2023-10-30","objectID":"/go%E8%AF%AD%E8%A8%80%E8%AE%BE%E8%AE%A1%E4%B8%8E%E5%AE%9E%E7%8E%B0%E4%B9%8B%E6%A0%88%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86/:2:4","tags":["go语言底层原理"],"title":"Go语言设计与实现之栈内存管理","uri":"/go%E8%AF%AD%E8%A8%80%E8%AE%BE%E8%AE%A1%E4%B8%8E%E5%AE%9E%E7%8E%B0%E4%B9%8B%E6%A0%88%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86/"},{"categories":["golang"],"content":"这里介绍go的垃圾回收机制：三色标记和混合写屏障","date":"2023-10-29","objectID":"/go%E8%AF%AD%E8%A8%80%E8%AE%BE%E8%AE%A1%E4%B8%8E%E5%AE%9E%E7%8E%B0%E4%B9%8B%E5%9E%83%E5%9C%BE%E5%9B%9E%E6%94%B6/","tags":["go语言底层原理"],"title":"Go语言设计与实现之垃圾回收","uri":"/go%E8%AF%AD%E8%A8%80%E8%AE%BE%E8%AE%A1%E4%B8%8E%E5%AE%9E%E7%8E%B0%E4%B9%8B%E5%9E%83%E5%9C%BE%E5%9B%9E%E6%94%B6/"},{"categories":["golang"],"content":"go的GC GC主要作用于堆区。 相比于C、C++等语言手动管理内存机制，go具备GC机制。GC的意思是垃圾回收，用以回收不再使用的内存空间。GC期间可能发生STW问题，go一直在迭代优化GC，以降低GC导致STW的时间。 ","date":"2023-10-29","objectID":"/go%E8%AF%AD%E8%A8%80%E8%AE%BE%E8%AE%A1%E4%B8%8E%E5%AE%9E%E7%8E%B0%E4%B9%8B%E5%9E%83%E5%9C%BE%E5%9B%9E%E6%94%B6/:0:0","tags":["go语言底层原理"],"title":"Go语言设计与实现之垃圾回收","uri":"/go%E8%AF%AD%E8%A8%80%E8%AE%BE%E8%AE%A1%E4%B8%8E%E5%AE%9E%E7%8E%B0%E4%B9%8B%E5%9E%83%E5%9C%BE%E5%9B%9E%E6%94%B6/"},{"categories":["golang"],"content":"设计原理 ","date":"2023-10-29","objectID":"/go%E8%AF%AD%E8%A8%80%E8%AE%BE%E8%AE%A1%E4%B8%8E%E5%AE%9E%E7%8E%B0%E4%B9%8B%E5%9E%83%E5%9C%BE%E5%9B%9E%E6%94%B6/:1:0","tags":["go语言底层原理"],"title":"Go语言设计与实现之垃圾回收","uri":"/go%E8%AF%AD%E8%A8%80%E8%AE%BE%E8%AE%A1%E4%B8%8E%E5%AE%9E%E7%8E%B0%E4%B9%8B%E5%9E%83%E5%9C%BE%E5%9B%9E%E6%94%B6/"},{"categories":["golang"],"content":"标记清除法 标记阶段：从根对象出发查找并标记堆中所有的存活的对象 清除阶段：回收未被标记的对象的内存空间，并追加到空闲链表里 标记清除法需要暂定程序运行（较长的STW），执行标记清除之后，才恢复程序执行。 ","date":"2023-10-29","objectID":"/go%E8%AF%AD%E8%A8%80%E8%AE%BE%E8%AE%A1%E4%B8%8E%E5%AE%9E%E7%8E%B0%E4%B9%8B%E5%9E%83%E5%9C%BE%E5%9B%9E%E6%94%B6/:1:1","tags":["go语言底层原理"],"title":"Go语言设计与实现之垃圾回收","uri":"/go%E8%AF%AD%E8%A8%80%E8%AE%BE%E8%AE%A1%E4%B8%8E%E5%AE%9E%E7%8E%B0%E4%B9%8B%E5%9E%83%E5%9C%BE%E5%9B%9E%E6%94%B6/"},{"categories":["golang"],"content":"三色标记法 三色标记法是对标记清除法的改进，进一步减少了STW的时间。 白色对象 潜在的垃圾，其内存可能会被垃圾收集器回收 黑色对象 活跃的对象，包括不存在任何引用外部指针的对象以及从根对象可达的对象 灰色对象 活跃的对象，因为存在指向白色对象的外部指针，垃圾收集器会扫描这些对象的子对象 工作过程： 从灰色对象的集合中选择一个灰色对象并将其标记成黑色； 将黑色对象指向的所有对象都标记成灰色，保证该对象和被该对象引用的对象都不会被回收； 重复上述两个步骤直到对象图中不存在灰色对象； 因为需要暂停整个程序以完成三色标记+清除，三色标记法依然需要较长的STW。 所以，为了减少垃圾回收带来的STW时间过长问题，我们需要可以采取增量或并发GC的方式，加快GC。 ","date":"2023-10-29","objectID":"/go%E8%AF%AD%E8%A8%80%E8%AE%BE%E8%AE%A1%E4%B8%8E%E5%AE%9E%E7%8E%B0%E4%B9%8B%E5%9E%83%E5%9C%BE%E5%9B%9E%E6%94%B6/:1:2","tags":["go语言底层原理"],"title":"Go语言设计与实现之垃圾回收","uri":"/go%E8%AF%AD%E8%A8%80%E8%AE%BE%E8%AE%A1%E4%B8%8E%E5%AE%9E%E7%8E%B0%E4%B9%8B%E5%9E%83%E5%9C%BE%E5%9B%9E%E6%94%B6/"},{"categories":["golang"],"content":"混合写屏障 要实现正确的增量或并发GC,需要保证三色不变性，而三色不变性是通过屏障技术实现的（go中实现的是混合写屏障） ","date":"2023-10-29","objectID":"/go%E8%AF%AD%E8%A8%80%E8%AE%BE%E8%AE%A1%E4%B8%8E%E5%AE%9E%E7%8E%B0%E4%B9%8B%E5%9E%83%E5%9C%BE%E5%9B%9E%E6%94%B6/:1:3","tags":["go语言底层原理"],"title":"Go语言设计与实现之垃圾回收","uri":"/go%E8%AF%AD%E8%A8%80%E8%AE%BE%E8%AE%A1%E4%B8%8E%E5%AE%9E%E7%8E%B0%E4%B9%8B%E5%9E%83%E5%9C%BE%E5%9B%9E%E6%94%B6/"},{"categories":["golang"],"content":"增量和并发收集 增量收集 增量地标记和清除垃圾，虽然会增加单次GC周期，但是会减少单次程序暂停的最长时间 并发收集 利用多核CPU的优势，加快垃圾收集 ","date":"2023-10-29","objectID":"/go%E8%AF%AD%E8%A8%80%E8%AE%BE%E8%AE%A1%E4%B8%8E%E5%AE%9E%E7%8E%B0%E4%B9%8B%E5%9E%83%E5%9C%BE%E5%9B%9E%E6%94%B6/:1:4","tags":["go语言底层原理"],"title":"Go语言设计与实现之垃圾回收","uri":"/go%E8%AF%AD%E8%A8%80%E8%AE%BE%E8%AE%A1%E4%B8%8E%E5%AE%9E%E7%8E%B0%E4%B9%8B%E5%9E%83%E5%9C%BE%E5%9B%9E%E6%94%B6/"},{"categories":["golang"],"content":"触发时机 runtime.sysmon 和 runtime.forcegchelper — 后台运行定时检查和垃圾收集（每2分钟）； runtime.GC — 用户程序手动触发垃圾收集； runtime.mallocgc — 只要申请内存时，堆大小达到控制器计算的触发堆的大小就会触发垃圾收集； ","date":"2023-10-29","objectID":"/go%E8%AF%AD%E8%A8%80%E8%AE%BE%E8%AE%A1%E4%B8%8E%E5%AE%9E%E7%8E%B0%E4%B9%8B%E5%9E%83%E5%9C%BE%E5%9B%9E%E6%94%B6/:2:0","tags":["go语言底层原理"],"title":"Go语言设计与实现之垃圾回收","uri":"/go%E8%AF%AD%E8%A8%80%E8%AE%BE%E8%AE%A1%E4%B8%8E%E5%AE%9E%E7%8E%B0%E4%B9%8B%E5%9E%83%E5%9C%BE%E5%9B%9E%E6%94%B6/"},{"categories":["golang"],"content":"介绍go内存分配策略","date":"2023-10-29","objectID":"/go%E8%AF%AD%E8%A8%80%E8%AE%BE%E8%AE%A1%E4%B8%8E%E5%AE%9E%E7%8E%B0%E4%B9%8B%E5%86%85%E5%AD%98%E5%88%86%E9%85%8D%E5%99%A8/","tags":["go语言底层原理"],"title":"Go语言设计与实现之内存分配器","uri":"/go%E8%AF%AD%E8%A8%80%E8%AE%BE%E8%AE%A1%E4%B8%8E%E5%AE%9E%E7%8E%B0%E4%B9%8B%E5%86%85%E5%AD%98%E5%88%86%E9%85%8D%E5%99%A8/"},{"categories":["golang"],"content":"go内存分配器 程序中的数据和变量都会被分配到程序所在的虚拟内存中，内存空间包含两个重要区域：栈区（Stack）和堆区（Heap）。函数调用的参数、返回值以及局部变量大都会被分配到栈上（可能发生内存逃逸），这部分内存会由编译器进行管理；不同编程语言使用不同的方法管理堆区的内存，C++ 等编程语言会由工程师主动申请和释放内存，Go 以及 Java 等编程语言会由工程师和编译器共同管理，堆中的对象由内存分配器分配并由垃圾收集器回收。 ","date":"2023-10-29","objectID":"/go%E8%AF%AD%E8%A8%80%E8%AE%BE%E8%AE%A1%E4%B8%8E%E5%AE%9E%E7%8E%B0%E4%B9%8B%E5%86%85%E5%AD%98%E5%88%86%E9%85%8D%E5%99%A8/:0:0","tags":["go语言底层原理"],"title":"Go语言设计与实现之内存分配器","uri":"/go%E8%AF%AD%E8%A8%80%E8%AE%BE%E8%AE%A1%E4%B8%8E%E5%AE%9E%E7%8E%B0%E4%B9%8B%E5%86%85%E5%AD%98%E5%88%86%E9%85%8D%E5%99%A8/"},{"categories":["golang"],"content":"内存分配方法 线性分配法 只在内存中维护一个指针，并指向剩余空闲内存地址，如果分配新的内存空间只需要将指针往后移即可，如果释放内存，需要通过额外操作来整理内存碎片。（线性分配器本身是无法合理利用已经被释放的内存。） 空闲链表分配法 空闲内存块之间通过指针相连，相比于线性分配法，可以重用已经被释放的内存。 go的内存分配策略：隔离适应策略 对不同大小的内存块分级，减少内存块链表的长度。分配时，先根据内存大小直接定位到特定的内存块链表头节点，然后在链表里查找空闲的内存块。（类似于二分的思想，可以减少遍历内存块的数量，提升效率） ","date":"2023-10-29","objectID":"/go%E8%AF%AD%E8%A8%80%E8%AE%BE%E8%AE%A1%E4%B8%8E%E5%AE%9E%E7%8E%B0%E4%B9%8B%E5%86%85%E5%AD%98%E5%88%86%E9%85%8D%E5%99%A8/:1:0","tags":["go语言底层原理"],"title":"Go语言设计与实现之内存分配器","uri":"/go%E8%AF%AD%E8%A8%80%E8%AE%BE%E8%AE%A1%E4%B8%8E%E5%AE%9E%E7%8E%B0%E4%B9%8B%E5%86%85%E5%AD%98%E5%88%86%E9%85%8D%E5%99%A8/"},{"categories":["golang"],"content":"多级缓存 内存分配会根据所需内存的大小，从不同级别缓存申请内存。 线程缓存 线程缓存属于每一个独立的线程内，并没有并发安全问题，申请很快。 中心缓存 中心缓存可能涉及到多个线程并发申请，需要加锁访问。 页堆 前面两种无法满足内存申请，则需要从页堆申请新的内存空间。 ","date":"2023-10-29","objectID":"/go%E8%AF%AD%E8%A8%80%E8%AE%BE%E8%AE%A1%E4%B8%8E%E5%AE%9E%E7%8E%B0%E4%B9%8B%E5%86%85%E5%AD%98%E5%88%86%E9%85%8D%E5%99%A8/:2:0","tags":["go语言底层原理"],"title":"Go语言设计与实现之内存分配器","uri":"/go%E8%AF%AD%E8%A8%80%E8%AE%BE%E8%AE%A1%E4%B8%8E%E5%AE%9E%E7%8E%B0%E4%B9%8B%E5%86%85%E5%AD%98%E5%88%86%E9%85%8D%E5%99%A8/"},{"categories":["golang"],"content":"分级分配 go划分了对象大小的级别，按照对象的不同大小，采用不同的分配策略。 微对象(0, 16B] 先使用微型分配器，再依次尝试线程缓存、中心缓存和堆分配内存； 小对象(16B, 32KB] 依次尝试使用线程缓存、中心缓存和堆分配内存； 大对象(32KB, +♾️) 大于32KB的内存分配申请，会直接在堆上申请。 ","date":"2023-10-29","objectID":"/go%E8%AF%AD%E8%A8%80%E8%AE%BE%E8%AE%A1%E4%B8%8E%E5%AE%9E%E7%8E%B0%E4%B9%8B%E5%86%85%E5%AD%98%E5%88%86%E9%85%8D%E5%99%A8/:3:0","tags":["go语言底层原理"],"title":"Go语言设计与实现之内存分配器","uri":"/go%E8%AF%AD%E8%A8%80%E8%AE%BE%E8%AE%A1%E4%B8%8E%E5%AE%9E%E7%8E%B0%E4%B9%8B%E5%86%85%E5%AD%98%E5%88%86%E9%85%8D%E5%99%A8/"},{"categories":["golang"],"content":"go的虚拟内存布局 暂略 ","date":"2023-10-29","objectID":"/go%E8%AF%AD%E8%A8%80%E8%AE%BE%E8%AE%A1%E4%B8%8E%E5%AE%9E%E7%8E%B0%E4%B9%8B%E5%86%85%E5%AD%98%E5%88%86%E9%85%8D%E5%99%A8/:4:0","tags":["go语言底层原理"],"title":"Go语言设计与实现之内存分配器","uri":"/go%E8%AF%AD%E8%A8%80%E8%AE%BE%E8%AE%A1%E4%B8%8E%E5%AE%9E%E7%8E%B0%E4%B9%8B%E5%86%85%E5%AD%98%E5%88%86%E9%85%8D%E5%99%A8/"},{"categories":["golang"],"content":"本文将介绍go语言是如何调度协程的","date":"2023-10-29","objectID":"/go%E8%AF%AD%E8%A8%80%E8%AE%BE%E8%AE%A1%E4%B8%8E%E5%AE%9E%E7%8E%B0%E4%B9%8B%E8%B0%83%E5%BA%A6%E5%99%A8/","tags":["go语言底层原理"],"title":"Go语言设计与实现之调度器","uri":"/go%E8%AF%AD%E8%A8%80%E8%AE%BE%E8%AE%A1%E4%B8%8E%E5%AE%9E%E7%8E%B0%E4%B9%8B%E8%B0%83%E5%BA%A6%E5%99%A8/"},{"categories":["golang"],"content":"go线程调度器 ","date":"2023-10-29","objectID":"/go%E8%AF%AD%E8%A8%80%E8%AE%BE%E8%AE%A1%E4%B8%8E%E5%AE%9E%E7%8E%B0%E4%B9%8B%E8%B0%83%E5%BA%A6%E5%99%A8/:0:0","tags":["go语言底层原理"],"title":"Go语言设计与实现之调度器","uri":"/go%E8%AF%AD%E8%A8%80%E8%AE%BE%E8%AE%A1%E4%B8%8E%E5%AE%9E%E7%8E%B0%E4%B9%8B%E8%B0%83%E5%BA%A6%E5%99%A8/"},{"categories":["golang"],"content":"抢占式调度器 go语言调度器的发展历程经过好几个版本，目前的实现是基于信号的抢占式调度器。go语言是在用户空间实现的协程调度器，相比于依赖于操作系统线程调度器，go的多个协程绑定到一个内核线程上去，内存开销很小，可以创建很多协程；并且没有用户态和内核态切换的成本。 但是go调度器并没有彻底解决STW问题。 ","date":"2023-10-29","objectID":"/go%E8%AF%AD%E8%A8%80%E8%AE%BE%E8%AE%A1%E4%B8%8E%E5%AE%9E%E7%8E%B0%E4%B9%8B%E8%B0%83%E5%BA%A6%E5%99%A8/:1:0","tags":["go语言底层原理"],"title":"Go语言设计与实现之调度器","uri":"/go%E8%AF%AD%E8%A8%80%E8%AE%BE%E8%AE%A1%E4%B8%8E%E5%AE%9E%E7%8E%B0%E4%B9%8B%E8%B0%83%E5%BA%A6%E5%99%A8/"},{"categories":["golang"],"content":"数据结构 go调度器主要由三个主要部分组成：G，M，P ","date":"2023-10-29","objectID":"/go%E8%AF%AD%E8%A8%80%E8%AE%BE%E8%AE%A1%E4%B8%8E%E5%AE%9E%E7%8E%B0%E4%B9%8B%E8%B0%83%E5%BA%A6%E5%99%A8/:2:0","tags":["go语言底层原理"],"title":"Go语言设计与实现之调度器","uri":"/go%E8%AF%AD%E8%A8%80%E8%AE%BE%E8%AE%A1%E4%B8%8E%E5%AE%9E%E7%8E%B0%E4%B9%8B%E8%B0%83%E5%BA%A6%E5%99%A8/"},{"categories":["golang"],"content":"G G指的是调度器中执行的任务，也就是goroutine，部分结构如下： type g struct { stack stack // 标识当前goroutine栈内存范围 stackguard0 uintptr preempt bool // 抢占信号 preemptStop bool // 抢占时将状态修改成 `_Gpreempted` preemptShrink bool // 在同步安全点收缩栈 _panic *_panic // 最内侧的 panic 结构体 _defer *_defer // 最内侧的延迟函数结构体 m *m // 当前G占用的线程 sched gobuf // 存储G的调度数据：其实就是保存上下文 atomicstatus uint32 // G的状态 goid int64 // 唯一标识G ... } type gobuf struct { sp uintptr // 栈指针 pc uintptr // 程序计数器 g guintptr ret sys.Uintreg // 系统调用返回值 ... } ","date":"2023-10-29","objectID":"/go%E8%AF%AD%E8%A8%80%E8%AE%BE%E8%AE%A1%E4%B8%8E%E5%AE%9E%E7%8E%B0%E4%B9%8B%E8%B0%83%E5%BA%A6%E5%99%A8/:2:1","tags":["go语言底层原理"],"title":"Go语言设计与实现之调度器","uri":"/go%E8%AF%AD%E8%A8%80%E8%AE%BE%E8%AE%A1%E4%B8%8E%E5%AE%9E%E7%8E%B0%E4%B9%8B%E8%B0%83%E5%BA%A6%E5%99%A8/"},{"categories":["golang"],"content":"M M指的是操作系统的线程，调度器最多可以创建10000个线程，但同时可以运行的线程只能有gomaxprocs个。（runtime.GOMAXPROCS可以设置，默认设置是线程数等于cpu核数，其实也是P的个数） type m struct { g0 *g // 负责调度P的goroutine：参与大内存分配、CGO函数执行等 curg *g // 获得线程执行权的goroutine p puintptr // 正在运行代码的处理器 nextp puintptr // 暂存的处理器 oldp puintptr // 执行系统调用前的处理器 ... } ","date":"2023-10-29","objectID":"/go%E8%AF%AD%E8%A8%80%E8%AE%BE%E8%AE%A1%E4%B8%8E%E5%AE%9E%E7%8E%B0%E4%B9%8B%E8%B0%83%E5%BA%A6%E5%99%A8/:2:2","tags":["go语言底层原理"],"title":"Go语言设计与实现之调度器","uri":"/go%E8%AF%AD%E8%A8%80%E8%AE%BE%E8%AE%A1%E4%B8%8E%E5%AE%9E%E7%8E%B0%E4%B9%8B%E8%B0%83%E5%BA%A6%E5%99%A8/"},{"categories":["golang"],"content":"P P是协程和线程的中间层，负责将协程映射到某个线程执行（提供给线程上下文环境），同时负责调度本地队列里的协程。 type p struct { m muintptr runqhead uint32 // runqtail uint32 // runq [256]guintptr // 等待运行队列，最多256个 runnext guintptr // 下一个需要被执行的G ... } ","date":"2023-10-29","objectID":"/go%E8%AF%AD%E8%A8%80%E8%AE%BE%E8%AE%A1%E4%B8%8E%E5%AE%9E%E7%8E%B0%E4%B9%8B%E8%B0%83%E5%BA%A6%E5%99%A8/:2:3","tags":["go语言底层原理"],"title":"Go语言设计与实现之调度器","uri":"/go%E8%AF%AD%E8%A8%80%E8%AE%BE%E8%AE%A1%E4%B8%8E%E5%AE%9E%E7%8E%B0%E4%B9%8B%E8%B0%83%E5%BA%A6%E5%99%A8/"},{"categories":["golang"],"content":"如何调度 ","date":"2023-10-29","objectID":"/go%E8%AF%AD%E8%A8%80%E8%AE%BE%E8%AE%A1%E4%B8%8E%E5%AE%9E%E7%8E%B0%E4%B9%8B%E8%B0%83%E5%BA%A6%E5%99%A8/:3:0","tags":["go语言底层原理"],"title":"Go语言设计与实现之调度器","uri":"/go%E8%AF%AD%E8%A8%80%E8%AE%BE%E8%AE%A1%E4%B8%8E%E5%AE%9E%E7%8E%B0%E4%B9%8B%E8%B0%83%E5%BA%A6%E5%99%A8/"},{"categories":["golang"],"content":"创建G时 创建好G时，如果允许G作为下一个处理器要执行的G，则直接设置g.runnext属性为G 如果不允许，则将G放入处理器的本地运行队列； 如果本地运行队列已满，则将一部分G和新加入的G放入全局的运行队列 ","date":"2023-10-29","objectID":"/go%E8%AF%AD%E8%A8%80%E8%AE%BE%E8%AE%A1%E4%B8%8E%E5%AE%9E%E7%8E%B0%E4%B9%8B%E8%B0%83%E5%BA%A6%E5%99%A8/:3:1","tags":["go语言底层原理"],"title":"Go语言设计与实现之调度器","uri":"/go%E8%AF%AD%E8%A8%80%E8%AE%BE%E8%AE%A1%E4%B8%8E%E5%AE%9E%E7%8E%B0%E4%B9%8B%E8%B0%83%E5%BA%A6%E5%99%A8/"},{"categories":["golang"],"content":"运行时调度循环 runtime.schedule 函数会从下面几个地方查找待执行的 Goroutine： 为了保证公平，当全局运行队列中有待执行的 Goroutine 时，通过 schedtick 保证有一定几率会从全局的运行队列中查找对应的 Goroutine； 从处理器本地的运行队列中查找待执行的 Goroutine； 如果前两种方法都没有找到 Goroutine，会通过 runtime.findrunnable 进行阻塞地查找 Goroutine； runtime.findrunnable 的实现非常复杂，这个 300 多行的函数通过以下的过程获取可运行的 Goroutine： 从本地运行队列、全局运行队列中查找； 从网络轮询器中查找是否有 Goroutine 等待运行； 通过 runtime.runqsteal 尝试从其他随机的处理器中窃取待运行的 Goroutine，该函数还可能窃取处理器的计时器； 因为函数的实现过于复杂，上述的执行过程是经过简化的，总而言之，当前函数一定会返回一个可执行的 Goroutine，如果当前不存在就会阻塞等待。 P获取到Goroutine之后，调度到当前线程上执行，执行完毕后，会执行一些清理工作，然后又进入下一轮调度，形成调度循环。 ","date":"2023-10-29","objectID":"/go%E8%AF%AD%E8%A8%80%E8%AE%BE%E8%AE%A1%E4%B8%8E%E5%AE%9E%E7%8E%B0%E4%B9%8B%E8%B0%83%E5%BA%A6%E5%99%A8/:3:2","tags":["go语言底层原理"],"title":"Go语言设计与实现之调度器","uri":"/go%E8%AF%AD%E8%A8%80%E8%AE%BE%E8%AE%A1%E4%B8%8E%E5%AE%9E%E7%8E%B0%E4%B9%8B%E8%B0%83%E5%BA%A6%E5%99%A8/"},{"categories":["golang"],"content":"调度时机 主动挂起 — runtime.gopark -\u003e runtime.park_m 系统调用 — runtime.exitsyscall -\u003e runtime.exitsyscall0 协作式调度 — runtime.Gosched -\u003e runtime.gosched_m -\u003e runtime.goschedImpl 系统监控 — runtime.sysmon -\u003e runtime.retake -\u003e runtime.preemptone ","date":"2023-10-29","objectID":"/go%E8%AF%AD%E8%A8%80%E8%AE%BE%E8%AE%A1%E4%B8%8E%E5%AE%9E%E7%8E%B0%E4%B9%8B%E8%B0%83%E5%BA%A6%E5%99%A8/:4:0","tags":["go语言底层原理"],"title":"Go语言设计与实现之调度器","uri":"/go%E8%AF%AD%E8%A8%80%E8%AE%BE%E8%AE%A1%E4%B8%8E%E5%AE%9E%E7%8E%B0%E4%B9%8B%E8%B0%83%E5%BA%A6%E5%99%A8/"},{"categories":["golang"],"content":"这里介绍一些关于go语言底层数据结构的原理与实现，如\\slice\\array\\channel\\map\\string go底层数据结构原理 ","date":"2023-10-27","objectID":"/go%E8%AF%AD%E8%A8%80%E8%AE%BE%E8%AE%A1%E4%B8%8E%E5%AE%9E%E7%8E%B0%E4%B9%8B%E5%9F%BA%E7%A1%80%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/:0:0","tags":["go语言底层原理"],"title":"Go语言设计与实现之基础数据结构","uri":"/go%E8%AF%AD%E8%A8%80%E8%AE%BE%E8%AE%A1%E4%B8%8E%E5%AE%9E%E7%8E%B0%E4%B9%8B%E5%9F%BA%E7%A1%80%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"},{"categories":["golang"],"content":"数组 数组是一个定长的顺序表，内存上元素地址是连续的。 初始化（不考虑逃逸分析） 初始化有两种方式：[...]{1,2,3,4}或[4]{1,2,3,4}。这两种方式初始化的方式仅仅只是语法糖而已，在SSA中间代码生成的时候，其实是一样的处理逻辑。 数组元素个数\u003c=4个时，那么数组就会直接在栈上分配； 数组元素个数\u003e4个时，那么就会在静态存储区初始化，然后拷贝回栈中； 访问和赋值 不论是访问操作还是赋值操作，首先就是确定索引。 对于常量索引，代码编译期间就可以判定是否越界；而对于变量索引，需要在运行时中插入检测函数检测是否越界。通过这两种方式，可以保证数组访问不会越界，一旦越界会报错。 ","date":"2023-10-27","objectID":"/go%E8%AF%AD%E8%A8%80%E8%AE%BE%E8%AE%A1%E4%B8%8E%E5%AE%9E%E7%8E%B0%E4%B9%8B%E5%9F%BA%E7%A1%80%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/:1:0","tags":["go语言底层原理"],"title":"Go语言设计与实现之基础数据结构","uri":"/go%E8%AF%AD%E8%A8%80%E8%AE%BE%E8%AE%A1%E4%B8%8E%E5%AE%9E%E7%8E%B0%E4%B9%8B%E5%9F%BA%E7%A1%80%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"},{"categories":["golang"],"content":"切片 切片相比数组，具备自扩容特点，不同的是切片是在数组之上进一步封装。在运行时中的结构如下： type SliceHeader struct { Data uintptr // 指向底层数组的指针 Len int // 当前切片长度 Cap int // 切片的容量，也就是底层数组大小 } 切片的特点： 封装：这样封装之后，底层数组的扩容、缩容等操作对于上层可以做到无感，也就是上层不需要关心扩容缩容的问题，由切片来实现。 运行时确定：与数组不同，是在编译期间就决定好结构了，读写位置是在编译期间就固定的；但是切片在运行时可能由于先前分配的容量不够，触发切片底层数组扩缩容，结构会发生改变。 初始化 初始化有三种方式： // 通过下标的方式获得数组或者切片的一部分； arr := [3]int{1, 2, 3} slice := arr[:] // 使用字面量初始化新的切片； slice:=[]int{1, 2, 3} // 使用关键字 make 创建切片： slice:=make([]int,3) slice[0]=1 slice[1]=2 slice[2]=3 其中第二种初始化方式在编译期间都会被展开成arr[:]和方式一一样的处理逻辑； 如果是第三种初始化方式，会在运行时对传入参数进行进一步检查和操作 是否cap\u003e=len 切片如果发生了逃逸或者需要的内存过大，就在堆上进行分配 切片如果没有发生逃逸或者需要的内存较小，make函数会在编译阶段变成和前面两种方式一样地处理逻辑，也就是分配到栈上。 访问元素 获取len、cap和访问切片中的元素 len(slice) 或者 cap(slice) 在一些情况下会直接替换成切片的长度或者容量，不需要在运行时获取；除了获取切片的长度和容量之外，访问切片中元素使用的 OINDEX 操作也会在SSA中间代码生成期间转换成对地址的直接访问 range切片也会被转化成更简单的for循环 追加和扩容 追加 append有两种方式：一种是覆盖原变量slice=append(slice,1,2,3)，另一种是不覆盖回原变量append(slice,1,2,3)。 后者如果底层数组发生扩容，那么slice指向就会失效了. 如果我们选择覆盖原有的变量，就不需要担心切片发生拷贝影响性能，因为 Go 语言编译器已经对这种常见的情况做出了优化。 扩容 在分配内存空间之前需要先确定新的切片容量，运行时根据切片的当前容量选择不同的策略进行扩容： 如果期望容量大于当前容量的两倍就会使用期望容量； 如果当前切片的长度小于 1024 就会将容量翻倍； 如果当前切片的长度大于 1024 就会每次增加 25% 的容量，直到新容量大于期望容量； 上述代码片段仅会确定切片的大致容量，下面还需要根据切片中的元素大小对齐内存，当数组中元素所占的字节大小为 1、8 或者 2 的倍数时，运行时会使用如下所示的代码对齐内存 尽量避免大切片的扩容或复制时发生的大规模内存拷贝。 ","date":"2023-10-27","objectID":"/go%E8%AF%AD%E8%A8%80%E8%AE%BE%E8%AE%A1%E4%B8%8E%E5%AE%9E%E7%8E%B0%E4%B9%8B%E5%9F%BA%E7%A1%80%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/:2:0","tags":["go语言底层原理"],"title":"Go语言设计与实现之基础数据结构","uri":"/go%E8%AF%AD%E8%A8%80%E8%AE%BE%E8%AE%A1%E4%B8%8E%E5%AE%9E%E7%8E%B0%E4%B9%8B%E5%9F%BA%E7%A1%80%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"},{"categories":["golang"],"content":"哈希表 哈希表是一种数据结构，具备O(1)级别的读写性能。但是设计一个优秀的哈希表，需要解决两个关键点：哈希冲突和哈希函数。 初始化 两种初始化方式：字面量map[string]int{\"1\":11}和make(map[string]int,1) 这两种初始化方式仅仅只是语法糖，最后都会转化为对runtime.makemap调用，这个函数主要干了： 计算哈希占用的内存是否溢出或者超出能分配的最大值； 调用 runtime.fastrand 获取一个随机的哈希种子； 根据传入的 hint 计算出需要的最小需要的桶的数量； 使用 runtime.makeBucketArray 创建用于保存桶的数组； 当桶的数量小于 2^4^ 时，由于数据较少、使用溢出桶的可能性较低，会省略创建的过程以减少额外开销； 当桶的数量多于 2^4^ 时，会额外创建 2^B^−2^4^ 个溢出桶； 读操作 两种方式：读操作返回一个参数v:=map[k]；读操作返回两个参数v,ok:=map[k]。底层实现上，一个是调用了runtime.mapaccess1，而另一个是调用了runtime.mapaccess2，这两个函数的实现差不多，只是后者多了个bool返回值。 先通过哈希表设置的哈希函数和哈希种子获取当前key的哈希值； 然后再拿到桶的序号和哈希的高8位数字；（哈希高8位数字是用来和桶中的tophash比较和快速定位桶中key的地址的） 依次遍历桶中的8个tophash，如果找到tophash与key哈希后的高8位数字相等，然后就直接指针偏移到对应的key[i]并与key比较，如果相等就再偏移拿到value[i]并返回。 桶中没有，会去溢出桶里找 写操作 首先还是先拿到key的哈希值和桶； 然后再比较桶中的存储的 tophash和key的哈希高8位，如果相同就会直接返回目标位置的地址； 如果在桶中没找到，就去单链表结构的溢出桶里继续寻找，如果有就返回value地址； 如果还没有找到key，就会为新的键值对规划存储的内存地址，并返回value的地址； 而后续的写入是由编译期间插入的汇编代码执行的。 哈希扩容 随着哈希元素的逐渐增多，特别是溢出桶的增加（溢出桶的遍历复杂度是O(n)级别的），哈希性能会逐渐恶化，我们需要扩容。扩容的条件是： 装载因子已经超过 6.5； 这个条件引起的扩容触发的是翻倍扩容，增加桶容量。扩容是增量触发的的，并不是一次性完成的。而且扩容期间读操作会使用旧桶的数据；写操作才会触发旧桶的分流。这样可以摊销掉扩容的时间复杂度。 哈希使用了太多溢出桶； 由这个条件引起的扩容是等量扩容sameSizeGrow：如果出现太多溢出桶，它会创建数量和旧桶一样数量的新桶保存数据，然后垃圾回收会清理老的溢出桶并释放内存。 删除操作 哈希表的删除逻辑与写入逻辑很相似，只是触发哈希的删除需要使用关键字，如果在删除期间遇到了哈希表的扩容，就会分流桶中的元素，分流结束之后会找到桶中的目标元素完成键值对的删除工作。 Go 语言使用拉链法来解决哈希碰撞的问题实现了哈希表，它的访问、写入和删除等操作都在编译期间转换成了运行时的函数或者方法。哈希在每一个桶中存储键对应哈希的前 8 位，当对哈希进行操作时，这些 tophash 就成为可以帮助哈希快速遍历桶中元素的缓存。 哈希表的每个桶都只能存储 8 个键值对，一旦当前哈希的某个桶超出 8 个，新的键值对就会存储到哈希的溢出桶中。随着键值对数量的增加，溢出桶的数量和哈希的装载因子也会逐渐升高，超过一定范围就会触发扩容，扩容会将桶的数量翻倍，元素再分配的过程也是在调用写操作时增量进行的，不会造成性能的瞬时巨大抖动。 ","date":"2023-10-27","objectID":"/go%E8%AF%AD%E8%A8%80%E8%AE%BE%E8%AE%A1%E4%B8%8E%E5%AE%9E%E7%8E%B0%E4%B9%8B%E5%9F%BA%E7%A1%80%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/:3:0","tags":["go语言底层原理"],"title":"Go语言设计与实现之基础数据结构","uri":"/go%E8%AF%AD%E8%A8%80%E8%AE%BE%E8%AE%A1%E4%B8%8E%E5%AE%9E%E7%8E%B0%E4%B9%8B%E5%9F%BA%E7%A1%80%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"},{"categories":["golang"],"content":"字符串 go语言字符串底层实现就是一个只读的数组结构[n]byte。字符串可以被索引，但是不能被直接修改。 数据结构 type StringHeader struct { Data uintptr // 底层数组指针 Len int // 底层数组大小 } 字符串拼接 + 性能最差的拼接方式。由于go语言字符串是只读数据结构，拼接字符串会将字符串重新拷贝分配到新的地址上。所以+会造成字符串的大量拷贝。 fmt.Sprintf 使用反射，也会涉及到拷贝，性能较差 strings.Builder 底层切片有复用机制，而且转化为字符串时，使用了零拷贝机制。 strings.Bytes 还行 类型转化 某些场景下，我们需要将string和[]byte互相转化，如果对于原结构不做任何改变的话，可以考虑使用unsafe ","date":"2023-10-27","objectID":"/go%E8%AF%AD%E8%A8%80%E8%AE%BE%E8%AE%A1%E4%B8%8E%E5%AE%9E%E7%8E%B0%E4%B9%8B%E5%9F%BA%E7%A1%80%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/:4:0","tags":["go语言底层原理"],"title":"Go语言设计与实现之基础数据结构","uri":"/go%E8%AF%AD%E8%A8%80%E8%AE%BE%E8%AE%A1%E4%B8%8E%E5%AE%9E%E7%8E%B0%E4%B9%8B%E5%9F%BA%E7%A1%80%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"},{"categories":["golang"],"content":"接口 接口也是 Go 语言中的一种类型，它能够出现在变量的定义、函数的入参和返回值中并对它们进行约束，不过 Go 语言中有两种略微不同的接口，一种是带有一组方法的接口，另一种是不带任何方法的 interface{}：Go 语言使用 runtime.iface 表示第一种接口，使用 runtime.eface 表示第二种不包含任何方法的接口 interface{} 值得注意的是：接口不是任意类型，接口也是一种类型。 接口零值 接口的零值是指类型和值都要为nil才表示零值。如： package main type TestStruct struct{} func NilOrNot(v interface{}) bool { return v == nil } func main() { var s *TestStruct fmt.Println(s == nil) // #=\u003e true fmt.Println(NilOrNot(s)) // #=\u003e false } $ go run main.go true false 数据结构 // 运行时的类型结构 type _type struct { size uintptr ptrdata uintptr hash uint32 // 快速判断类型是否相等 tflag tflag align uint8 fieldAlign uint8 kind uint8 equal func(unsafe.Pointer, unsafe.Pointer) bool gcdata *byte str nameOff ptrToThis typeOff } 空接口 type eface struct { // 16 字节 _type *_type // 指向底层数据类型：类型大小、hash、对齐等 data unsafe.Pointer // 指向数据的指针 } 非空接口 type iface struct { // 16 字节 tab *itab data unsafe.Pointer // 指向数据 } type itab struct { // 32 字节 inter *interfacetype // 指向接口类型 _type *_type // 指向真实数据类型 hash uint32 // 对_type字段中hash的拷贝，可以减少指针解引用的开销 _ [4]byte // pad fun [1]uintptr // 虽然只有一个元素，但是其实指针，指向一组固定大小的方法数组 } 类型转化 无论是指针接受者还是值接受者实现转化为接口类型时，会在运行时获取接受者的类型以及数据，组装好运行时的eface或iface结构体。 类型断言 断言的实质其实就是比较目标类型的hash和接口类型的hash是否相等； 动态派发 动态派发（Dynamic dispatch）是在运行期间选择具体多态操作（方法或者函数）执行的过程，它是面向对象语言中的常见特性6。 在如下所示的代码中，main 函数调用了两次 Quack 方法： 第一次以 Duck 接口类型的身份调用，调用时需要经过运行时的动态派发； 第二次以 *Cat 具体类型的身份调用，编译期就会确定调用的函数： func main() { var c Duck = \u0026Cat{Name: \"draven\"} c.Quack() // 动态派发 c.(*Cat).Quack() } 动态派发会进行额外的指令来选择函数。使用动态派发会对程序性能造成一定影响，建议都使用指针实现，因为指针实现的动态派发成本更小一点。而且给项目带来的好处时要大于性能上的一点损失的。 ","date":"2023-10-27","objectID":"/go%E8%AF%AD%E8%A8%80%E8%AE%BE%E8%AE%A1%E4%B8%8E%E5%AE%9E%E7%8E%B0%E4%B9%8B%E5%9F%BA%E7%A1%80%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/:5:0","tags":["go语言底层原理"],"title":"Go语言设计与实现之基础数据结构","uri":"/go%E8%AF%AD%E8%A8%80%E8%AE%BE%E8%AE%A1%E4%B8%8E%E5%AE%9E%E7%8E%B0%E4%B9%8B%E5%9F%BA%E7%A1%80%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"},{"categories":["golang"],"content":"通道 chan是go语言特有的数据结构，是go实现CSP并发编程模型的关键数据结构。值得注意的是，chan并不是完全无锁的结构，只是在某些关键路径上实现了无锁。 数据结构 type hchan struct { qcount uint // 元素个数 dataqsiz uint // 循环队列的长度 buf unsafe.Pointer// 缓冲区数据指针 elemsize uint16 // 元素大小 closed uint32 elemtype *_type // 元素类型 sendx uint // 发送操作处理到的位置 recvx uint // 接收操作处理到的位置 recvq waitq // 缓冲区空间为空而阻塞的接收goroutine双向链表 sendq waitq // 缓冲区空间不足而阻塞的发送goroutine双向链表 lock mutex } type waitq struct { first *sudog last *sudog } 创建管道 创建管道只能通过make(vhan int,n)的方式 如果是创建无缓冲的管道，也就是hchan.buf为nil，不会分配内存 如果是channel中存储的类型是指针，那么会直接给hchan.buf分配内存 如果是有缓冲并且基础类型不是指针的，则也会直接给hchan.buf分配内存 发送数据 发送数据之前，会给chan加锁，防止并发安全问题。发送的逻辑如下： 如果hchan.recvq非空，表示有goroutine阻塞等待接收管道数据，则会取出最先陷入等待的goroutine并直接向它发送数据，并且将该goroutine标记为可运行，等待处理器调度执行； 如果hchan.recvq为空，表示没有goroutine阻塞等待接收管道数据，且缓冲区有空余空间，就会将发送数据写入hchan.buf中； 如果缓冲区已满或者没有接收者来接收数据，那么执行发送操作的goroutine就会一直阻塞下去 调度时机：第一步并没有立即调度执行；还有就是第三步阻塞其实就是调用gopark让出处理器使用权。 接收数据 接收数据之前也会给chan加锁，接收逻辑如下： 当hchan为空时，会直接挂起当前goroutine 如果当前hchan已经被关闭并且缓冲区没有数据，会直接返回 如果hchan.sendq队列存在挂起的goroutine，表示存在期待发送数据的goroutine，那么可以直接将该goroutine取出来，然后去接收数据 如果缓冲区存在数据，则去缓冲区取数据 如果缓冲区也没有数据，那么就会阻塞挂起等待，将当前goroutine加入hchan.recvq队列里等待调度器唤醒。 调度时机：第一步挂起协程；第五步也会挂起协程 关闭管道 一旦管道关闭，管道还能读取数据，但是不能写入数据，而且管道也不会发生阻塞 ","date":"2023-10-27","objectID":"/go%E8%AF%AD%E8%A8%80%E8%AE%BE%E8%AE%A1%E4%B8%8E%E5%AE%9E%E7%8E%B0%E4%B9%8B%E5%9F%BA%E7%A1%80%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/:6:0","tags":["go语言底层原理"],"title":"Go语言设计与实现之基础数据结构","uri":"/go%E8%AF%AD%E8%A8%80%E8%AE%BE%E8%AE%A1%E4%B8%8E%E5%AE%9E%E7%8E%B0%E4%B9%8B%E5%9F%BA%E7%A1%80%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"},{"categories":["设计模式"],"content":" 桥接模式 一个类存在多个独立变化维度，我们通过组合的方式让多个维度可以独立进行扩展。桥接模式的目的是将抽象部分与实现部分解耦，使它们可以独立地变化。 举例：实现一个告警系统：告警系统含有多个告警类别和多种告警方式，告警类别和告警方式之间可以任意对应使用，方便灵活调整。也就是说，告警方式和告警类别可以独立变化，因为这两个没有依赖关系。 // @author cold bin // @date 2023/9/2 package bridge import \"fmt\" // AlertMethod 告警方式的接口 type AlertMethod interface { SendAlert(message string) } // 具体的告警方式 type EmailAlert struct{} func (e *EmailAlert) SendAlert(message string) { fmt.Println(\"通过邮件发送告警：\", message) } type SMSAlert struct{} func (s *SMSAlert) SendAlert(message string) { fmt.Println(\"通过短信发送告警：\", message) } // AlertLevel 告警级别的接口 type AlertLevel interface { SetAlertMethod(method AlertMethod) Alert(message string) } // 具体的告警级别 type WarningAlert struct { method AlertMethod } func (w *WarningAlert) SetAlertMethod(method AlertMethod) { w.method = method } func (w *WarningAlert) Alert(message string) { w.method.SendAlert(\"[Warning] \" + message) } type ErrorAlert struct { method AlertMethod } func (e *ErrorAlert) SetAlertMethod(method AlertMethod) { e.method = method } func (e *ErrorAlert) Alert(message string) { e.method.SendAlert(\"[Error] \" + message) } ","date":"2023-09-02","objectID":"/go%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F%E4%B9%8B%E6%A1%A5%E6%8E%A5%E6%A8%A1%E5%BC%8F/:0:0","tags":[],"title":"Go设计模式之桥接模式","uri":"/go%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F%E4%B9%8B%E6%A1%A5%E6%8E%A5%E6%A8%A1%E5%BC%8F/"},{"categories":["设计模式"],"content":" 代理模式 单例模式、工厂模式、建造者模式、原型模式、函数选项模式都是属于创建型模式，指导如何创建对象。 而结构型模式主要指导如何将对象或类组合在一起，有代理模式、桥接模式、装饰器模式、门面模式、组合模式、享元模式。 什么是代理模式呢？顾名思义，在不改变原始类（被代理类）代码的情况下，通过引入代理类给原始类附加功能。 ","date":"2023-09-01","objectID":"/go%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F%E4%B9%8B%E4%BB%A3%E7%90%86%E6%A8%A1%E5%BC%8F/:0:0","tags":[],"title":"Go设计模式之代理模式","uri":"/go%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F%E4%B9%8B%E4%BB%A3%E7%90%86%E6%A8%A1%E5%BC%8F/"},{"categories":["设计模式"],"content":"应用场景 RPC 缓存 监控 鉴权 限流 事务 … 这里给出一个文件上传代理模式框架实现 // @author cold bin // @date 2023/9/1 package proxy import ( \"context\" ) // Uploader 上传文件的抽象 type Uploader interface { UploadSingle(ctx context.Context, key string, value []byte) UploadMultiple(ctx context.Context, keys []string, values [][]byte) } // UploaderProxy 上传的代理者 // // 将所有实现都进行代理，我们可以在Up执行前或后做一些事情，例如实现一些hook type UploaderProxy struct { Up Uploader } func (u *UploaderProxy) UploadSingle(ctx context.Context, key string, value []byte) { // 这里可以做一些事情： // 1、校验文件名是否正确 // 2、记录图床开始上传时间 // 3、统计上传频率，做限流等 u.Up.UploadSingle(ctx, key, value) // 这里也可以做一些事情： // 1、hook结果 // 2、记录图床结束上传时间 // 3、记录图床上传时间 } func (u *UploaderProxy) UploadMultiple(ctx context.Context, keys []string, values [][]byte) { u.Up.UploadMultiple(ctx, keys, values) } type QiniuOss struct { // 注入七牛的依赖 } func (q *QiniuOss) UploadSingle(ctx context.Context, key string, value []byte) { } func (q *QiniuOss) UploadMultiple(ctx context.Context, keys []string, values [][]byte) { } type AliOss struct { // 注入阿里云oss依赖 } func (a *AliOss) UploadSingle(ctx context.Context, key string, value []byte) { } func (a *AliOss) UploadMultiple(ctx context.Context, keys []string, values [][]byte) { } ","date":"2023-09-01","objectID":"/go%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F%E4%B9%8B%E4%BB%A3%E7%90%86%E6%A8%A1%E5%BC%8F/:1:0","tags":[],"title":"Go设计模式之代理模式","uri":"/go%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F%E4%B9%8B%E4%BB%A3%E7%90%86%E6%A8%A1%E5%BC%8F/"},{"categories":["设计模式"],"content":"总结 看的出来，代理模式的实现是在“基于接口而非实现”的抽象之上的 可以说，代理模式也实现了基于接口而非实现的设计原则，并且在此基础上，通过实现代理类，来给原有业务实现添加一些功能。添加的功能作用于局限于原业务执行前后，但是不能作用于原业务中。 其实，作用于原业务中就不使用代理模式了，只能重构原业务代码实现。 代理模式指的是：通过引入原始类的方式来给原始类增加功能。我们也可以引入原始类实现的接口，这样可以对所有的实现进行统一代理 ","date":"2023-09-01","objectID":"/go%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F%E4%B9%8B%E4%BB%A3%E7%90%86%E6%A8%A1%E5%BC%8F/:2:0","tags":[],"title":"Go设计模式之代理模式","uri":"/go%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F%E4%B9%8B%E4%BB%A3%E7%90%86%E6%A8%A1%E5%BC%8F/"},{"categories":["设计模式"],"content":" 原型模式 如果对象创建成本比较大（有些字段赋值可能需要rpc、网络、磁盘读取等），而且同一个类的对象差异不大（大部分字段都相同）。在这种情况下，可以从已有对象拷贝出新的对象使用，可以减少对象创建时的成本。 ","date":"2023-08-31","objectID":"/go%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F%E4%B9%8B%E5%8E%9F%E5%9E%8B%E6%A8%A1%E5%BC%8F/:0:0","tags":[],"title":"Go设计模式之原型模式","uri":"/go%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F%E4%B9%8B%E5%8E%9F%E5%9E%8B%E6%A8%A1%E5%BC%8F/"},{"categories":["设计模式"],"content":"实现方法 浅拷贝 针对引用或指针类型的浅拷贝，如果对象发生变化，所有指向对象的指针所得值都会跟着变化。 深拷贝 拷贝得到的对象完全独立，不会受源对象的影响。 深度递归实现（注意环路数据结构） 序列后与反序列化 ","date":"2023-08-31","objectID":"/go%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F%E4%B9%8B%E5%8E%9F%E5%9E%8B%E6%A8%A1%E5%BC%8F/:1:0","tags":[],"title":"Go设计模式之原型模式","uri":"/go%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F%E4%B9%8B%E5%8E%9F%E5%9E%8B%E6%A8%A1%E5%BC%8F/"},{"categories":["设计模式"],"content":"例子 需求: 假设现在数据库中有大量数据，包含了关键词，关键词被搜索的次数等信息，模块 A 为了业务需要 会在启动时加载这部分数据到内存中 并且需要定时更新里面的数据 同时展示给用户的数据每次必须要是相同版本的数据，不能一部分数据来自版本 1 一部分来自版本 2 // @author cold bin // @date 2023/8/31 package prototype import ( \"encoding/json\" \"time\" ) // Keyword 搜索关键字 type Keyword struct { Word string Visit int UpdatedAt *time.Time } // Clone 这里使用序列化与反序列化的方式深拷贝 func (k *Keyword) Clone() *Keyword { var newKeyword Keyword b, _ := json.Marshal(k) json.Unmarshal(b, \u0026newKeyword) return \u0026newKeyword } // Keywords 关键字 map // 数据库缓存 type Keywords map[string]*Keyword // Clone 复制一个新的 keywords // updatedWords: 需要更新的关键词列表，由于从数据库中获取数据常常是数组的方式 func (words Keywords) Clone(updatedWords []*Keyword) Keywords { newKeywords := Keywords{} for k, v := range words { // 这里是浅拷贝，直接拷贝了地址 newKeywords[k] = v } // 替换掉需要更新的字段，这里用的是深拷贝 for _, word := range updatedWords { newKeywords[word.Word] = word.Clone() } return newKeywords } ","date":"2023-08-31","objectID":"/go%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F%E4%B9%8B%E5%8E%9F%E5%9E%8B%E6%A8%A1%E5%BC%8F/:2:0","tags":[],"title":"Go设计模式之原型模式","uri":"/go%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F%E4%B9%8B%E5%8E%9F%E5%9E%8B%E6%A8%A1%E5%BC%8F/"},{"categories":["设计模式"],"content":" 来源于topgoer 函数选项模式 ","date":"2023-08-30","objectID":"/go%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F%E4%B9%8B%E5%87%BD%E6%95%B0%E9%80%89%E9%A1%B9%E6%A8%A1%E5%BC%8F/:0:0","tags":[],"title":"Go设计模式之函数选项模式","uri":"/go%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F%E4%B9%8B%E5%87%BD%E6%95%B0%E9%80%89%E9%A1%B9%E6%A8%A1%E5%BC%8F/"},{"categories":["设计模式"],"content":"默认值 有时候一个函数会有很多参数，为了方便函数的使用，我们会给希望给一些参数设定默认值，调用时只需要传与默认值不同的参数即可，类似于 python 里面的默认参数和字典参数，虽然 golang 里面既没有默认参数也没有字典参数，但是我们有选项模式。 ","date":"2023-08-30","objectID":"/go%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F%E4%B9%8B%E5%87%BD%E6%95%B0%E9%80%89%E9%A1%B9%E6%A8%A1%E5%BC%8F/:1:0","tags":[],"title":"Go设计模式之函数选项模式","uri":"/go%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F%E4%B9%8B%E5%87%BD%E6%95%B0%E9%80%89%E9%A1%B9%E6%A8%A1%E5%BC%8F/"},{"categories":["设计模式"],"content":"应用 从这里可以看到，为了实现选项的功能，我们增加了很多的代码，实现成本相对还是较高的，所以实践中需要根据自己的业务场景去权衡是否需要使用。个人总结满足下面条件可以考虑使用选项模式 参数确实比较复杂，影响调用方使用 参数确实有比较清晰明确的默认值 为参数的后续拓展考虑 ","date":"2023-08-30","objectID":"/go%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F%E4%B9%8B%E5%87%BD%E6%95%B0%E9%80%89%E9%A1%B9%E6%A8%A1%E5%BC%8F/:2:0","tags":[],"title":"Go设计模式之函数选项模式","uri":"/go%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F%E4%B9%8B%E5%87%BD%E6%95%B0%E9%80%89%E9%A1%B9%E6%A8%A1%E5%BC%8F/"},{"categories":["设计模式"],"content":"代码实现 // @author cold bin // @date 2023/8/30 package option const ( DefaultMaxTotal = 10 DefaultMaxIdle = 9 DefaultMinIdle = 1 ) type Pool struct { name string maxTotal int maxIdle int minIdle int } func NewPool(name string, opts ...Option) *Pool { // 填必须参数 p := \u0026Pool{name: name, maxTotal: DefaultMaxTotal, maxIdle: DefaultMaxIdle, minIdle: DefaultMinIdle} // 填入选项 for _, opt := range opts { opt(p) } return p } type Option func(*Pool) func WithMaxTotal(maxTotal int) Option { return func(pool *Pool) { pool.maxTotal = maxTotal } } func WithMaxIdle(maxIdle int) Option { return func(pool *Pool) { pool.maxIdle = maxIdle } } func WithMinIdle(minIdle int) Option { return func(pool *Pool) { pool.minIdle = minIdle } } ","date":"2023-08-30","objectID":"/go%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F%E4%B9%8B%E5%87%BD%E6%95%B0%E9%80%89%E9%A1%B9%E6%A8%A1%E5%BC%8F/:3:0","tags":[],"title":"Go设计模式之函数选项模式","uri":"/go%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F%E4%B9%8B%E5%87%BD%E6%95%B0%E9%80%89%E9%A1%B9%E6%A8%A1%E5%BC%8F/"},{"categories":["设计模式"],"content":" 建造者模式 与工厂模式不同，建造者模式只创建一种类型的复杂对象，可以通过设置可选参数，定制化地创建不同对象。 简而言之，创建参数复杂的对象 ","date":"2023-08-30","objectID":"/go%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F%E4%B9%8B%E5%BB%BA%E9%80%A0%E8%80%85%E6%A8%A1%E5%BC%8F/:0:0","tags":[],"title":"Go设计模式之建造者模式","uri":"/go%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F%E4%B9%8B%E5%BB%BA%E9%80%A0%E8%80%85%E6%A8%A1%E5%BC%8F/"},{"categories":["设计模式"],"content":"应用场景 类属性较多 类属性之间含有依赖关系 存在必选和非必选参数 希望创建不可变对象 ","date":"2023-08-30","objectID":"/go%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F%E4%B9%8B%E5%BB%BA%E9%80%A0%E8%80%85%E6%A8%A1%E5%BC%8F/:1:0","tags":[],"title":"Go设计模式之建造者模式","uri":"/go%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F%E4%B9%8B%E5%BB%BA%E9%80%A0%E8%80%85%E6%A8%A1%E5%BC%8F/"},{"categories":["设计模式"],"content":"代码实现 // @author cold bin // @date 2023/8/30 package builder import \"errors\" const ( DefaultMaxTotal = 10 DefaultMaxIdle = 9 DefaultMinIdle = 1 ) type Build struct { name string maxTotal int maxIdle int minIdle int } // Build 建造器 // // 这里负责参数校验，通过后真正new对象 func (b *Build) Build() (*Pool, error) { if b.maxIdle \u003c= 0 { b.maxIdle = DefaultMaxIdle } if b.minIdle \u003c= 0 { b.minIdle = DefaultMinIdle } if b.maxTotal \u003c= 0 { b.maxTotal = DefaultMaxTotal } if b.minIdle \u003e b.maxIdle { return nil, errors.New(\"minIdle is more than maxIdle\") } if b.maxIdle \u003e b.maxTotal { return nil, errors.New(\"maxIdle is more than maxTotal\") } return \u0026Pool{ name: b.name, maxTotal: b.maxTotal, maxIdle: b.maxIdle, minIdle: b.minIdle, }, nil } func (b *Build) SetName(name string) { b.name = name } func (b *Build) SetMaxTotal(maxTotal int) { b.maxTotal = maxTotal } func (b *Build) SetMaxIdle(maxIdle int) { b.maxIdle = maxIdle } func (b *Build) SetMinIdle(minIdle int) { b.minIdle = minIdle } type Pool struct { name string maxTotal int maxIdle int minIdle int } ","date":"2023-08-30","objectID":"/go%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F%E4%B9%8B%E5%BB%BA%E9%80%A0%E8%80%85%E6%A8%A1%E5%BC%8F/:2:0","tags":[],"title":"Go设计模式之建造者模式","uri":"/go%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F%E4%B9%8B%E5%BB%BA%E9%80%A0%E8%80%85%E6%A8%A1%E5%BC%8F/"},{"categories":["设计模式"],"content":" 工厂模式 与单例模式不同，工厂模式根据传入参数不同，会创建出不同的但是相关联的对象，由给定参数来决定是哪一种对象。像一个工厂一样，传入什么，生产什么，不止一种。 ","date":"2023-08-28","objectID":"/go%E8%AF%AD%E8%A8%80%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F%E4%B9%8B%E5%B7%A5%E5%8E%82%E6%A8%A1%E5%BC%8F/:0:0","tags":[],"title":"Go语言设计模式之工厂模式","uri":"/go%E8%AF%AD%E8%A8%80%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F%E4%B9%8B%E5%B7%A5%E5%8E%82%E6%A8%A1%E5%BC%8F/"},{"categories":["设计模式"],"content":"简单工厂 传出不同对象需要，使用接口多态特性 // @author cold bin // @date 2023/8/28 package factory type Config interface { Parse(data []byte) error UnParse(src []byte, dst []byte) error } type Json struct { } func (j Json) Parse(data []byte) error { panic(\"implement me\") } func (j Json) UnParse(src []byte, dst []byte) error { panic(\"implement me\") } type Yaml struct { } func (y Yaml) Parse(data []byte) error { panic(\"implement me\") } func (y Yaml) UnParse(src []byte, dst []byte) error { panic(\"implement me\") } // NewConfig 这里直接使用简单工厂方法创建最终对象 // // 如果创建对象不复杂，不涉及对象之间的组合，可以使用 func NewConfig(name, typ string) Config { switch typ { case \"json\": return \u0026Json{} case \"yaml\": return \u0026Yaml{} } return nil } ","date":"2023-08-28","objectID":"/go%E8%AF%AD%E8%A8%80%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F%E4%B9%8B%E5%B7%A5%E5%8E%82%E6%A8%A1%E5%BC%8F/:1:0","tags":[],"title":"Go语言设计模式之工厂模式","uri":"/go%E8%AF%AD%E8%A8%80%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F%E4%B9%8B%E5%B7%A5%E5%8E%82%E6%A8%A1%E5%BC%8F/"},{"categories":["设计模式"],"content":"工厂方法 简单工厂适合直接创建一些较简单的对象，如果涉及多个对象之间的组合以及初始化，可以考虑使用工厂方法。工厂方法并不是直接拿的直接对象，而是拿的工厂，拿到之后，可以在有需要的时候根据工厂来创建对象并组合。 而且，工厂可以复用，避免重复创建工厂 // @author cold bin // @date 2023/8/29 package factory // ConfigFactory Config 的工厂方法抽象 type ConfigFactory interface { Create() Config } type JsonFactory struct{} func (j JsonFactory) Create() Config { return Json{} } type YamlFactory struct{} func (y YamlFactory) Create() Config { return Json{} } // NewConfigFactory 这里使用简单工厂的方式创建工厂 // // 与简单工厂不同的是，这里拿的还是工厂，而不是直接的对象 func NewConfigFactory(typ string) ConfigFactory { switch typ { case \"json\": return JsonFactory_ case \"yaml\": return YamlFactory_ } return nil } var ( JsonFactory_ = JsonFactory{} YamlFactory_ = YamlFactory{} ) ","date":"2023-08-28","objectID":"/go%E8%AF%AD%E8%A8%80%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F%E4%B9%8B%E5%B7%A5%E5%8E%82%E6%A8%A1%E5%BC%8F/:2:0","tags":[],"title":"Go语言设计模式之工厂模式","uri":"/go%E8%AF%AD%E8%A8%80%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F%E4%B9%8B%E5%B7%A5%E5%8E%82%E6%A8%A1%E5%BC%8F/"},{"categories":["设计模式"],"content":"抽象工厂 略 ","date":"2023-08-28","objectID":"/go%E8%AF%AD%E8%A8%80%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F%E4%B9%8B%E5%B7%A5%E5%8E%82%E6%A8%A1%E5%BC%8F/:3:0","tags":[],"title":"Go语言设计模式之工厂模式","uri":"/go%E8%AF%AD%E8%A8%80%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F%E4%B9%8B%E5%B7%A5%E5%8E%82%E6%A8%A1%E5%BC%8F/"},{"categories":["设计模式"],"content":"应用——DI DI是依赖注入的意思。DI的底层设计其实就是工厂模式的应用。 ","date":"2023-08-28","objectID":"/go%E8%AF%AD%E8%A8%80%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F%E4%B9%8B%E5%B7%A5%E5%8E%82%E6%A8%A1%E5%BC%8F/:4:0","tags":[],"title":"Go语言设计模式之工厂模式","uri":"/go%E8%AF%AD%E8%A8%80%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F%E4%B9%8B%E5%B7%A5%E5%8E%82%E6%A8%A1%E5%BC%8F/"},{"categories":["设计模式"],"content":"总结 工厂模式是用以创建不同但是相关联的对象，根据传入参数来决定创建那种对象。 不用工厂模式创建多个有关联对象：if-else逻辑过多、创建逻辑、业务代码耦合在一起 简单工厂可以将多个对象的创建逻辑放到一个工厂类里 工厂方法可以将不同创建逻辑拆分到不同工厂类里，然后可以通过特定工厂创建对象 适合简单工厂创建对象过于复杂的情形 ","date":"2023-08-28","objectID":"/go%E8%AF%AD%E8%A8%80%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F%E4%B9%8B%E5%B7%A5%E5%8E%82%E6%A8%A1%E5%BC%8F/:5:0","tags":[],"title":"Go语言设计模式之工厂模式","uri":"/go%E8%AF%AD%E8%A8%80%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F%E4%B9%8B%E5%B7%A5%E5%8E%82%E6%A8%A1%E5%BC%8F/"},{"categories":["设计模式"],"content":"单例模式 简而言之：一个类只允许创建一个对象或示例。 ","date":"2023-08-28","objectID":"/go%E8%AF%AD%E8%A8%80%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F%E4%B9%8B%E5%8D%95%E4%BE%8B%E6%A8%A1%E5%BC%8F/:0:0","tags":[],"title":"Go语言设计模式之单例模式","uri":"/go%E8%AF%AD%E8%A8%80%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F%E4%B9%8B%E5%8D%95%E4%BE%8B%E6%A8%A1%E5%BC%8F/"},{"categories":["设计模式"],"content":"饿汉式 项目初始化的时候加载并初始化对象。创建过程线程安全，而且使得问题尽早暴露。 // @author cold bin // @date 2023/8/28 package singleton /* 单例模式的饿汉式:项目编译运行的初期就已经初始化了对象 */ type Logger interface { print(...any) error } type log struct{} func (l *log) print(...any) error { return nil } var log_ *log // 只要导入包，编译时便会初始化这个对象 func init() { log_ = \u0026log{} } func GetInstance() Logger { return log_ } ","date":"2023-08-28","objectID":"/go%E8%AF%AD%E8%A8%80%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F%E4%B9%8B%E5%8D%95%E4%BE%8B%E6%A8%A1%E5%BC%8F/:1:0","tags":[],"title":"Go语言设计模式之单例模式","uri":"/go%E8%AF%AD%E8%A8%80%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F%E4%B9%8B%E5%8D%95%E4%BE%8B%E6%A8%A1%E5%BC%8F/"},{"categories":["设计模式"],"content":"懒汉式（双重检测） 支持延迟加载，在第一次使用对象的时候创建对象，也就是有需要使用的时候再创建对象。相比懒汉式，支持对象创建的并发安全。代码实现思路：一是通过sync.Mutex直接加锁；二是使用sync.Once保证对象创建只调用一次。 // @author cold bin // @date 2023/8/28 package singleton import \"sync\" /* 单例模式的懒汉式：支持延迟加载，实现范式会导致频繁加锁和释放锁，而且并发度也低。双重检测版本 */ var ( log__ *log once sync.Once ) func GetLazyInstance() Logger { if log__ == nil { once.Do(func() { log__ = \u0026log{} }) } return log__ } ","date":"2023-08-28","objectID":"/go%E8%AF%AD%E8%A8%80%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F%E4%B9%8B%E5%8D%95%E4%BE%8B%E6%A8%A1%E5%BC%8F/:2:0","tags":[],"title":"Go语言设计模式之单例模式","uri":"/go%E8%AF%AD%E8%A8%80%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F%E4%B9%8B%E5%8D%95%E4%BE%8B%E6%A8%A1%E5%BC%8F/"},{"categories":["设计模式"],"content":" 设计原则 SOLID原则 SRP、OCP、LSP、ISP、DIP KISS原则 YAGNI原则 DRY原则 LOD原则 ","date":"2023-08-28","objectID":"/go%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F%E4%B9%8B%E8%AE%BE%E8%AE%A1%E5%8E%9F%E5%88%99/:0:0","tags":[],"title":"Go设计模式之设计原则","uri":"/go%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F%E4%B9%8B%E8%AE%BE%E8%AE%A1%E5%8E%9F%E5%88%99/"},{"categories":["设计模式"],"content":"SRP 单一职责原则（Single Responsibility Principle）：一个模块或类只负责完成一个功能。不要设计大而全的类。 是否越单一越好？ 越单一的模块，会使得代码内聚性变低。我们要做到高内聚，低耦合。 如何判断模块设计的是否满足SRP原则？ 以业务而定。 ","date":"2023-08-28","objectID":"/go%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F%E4%B9%8B%E8%AE%BE%E8%AE%A1%E5%8E%9F%E5%88%99/:1:0","tags":[],"title":"Go设计模式之设计原则","uri":"/go%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F%E4%B9%8B%E8%AE%BE%E8%AE%A1%E5%8E%9F%E5%88%99/"},{"categories":["设计模式"],"content":"OCP 开闭原则（Open / Closed Principle）：通过在已有代码的基础上扩展代码（添加新的模块、类、方法、属性等），而非修改代码（修改已有的模块、类、方法、属性等）。 如何理解开闭原则？ 扩展和修改的定义是相对于作用对象而言的。 添加一个方法或属性在类的层面就是修改类，但是在方法或属性层面就是扩展了属性或方法。换言之，同样的代码改动，在不同对象粒度上，会有不同看法。 OCP并非完全排斥修改，而是将修改对代码可维护性的影响降至最低 怎么做到开闭原则？ 提前了解项目需求，预测短期内可能出现的业务变更，组件变更等，在这些地方预留扩展点。怎么留扩展点？运用一些设计模式技巧、基于接口而非实现编程、依赖注入、多态等 ","date":"2023-08-28","objectID":"/go%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F%E4%B9%8B%E8%AE%BE%E8%AE%A1%E5%8E%9F%E5%88%99/:2:0","tags":[],"title":"Go设计模式之设计原则","uri":"/go%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F%E4%B9%8B%E8%AE%BE%E8%AE%A1%E5%8E%9F%E5%88%99/"},{"categories":["设计模式"],"content":"LSP 里氏替换原则（Liskov Subsititution Principle）：子类对象能够替换程序中对父类对象出现的任何地方，并且保证原来程序的逻辑行为不变及正确不被破坏。 哪些子类设计违反LSP 子类违背父类声明要实现的功能 子类违背父类对输入、输出和异常的要求 子类违背父类注释中所罗列的任何特殊声明 简而言之，子类对父类方法重构的实际输入输出、功能需求不能发生变化。 LSP和多态有什么区别？ LSP是设计原则，多态是编程方法。LSP指导子类如何设计，以让子类可以替换父类出现的任何位置。 ","date":"2023-08-28","objectID":"/go%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F%E4%B9%8B%E8%AE%BE%E8%AE%A1%E5%8E%9F%E5%88%99/:3:0","tags":[],"title":"Go设计模式之设计原则","uri":"/go%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F%E4%B9%8B%E8%AE%BE%E8%AE%A1%E5%8E%9F%E5%88%99/"},{"categories":["设计模式"],"content":"ISP 接口隔离原则（Interface Segregation Principle）：客户端不应该强迫依赖它不需要的接口。 三种理解： 一组API集合 某组API只被某个客户端使用，那就没有必要将这组API暴露给其他客户端，将它隔离出来 一个API或函数 类似于单一职责原则，函数或API功能单一，不应该包含其他功能或业务。 OOP里的接口 语法上的接口，应该尽可能使得接口里面的方法少，只包含必要的方法 ","date":"2023-08-28","objectID":"/go%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F%E4%B9%8B%E8%AE%BE%E8%AE%A1%E5%8E%9F%E5%88%99/:4:0","tags":[],"title":"Go设计模式之设计原则","uri":"/go%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F%E4%B9%8B%E8%AE%BE%E8%AE%A1%E5%8E%9F%E5%88%99/"},{"categories":["设计模式"],"content":"DIP 依赖反转原则（Dependency Inversion Principle）：高层模块不要依赖低层模块，高层模块和低层模块应该通过抽象来相互依赖。抽象不要依赖具体实现细节，具体实现以来抽象。 ","date":"2023-08-28","objectID":"/go%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F%E4%B9%8B%E8%AE%BE%E8%AE%A1%E5%8E%9F%E5%88%99/:5:0","tags":[],"title":"Go设计模式之设计原则","uri":"/go%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F%E4%B9%8B%E8%AE%BE%E8%AE%A1%E5%8E%9F%E5%88%99/"},{"categories":["设计模式"],"content":"IOC 控制反转（Inversion Of Control）：通过框架约束程序执行流程。 ","date":"2023-08-28","objectID":"/go%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F%E4%B9%8B%E8%AE%BE%E8%AE%A1%E5%8E%9F%E5%88%99/:5:1","tags":[],"title":"Go设计模式之设计原则","uri":"/go%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F%E4%B9%8B%E8%AE%BE%E8%AE%A1%E5%8E%9F%E5%88%99/"},{"categories":["设计模式"],"content":"DI 依赖注入（Dependency Injection）：是一种实现IOC的且用于解决依赖性问题的设计模式，我们不在内部创建对象并使用它，而是将依赖对象直接注入来使用，这样我们就只依赖依赖注入的对象。 下面是摘自飞书技术。 依赖注入前 // dal/user.go func (u *UserDal) Create(ctx context.Context, data *UserCreateParams) error { db := mysql.GetDB().Model(\u0026entity.User{}) user := entity.User{ Username: data.Username, Password: data.Password, } return db.Create(\u0026user).Error } // service/user.go func (u *UserService) Register(ctx context.Context, data *schema.RegisterReq) (*schema.RegisterRes, error) { params := dal.UserCreateParams{ Username: data.Username, Password: data.Password, } err := dal.GetUserDal().Create(ctx, params) if err != nil { return nil, err } registerRes := schema.RegisterRes{ Msg: \"register success\", } return \u0026registerRes, nil } 在这段代码里，层级依赖关系为 service -\u003e dal -\u003e db，上游层级通过 Getxxx实例化依赖。但在实际生产中，我们的依赖链比较少是垂直依赖关系，更多的是横向依赖。即我们一个方法中，可能要多次调用Getxxx的方法，这样使得我们代码极不简洁。 不仅如此，我们的依赖都是写死的，即依赖者的代码中写死了被依赖者的生成关系。当被依赖者的生成方式改变，我们也需要改变依赖者的函数，这极大的增加了修改代码量以及出错风险。 接下来我们用依赖注入的方式对代码进行改造： 依赖注入后 // dal/user.go type UserDal struct{ DB *gorm.DB } func NewUserDal(db *gorm.DB) *UserDal{ return \u0026UserDal{ DB: db } } func (u *UserDal) Create(ctx context.Context, data *UserCreateParams) error { db := u.DB.Model(\u0026entity.User{}) user := entity.User{ Username: data.Username, Password: data.Password, } return db.Create(\u0026user).Error } // service/user.go type UserService struct{ UserDal *dal.UserDal } func NewUserService(userDal dal.UserDal) *UserService{ return \u0026UserService{ UserDal: userDal } } func (u *UserService) Register(ctx context.Context, data *schema.RegisterReq) (*schema.RegisterRes, error) { params := dal.UserCreateParams{ Username: data.Username, Password: data.Password, } err := u.UserDal.Create(ctx, params) if err != nil { return nil, err } registerRes := schema.RegisterRes{ Msg: \"register success\", } return \u0026registerRes, nil } // main.go db := mysql.GetDB() userDal := dal.NewUserDal(db) userService := dal.NewUserService(userDal) 如上编码情况中，我们通过将 db 实例对象注入到 dal 中，再将 dal 实例对象注入到 service 中，实现了层级间的依赖注入。解耦了部分依赖关系。 在系统简单、代码量少的情况下上面的实现方式确实没什么问题。但是项目庞大到一定程度，结构之间的关系变得非常复杂时，手动创建每个依赖，然后层层组装起来的方式就会变得异常繁琐，并且容易出错。这个时候勇士 wire 出现了！ ","date":"2023-08-28","objectID":"/go%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F%E4%B9%8B%E8%AE%BE%E8%AE%A1%E5%8E%9F%E5%88%99/:5:2","tags":[],"title":"Go设计模式之设计原则","uri":"/go%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F%E4%B9%8B%E8%AE%BE%E8%AE%A1%E5%8E%9F%E5%88%99/"},{"categories":["设计模式"],"content":"KISS 尽量保持简单（Keep It Simple and Stupid） 如何写出满足KISS原则的代码？ 注重代码可读性 不要重复造轮子（生产开发 不要过度优化 ","date":"2023-08-28","objectID":"/go%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F%E4%B9%8B%E8%AE%BE%E8%AE%A1%E5%8E%9F%E5%88%99/:6:0","tags":[],"title":"Go设计模式之设计原则","uri":"/go%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F%E4%B9%8B%E8%AE%BE%E8%AE%A1%E5%8E%9F%E5%88%99/"},{"categories":["设计模式"],"content":"YAGNI 你不会需要它（You Aren’t Gonna Need It）：核心就是不要过度设计，把一些目前没必要使用的模块也引入 ","date":"2023-08-28","objectID":"/go%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F%E4%B9%8B%E8%AE%BE%E8%AE%A1%E5%8E%9F%E5%88%99/:7:0","tags":[],"title":"Go设计模式之设计原则","uri":"/go%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F%E4%B9%8B%E8%AE%BE%E8%AE%A1%E5%8E%9F%E5%88%99/"},{"categories":["设计模式"],"content":"DRY 不要重复自己（Don’t repeat yourself）： 实现逻辑重复，但功能逻辑不重复，并不违反DRY原则 实现逻辑不重复，但功能逻辑重复，违反DRY原则 代码执行重复也算违背DRY原则 如何提高代码复用性？ 减少代码耦合 满足单一职责原则 模块化 业务与非业务逻辑分离 通用代码下沉 抽象、多态和封装 使用合适的设计模式 ","date":"2023-08-28","objectID":"/go%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F%E4%B9%8B%E8%AE%BE%E8%AE%A1%E5%8E%9F%E5%88%99/:8:0","tags":[],"title":"Go设计模式之设计原则","uri":"/go%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F%E4%B9%8B%E8%AE%BE%E8%AE%A1%E5%8E%9F%E5%88%99/"},{"categories":["设计模式"],"content":"LOD 迪米特法则（Law Of Demeter）：每个模块只应该了解哪些与它关系密切模块的优先知识。 如何理解高内聚、松耦合？ 高内聚就是指把功能相近或为一体的放到同一个模块实现，松耦合就是减少模块之间不必要的依赖。 ","date":"2023-08-28","objectID":"/go%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F%E4%B9%8B%E8%AE%BE%E8%AE%A1%E5%8E%9F%E5%88%99/:9:0","tags":[],"title":"Go设计模式之设计原则","uri":"/go%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F%E4%B9%8B%E8%AE%BE%E8%AE%A1%E5%8E%9F%E5%88%99/"},{"categories":["设计模式"],"content":"OOP OOP指的就是面向对象编程。有三大特性：封装、继承和多态 封装：通过对象访问控制权限实现，只对外暴露必要的方法修改对象，不能直接将对象全部暴露，也不能暴露可能存在安全隐患的字段。 继承：继承父类对象的属性和方法，达到代码复用的目的 多态：go语言里可以这么认为，实现之前先定义一个好的抽象的接口，然后再让对象去实现这个接口，如果后面要迁移或者换一个对象实现，只需要重新用新对象实现这个接口即可，可以保证不修改调用者代码。 ","date":"2023-08-27","objectID":"/go%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F%E4%B9%8Boop/:0:0","tags":[],"title":"Go设计模式之OOP","uri":"/go%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F%E4%B9%8Boop/"},{"categories":["杂"],"content":"记一次picgo+github图床上传失败的过程 ","date":"2023-08-22","objectID":"/%E8%AE%B0%E4%B8%80%E6%AC%A1github%E5%9B%BE%E5%BA%8A%E4%B8%8A%E4%BC%A0%E5%BC%82%E5%B8%B8%E7%9A%84bug/:1:0","tags":[],"title":"记一次github图床上传异常的bug","uri":"/%E8%AE%B0%E4%B8%80%E6%AC%A1github%E5%9B%BE%E5%BA%8A%E4%B8%8A%E4%BC%A0%E5%BC%82%E5%B8%B8%E7%9A%84bug/"},{"categories":["杂"],"content":"问题 这次遇到一个非常非常奇怪的问题。我和往常一样使用picgo在github上上传图片。但是突然没有预兆的给我报了err: connected etimedout的错误（指连接超时）。 我ping了一下api.github.com，发现链路不通，数据包送不过去，但是浏览器还可以请求api.github.com。 上网冲浪后得知：应该是服务端设置了相关策略对网络层icmp回显请求报文进行了限制；而访问网页用的是http协议，因此会出现此现象。 所以这个现象聊胜于无。 后来看到picgo的issue中，有不少人都提了这个问题。 ","date":"2023-08-22","objectID":"/%E8%AE%B0%E4%B8%80%E6%AC%A1github%E5%9B%BE%E5%BA%8A%E4%B8%8A%E4%BC%A0%E5%BC%82%E5%B8%B8%E7%9A%84bug/:1:1","tags":[],"title":"记一次github图床上传异常的bug","uri":"/%E8%AE%B0%E4%B8%80%E6%AC%A1github%E5%9B%BE%E5%BA%8A%E4%B8%8A%E4%BC%A0%E5%BC%82%E5%B8%B8%E7%9A%84bug/"},{"categories":["杂"],"content":"解决 其实这个就是系统代理本身的问题了：并不是所有软件或工具的网络请求都会走系统代理，有些应用的网络请求可能绕过代理，直接与网络通信。 所以，我们需要给picgo手动设置代理，让picgo的所有请求一定要经过代理。如下图： 我们在picgo中设置了vpn的服务端口。这样所有请求就可以转发到这里，包括一些外网的请求。 因此，解决了代理问题。 ","date":"2023-08-22","objectID":"/%E8%AE%B0%E4%B8%80%E6%AC%A1github%E5%9B%BE%E5%BA%8A%E4%B8%8A%E4%BC%A0%E5%BC%82%E5%B8%B8%E7%9A%84bug/:2:0","tags":[],"title":"记一次github图床上传异常的bug","uri":"/%E8%AE%B0%E4%B8%80%E6%AC%A1github%E5%9B%BE%E5%BA%8A%E4%B8%8A%E4%BC%A0%E5%BC%82%E5%B8%B8%E7%9A%84bug/"},{"categories":["杂"],"content":"意外惊喜 我最近一直遇到这样问题：我明明已经开了梯子，但为什么我在bash中git pull或git push时总是报这个两个错： fatal: unable to access 'https://github.com/cold-bin/cold-bin.github.io.git/': OpenSSL SSL_read: Connection was reset, errno 10054 or fatal: unable to access 'https://github.com/cold-bin/cold-bin.github.io.git/': Failed to connect to github.com port 443 after 21109 ms: Timed out 其实和上面的问题一样，有些软件不走你的系统代理，直接走网络通信。解决方案也和上面一样：给git设置vpn代理的端口。 git config --global http.proxy http://127.0.0.1:7890 git config --global https.proxy http://127.0.0.1:7890 # 我的clash在7890上系统代理 ","date":"2023-08-22","objectID":"/%E8%AE%B0%E4%B8%80%E6%AC%A1github%E5%9B%BE%E5%BA%8A%E4%B8%8A%E4%BC%A0%E5%BC%82%E5%B8%B8%E7%9A%84bug/:3:0","tags":[],"title":"记一次github图床上传异常的bug","uri":"/%E8%AE%B0%E4%B8%80%E6%AC%A1github%E5%9B%BE%E5%BA%8A%E4%B8%8A%E4%BC%A0%E5%BC%82%E5%B8%B8%E7%9A%84bug/"},{"categories":["分布式系统"],"content":"lab1——实现简易版的mapreduce框架 ","date":"2023-08-10","objectID":"/mit6.824%E4%B9%8Blab1/:1:0","tags":["mit6.824"],"title":"Mit6.824之lab1","uri":"/mit6.824%E4%B9%8Blab1/"},{"categories":["分布式系统"],"content":"论文回顾 mapreduce架构 严格来讲，MapReduce是一种分布式计算模型，用于解决大于1TB数据量的大数据计算处理。著名的开源项目Hadoop和Spark在计算方面都实现的是MapReduce模型。从论文中可以看到花了不少篇幅在讲解这个模型的原理和运行过程，但同时也花了一点篇幅来讲解处理分布式系统实现中可能遇到的问题。 MapReduce的模型设计很容易进行水平横向扩展以加强系统的能力，基本分为两种任务：map和reduce，通过map任务完成程序逻辑的并发，通过reduce任务完成并发结果的归约和收集，使用这个框架的开发者的任务就是把自己的业务逻辑先分为这两种任务，然后丢给MapReduce模型去运行。设计上，执行这两种任务的worker可以运行在普通的PC机器上，不需要使用太多资源。当系统整体能力不足时，通过增加worker即可解决。 性能瓶颈 那么什么更容易导致系统性能扩展的瓶颈？CPU？内存？磁盘？还是网络？在2004年这篇文章问世的时候回答还是”网络带宽“最受限，论文想方设法的减少数据在系统内的搬运与传输，而到如今数据中心的内网速度要比当时快多了，因此如今更可能的答案恐怕就是磁盘了，新的架构会减少数据持久化到磁盘的次数，更多的利用内存甚至网络（这正是Spark的设计理念） 如何处理较慢的网络？参考论文3.4节减少网络带宽资源的浪费，都尽量让输入数据保存在构成集群机器的本地硬盘上，并通过使用分布式文件系统GFS进行本地磁盘的管理。尝试分配map任务到尽量靠近这个任务的输入数据库的机器上执行，这样从GFS读时大部分还是在本地磁盘读出来。中间数据传输（map到reduce）经过网络一次，但是分多个key并行执行 负载均衡 某个task运行时间比较其他N-1个都长，大家都必须等其结束那就尴尬了，因此参考论文3.5节、3.6节系统设计保证task比worker数量要多，做的快的worker可以继续先执行其他task，减少等待。（框架的任务调度后来发现更值得研究） 容错 参考论文3.3节重新执行那些失败的MR任务即可，因此需要保证MR任务本身是幂等且无状态的。 更特别一些，worker失效如何处理？将失败任务调配到其他worker重新执行，保证最后输出到GFS上的中间结果过程是原子性操作即可。（减少写错数据的可能） Master失效如何处理？因为master是单点，只能人工干预，系统干脆直接终止，让用户重启重新执行这个计算 其他 其实还有部分工程问题，这篇文章中并没有讨论，可能因为这些更偏重工程实践，比如：task任务的状态如何监控、数据如何移动、worker故障后如何恢复等。 ","date":"2023-08-10","objectID":"/mit6.824%E4%B9%8Blab1/:1:1","tags":["mit6.824"],"title":"Mit6.824之lab1","uri":"/mit6.824%E4%B9%8Blab1/"},{"categories":["分布式系统"],"content":"实现思路 lec1中讲到，mapreduce隐藏的细节，我们需要实现下面的这些： 将应用程序代码发送到服务器 跟踪哪些任务已完成 将中间数据从 Maps“移到” Reduces 平衡服务器负载 从失败中恢复 coordinator 在lab1中，coordinator类似于论文中提到的master，是集群的管理者，负责分配job的task给worker进程。 task分两种： 一种是map任务，负责将给定分区文件的数据处理成中间结果，然后将中间结果输出到本地磁盘。输出时，就进行分区。（分区映射函数hash(key) mod R） 另一种是reduce任务，负责将中间结果收集合并，输出到文件里。 一般来说，lab1中的一个reduce任务就对应一个输出文件，在map任务输出时，就已经在map worker磁盘本地分好区了。后面reduce任务就会从所有map worker里去取自己分区的中间结果集。 // TaskType 任务类型，worker会承担map任务或者reduce任务 type TaskType uint8 const ( Map TaskType = iota + 1 Reduce Done ) coordinator管理过程 首先将map任务分配给worker，直到所有map任务完成为止 所有map任务完成后，coordinator才开始分发reduce任务 coordinator数据结构 在上面谈到，lab里并没有要求我们实现一个工业级别的mapreduce，仅仅要求我们实现简易版的demo 所有的map任务完成时，才能分配reduce任务 这也就意味着，如果只是部分map任务执行完毕时，我们需要等待其他map任务都执行完毕，才能执行reduce任务。怎么实现呢？在lab的hints里，给出了提示:使用sync.Cond条件变量实现 当所有的task没有完成时，我们需要调用`sync.Cond`的`Wait`方法等待`Boradcast`唤醒 \u003e 这样做的好处是只需要满足所有map worker都执行完毕才唤醒当前线程，使其执行下一步，也就是分配reduce任务，这条可以避免轮询cpu带来的性能消耗 上面的点提到”当所有task没有完成“这句话，显然，我们还需要记录map任务和reduce任务分别的完成情况。 以前google内部的mapreduce（2004）实现在输入前，还会分区。但是lab里只需要将不同文件直接当作不同分区，不再细分为64m的block了，而且一个分区对应一个文件，一个文件对应一个map任务。所以，需要给出分区列表（也就是文件名列表）。 lab里还要求解决”落伍者“问题。”落伍者“的大概意思就是，如果worker执行太久（lab里规定为10s）而没有finished，那么就认为这个worker寄掉了，此时，我们需要将task重新分配给其他的worker。 lab中提到，我们需要探测集群中的所有worker是否存活。这里我们是否有必要给worker额外起一个协程来Ping/Pong吗？ 其实我们可以减少这个网络成本。 worker首先会从coordinator拿task去执行，执行完毕后又会返回task的完成信息。我们认为只要在10s内，worker没有返回了完成信息，那么这个worker就寄掉了。 如果10s过后，完成信息返回到了coordinator，又该怎么办呢？ 我们保证确认完成信息是幂等性的就可以了。例如我要在coodinator中更新map任务的完成情况mapTasksFinished的核心代码就是 mapTasksFinished[task_id] = true 我们可以看到，这段代码是幂等的。 所以，我们需要记录worker的完成时间点和拿取任务时间点的差值是否超过了10s，就代表worker是否寄掉。 还有其他数据也需要附带，例如：锁、当前map和reduce任务的数目，job状态 type Coordinator struct { // Your definitions here. mu sync.Mutex cond *sync.Cond mapFiles []string // map任务的输入文件 nMapTasks int // map任务数，lab里规定的一个map任务对应一个worker。len(mapFiles) = nMapTasks nReduceTasks int // 记录任务的完成情况和何时分配，用以跟踪worker的处理进度 mapTasksFinished []bool // 记录map任务完成情况 mapTasksWhenAssigned []time.Time // 记录map任务何时分配 reduceTasksFinished []bool // 记录reduce任务完成情况 reduceTasksWhenAssigned []time.Time // 记录reduce任务何时分配 done bool // 所有reduce任务是否已经完成，也就是整个job是否完成 } coordinator提供的rpc 由上面得知，我们不需要提供pingrpc。我们需要提供申请task和完成task这两个rpc。 申请task rpc task有两种（map和reduce），刚开始我们只能分配map任务，map任务执行完毕过后，才能分配reduce任务。通过sync.Cond来实现”等待“。别忘了落伍者的处理。 func (c *Coordinator) GetTask(args *GetTaskArgs, reply *GetTaskReply) error { c.mu.Lock() defer c.mu.Unlock() reply.NMapTasks = c.nMapTasks reply.NReduceTasks = c.nReduceTasks // 分配worker map任务，直到所有map任务执行完毕 for { mapDone := true for i, done := range c.mapTasksFinished { if !done /*task没有完成*/ { if c.mapTasksWhenAssigned[i].IsZero() || /*任务是否被分配*/ time.Since(c.mapTasksWhenAssigned[i]).Seconds() \u003e 10 /*分配出去10s还没完成，认为worker寄掉了*/ { reply.TaskType = Map reply.TaskId = i reply.MapFileName = c.mapFiles[i] c.mapTasksWhenAssigned[i] = time.Now() return nil } else { mapDone = false } } } if !mapDone { /*没有完成，我们需要阻塞*/ c.cond.Wait() } else { /*全部map任务已经完成了*/ break } } // 此时，所有的map任务已经做完了，可以开始reduce任务了 for { rDone := true for i, done := range c.reduceTasksFinished { if !done /*task没有完成*/ { if c.reduceTasksWhenAssigned[i].IsZero() || /*任务是否被分配*/ time.Since(c.reduceTasksWhenAssigned[i]).Seconds() \u003e 10 /*分配出去10s还没完成，认为worker寄掉了*/ { reply.TaskType = Reduce reply.TaskId = i c.reduceTasksWhenAssigned[i] = time.Now() return nil } else { rDone = false } } } if !rDone { /*没有完成，我们需要等待*/ c.cond.Wait() } else { /*全部map任务已经完成了*/ break } } // if the job is done reply.TaskType = Done c.done = true return nil } 完成task rpc 当任务完成时，需要回传完成信息。并且，需要唤醒阻塞，进入下一步（可能是等待所有map任务的阻塞，或者是最终等待所有reduce任务的阻塞） func (c *Coordinator) FinishedTask(args *FinishedTaskArgs, reply *FinishedTaskReply) error { c.mu.Lock() defer c.mu.Unlock() switch args.TaskType { case Map: c.mapTasksFinished[args.TaskId] = true case Reduce: c.reduceTasksFinished[args.TaskId] = true default: return errors.New(\"coordinator: not support this task type\") } c.cond.Broadcast() return nil } 其他 定时轮询 我们其实并不知道何时所有map任务完成或所有reduce任务完成，所以，我们需要轮询去唤醒阻塞，然后检测是否满足条件，不满就继续阻塞","date":"2023-08-10","objectID":"/mit6.824%E4%B9%8Blab1/:1:2","tags":["mit6.824"],"title":"Mit6.824之lab1","uri":"/mit6.824%E4%B9%8Blab1/"},{"categories":["分布式系统"],"content":"感言 论文其实大部分还是看得懂，最初的时候实现起来毫无头绪，论文给的是一个参考的、极其模糊的实现，而且还是分布式下的。但是lab里没有要求我们需要通过网络来把map worker的中间结果集输送到reduce worker机子里，仅仅只是实现一个单机多进程版本的mapreduce。 这里的实现也是参考了一下lab1 Q\u0026A 才知道我们仅仅只是做一个单机多进程版本的mapreduce，不需要考虑在网络传输map worker的中间结果集。 实现上有很多有意思的地方： 中间任务的输出结果，我们写入临时文件里，处理完毕过后，我们再重命名为最终结果文件；如果中途失败，那么我们得不到这个中间文件，因为该文件是临时文件。这样可以保证我们的map任务或reduce任务要么失败没有输出，要么成功有输出，从而保证了原子性 lab并不像论文中描述的：一旦map任务有一个执行完毕了，那么reduce任务就可以开始启动了。而是采取更简单的实现策略：所有map任务都完成了，才可以启动reduce任务。所以，这里需要有一个等待机制，满足“所有map任务执行完毕”的条件时，我们才能分配reduce任务。所以，这里使用sync.Cond来实现。 如何实现“落伍者”的检测呢？我们可以记录分配任务时和完成任务时的时间差，太长就认为worker超时了。 代码传输门 ","date":"2023-08-10","objectID":"/mit6.824%E4%B9%8Blab1/:2:0","tags":["mit6.824"],"title":"Mit6.824之lab1","uri":"/mit6.824%E4%B9%8Blab1/"},{"categories":["计算机网络"],"content":"cs144 lab的思路与实现 lab0 ","date":"2023-07-31","objectID":"/cs144-lab/:0:0","tags":["cs144"],"title":"Cs144 Lab","uri":"/cs144-lab/"},{"categories":["计算机网络"],"content":"reference link lab0实验手册 ","date":"2023-07-31","objectID":"/cs144-lab/:1:0","tags":["cs144"],"title":"Cs144 Lab","uri":"/cs144-lab/"},{"categories":["计算机网络"],"content":"implementation ","date":"2023-07-31","objectID":"/cs144-lab/:2:0","tags":["cs144"],"title":"Cs144 Lab","uri":"/cs144-lab/"},{"categories":["计算机网络"],"content":"Set up GNU/Linux on your computer 简单地安装CS144 VirtualBox和c++环境，以方便后续地测试。 ","date":"2023-07-31","objectID":"/cs144-lab/:2:1","tags":["cs144"],"title":"Cs144 Lab","uri":"/cs144-lab/"},{"categories":["计算机网络"],"content":"Networking by hand 接下来就是使用简单地telnet来构造HTTP请求和SMTP请求，并得到对应的响应。 （亲手写出一部分报文 ","date":"2023-07-31","objectID":"/cs144-lab/:2:2","tags":["cs144"],"title":"Cs144 Lab","uri":"/cs144-lab/"},{"categories":["计算机网络"],"content":"Listening and connecting 通过netcat建立一个全双工通信的服务端，感受netcat的使用。 （类似于即时通讯服务 ","date":"2023-07-31","objectID":"/cs144-lab/:2:3","tags":["cs144"],"title":"Cs144 Lab","uri":"/cs144-lab/"},{"categories":["计算机网络"],"content":"Writing a network program by using an OS stream socket part 1 lab0的最后一项任务就是使用原生的tcp socket来编写http报文结构，从而在socket的基础上发起一次http请求 详见lab0/minow/apps/webget.cc代码实现。 part2 part2实现可靠字节流，使用一个普通string来存储字节流 lab1 ","date":"2023-07-31","objectID":"/cs144-lab/:2:4","tags":["cs144"],"title":"Cs144 Lab","uri":"/cs144-lab/"},{"categories":["计算机网络"],"content":"reference link lab1实验手册 博客 ","date":"2023-07-31","objectID":"/cs144-lab/:3:0","tags":["cs144"],"title":"Cs144 Lab","uri":"/cs144-lab/"},{"categories":["计算机网络"],"content":"implementation 在tcp/ip协议栈中，数据传输的可靠性不是网络来承担的，而是交由端系统来承担的。网络中的有序字节流从一端发送到另一端，跨越了诸多网络的路由器 ，期间难免出现字节流传输的乱序、重复、重叠等。而lab1就是需要在这样不可靠字节流中去建立可靠的字节流。 ","date":"2023-07-31","objectID":"/cs144-lab/:4:0","tags":["cs144"],"title":"Cs144 Lab","uri":"/cs144-lab/"},{"categories":["计算机网络"],"content":"具体实现 这里参考了上面的博客 lab2 ","date":"2023-07-31","objectID":"/cs144-lab/:4:1","tags":["cs144"],"title":"Cs144 Lab","uri":"/cs144-lab/"},{"categories":["计算机网络"],"content":"reference link lab2实验手册 ","date":"2023-07-31","objectID":"/cs144-lab/:5:0","tags":["cs144"],"title":"Cs144 Lab","uri":"/cs144-lab/"},{"categories":["计算机网络"],"content":"implementation ","date":"2023-07-31","objectID":"/cs144-lab/:6:0","tags":["cs144"],"title":"Cs144 Lab","uri":"/cs144-lab/"},{"categories":["计算机网络"],"content":"wrap与unwrap wrap是将absolute seqno转化为seqno 由于absolute seqno是非循环序号，seqno是循环序号，所以需要取模转化。（当然也可以直接截断。 unwrap是将seqno转化为absolute seqno checkpoint其实就是first_unassembled_index 这里的处理比较麻烦。我最开始的想法是，循环找出最小的checkpoint - (seqno+x * 2^32)，也就是离得最近的x。 但是复杂度比较高，看了大佬的博客，利用位运算，可以将O(x)的时间复杂度降低为O(1). 官方让我们找到离checkpoint最近的absolute seqno，因为给出seqno，会有多个absolute seqno与之对应， ","date":"2023-07-31","objectID":"/cs144-lab/:6:1","tags":["cs144"],"title":"Cs144 Lab","uri":"/cs144-lab/"},{"categories":["计算机网络"],"content":"TCP Receiver 参考上面seqno、absolute seqno与stream index的对应关系图 receive时，直接在reassembler中插入absolute seqno，显然不是期待的stream_index； send时，需要考虑到available_capacity不能超过UINT_MAX以及close时，发送的fin报文也要占据一个序号 lab3 ","date":"2023-07-31","objectID":"/cs144-lab/:6:2","tags":["cs144"],"title":"Cs144 Lab","uri":"/cs144-lab/"},{"categories":["计算机网络"],"content":"reference link lab3实验手册 博客 ","date":"2023-07-31","objectID":"/cs144-lab/:7:0","tags":["cs144"],"title":"Cs144 Lab","uri":"/cs144-lab/"},{"categories":["计算机网络"],"content":"implementation 这里参考了博客实现 实现ARQ重传机制 lab4 ","date":"2023-07-31","objectID":"/cs144-lab/:8:0","tags":["cs144"],"title":"Cs144 Lab","uri":"/cs144-lab/"},{"categories":["计算机网络"],"content":"reference link lab4实验手册 博客 ","date":"2023-07-31","objectID":"/cs144-lab/:9:0","tags":["cs144"],"title":"Cs144 Lab","uri":"/cs144-lab/"},{"categories":["计算机网络"],"content":"implementation Lab4 要求实现网络接口部分， 打通网络数据报 （Internet datagrams） 和链路层的以太网帧（link-layer Ethernet frames） 之间的桥梁。 也就是，实现ip数据报转化为mac帧。ip数据报在转化为mac帧时，最重要的就是要知道目的mac地址，而mac地址我们可以通过arp协议来学习到。 所以，这里就牵扯到了arp协议的实现： 目的ip地址与mac地址的缓存映射。最多保存30s； 端系统可以组装ip报文和arp请求与响应报文，也能解析之； 不论是arp请求还是arp响应，端系统拿到过后都可以学习到对等端的ip的其mac地址的映射关系 而且，我们并不能任性发送arp请求，我们只能等待相同的arp请求发出去5秒后没有收到arp响应才再次发送，这是为了防止频繁地arp广播导致链路阻塞 端系统在接收报文时，收到ip数据报自然不用说，该怎么处理就怎么处理，但是收到arp报文时，需要进一步处理： 如果是arp请求报文 那么我们在校验合法性通过后，还需要学习arp请求的来源ip地址和来源mac地址的映射关系，并且构造arp响应，返回自己的ip与mac地址的映射关系，以供arp请求方学习 如果是arp响应报文 那么我们在校验合法性通过后，再从arp响应报文中学习到目的ip地址和目的mac地址的映射关系。除了mac地址学习外，我们还需要将arp请求等待列表清空 总之，我们通过arp协议拿到了目的mac地址过后，剩下的事情就非常简单了（将ip数据报组装成mac帧，以方便发送到数据链路上 实验手册给的实现已经很详细了，翻译成代码即可。 lab5 ","date":"2023-07-31","objectID":"/cs144-lab/:10:0","tags":["cs144"],"title":"Cs144 Lab","uri":"/cs144-lab/"},{"categories":["计算机网络"],"content":"reference link lab5实验手册 博客 ","date":"2023-07-31","objectID":"/cs144-lab/:11:0","tags":["cs144"],"title":"Cs144 Lab","uri":"/cs144-lab/"},{"categories":["计算机网络"],"content":"implementation lab5的要求是在lab4实现的网络接口上，实现ip router.路由器有多个网络接口，可以在其中任何一个接口上接收 Internet 数据报。 路由器的工作是根据路由表转发它获得的数据报：路由表是一个规则列表，告诉路由器对于任何给定的数据报: 在哪个network interface发出去 确定next hop 实验手册的Q \u0026 A中对route table的数据结构要求很低，允许实现O(N)时间复杂度。所以，这里直接使用std:list\u003ctype\u003e。 lab5的要求其实很简单了。我们只需要实现路由最长前缀匹配即可，并不需要实现动态路由的一些协议（RIP、OSPF、BGP 或 SDN 控制器） 当然除了这些，我们还需要注意： 每次路由转发ip数据报时，ttl需要减一，直至ttl等于0时，路由器会自动丢弃它 如果路由表里没有找到next hop，也会丢弃ip数据报 如果路由的数据报缓存已满，也会丢弃掉（lab5里并没有说 lab6 no code 使用以前的所有实现的lab来创建一个真实的网络，其中包括网络堆栈（主机和路由器），与另一台主机上实现的网络堆栈进行通信。 ","date":"2023-07-31","objectID":"/cs144-lab/:12:0","tags":["cs144"],"title":"Cs144 Lab","uri":"/cs144-lab/"},{"categories":["计算机网络"],"content":"how to do lab6实验手册 ","date":"2023-07-31","objectID":"/cs144-lab/:13:0","tags":["cs144"],"title":"Cs144 Lab","uri":"/cs144-lab/"},{"categories":["计算机网络"],"content":"[toc] ","date":"2023-07-13","objectID":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E7%BD%91%E7%BB%9C%E5%AE%89%E5%85%A8/:0:0","tags":[],"title":"计算机网络之网络安全","uri":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E7%BD%91%E7%BB%9C%E5%AE%89%E5%85%A8/"},{"categories":["计算机网络"],"content":"一、网络攻击的类型 网络攻击分为两大类：被动攻击和主动攻击。 截获：从网络上窃听他人的通信内容，但不干扰原报文在源端和目的端的传送。 篡改：捕获报文并篡改报文后再发送给目的站。 恶意程序：计算机病毒。 拒绝服务 DoS：指攻击者向互联网上的某个服务器不停地发送大量分组，该服务器的网络资源耗尽而无法向其他正常客户端主机提供服务。若从互联网上的成百上千的网站集中攻击一个网站，则称为分布式拒绝服务 DDoS 。 有时也把这种攻击称为网络带宽攻击。 对付被动攻击：采用加密技术（让截获者读不懂报文内容） 对付主动攻击：采用加密技术 + 鉴别技术（指鉴别对方身份） ","date":"2023-07-13","objectID":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E7%BD%91%E7%BB%9C%E5%AE%89%E5%85%A8/:1:0","tags":[],"title":"计算机网络之网络安全","uri":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E7%BD%91%E7%BB%9C%E5%AE%89%E5%85%A8/"},{"categories":["计算机网络"],"content":"二、密钥密码体制 安全的计算机网络应达到以下目标 保密性：只有信息的发送方和接收方才能懂得所发送信息的内容。 端点鉴别（身份鉴别）：鉴别信息的发送方和接收方的真实身份。 信息的完整性：确保信息的内容未被篡改过。 可以通过密钥加密来做到以上3点。下面是数据加密模型： 通信双方：A 和 B。 A 和 B 分别持有密钥 K(E) 和 K(D)，这可以是一对对称密钥或者一对非对称密钥。 A 和 B 分别持有一对加密算法 E 和 D。 A将明文X通过密钥K(e) 和 算法E进行加密得到密文Y，可以把E看做是一个函数，密钥 K(e)和明文X作为这个函数的参数，返回值是Y。 传输给B后，用D(K(d), Y)解密得到X。 需要注意： 0、密钥 K本质是一串字符串。 1、D 和 E 都既可以是加密算法，也可以是解密算法，其本质都是将传入的字符串数据变为另一个字符串，所以我更倾向于说成是 D运算和E运算，而不是说 加密算法 和 解密算法。 2、D 和 E算法 是一对算法，即不论密钥K(e)和K(d)是对称密钥（即 K(e) = K(d)）还是非对称密钥（即 K(e) ≠ K(d)）都有： D运算是E运算的逆运算，E运算也是D运算的逆运算。 E(K(e), X) = Y D(K(d), Y) = X 所以： D(K(d), E(K(e), X)) = X 3、使用密钥和加密算法来加密或解密一串数据时，所花费的时间和数据本身的长度成正比。 4、加密算法 E 和解密算法 D 是公开的。而密钥 K(e) 和 K(d)如果是对称密钥，则这两个密钥是保密的，如果密钥 K(e) 和 K(d)如果是非对称密钥则公钥公开，私钥保密。 下面介绍两种密码体制：对称密钥密码体制 和 非对称密钥密码体制 ","date":"2023-07-13","objectID":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E7%BD%91%E7%BB%9C%E5%AE%89%E5%85%A8/:2:0","tags":[],"title":"计算机网络之网络安全","uri":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E7%BD%91%E7%BB%9C%E5%AE%89%E5%85%A8/"},{"categories":["计算机网络"],"content":"1、对称密钥密码体制 该体制是 加密密钥与解密密钥都使用相同密钥 的密码体制。对称密钥又称为共享密钥。 加密过程如下： E(K, X) = Y D(K, Y) = X 所以： D(K, E(K, X)) = X ","date":"2023-07-13","objectID":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E7%BD%91%E7%BB%9C%E5%AE%89%E5%85%A8/:2:1","tags":[],"title":"计算机网络之网络安全","uri":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E7%BD%91%E7%BB%9C%E5%AE%89%E5%85%A8/"},{"categories":["计算机网络"],"content":"2、非对称密钥密码体制 通信双方持有一对密钥分别是对外公开的公钥PK 和 只有自己知道的私钥SK。例如通信双方 A 和 B，A有一对密钥 PK(a) 和 SK(a)，B也有一对密钥 PK(b) 和 SK(b)。 当A作为发送方向B发送数据时，A要用B的公钥 PK(b) 对明文X加密得到Y，B用私钥 SK(b) 对Y解密得到X。当B作为发送方向A发送数据时，则和上述过程一样要用A的一对密钥加解密。 目前最流行的公钥密码体制是RSA体制。 公钥算法的特点： (1) 密钥对产生器产生出接收者 B 的一对密钥： 加密密钥 PKB 和 解密密钥 SKB 。 加密密钥 PKB 就是接收者 B 的公钥，向公众公开。 解密密钥 SKB 就是接收者 B 的私钥，对其他任何人都保密。 (2) 发送者 A 用 B 的公钥 PKB 对明文 X 加密（E 运算），然后发送给 B。 接收者 B 用自己的私钥 SKB 解密（D 运算），即可恢复出明文：D(SK(b), E(PK(b), X)) = X (3) 从已知的 PKB 实际上不可能推导出 SKB。 (4) 加密密钥是公开的，但不能用来解密：D(PK(b), E(PK(b), X)) ≠ X ","date":"2023-07-13","objectID":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E7%BD%91%E7%BB%9C%E5%AE%89%E5%85%A8/:2:2","tags":[],"title":"计算机网络之网络安全","uri":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E7%BD%91%E7%BB%9C%E5%AE%89%E5%85%A8/"},{"categories":["计算机网络"],"content":"3、对称密钥和公开密钥的区别 使用对称密钥： 在通信信道上可以进行一对一的双向保密通信。即拥有公钥的只有一个客户端和一个服务端。 使用公开密钥： 在通信信道上可以是多对一的单向保密通信。 即拥有公钥的客户端可以有多个，这些客户端都可以和有私钥的服务端通信，单向是指 一对密钥只能用在一个方向的通信（A-\u003eB用B的一对密钥），另一个方向的通信要用另一对密钥才行。 公钥加密算法的开销（运算量）较大（是对称密钥算法的3倍）。因此现实中都是使用对称密钥进行加密。 使用密钥进行加密满足了保密性的要求，但还没有满足实体鉴别和完整性鉴别的要求。不过密钥不仅可以用来加密，还可以用来签名，密钥用来签名可以做到实体鉴别。 ","date":"2023-07-13","objectID":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E7%BD%91%E7%BB%9C%E5%AE%89%E5%85%A8/:2:3","tags":[],"title":"计算机网络之网络安全","uri":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E7%BD%91%E7%BB%9C%E5%AE%89%E5%85%A8/"},{"categories":["计算机网络"],"content":"三、报文鉴别 报文鉴别需要做到两点：验证通信的对方是否是自己要通信的对象而不是篡改者 和 验证报文内容是否被篡改。 报文鉴别通过数字签名实现。 ","date":"2023-07-13","objectID":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E7%BD%91%E7%BB%9C%E5%AE%89%E5%85%A8/:3:0","tags":[],"title":"计算机网络之网络安全","uri":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E7%BD%91%E7%BB%9C%E5%AE%89%E5%85%A8/"},{"categories":["计算机网络"],"content":"1、数字签名原理 例如 A 发送报文X给 B，B要鉴别发送者是A： 1、首先，A 用其私钥 SKA 对报文 X 进行 D 运算得到的密文，这个过程就是签名。 2、B 为了核实签名，用 A 的公钥 PKA 对密文Y进行 E 运算，还原出明文 X 为什么这样可以做到实体鉴别？ 因为SKA只有A才有，B如果能用PKA解密成功（得到的明文是可读的），就说明对方是A。如果B解密出来的明文是不可读的，这段不可读报文也不会对B产生危害。 签名的本质是什么？ 签名的本质是在报文中加一些只有发送方才有的特殊信息，接收方看到报文中的这个特殊信息就知道一定是A发过来的，例如上面对报文用A的私钥进行D运算，A的私钥就是A独有的东西。 所以具体来说，签名就是用自己的私钥给 待签名的对象 进行签名运算。 核实签名的本质是对签名运算的逆运算（即图中的E运算）。 这样做达到了实体鉴别的目的，但没有做到保密性，因为任何一个接收方收到报文都可以用A的公钥（公钥是公开的）解密得到明文。假如有中间者截获，依旧可以直到报文的内容。 为了同时做到实体鉴别和保密，应该使用A的私钥进行数字签名，用B的公钥进行内容加密，这么一来只有B才能用B的私钥解密。这里A的私钥和B的公钥的作用是不同的。 这里我们无需深究使用的是D运算还是E运算，只要知道签名和核实签名使用的是互逆的运算，加密和解密用的也是互逆的运算即可。 综上总结：签名用发送者的私钥，加密用接收者的公钥。 这种方式的很少用，因为需要对报文进行两次 D 运算和两次 E 运算，运算量太大，花费非常多的 CPU 时间。 现实应用中，普遍使用开销小得多的对称密钥实现报文加密，使用非对称密钥实现数字签名，而且一定要设法减小公钥密码算法的开销。 ","date":"2023-07-13","objectID":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E7%BD%91%E7%BB%9C%E5%AE%89%E5%85%A8/:3:1","tags":[],"title":"计算机网络之网络安全","uri":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E7%BD%91%E7%BB%9C%E5%AE%89%E5%85%A8/"},{"categories":["计算机网络"],"content":"2、报文鉴别码 现在我们遇到的问题是：签名需要用发送者的私钥对整个明文进行签名运算（为了避免签名和加密混淆，所以这里说成是签名运算而不是加密运算），因而会消耗很多时间。 为了减小签名的开销，可以使用散列函数H()代替签名算法（签名函数）对明文X进行散列处理（如MD5、SHA-1、SHA-2x）。 散列函数的特点如下（和D、E这样的签名/加密运算对比）： 返回值的长度较短和固定 (签名运算返回值的长度和输入值的长度成正比，输入值越长，返回值越长)。 不同的输入产生相同的输出的可能性极低 (签名运算不会有重复，因为其返回值是什么样的和它输入值是什么样的直接相关)。 单向运算，不能逆向变换，因此严格来说，散列函数不能算一个加密函数 (签名运算可逆运算，可以根据密文还原成原文)。 仅改动输入的一个比特，输出也会相差极大。 为什么散列函数可能出现重复值？ 原因很简单，散列函数的输入值可以是无限多的任意取值，而返回值是固定位数，例如MD5的返回值是 128位，产生的值的范围在 0~2^128-1，以无限多的输入值得到有限个数的返回值，那么肯定会发生2个不同的输入值对应同一个返回值的重复现象。 下面我们可以简单看看MD5加密的原理： 1、附加：把任意长的报文按模 2^64 计算其余数（余数的长度64位），追加在报文的后面。 2、 填充：在报文和余数之间填充 1～512 位，使得填充后的总长度是 512 的整数倍。填充的首位是 1，后面都是 0。 3、分组：把追加和填充后的报文分割为多个 512 位的数据块，每个 512 位的报文数据再分成 4 个 128 位的数据块。 4、计算：将 4 个 128 位的数据块依次送到不同的散列函数进行 4 轮计算。每一轮都按 32 位的小数据块进行复杂的位运算，而且一轮运算的结果会作为下一轮运算的参数。一直到最后计算出 MD5 结果（128 位）。 散列函数的开销相比于签名函数的开销可以忽略不计。 回到之前的问题，如何借助散列函数进行报文鉴别？ 1、A对X使用散列函数处理得到一个散列值 H(X)。 2、A对 H(x)用A的私钥经过D签名运算得到报文鉴别码MAC，这是一个经过签名的鉴别码。 3、将 报文X + MAC 拼接起来，经过B的公钥对整个扩展报文加密（图中没有画出来）并发送。 4、B用B的私钥对扩展报文解密，将MAC和X分开，用A的公钥对MAC逆运算得到 H(X0)。 5、B用 H()散列运算对报文X进行计算得到 H(X1) ,如果H(X1) == H(X2)，就既能证明报文没有被篡改，又能证明对方是A。 需要注意：第二步没有对报文进行加密（D运算），而是对很短的散列 H(X) 进行 D 运算，因为 H(X)很短，所以D(H(X))的开销和耗时很小，这样就解决了之前开销大的问题。 上图是非对称密钥的签名过程，下图是对称密钥的签名过程： 对称密钥的签名方式就不是使用 D 运算，而是直接 把对称密钥K 拼在报文 X 之后，直接进行散列运算。图中也是省略了加密这一步，其实也应该要加密。 TLS会话阶段就是用的对称密钥对报文签名的。 ","date":"2023-07-13","objectID":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E7%BD%91%E7%BB%9C%E5%AE%89%E5%85%A8/:3:2","tags":[],"title":"计算机网络之网络安全","uri":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E7%BD%91%E7%BB%9C%E5%AE%89%E5%85%A8/"},{"categories":["计算机网络"],"content":"四、实体鉴别 实体鉴别与报文鉴别不同。 报文鉴别：对每一个收到的报文都要鉴别报文的发送者和完整性。 实体鉴别：在通信开始前，对和自己通信的对方实体只需验证一次。 最简单的实体鉴别就是使用共享的对称密钥 K(AB)，因为这个密钥只有A和B才有，因此B如果能用K(AB)这个密钥解密成功，就知道对方肯定只能是A。 该方法存在明显漏洞：不能抵抗重放攻击。 重放攻击 ：原理是把以前窃听到的数据原封不动地重新发送给接收方。入侵者 C 捕获到报文后不需要破译报文，而是在之后攻击者想攻击的时候直接把由 A 加密的报文发送给 B，使 B 误认为 C 就是 A。B 就会向伪装成 A 的 C 发送许多本来应当发给 A 的报文。 回到上图，C不用拥有K(AB)这个密钥，C只需截获这个报文，之后想攻击的时候，就发送这个报文给B就让B以为C拥有K(AB)。从而让B以为C是A。 为了防御重放攻击，可以使用不重数（即不重复使用的大随机数）。 如下图所示： 1、A发送明文随机数Ra，并且Ra保存在A的内存中。 2、B用私钥SKb对Ra进行签名（是签名不是加密，这样A就知道对方是B）并加上自己的随机数Rb（Rb也会存在B的内存）。A会验证B发过来的签名了的随机数是不是A上次所发的Ra。 3、A发送用SKa签名的Rb，B就知道对方是A。B再验证Rb是否就是上次B发出去的Rb，如果验证成功就在内存中删除存储的Rb。 4、A和B开始通信。 如果C要使用重放攻击，C就要捕获第三次报文，这是因为C没有A的私钥，B就知道C不是A，而第三次报文是用 A 的私钥签名过的，因此C必须捕获第三次报文。下次重放这个报文B用A的公钥核实签名成功，就以为C是A。但即使如此，B核实完签名又发现里面的随机数Rb是自己不认识的数（因为Rb已经过期和失效），因此知道有内鬼，终止交易。 此外，还有一种更难防御的“中间人攻击”。即使使用不重数也无法防御： 1、C 冒充是 A，发送报文给 B，说：“我是 A”。 2、 B 选择一个不重数 RB，发送给 A，但被 C 截获了。 C 用自己的私钥 SKC 冒充是 A 的私钥，对 RB 签名，并发送给 B。此时B还无法核实签名，因为B没有C的公钥。 3、 B 向 A 发送报文，要求对方把解密（不应该是解密，应该是核实签名）用的公钥发送过来，但这报文也被 C 截获了。 C 把自己的公钥 PKC 冒充是 A 的公钥发送给 B。 B 用收到的公钥 PKC 对收到的加密的 RB 进行解密，其结果当然正确。 4、于是 B 相信通信的对方是 A，接着就向 A 发送许多敏感数据，但都被 C 截获了。 ","date":"2023-07-13","objectID":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E7%BD%91%E7%BB%9C%E5%AE%89%E5%85%A8/:4:0","tags":[],"title":"计算机网络之网络安全","uri":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E7%BD%91%E7%BB%9C%E5%AE%89%E5%85%A8/"},{"categories":["计算机网络"],"content":"五、公钥的分配 我们知道 发送者A 发送消息 给接收者B的时候，要用B的公钥对报文加密（在TLS传输中，对数据传输阶段的数据加密不使用不对称密钥，而是使用对称密钥，因此通信前需要传输对称密钥。为了安全传输对称密钥，需要用B的公钥对要传输的对称密钥进行加密），那么A怎么得到B的公钥呢？ 有一个权威第三方机构：认证中心 CA。它负责为拥有公钥的实体（人或者机器）提供数字证书，数字证书 = 实体的公钥 + 实体的信息（如主机名，实体的IP地址，实体人姓名等），数字证书（又称公钥证书）实际上是对公钥和对应实体的绑定。 这是一个数字证书的样子： 数字整数是公开的（因为里面放的是公钥，公钥本来就是公开的），无需加密，但是数字整数需要签名，CA用自己的私钥K(ca)对数字证书签名，目的是让大家知道这个证书是CA出品的，是正规的证书。 数字证书的制作过程如下： 1、CA对B的未签名证书散列运算得到 H(X) 2、CA用自己的私钥K(ca)对H(X)进行签名（D运算)。 3、把签名追加到明文证书的后面，得到签名后的证书。 核实：A 拿到 B 的数字证书后，使用数字证书上给出的 CA 的公钥，对数字证书中 CA 的数字签名进行 E 运算，得出一个数值。再对 B 的数字证书 (CA 数字签名除外的部分) 进行散列运算，又得出一个H(X)。比较这两个H(X)。若一致，则数字证书是真的。 ","date":"2023-07-13","objectID":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E7%BD%91%E7%BB%9C%E5%AE%89%E5%85%A8/:5:0","tags":[],"title":"计算机网络之网络安全","uri":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E7%BD%91%E7%BB%9C%E5%AE%89%E5%85%A8/"},{"categories":["计算机网络"],"content":"六、TLS 运输层安全协议 TLS协议是SSL协议（安全套接字层协议）的改进，现在我们所说的SSL是 SSL/TLS的统称，实际上用的是TLS协议，真正的SSL协议在多年前已经被废弃。 TLS 安全运输层位于应用层和运输层之间，作用是为应用层提供保密性、数据完整性和鉴别功能安全传输服务（安全会话）。TLS层具体要做的是：在发送方，TLS 接收应用层的数据，对数据进行加密，然后把加密后的数据送往 TCP 套接字。 不要搞混可靠传输服务和安全服务，前者是指数据传输的有序、不丢失和无差错，后者指保密性、数据完整性和鉴别通信双方。 应用层使用 TLS 最多的就是 HTTP（即HTTPS）。TLS 可用于任何应用层协议。TCP 的 HTTPS 端口号是 443，而不是平时使用的端口号 80。 TLS 具有双向鉴别的功能，但实际应用中只会用到单向鉴别：客户端（浏览器）需要鉴别服务器（即安全服务的第三点），具体来说客户端要对方证明他就是 http://www.baidu.com这个站点的服务。这就需要服务端提供CA证书，CA 证书是运输层安全协议 TLS 的基石。 TLS如何建立安全会话？ 建立安全会话两个阶段： 握手阶段：使用握手协议，这个阶段需要做三件事：协商加密算法、鉴别接收方的CA证书和生成用于加密的对称密钥。 会话阶段：使用记录协议。 ","date":"2023-07-13","objectID":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E7%BD%91%E7%BB%9C%E5%AE%89%E5%85%A8/:6:0","tags":[],"title":"计算机网络之网络安全","uri":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E7%BD%91%E7%BB%9C%E5%AE%89%E5%85%A8/"},{"categories":["计算机网络"],"content":"1、握手阶段 1、协商加密算法 浏览器 A 向服务器 B 发送浏览器的 TLS 版本号和一些可选的加密算法（是在TCP第三次握手的ACK报文段中顺带传输这些信息的），以及A的不重数Ra。 B 从中选定自己所支持的加密算法（如 RSA）和生成主密钥的算法，并告知 A，同时把自己的 CA 数字证书、B的不重数Rb 和 用B私钥加密（应该说是签名）的SKB(Ra) 发送给 A。 2、服务器鉴别。客户 A 用数字证书中 CA 的公钥对数字证书进行验证鉴别（校验过程请参考上文）。鉴别成功后，A就从证书中得到了B的公钥PKB。 3、客户 A生成了预主密钥PMS，并且根据预主密钥 PMS 、Ra 和 Rb生成主密钥MS。A再用 B 的公钥 PKB 对PMS 加密，得出加密的预主密钥 PKB(PMS)，发送给服务器 B。 4、服务器 B 用自己的私钥SKB把预主密钥PMS解密出来 。这样，客户 A 和服务器 B 都有了PMS，将预主密钥 PMS 、Ra 和 Rb通过双方已商定的密钥生成算法，生成为后面数据传输用的对称主密钥 MS。 5、这一步时图中没有画出来的，服务端B最后发送一个加密的\"Finished\"消息，表示握手阶段结束。A和B使用对称密钥MS进行加密传输，而不是用非对称密钥加密，这是因为用对称密钥加密的开销和时间远小于非对称加密。 6、为了使双方的通信更加安全，客户 A 和服务器 B 最好使用不同的密钥。主密钥被分割成 4 个不同的密钥。 每一方都拥有这样 4 个密钥（注意：这些都是对称密钥）： 客户 A 发送数据时使用的会话密钥 KA （用于A对数据加密） 客户 A 发送数据时使用的 MAC 密钥MA（用于A对数据签名） 服务器 B 发送数据时使用的会话密钥 KB （用于B对数据加密） 服务器 B 发送数据时使用的 MAC 密钥 MB （用于B对数据签名） 也就是说数据收发阶段，每个报文都要加密和签名这两步。 最后总结：使用了TLS安全传输的会比传统的TCP三次握手多出三次握手（对于客户端多出2个RTT时延， 对于服务端多出1个RTT）才能开始数据发送阶段。如下图绿色部分： TLS 的浏览器端和服务端都会保存之前的TLS会话参数（包括协商好的加密算法和主密钥等等），以及这些参数的会话ID（会话ID - 会话参数的映射）。下次浏览器再访问同一个服务器时，直接发送会话ID就能够通知服务端使用上一次的密钥和加密算法进行通信，节省了一个RTT时间和生成主密钥的时间： ","date":"2023-07-13","objectID":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E7%BD%91%E7%BB%9C%E5%AE%89%E5%85%A8/:6:1","tags":[],"title":"计算机网络之网络安全","uri":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E7%BD%91%E7%BB%9C%E5%AE%89%E5%85%A8/"},{"categories":["计算机网络"],"content":"2、会话阶段 即数据报文传输阶段。以A发送数据给B这一个传输方向为例： 1、把长的数据划分为较小的数据块（TCP把一个应用层消息分隔为多个报文段），叫做记录。 对每一个记录进行鉴别运算(用密钥MA)和加密运算（用密钥KA）。 2、记录协议对每一个记录按发送顺序赋予序号，第一个记录作为 0。发送下一个记录时序号就加 1，序号最大值不得超过 264 – 1，且不允许序号绕回。 序号未写在记录之中，而是在进行散列运算时，把序号包含进去。客户 A 向服务器 B 发送一个记录前，对 密钥 MA 、记录的当前序号和明文记录进行散列运算得到数字签名MAC，MAC在拼到明文记录后面，附有签名的数据记录就生成好了。 使用会话密钥 KA 对附有签名的数据记录进行加解密。 上面过程，在握手阶段使用了实体鉴别，在会话阶段使用了报文鉴别。 关闭 TLS 连接： 关闭 TLS 连接之前，A 或 B 应当先发送关闭 TLS 的记录，以防止截断攻击 。 截断攻击：在 A 和 B 正在进行会话时，入侵者突然发送 TCP 的 FIN 报文段来关闭 TCP 连接。 如果 A 或 B 没有发送一个要关闭 TLS 的记录的情况下收到了 TCP 的 FIN 报文段时，就知道这是入侵者的截断攻击了。因为入侵者无法伪造关闭 TLS 的记录。 ","date":"2023-07-13","objectID":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E7%BD%91%E7%BB%9C%E5%AE%89%E5%85%A8/:6:2","tags":[],"title":"计算机网络之网络安全","uri":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E7%BD%91%E7%BB%9C%E5%AE%89%E5%85%A8/"},{"categories":["计算机网络"],"content":"[toc] 应用层 ","date":"2023-07-13","objectID":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E5%BA%94%E7%94%A8%E5%B1%82/:0:0","tags":[],"title":"计算机网络之应用层","uri":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E5%BA%94%E7%94%A8%E5%B1%82/"},{"categories":["计算机网络"],"content":"应用层协议原理 ","date":"2023-07-13","objectID":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E5%BA%94%E7%94%A8%E5%B1%82/:1:0","tags":[],"title":"计算机网络之应用层","uri":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E5%BA%94%E7%94%A8%E5%B1%82/"},{"categories":["计算机网络"],"content":"应用程序架构 应用层程序的体系结构一般包括：P2P、CS这两种架构。 P2P：P2P（Peer-to-Peer）是一种分布式计算和通信模型，它允许计算机之间直接连接和通信，而不需要通过中央服务器进行中转。在P2P网络中，所有的计算机节点都可以充当客户端和服务器的角色，可以共享资源、提供服务和请求服务。 典型例子：p2p的下载等 CS：一直打开的一端被称为服务端（S），它会处理来自客户端请求；主动请求的一方就是客户端（C）。 典型CS架构的应用：web服务 ","date":"2023-07-13","objectID":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E5%BA%94%E7%94%A8%E5%B1%82/:1:1","tags":[],"title":"计算机网络之应用层","uri":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E5%BA%94%E7%94%A8%E5%B1%82/"},{"categories":["计算机网络"],"content":"进程通信与套接字 进程通信 应用层解决的是两端应用程序（进程）通信的问题。两端进程是通过套接字通信，为用户进程提供套接字也是应用层的主要功能。网络通信的用户进程是运行在应用层上的。 套接字 套接字是同一台主机内应用层与运输层之间的接口，也是提供给应用程序的可编程接口，应用程序可以通过套接字控制和使用应用层的一切功能，但无法通过套接字控制运输层。 ","date":"2023-07-13","objectID":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E5%BA%94%E7%94%A8%E5%B1%82/:1:2","tags":[],"title":"计算机网络之应用层","uri":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E5%BA%94%E7%94%A8%E5%B1%82/"},{"categories":["计算机网络"],"content":"进程寻址 通过IP地址和端口号寻找接收方进程（具体来说是寻找接收方进程对应的套接字）。 ","date":"2023-07-13","objectID":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E5%BA%94%E7%94%A8%E5%B1%82/:1:3","tags":[],"title":"计算机网络之应用层","uri":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E5%BA%94%E7%94%A8%E5%B1%82/"},{"categories":["计算机网络"],"content":"动态主机配置协议：DHCP DHCP 的全称是 Dynamic Host Configuration Protocol 动态主机配置协议。使用 DHCP 就能实现自动设置 IP 地址、统一管理 IP 地址分配。也就是不管你是在开会还是在工位干活，都省去了手动配置 IP 地址这一步繁琐的操作，同时 DHCP 也大大减少了可能由于你手动分配 IP 地址导致错误的几率。 DHCP 与 IP 密切相关，它是 IP 网络上所使用的协议。如果你想要使用 DHCP 提供服务的话，那么在整条通信链路上就需要 DHCP 服务器的存在，连接到网络的设备使用 DHCP 协议从 DHCP 服务器请求 IP 地址。DHCP 服务器会为设备分配一个唯一的 IP 地址。 除了 IP 地址外，DHCP 服务器还会把子网掩码，默认路由，DNS 服务器告诉你。 ","date":"2023-07-13","objectID":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E5%BA%94%E7%94%A8%E5%B1%82/:2:0","tags":[],"title":"计算机网络之应用层","uri":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E5%BA%94%E7%94%A8%E5%B1%82/"},{"categories":["计算机网络"],"content":"DHCP 服务器 现在，你不需要手动配置 IP 地址，也不再需要管理 IP 地址了，管理权已经移交给了 DHCP 服务器，DHCP 服务器会维护 IP 地址池，在网络上启动时会将地址租借给启用 DHCP 的客户端。 由于 IP 地址是动态的(临时分配)**而不是**静态的(永久分配)，因此不再使用的 IP 地址会自动返回 IP 地址池中进行重新分配。 那么 DHCP 服务器由谁维护呢？ 网络管理员负责建立 DHCP 服务器，并以租约的形式向启用 DHCP 的客户端提供地址配置，啊，既然不需要我管理，那就很舒服了～ 好了，现在你能舒舒服服的开发了，你用 postman 配了一条 192.168.1.4/x/x 的接口进行请求，请求能够顺利进行，但是过了一段时间后，你发现 192.168.1.4/x/x 这个接口请求不通了，这是为啥呢？然后你用 ipconfig 查询了一下自己的 IP 地址，发现 IP 地址变成了 192.168.1.7，怎么我用着用着 IP 地址还改了？DHCP 是个垃圾，破玩意！！@#¥%¥%……¥% 其实，这也是一个 DHCP 服务器的一个功能，DHCP 服务器通常为每个客户端分配一个唯一的动态 IP 地址，当该 IP 地址的客户端租约到期时，该地址就会更改。 唯一意思说的就是，如果你手动设置了一个静态 IP，同时 DHCP 服务器分配了一个动态 IP，这个动态 IP 和静态 IP 一样，那么必然会有一个客户端无法上网。 我就遇到过这种情况，我使用虚拟机配置的静态 IP 是192.168.1.8，手机使用 DHCP 也同样配置了 192.168.1.8 的 IP 地址，此时我的虚拟机还没有接入网络，当我接入网络时，我怎样也连不上虚拟机了，一查才发现 IP 地址冲突了 … 虽然 DHCP 服务器能提供 IP 地址，但是他怎么知道哪些 IP 地址空闲，哪些 IP 地址正在使用呢？ 实际上，这些信息都配置在了数据库中，下面我们就来一起看一下 DHCP 服务器维护了哪些信息。 网络上所有有效的 TCP/IP 配置参数 这些参数主要包括主机名（Host name）、DHCP 客户端（DHCP client）、域名（Domain name）、IP 地址IP address）、网关（Netmask）、广播地址（Broadcast address）、默认路由（default rooter）。 有效的 IP 地址和排除的 IP 地址，保存在 IP 地址池中等待分配给客户端 为某些特定的 DHCP 客户端保留的地址，这些地址是静态 IP，这样可以将单个 IP 地址一致地分配给单个DHCP 客户端 好了，现在你知道 DHCP 服务器都需要保存哪些信息了，并且看过上面的内容，你应该知道一个 DHCP 的组件有哪些了，下面我们就来聊一聊 DHCP 中都有哪些组件，这些组件缺一不可。 ","date":"2023-07-13","objectID":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E5%BA%94%E7%94%A8%E5%B1%82/:2:1","tags":[],"title":"计算机网络之应用层","uri":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E5%BA%94%E7%94%A8%E5%B1%82/"},{"categories":["计算机网络"],"content":"DHCP 的组件 使用 DHCP 时，了解所有的组件很重要，下面我为你列出了一些 DHCP 的组件和它们的作用都是什么。 DHCP Server，DHCP 服务器，这个大家肯定都知道，因为我们上面就一直在探讨 DHCP 服务器的内容，使用 DHCP ，是一定要有 DHCP 服务器的，要不然谁给你提供服务呢？ DHCP Client，DHCP 客户端，这个大家应该也知道，毕竟只有一个服务端不行啊，没有客户端你为谁服务啊？DHCP 的客户端可以是计算机、移动设备或者其他需要连接到网络的任何设备，默认情况下，大多数配置为接收 DHCP 信息。 Ip address pool: 你得有 IP 地址池啊，虽然说你 DHCP 提供服务，但是你也得有工具啊，没有工具玩儿啥？IP 地址池是 DHCP 客户端可用的地址范围，这个地址范围通常由最低 -\u003e 最高顺序发送。 Subnet：这个组件是子网，IP 网络可以划分一段一段的子网，子网更有助于网络管理。 Lease：租期，这个表示的就是 IP 地址续约的期限，同时也代表了客户端保留 IP 地址信息的时间长度，一般租约到期时，客户端必须续约。 DHCP relay：DHCP 中继器，这个一般比较难想到，DHCP 中继器一般是路由器或者主机。DHCP 中继器通常应对 DHCP 服务器和 DHCP 客户端不再同一个网断的情况，如果 DHCP 服务器和 DHCP 客户端在同一个网段下，那么客户端可以正确的获得动态分配的 IP 地址；如果不在的话，就需要使用 DHCP 中继器进行中继代理。 现在 DHCP 的组件你了解后，下面我就要和你聊聊 DHCP 的工作机制了。 ","date":"2023-07-13","objectID":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E5%BA%94%E7%94%A8%E5%B1%82/:2:2","tags":[],"title":"计算机网络之应用层","uri":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E5%BA%94%E7%94%A8%E5%B1%82/"},{"categories":["计算机网络"],"content":"DHCP 工作机制 在聊 DHCP 工作机制前，先来看一下 DHCP 的报文消息 ","date":"2023-07-13","objectID":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E5%BA%94%E7%94%A8%E5%B1%82/:2:3","tags":[],"title":"计算机网络之应用层","uri":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E5%BA%94%E7%94%A8%E5%B1%82/"},{"categories":["计算机网络"],"content":"DHCP 报文 DHCP 报文共有一下几种： DHCP DISCOVER ：客户端开始 DHCP 过程发送的包，是 DHCP 协议的开始 DHCP OFFER ：服务器接收到 DHCPDISCOVER 之后做出的响应，它包括了给予客户端的 IP 租约过期时间、服务器的识别符以及其他信息 DHCP REQUEST ：客户端对于服务器发出的 DHCPOFFER 所做出的响应。在续约租期的时候同样会使用。 DHCP ACK ：服务器在接收到客户端发来的 DHCPREQUEST 之后发出的成功确认的报文。在建立连接的时候，客户端在接收到这个报文之后才会确认分配给它的 IP 和其他信息可以被允许使用。 DHCP NAK ：DHCPACK 的相反的报文，表示服务器拒绝了客户端的请求。 DHCP RELEASE ：一般出现在客户端关机、下线等状况。这个报文将会使 DHCP 服务器释放发出此报文的客户端的 IP 地址 DHCP INFORM ：客户端发出的向服务器请求一些信息的报文 DHCP DECLINE :当客户端发现服务器分配的 IP 地址无法使用（如 IP 地址冲突时），将发出此报文，通知服务器禁止使用该 IP 地址。 DHCP 的工作机制比较简单，无非就是客户端向服务器租借 IP ，服务器提供 IP 给客户端的这个过程呗。嗯，你很聪明，大致是这样的，不过有一些细节需要注意下，下面我通过两张图来和你聊一下。 关于从 DHCP 中获取 IP 地址的流程，主要分为两个阶段。 第一个阶段是 DHCP 查找包的阶段 查找包的阶段主要分为两步：第一步是 DHCP 发现包，第二步是 DHCP 提供包。 DHCP 客户端在通信链路上发起广播，看看链路上有没有能提供 DHCP 包的服务器，然后通信链路上的各个节点会检查自己是否能够提供 DHCP 包，这时 DHCP 服务器说它能够提供 DHCP 包，然后 DHCP 就发出一个 DHCP 包沿着通信链路返回给 DHCP 客户端。 第二个阶段是 DHCP 的请求阶段。 DHCP 的请求包也分为两步：第一步是 DHCP 请求包，第二步是 DHCP 确认包。 DHCP 客户端在通信链路上发起 DHCP 请求包，请求包主要是告诉 DHCP 服务器，它想要用上一步提供的网络设置，然后 DHCP 服务器向 DHCP 客户端发送确认包，表示允许 DHCP 客户端使用第二步发送的网络设置。 至此，DHCP 的网络设置就结束了，然后通信链路上的主机之间就可以进行 TCP/IP 通信了。 当不需要 IP 地址时，可以发送 DHCP 解除包(DHCP RELEASE)进行解除。另外，DHCP 的设置中通常会有一个租期时间的设定，DHCP 客户端在这个时限内可以发送 DHCP 请求包通知想要延长这个期限。 ","date":"2023-07-13","objectID":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E5%BA%94%E7%94%A8%E5%B1%82/:2:4","tags":[],"title":"计算机网络之应用层","uri":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E5%BA%94%E7%94%A8%E5%B1%82/"},{"categories":["计算机网络"],"content":"超文本传输协议：HTTP ","date":"2023-07-13","objectID":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E5%BA%94%E7%94%A8%E5%B1%82/:3:0","tags":[],"title":"计算机网络之应用层","uri":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E5%BA%94%E7%94%A8%E5%B1%82/"},{"categories":["计算机网络"],"content":"HTTP概况 http概况 HTTP是一个在计算机的世界里专门在两点之间传输超文本数据的约定和规范。 ","date":"2023-07-13","objectID":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E5%BA%94%E7%94%A8%E5%B1%82/:3:1","tags":[],"title":"计算机网络之应用层","uri":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E5%BA%94%E7%94%A8%E5%B1%82/"},{"categories":["计算机网络"],"content":"非持续连接与持续连接 RTT RTT(Round-Trip Time) 往返时间指的是一个段分组从客户到服务器然后再返回客户所花费的时间。我们尝试用RTT来表示客户点击超链接后页面出现的时间，如下图所示： 粗略地讲，总的响应时间就是两个RTT 再加上服务器传输HTML文件的时间 非持续连接 每个请求/响应都是一个单独的TCP连接，每次请求完毕，都会释放连接。 非持续连接的缺点： 必须为每个请求的对象都创建对象，建立和维护一个全新的连接。这对web服务端来说，会引起内存开销大，耗损性能的问题 每个请求的对象经受两倍的RTT交付时延，累积起来后，成本依旧不菲. 持续连接 每个请求/响应都是经过相同的TCP连接发送的，复用了这个TCP连接。 ","date":"2023-07-13","objectID":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E5%BA%94%E7%94%A8%E5%B1%82/:3:2","tags":[],"title":"计算机网络之应用层","uri":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E5%BA%94%E7%94%A8%E5%B1%82/"},{"categories":["计算机网络"],"content":"HTTP性能 HTTP事务延时 一个HTTP事务是指，一个HTTP请求从发出到响应到达发送端的过程，如下所示： 上图显示一个HTTP事务的时延主要包括4部分： 1、DNS查询：如果从本地DNS缓存就能得到目标IP地址，则耗时0RTT；如果从本地DNS服务器就能得到目标IP地址，则耗时1个RTT；如果涉及到DNS的递归和迭代请求，则耗时多个RTT。 2、TCP连接的三次握手，占 1 个RTT，如果使用TLS安全连接则占3个RTT。 3、请求到达服务器和响应返回的时间，占1个RTT。 4、服务器应用程序处理请求的时间，假设请求的是一个静态文件，则处理时间可以忽略不记。 TCP相关的时延 HTTP协议是基于TCP协议的，因此一个HTTP事务的性能很大程度取决于TCP通道的性能，下面是一些TCP连接相关的可能会拖慢HTTP的地方： 1、TCP连接的3次握手时延 花费 1个 RTT。越小的HTTP事务在TCP连接下的效率越低。 2、捎带机制 接收方在接收到一个序号的分组后不会马上返回ACK报文，而是允许接收方在有数据发往发送方的分组中“捎带”这个ACK消息（ACK标志位和ACK号），这是考虑到ACK确认报文很小，单独发一个报文会比较浪费（TCP头部就占了40~60字节）。 这也会给HTTP响应带来时延。 3、TCP慢启动 TCP慢启动使 刚打开的连接 传输分组的速度很慢。 4、Nagle算法 和 TCP_NODELAY Nagle算法是为了防止一次分组发送的实际数据量太少，造成网络效率降低，所以要求缓冲区需要到达MSS及以上大小的数据时才发送分组（或者发送计时器到时）。如果缓冲区里的数据不足一个MSS大小，那么要等到在前一个分组被确认之后或者发送计时器超时才能发下一个分组。 Naggle会引发几种HTTP性能问题：小的HTTP报文无法填满一个MSS，所以只能等到计时器到达才能发送；延迟确认会阻止Naggle发送下一个分组（延迟确认和Naggle共同作用）。 HTTP应用程序会设置参数TCP_NODELAY禁用Naggle算法。当然，TCP应用Nagle算法时，要保证应用程序每次向缓冲区输送的数据块得是较大的数据块（如512B）而不能时逐字节发送。 5、TIME_WAIT 累积和端口耗尽 主动关闭连接的一方（不论是服务器还是客户端）进入TIME_WAIT后，主动关闭方会在内存中记录最近所关闭的对端IP地址和端口，记录有效期维持2MSL，2MSL内连接并不会真正关闭。在这段期间内，该端不会与对端建立相同IP和端口的连接。 假如A和B以很快的速率创建和关闭连接（也就是在2MSL时间内，端口还是被占用了），B就没有端口可以和A的80服务建立连接了，这就是端口耗尽（是客户端B的端口耗尽），当然这种情况很少出现，只会出现在性能测试的时候。 对于服务端，服务端如果有大量的TIME_WAIT未关闭连接也会使操作系统的速度严重减慢（如果使用多线程则一个连接可能就是一个线程）和消耗大量内存（毕竟一个连接是一个套接字）。 TCP连接过多意味着什么？ 1、大量的套接字会占用客户端、服务器以及代理的内存（输入输出缓冲区要预先分配的）和CPU； 2、并行 TCP 连接接收和发送数据时竞争共享的带宽； 3、应用的并行能力也受限制（管理一个套接字可能就要用一个线程）。 HTTP优化建议 从大方向上，HTTP优化只会围绕2点进行：减少 网络延迟 和 减少要传输的字节。具体可以通过以下几点进行优化： 1、减少DNS查询次数 尽量命中本地DNS缓存和加长DNS缓存记录的过期时间。 2、减少HTTP请求 任何请求都不如没有请求更快，因此要去掉页面上没有必要的资源，多次请求尽可能合并为1次请求。 3、使用CDN 从地理上把数据放到接近客户端的地方，可以显著减少每次请求的传播时延。 4、添加Expires首部并配置ETag标签 相关资源应该缓存，以避免重复请求每个页面中相同的资源。Expires 首部可用 于指定缓存时间，在这个时间内可以直接从缓存取得资源，完全避免 HTTP 请求。ETag 及 Last-Modified 首部提供了一个与缓存相关的机制，相当于最后一次 更新的指纹或时间戳。 5、Gzip资源 所有文本资源都应该使用 Gzip 压缩，再传输。 6、避免HTTP重定向 HTTP 重定向极其耗时，特别是把客户端定向到一个完全不同的域名的情况下， 还会导致额外的 DNS 查询、TCP 连接延迟，等等。 ","date":"2023-07-13","objectID":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E5%BA%94%E7%94%A8%E5%B1%82/:3:3","tags":[],"title":"计算机网络之应用层","uri":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E5%BA%94%E7%94%A8%E5%B1%82/"},{"categories":["计算机网络"],"content":"web缓存 web缓存器也叫代理服务器，主要用于缓存本地对于远端服务器响应的内容，减少对远端服务器的请求。 ","date":"2023-07-13","objectID":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E5%BA%94%E7%94%A8%E5%B1%82/:3:4","tags":[],"title":"计算机网络之应用层","uri":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E5%BA%94%E7%94%A8%E5%B1%82/"},{"categories":["计算机网络"],"content":"HTTP协议演进 HTTP1.0 特点 无状态：服务器不跟踪不记录请求过的状态 无连接：浏览器每次请求都需要建立TCP连接 HTTP/1.0规定浏览器和服务器保持短连接。浏览器的每次请求都需要与服务器建立一个TCP连接，服务器处理完成后立即断开TCP连接（无连接），服务器不跟踪每个客户端也不记录过去的请求（无状态） 无状态导致的问题可以借助cookie/session机制来做身份认证和状态记录解决。 然而，无连接特性将会导致以下性能缺陷： 无法复用连接 每次发送请求的时候，都需要进行一次TCP连接，而TCP的连接释放过程又是比较费事的。这种无连接的特性会导致网络的利用率非常低。 队头堵塞(head of line blocking) 由于HTTP/1.0规定下一个请求必须在前一个请求响应到达之前才能发送。假设一个请求响应一直不到达，那么下一个请求就不发送，就到导致阻塞后面的请求。 为了解决这些问题，HTTP/1.1出现了。 HTTP1.1 长连接 HTTP/1.1增加了一个Connection字段，通过设置Keep-alive（默认已设置）可以保持连接不断开，避免了每次客户端与服务器请求都要重复建立释放TCP连接，提高了网络的利用率。如果客户端想关闭HTTP连接，可以在请求头中携带Connection:false来告知服务器关闭请求 支持断点续传 通过使用请求头中的 Range 来实现。 支持请求管道化（pipelining） 基于HTTP/1.1的长连接，使得请求管线化成为可能。多个请求可以同时发送，但是服务器还是按照顺序，先回应 A 请求，完成后再回应 B 请求。要是 前面的回应特别慢，后面就会有许多请求排队等着。这称为「队头堵塞」。 HTTP2 http2.0是一种安全高效的下一代http传输协议。安全是因为http2.0建立在https协议的基础上，高效是因为它是通过二进制分帧来进行数据传输。正因为这些特性，http2.0协议也在被越来越多的网站支持。 特点 对1.x协议语意的完全兼容 2.0协议是在1.x基础上的升级而不是重写，1.x协议的方法，状态及api在2.0协议里是一样的。 建立在HTTPS上 性能的大幅提升 2.0协议重点是对终端用户的感知延迟、网络及服务器资源的使用等性能的优化。 二进制分帧 http2.0之所以能够突破http1.X标准的性能限制，改进传输性能，实现低延迟和高吞吐量，就是因为其新增了二进制分帧层。 帧(frame)包含部分：类型Type, 长度Length, 标记Flags, 流标识Stream和frame payload有效载荷。 消息(message)：一个完整的请求或者响应，比如请求、响应等，由一个或多个 Frame 组成。 流是连接中的一个虚拟信道，可以承载双向消息传输。每个流有唯一整数标识符。为了防止两端流ID冲突，客户端发起的流具有奇数ID，服务器端发起的流具有偶数ID。 流标识是描述二进制frame的格式，使得每个frame能够基于http2发送，与流标识联系的是一个流，每个流是一个逻辑联系，一个独立的双向的frame存在于客户端和服务器端之间的http2连接中。一个http2连接上可包含多个并发打开的流，这个并发流的数量能够由客户端设置。 在二进制分帧层上，http2.0会将所有传输信息分割为更小的消息和帧，并对它们采用二进制格式的编码将其封装，新增的二进制分帧层同时也能够保证http的各种动词，方法，首部都不受影响，兼容上一代http标准。其中，http1.X中的首部信息header封装到Headers帧中，而request body将被封装到Data帧中。 多路复用/连接共享 而http2.0中的多路复用优化了这一性能。多路复用允许同时通过单一的http/2 连接发起多重的请求-响应消息。有了新的分帧机制后，http/2 不再依赖多个TCP连接去实现多流并行了。每个数据流都拆分成很多互不依赖的帧，而这些帧可以交错（乱序发送），还可以分优先级，最后再在另一端把它们重新组合起来。 http 2.0 连接都是持久化的，而且客户端与服务器之间也只需要一个连接（每个域名一个连接）即可。http2连接可以承载数十或数百个流的复用，多路复用意味着来自很多流的数据包能够混合在一起通过同样连接传输。当到达终点时，再根据不同帧首部的流标识符重新连接将不同的数据流进行组装。 上图展示了一个连接上的多个传输数据流：客户端向服务端传输数据帧stream5，同时服务端向客户端乱序发送stream1和stream3。这次连接上有三个响应请求乱序并行交换。 上图就是http1.X和http2.0在传输数据时的区别。以货物运输为例再现http1.1与http2.0的场景： http1.1过程：货轮1从A地到B地去取货物，取到货物后，从B地返回，然后货轮2在A返回并卸下货物后才开始再从A地出发取货返回，如此有序往返。 http2.0过程：货轮1、2、3、4、5从A地无序全部出发，取货后返回，然后根据货轮号牌卸载对应货物。 显然，第二种方式运输货物多，河道的利用率高。 头部压缩 缓存与压缩头部 http1.x的头带有大量信息，而且每次都要重复发送。http/2使用encoder来减少需要传输的header大小，通讯双方各自缓存一份头部字段表，既避免了重复header的传输，又减小了需要传输的大小。 当然，HTTP1.x的GIZP压缩是对body的压缩，与http2的头部压缩不冲突。 请求优先级 http2采用二进制分帧，所以可以允许帧乱序传输，会在客户端重新组装成正确的序列。因此，我们可以在建立好的HTTP2连接里，可以为优先级高的请求优先发送数据帧。 例如： ●优先级最高：主要的html ●优先级高：CSS文件 ●优先级中：js文件 ●优先级低：图片 服务器端推送 服务器可以对一个客户端请求发送多个响应，服务器向客户端推送资源无需客户端明确地请求。并且，服务端推送能把客户端所需要的资源伴随着index.html一起发送到客户端，省去了客户端重复请求的步骤。 HTTP2的性能瓶颈 启用http2.0后会给性能带来很大的提升，但同时也会带来新的性能瓶颈。因为现在所有的压力集中在底层一个TCP连接之上，TCP很可能就是下一个性能瓶颈，比如TCP分组的队首阻塞问题，单个TCP packet丢失导致整个连接阻塞，无法逃避，此时所有消息都会受到影响。 HTTP/1.x keep-alive 与 HTTP/2 多路复用区别 HTTP/1.x 是基于文本的，只能整体去传；HTTP/2 是基于二进制流的，可以分解为独立的帧，交错发送 HTTP/1.x keep-alive 必须按照请求发送的顺序返回响应；HTTP/2 多路复用不按序响应 HTTP/1.x keep-alive 为了解决队头阻塞，将同一个页面的资源分散到不同域名下，开启了多个 TCP 连接；HTTP/2 同域名下所有通信都在单个连接上完成 HTTP/1.x keep-alive 单个 TCP 连接在同一时刻只能处理一个请求（两个请求的生命周期不能重叠）；HTTP/2 单个 TCP 同一时刻可以发送多个请求和响应 HTTP3 基于Google的QUIC，HTTP3 背后的主要思想是放弃 TCP，转而使用基于 UDP 的 QUIC 协议。 与 HTTP2 在技术上允许未加密的通信不同，QUIC 严格要求加密后才能建立连接。此外，加密不仅适用于 HTTP 负载，还适用于流经连接的所有数据，从而避免了一大堆安全问题。建立持久连接、协商加密协议，甚至发送第一批数据都被合并到 QUIC 中的单个请求/响应周期中，从而大大减少了连接等待时间。如果客户端具有本地缓存的密码参数，则可以通过简化的握手（0-RTT）重新建立与已知主机的连接。 为了解决传输级别的线头阻塞问题，通过 QUIC 连接传输的数据被分为一些流。流是持久性 QUIC 连接中短暂、独立的“子连接”。每个流都处理自己的错误纠正和传递保证，但使用连接全局压缩和加密属性。每个客户端发起的 HTTP 请求都在单独的流上运行，因此丢失数据包不会影响其他流/请求的数据传输。 特点 基于google的QUIC协议，而quic协议是使用udp实现的； 减少了tcp三次握手时间，以及tls握手时间； 解决了http 2.0中前一个stream丢包导致后一个stream被阻塞的问题； 优化了重传策略，重传包和原包的编号不同，降低后续重传计算的消耗； 连接迁移，不再用tcp四元组确定一个连接，而是用一个64位随机数来确定这个连接； 更合适的流量控制。 ","date":"2023-07-13","objectID":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E5%BA%94%E7%94%A8%E5%B1%82/:3:5","tags":[],"title":"计算机网络之应用层","uri":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E5%BA%94%E7%94%A8%E5%B1%82/"},{"categories":["计算机网络"],"content":"简单邮件传输协议：SMTP 下图给出了电子邮件系统的总体情况，我们可以看到它有3个主要的组成部分：用户代理、邮件服务器、SMTP 用户代理，比如说：Foxmail，Outlook，Apple Mail 这类的软件 邮件服务器：qq邮箱，163邮箱，Gmail之类的邮件服务 SMTP则是简单邮件传输协议。 SMTP是因特网电子邮件的核心。它使用TCP可靠数据传输服务。每台邮件服务器上既运行着SMTP的客户端也运行着SMTP的服务器端。当一个邮件服务器向其他邮件服务器发送邮件时，他就是SMTP的客户；当邮件服务器从其他邮件服务器上接收邮件时，他就表现为一个SMTP的服务器。 假设Alice 想给 Bob发送一份简单的报文。 Alice调用它的邮件代理程序并提供Bob的邮件地址，撰写报文，然后指示用户代理发送该报文 Alice的用户代理把报文发给她的邮件服务器，在那里，该报文被放在报文队列中 运行在Alice的邮件服务器上的SMTP客户端发现了报文队列中的这个报文，他就创建一个到运行在Bob的邮件服务器上的SMTP服务器的TCP连接 在经过一些初始的SMTP握手后，SMTP客户通过该TCP连接发送Alice的报文。 在Bob的邮件服务器上，SMTP的服务器端接收该报文。Bob的邮件服务器然后将该报文放入Bob的邮箱中。 在Bob有时间的时候，它调用用户代理阅读该报文。 注意：SMTP 一般不适用中间邮件服务器发送邮件，如果Bob的邮件服务器没有开机，该报文会保留在ALice的邮件服务器上并等待新的尝试。 现在你知道了两台邮件服务器邮件发送的大体过程，那么，SMTP 是如何将邮件从 Alice 邮件服务器发送到 Bob 的邮件服务器的呢？ 主要分为下面三个阶段： 建立连接：在这一阶段，SMTP 客户请求与服务器的25端口建立一个 TCP 连接。一旦连接建立，SMTP 服务器和客户就开始相互通告自己的域名，同时确认对方的域名。 邮件传送：一旦连接建立后，就开始邮件传输。SMTP 依靠 TCP 能够将邮件准确无误地传输到接收方的邮件服务器中。SMTP 客户将邮件的源地址、目的地址和邮件的具体内容传递给 SMTP 服务器，SMTP 服务器进行相应的响应并接收邮件。 连接释放：SMTP 客户发出退出命令，服务器在处理命令后进行响应，随后关闭 TCP 连接。 ","date":"2023-07-13","objectID":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E5%BA%94%E7%94%A8%E5%B1%82/:4:0","tags":[],"title":"计算机网络之应用层","uri":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E5%BA%94%E7%94%A8%E5%B1%82/"},{"categories":["计算机网络"],"content":"HTTP vs SMTP 相同点 HTTP 是从Web服务器向Web客户传送文件；SMTP事一个邮件服务器向另一个邮件服务器传送文件 当进行文件传送时，持续的HTTP和SMTP都采用持续连接 不同点 HTTP主要是 拉协议(pull protocol)，即用户使用HTTP从该服务器拉取信息;而SMTP基本上是一个推协议(push protocol)，及发送邮件服务器把文件推向接收邮件服务器 SMTP要求每个报文采用 7 比特的ASCII 码格式。HTTP数据则不受这些限制 HTTP把每个对象分装到它自己的HTTP响应报文中去，而SMTP则把所有报文对象放在一个报文之中。 ","date":"2023-07-13","objectID":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E5%BA%94%E7%94%A8%E5%B1%82/:4:1","tags":[],"title":"计算机网络之应用层","uri":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E5%BA%94%E7%94%A8%E5%B1%82/"},{"categories":["计算机网络"],"content":"因特网邮件访问协议 Bob的用户代理不能使用SMTP得到报文，因为获取报文是一个拉操作，而SMTP协议是一个推协议。这就需要引入一个特殊的邮件访问协议来解决，该协议将Bob邮件服务器上的报文传送给他本地的PC。目前有一些流行的访问协议：POP3、IMAP以及HTTP POP3 POP3 是一个极为简单的邮件访问协议。POP3按照三个阶段进行工作： Authorization(特许)，用户代理发送用户名和口令以鉴别用户 事务处理阶段，用户代理取回报文。同时还能进行如下操作：对报文做删除标记，取消报文删除标记，获取邮件统计信息 更新阶段，出现在客户发出了quit命令之后，目的是结束该POP3会话。这时，该邮件服务器会删除那些被标记为删除的报文 在POP3的事务处理过程中，用户代理的回答可能有OK(正常)和-ERR(出现差错) 在事务处理阶段，POP3的用户代理通常被配置为”下载并删除”或者“下载并保留”方式。 下载并删除 这种方式存在的问题是，邮件接收方Bob可能是移动的，可能希望从多个不同的机器访问他的邮件报文，如从办公室的PC和笔记本来访问邮件。那么，如果Bob先在办公室的PC上收取了一条邮件，那么晚上当他在家里时，他便不能再通过笔记本收取该邮件 下载并保留 用户代理下载某邮件之后该邮件仍然保留在邮件服务器上。这样Bob就能通过不同的及其重新读取这些邮件 IMAP IMAP 是另一个邮件访问协议，它比 POP3具有更多的特色，不过也比POP3复杂得多。 POP3 会对移动用户带来问题。IMAP更喜欢使用一个在远程服务器上的层次文件夹，这样他可以从任何一台机器上对所有报文进行访问，但是POP3协议并没有给用户提供任何创建远程文件夹并为报文指派文件夹的方法。 IMAP服务器把每个报文与一个文件夹联系起来，当报文第一次到达服务器时，它与收件人的INBOX文件夹相关联。收件人作为能够把邮件移到一个新的用户创建的文件夹中来阅读邮件、删除邮件等。 IMAP协议为用户提供了创建文件夹以及将邮件从一个文件夹移动到另一个文件夹的命令 IMAP 还为用户提供了在远程文件夹中查询邮件的命令 IMAP 服务器维护了IMAP会话的用户状态信息 IMAP 具有允许用户代理获取报文某些部分的命令。例如，一个用户代理可以只读取一个报文的首部。 基于Web的电子邮件 比如网页端的QQ邮箱，网易邮箱。使用这种服务，用户代理就是普通的浏览器，用户和他远程邮箱之间的通信通过HTTP进行： 收件人从邮箱中访问一个报文时，该电子邮件报文从Bob的邮件服务器发送到他的浏览器，使用的是HTTP而不是POP3或者IMAP协议 当发件人要发送一封电子邮件报文时，该电子邮件豹纹从浏览器发送到邮件服务器使用的是HTTP报文而不是SMTP 但是，邮件服务器之间发送和接收邮件时，仍然使用SMTP ","date":"2023-07-13","objectID":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E5%BA%94%E7%94%A8%E5%B1%82/:4:2","tags":[],"title":"计算机网络之应用层","uri":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E5%BA%94%E7%94%A8%E5%B1%82/"},{"categories":["计算机网络"],"content":"文件传输协议：FTP ","date":"2023-07-13","objectID":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E5%BA%94%E7%94%A8%E5%B1%82/:5:0","tags":[],"title":"计算机网络之应用层","uri":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E5%BA%94%E7%94%A8%E5%B1%82/"},{"categories":["计算机网络"],"content":"FTP 概述 文件传送协议 FTP (File Transfer Protocol) 是因特网上使用得最广泛的文件传送协议。 FTP 提供交互式的访问，允许客户指明文件的类型与格式，并允许文件具有存取权限。 FTP 屏蔽了各计算机系统的细节，因而适合于在异构网络中任意计算机之间传送文件。 ","date":"2023-07-13","objectID":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E5%BA%94%E7%94%A8%E5%B1%82/:5:1","tags":[],"title":"计算机网络之应用层","uri":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E5%BA%94%E7%94%A8%E5%B1%82/"},{"categories":["计算机网络"],"content":"FTP 的基本工作原理 FTP 使用 TCP 进行连接，它需要两个连接来传送一个文件： 控制连接：服务器打开端口号 21 等待客户端的连接，客户端主动建立连接后，使用这个连接将客户端的命令传送给服务器，并传回服务器的应答。 数据连接：用来传送一个文件数据。 根据数据连接是否是服务器端主动建立，FTP 有主动和被动两种模式： 主动模式：服务器端主动建立数据连接，其中服务器端的端口号为 20，客户端的端口号随机，但是必须大于 1024，因为 0~1023 是熟知端口号。 被动模式：客户端主动建立数据连接，其中客户端的端口号由客户端自己指定，服务器端的端口号随机。 主动模式要求客户端开放端口号给服务器端，需要去配置客户端的防火墙。被动模式只需要服务器端开放端口号即可，无需客户端配置防火墙。但是被动模式会导致服务器端的安全性减弱，因为开放了过多的端口号。 当NAT(Network Address Translation)设备以主动模式访问FTP服务器时，由于NAT设备不会聪明的变更FTP包中的IP地址，从而导致无法访问服务器。 ","date":"2023-07-13","objectID":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E5%BA%94%E7%94%A8%E5%B1%82/:5:2","tags":[],"title":"计算机网络之应用层","uri":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E5%BA%94%E7%94%A8%E5%B1%82/"},{"categories":["计算机网络"],"content":"简单文件传送协议 TFTP TFTP 是一个很小且易于实现的文件传送协议。 TFTP 使用客户服务器方式和使用 UDP 数据报，因此 TFTP 需要有自己的差错改正措施。 TFTP 只支持文件传输而不支持交互。 TFTP 没有一个庞大的命令集，没有列目录的功能，也不能对用户进行身份鉴别。 TFTP 的主要特点是 (1) 每次传送的数据 PDU 中有 512 字节的数据，但最后一次可不足 512 字节。 (2) 数据 PDU 也称为文件块(block)，每个块按序编号，从 1 开始。 (3) 支持 ASCII 码或二进制传送。 (4) 可对文件进行读或写。 (5) 使用很简单的首部。 ","date":"2023-07-13","objectID":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E5%BA%94%E7%94%A8%E5%B1%82/:5:3","tags":[],"title":"计算机网络之应用层","uri":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E5%BA%94%E7%94%A8%E5%B1%82/"},{"categories":["计算机网络"],"content":"远程终端协议 TELNET ","date":"2023-07-13","objectID":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E5%BA%94%E7%94%A8%E5%B1%82/:6:0","tags":[],"title":"计算机网络之应用层","uri":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E5%BA%94%E7%94%A8%E5%B1%82/"},{"categories":["计算机网络"],"content":"简述 TELNET 是一个简单的远程终端协议，也是因特网的正式标准。 用户用 TELNET 就可在其所在地通过 TCP 连接注册（即登录）到远地的另一个主机上（使用主机名或 IP 地址）。 TELNET 能将用户的击键传到远地主机，同时也能将远地主机的输出通过 TCP 连接返回到用户屏幕。这种服务是透明的，因为用户感觉到好像键盘和显示器是直接连在远地主机上。 ","date":"2023-07-13","objectID":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E5%BA%94%E7%94%A8%E5%B1%82/:6:1","tags":[],"title":"计算机网络之应用层","uri":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E5%BA%94%E7%94%A8%E5%B1%82/"},{"categories":["计算机网络"],"content":"域名系统：DNS 首先我们要搞清楚 主机名和IP地址的关系。 主机名如：facebook.com、google.com 等 但是，主机名几乎没有提供关于主机在因特网中位置的信息，这让路由器难以处理 IP 地址： 由4个字节组成，并有着严格的层次结构。例如 121.7.106.83 这样一个 IP 地址，其中的每个字节都可以用 . 进行分割，表示了 0 - 255 的十进制数字。 我们需要一种进行主机名到IP地址转换的目录服务，这就是DNS的主要任务。DNS是： 一个由分层的DNS服务器实现的分布式数据库 一个使得主机能够查询分布式数据库的应用层协议 如下图所示： 总的来说，在单一DNS服务器上运行集中式数据库完全没有可扩展能力。因此，DNS采用了分布式设计方案。 DNS 是一个复杂的系统，我们在这里只是就其运行的主要方面进行学习，下面给出一个 DNS 工作过程的总体概述 假设运行在用户主机上的某些应用程序（如 Web 浏览器或邮件阅读器） 需要将主机名转换为 IP 地址。这些应用程序将调用 DNS 的客户端，并指明需要被转换的主机名。用户主机上的 DNS 收到后，会使用 UDP 通过 53 端口向网络上发送一个 DNS 查询报文，经过一段时间后，用户主机上的 DNS 会收到一个主机名对应的 DNS 回答报文。因此，从用户主机的角度来看，DNS 就像是一个黑盒子，其内部的操作你无法看到。但是实际上，实现 DNS 这个服务的黑盒子非常复杂，它由分布于全球的大量 DNS 服务器以及定义了 DNS 服务器与查询主机通信方式的应用层协议组成。 DNS 最早的一种简单设计只是在因特网上使用一个 DNS 服务器。该服务器会包含所有的映射。这是一种集中式的设计，这种设计并不适用于当今的互联网，因为互联网有着数量巨大并且持续增长的主机，这种集中式的设计会存在以下几个问题 单点故障(a single point of failure)，如果 DNS 服务器崩溃，那么整个网络随之瘫痪。 通信容量(traaffic volume)，单个 DNS 服务器不得不处理所有的 DNS 查询，这种查询级别可能是上百万上千万级 远距离集中式数据库(distant centralized database)，单个 DNS 服务器不可能 邻近 所有的用户，假设在美国的 DNS 服务器不可能临近让澳大利亚的查询使用，其中查询请求势必会经过低速和拥堵的链路，造成严重的时延。 维护(maintenance)，维护成本巨大，而且还需要频繁更新。 所以 DNS 不可能集中式设计，它完全没有可扩展能力，因此采用分布式设计，所以这种设计的特点如下 ","date":"2023-07-13","objectID":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E5%BA%94%E7%94%A8%E5%B1%82/:7:0","tags":[],"title":"计算机网络之应用层","uri":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E5%BA%94%E7%94%A8%E5%B1%82/"},{"categories":["计算机网络"],"content":"分布式、层次数据库 没有一台DNS服务器拥有因特网上所有主机的映射。大致来说，有3种类型的DNS服务器：根DNS服务器、顶级域DNS服务器和权威DNS服务器。 根DNS服务器 有400多个根服务器遍及全世界。根名字服务器提供TLD服务器的IP地址 顶级域DNS服务器。 对于每个顶级域(如com、org、edu) 和所有国家的顶级域(uk,cn)等，都有TLD(Top-Level Domain)服务器. TLD服务器提供了权威DNS服务器的IP地址 权威DNS服务器 一个组织机构的权威DNS收藏了DNS记录。另一种方法是，该组织能够支付费用，让这些记录存储在某个服务提供商的一个权威DNS服务器中。多数大学和大公司实现和维护他们自己基本和备份的权威DNS服务器。 ","date":"2023-07-13","objectID":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E5%BA%94%E7%94%A8%E5%B1%82/:7:1","tags":[],"title":"计算机网络之应用层","uri":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E5%BA%94%E7%94%A8%E5%B1%82/"},{"categories":["计算机网络"],"content":"查询方式 第一个步骤是本机向本地域名服务器发出一个DNS请求报文，报文里携带需要查询的域名；第二个步骤是本地域名服务器向本机回应一个DNS响应报文，里面包含域名对应的IP地址或者别名等。由两种查询方法： **递归查询：**本机向本地域名服务器发出一次查询请求，就静待最终的结果。如果本地域名服务器无法解析，自己会以DNS客户机的身份向其它域名服务器查询，直到得到最终的IP地址告诉本机 **迭代查询：**本地域名服务器向根域名服务器查询，根域名服务器告诉它下一步到哪里去查询，然后它再去查，每次它都是以客户机的身份去各个服务器查询 注意：理论上，任何DNS查询机可以是迭代的也可以是递归的。 在实践中，第一个步骤从主机到本地域名服务器是递归查询；第二大步骤中采用的是迭代查询，其实是包含了很多小步骤的，如下图所示。 ","date":"2023-07-13","objectID":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E5%BA%94%E7%94%A8%E5%B1%82/:7:2","tags":[],"title":"计算机网络之应用层","uri":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E5%BA%94%E7%94%A8%E5%B1%82/"},{"categories":["计算机网络"],"content":"[toc] ","date":"2023-07-11","objectID":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E8%BF%90%E8%BE%93%E5%B1%82/:0:0","tags":[],"title":"计算机网络之运输层","uri":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E8%BF%90%E8%BE%93%E5%B1%82/"},{"categories":["计算机网络"],"content":"运输层概述 ","date":"2023-07-11","objectID":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E8%BF%90%E8%BE%93%E5%B1%82/:1:0","tags":[],"title":"计算机网络之运输层","uri":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E8%BF%90%E8%BE%93%E5%B1%82/"},{"categories":["计算机网络"],"content":"运输层的基本功能 运输层协议为运行在不同主机上的应用进程之间提供了逻辑通信功能，使得运行在不同主机上的进程像直连一样。 复用和分用 差错检验 进程到进程的数据交付（多路复用和多路分解）和差错检查是两个最低限度的运输层服务，也是UDP能提供的仅有的两种服务。 ","date":"2023-07-11","objectID":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E8%BF%90%E8%BE%93%E5%B1%82/:1:1","tags":[],"title":"计算机网络之运输层","uri":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E8%BF%90%E8%BE%93%E5%B1%82/"},{"categories":["计算机网络"],"content":"多路复用和多路分解 多路复用 在数据的发送端，传输层收集各个套接字中需要发送的数据，将它们封装上首部信息后（之后用于分解），交给网络层； 多路分解 在数据的接收端，传输层接收到网络层的报文后，将它交付到正确的套接字上； 复用强调的是多个用户进程能够复用相同的运输层协议，例如不同进程的UDP套接字的发送缓冲区数据能被收集后统一交给运输层的UDP协议处理和封装。 ","date":"2023-07-11","objectID":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E8%BF%90%E8%BE%93%E5%B1%82/:1:2","tags":[],"title":"计算机网络之运输层","uri":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E8%BF%90%E8%BE%93%E5%B1%82/"},{"categories":["计算机网络"],"content":"运输层和网络层的关系 运输层为不同主机的应用程序提供了逻辑通信和数据交付服务 网络层为主机和主机提供了逻辑通信和数据交付服务。 运输层是基于网络层的服务的，只有实现了主机间逻辑通信才能再此基础上实现端到端进程间通信；运输层是网络层的功能扩展，网络层不能保证数据的可靠传输，而运输层则扩展了这个功能。 总结就是：网络层是运输层的基础和服务者，运输层是网络层的扩展，其实协议栈的每个上下层都是这个关系。 ","date":"2023-07-11","objectID":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E8%BF%90%E8%BE%93%E5%B1%82/:1:3","tags":[],"title":"计算机网络之运输层","uri":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E8%BF%90%E8%BE%93%E5%B1%82/"},{"categories":["计算机网络"],"content":"端口号 在TCP/IP体系的运输层中，不同操作系统采用的进程标识符结构不一样，所以，为了保证主机与主机中各个进程之间的通信，所以我们采用16位端口号来映射主机内的某个进程。 端口号分为3类 熟知端口（0~1023号端口）：作为指定用途的端口（如80是Web服务器端口），不会随机对口进行分配 登记端口（1024~49151）：可以作为服务器进程被随机分配的端口 短暂端口（49152~65535）：客户端在与对端连接通信时动态选择，连接关闭后端口就关闭，被系统收回，因此又叫短暂端口。 ","date":"2023-07-11","objectID":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E8%BF%90%E8%BE%93%E5%B1%82/:1:4","tags":[],"title":"计算机网络之运输层","uri":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E8%BF%90%E8%BE%93%E5%B1%82/"},{"categories":["计算机网络"],"content":"UDP协议 UDP是一种无连接的、面向报文的、尽最大努力交付（不保证可靠交付）的运输层协议。 ","date":"2023-07-11","objectID":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E8%BF%90%E8%BE%93%E5%B1%82/:2:0","tags":[],"title":"计算机网络之运输层","uri":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E8%BF%90%E8%BE%93%E5%B1%82/"},{"categories":["计算机网络"],"content":"特点 不建立连接：减少了3次握手的时延。 不保证可靠交付：报文可能丢失、乱序（但不会比特差错），而且不负责重发。丢失时可以通知应用层，让应用层组织重发。因为不保证可靠交付，所以也没有确认应答机制。 面向报文：一次交付一个完整报文。UDP协议不会对应用层交付给运输层的报文切分（即使这个应用层报文很大），而是直接对应用层报文加上首部就交给网络层（但网络层会对其分片的），保留了报文的边界。需要应用层决定一次发送多少数据以避免发生IP分片。 没有拥塞控制：网络出现拥塞时也不限制发送端的发送速率（对发送端自己有利，但对其所在的网络不利）。这意味着发送速率稳定，但发生拥塞时会因为路由器缓存溢出而丢失分组，而且不限速的发送也会导致拥塞和加重拥塞。 支持多种交互通信：支持一对一，一对多，多对多，多对一的交互通信，而TCP只能做到一对一。这里的一对一和一对多是指socket，具体是指一个服务端的socket可以为多个客户端的socket通信和服务。而TCP服务端的一个client套接字只能为一个客户端socket收发消息。 首部开销小：8个字节，比TCP的20字节首部短。 ","date":"2023-07-11","objectID":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E8%BF%90%E8%BE%93%E5%B1%82/:2:1","tags":[],"title":"计算机网络之运输层","uri":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E8%BF%90%E8%BE%93%E5%B1%82/"},{"categories":["计算机网络"],"content":"UDP的使用场景 实时应用：语音电话或视频会议。利用了UDP开销小、效率高、没有拥塞的特点。 一次性传小量数据的应用：如果一次性大量数据则不利于UDP不切分数据的特点。 多媒体应用：如播放视频。因为视频对传输可靠性没那么高，即使小部分数据丢失也不影响视频播放。 ","date":"2023-07-11","objectID":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E8%BF%90%E8%BE%93%E5%B1%82/:2:2","tags":[],"title":"计算机网络之运输层","uri":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E8%BF%90%E8%BE%93%E5%B1%82/"},{"categories":["计算机网络"],"content":"UDP协议报文格式 每个UDP报文分为UDP报头和UDP数据区两部分。报头由4个16位长(2字节) 字段组成，分别说明该报文的源端口、目的端口、报文长度、校验值 ","date":"2023-07-11","objectID":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E8%BF%90%E8%BE%93%E5%B1%82/:2:3","tags":[],"title":"计算机网络之运输层","uri":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E8%BF%90%E8%BE%93%E5%B1%82/"},{"categories":["计算机网络"],"content":"TCP协议 TCP协议是一种面向连接、面向字节流、提供可靠传输服务的一对一通信传输层协议。 ","date":"2023-07-11","objectID":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E8%BF%90%E8%BE%93%E5%B1%82/:3:0","tags":[],"title":"计算机网络之运输层","uri":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E8%BF%90%E8%BE%93%E5%B1%82/"},{"categories":["计算机网络"],"content":"特点 面向连接：数据收发前需要建立连接，数据收发要通过这个逻辑的虚拟信道。 全双工通信：通信两端都可收可发，而且双向的收发可以同时发生。 一对一通信：一个client socket只能与一个server socket进行数据收发。如果要实现多个client数据接收，只能开启相应数量的server 面向字节流：TCP只把数据看成一连串有序而无结构的字节流。 可靠传输服务：数据不丢失、不重复、无差错、不乱序。 TCP功能在UDP功能（数据交付和差错检验）的基础上加上了连接管理、超时重传、流量控制和拥塞控制。 ","date":"2023-07-11","objectID":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E8%BF%90%E8%BE%93%E5%B1%82/:3:1","tags":[],"title":"计算机网络之运输层","uri":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E8%BF%90%E8%BE%93%E5%B1%82/"},{"categories":["计算机网络"],"content":"TCP报文段格式 TCP首部的前20个字节是固定的（TCP首部最小长度为20字节，最大长度60），后面4n字节是按需增加的选项。 序号（seq）：TCP数据的每个字节是按序编号的，TCP首部的“序号”字段就是本报文携带数据的第一个字节的编号，范围在0~2^32-1， 超过则下一个序号会回到0重新增长。 初始序号（ISN）在建立连接时设置，ISN是一个随时间动态增长的非0序号，这也是为了防止被攻击者伪造初始序号和TCP报文。 序号是TCP实现可靠传输的基础，其作用有：防止接收方接收重复分组；确认应答机制基于序号，给字节标记序号便于超时重传触发时发送方知道自己应该重发那哪分组。 确认号（ACK）：是一端期望收到对端下一个报文段数据的第一个字节的序号，也是本端上一次发送的报文段的最后一个字节序号+1。 数据偏移：表示TCP首部的长度，占4个比特，每个比特的单位是4字节。因此TCP首部最大长度为60字节。 6个标志位 紧急位 URG：URG = 1时报头的紧急指针字段生效，表示报文段有紧急数据要尽快传送。 socket有发送缓冲区，运输层会择机将发送缓冲区的数据发送出去（3个时机），但如果URG = 1的报文段则无需等待这3个时机，可以直接发送。 确认位 ACK：ACK = 1时，报头的确认号(ACK)字段才有效，表示本报文是一个确认报文（tcp的应答确认机制）。TCP规定在连接建立后所有传送的报文段都必须把ACK置为1。 推送位 PSH：PSH=1的报文段，接收方会尽快上交给应用进程（不要在接收缓冲区中缓存），该标志位是针对接收方的。 重置连接位 RST：RST =1 的报文段标明TCP连接出现差错，必须释放连接，然后重新建立连接。 同部位 SYN：SYN = 1表示这是一个连接请求报文。 终止位 FIN：FIN = 1表示这是一个请求释放连接的报文 窗口字段：接收方告诉发送方，下一次容许发送方能够发送的最大数据长度（取决于接收方的接收缓冲区大小）。 检验和：同UDP校验和。 紧急指针：指出本报文段中紧急数据有多少字节，紧急数据放在报文段数据的最前面，所以紧急指针等于紧急数据在本报文的最后一个字节的位置。窗口为0时也可以发送紧急数据。 选项字段（最大40字节）：该字段长度可变。有如下可选项： 所有可选选项都包含该选项的类型长度还有实际内容（例如下面的时间戳选项，kind=8表示这是一个时间戳选项，length表示时间戳选项长度为10字节).所有可选选项都在建立连接时的SYN报文指定开启。 窗口扩大选项 使用窗口扩大项后，可以是窗口大小从原本最大 2^16 - 1 扩大到最大 2^30 - 1个字节。 这是考虑到链路可能有长又肥（即带宽很大，且链路很长(即时延很长)），如果一次发的数据太少就无法充分利用带宽，吞吐率也低。发送端的发送窗口（TCP头部的窗口大小）由接收方的接收缓存大小 **（流量窗口）**以及 **链路带宽和拥塞情况（拥塞窗口）**共同决定的。 选择确认选项：接收方告诉发送方自己收到的连续字节块。用于数据段失序到达时，发送端重复发送数据段。 时间戳选项：占10字节，包含最主要的是时间戳值字段（4字节）和时间戳回送回答字段（4字节）。 有两个作用： A. 计算报文在两端传输层的往返时延（接近于RTT） A发送报文时会将发送时间戳放入 timestamp（时间戳值字段）， B接收到报文后将timestamp复制到timestamp echo（时间戳回显重试字段），并在返回ACK报文时将当前时间戳放入timestamp。 回复报文到达A后，A可以用当前时间戳 - timestamp echo得到往返时间，而且该往返时间可认为就是RTT。 B. 防止序号回绕带来的问题 需要注意，填充的时间戳不是真实的时间戳，而是一个自增的整型，而且发送方填入的timestamp和接收方填入的timestamp可以是独立的，例如发送方填入timestamp = 5012， 接收方填入timestamp = 197720862，也就是说两端的时间戳可以不用同步。 MSS 最大报文段大小 MSS 最大报文段数据大小，用于告诉对端，我所在的局域网链路能容纳的最大报文段的数据长度。MSS和窗口无关，和网络带宽有关。在建立TCP连接时，通信双方都要在SYN报文指明自己允许的MSS大小，MSS是双向的。 MSS可以控制TCP的传输效率，MSS太大可能导致报文段在网络层分片，太小可能导致传输效率降低（假设MSS设为1个字节，那么一个报文段的数据包含只有1个字节但头部有20个字节，你说效率低不低）。应该尽量设置MSS接近网络层一个分片的大小，使得该报文段刚好不用分片，这取决于从源主机到目的主机链路的最小MTU（MTU是网络层的包的最大长度）。 MSS + TCP头部 = TCP报文段长度。 MSS + TCP头部 + IP头部 \u003c= MTU MSS默认是536字节（这也是合理的最小MSS），因为任何主机都应该至少处理576字节的IPv4数据报（含IP头部），如果按最小的IPv4和TCP头部计算，最小IPv4数据报下的最大MSS = 576 - 20 - 20 = 536。 在以太网中IPv4协议下，MSS应该设置的比较合适的值是略小于1460，因为以太网MTU=1500, 而TCP和IP头部分别为20字节，1460 + 20 +20 = 1500，刚好达到网络层不用对IP包分片的最大包大小，也刚好到达以太网链路的最大报文数据的传输大小。 在以太网中IPv6协议下，MSS应该设置的比较合适的值是略小于1440，因为IPv6的头部为40字节。 ","date":"2023-07-11","objectID":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E8%BF%90%E8%BE%93%E5%B1%82/:3:2","tags":[],"title":"计算机网络之运输层","uri":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E8%BF%90%E8%BE%93%E5%B1%82/"},{"categories":["计算机网络"],"content":"TCP可靠传输原理 停止等待协议 简单来说，就是发一个数据包，就等待一个ACK，然后再发一个数据包；如果等不到ACK就需要重发一个数据包。 停止等待协议，是一个保证可靠传输的协议，主要有以下机制构成： 确认应答机制 接收方接收到一个正确分组时，需要回复一个带确认号和ACK位为1的确认报文给发送方。此时发送方就知道分组已到达； 分组由于比特差错被接收方检测到会被直接丢弃，不发送ACK报文（在TCP协议中即使一个分组发生比特错误，对端也会发送ACK，但确认的是出错报文之前的报文）； 分组由于丢失无法到达接收方则不发送ACK报文； 停等机制 指发送方每发送完一个分组就停止发送，等待对方确认，收到确认后再发下一个分组。 自动超时重传机制（ARQ） 发送方为每个分组设置一个超时计时器（TCP中是为每个窗口设置一个超时计时器），超过指定重传时间（RTO）未收到接收方的ACK报文，发送方就会重发报文。 ARQ的意思是不用接收方请求重传，而是发送方通过计时器计时来自动重传。 这里需要注意3点： 发送方必须暂时保留已发送的分组的副本以便重传时使用，收到相应分组的确认才可以清除副本。 分组必须编号，这才能明确哪个分组得到确认（而且编号也有助于接收方丢弃重复接收过的分组）。 重传时间RTO 应该略大于RTT 一个问题：超时重传时间应该定为多少？ 答：超时重传时间太大会使网络链路空闲，降低传输效率；太小会使报文不必要的重传，加大网络负荷引起拥塞。TCP采用了一种自适应算法计算超时时间RTO。RTO应该略大于报文往返时间RTT。 新的RTTS = (1-α) * (旧的RTTS) + α * (新的RTT样本) RTTD = （1-β）* 旧的RTTD + β * | RTTS - 新的RTT样本 | RTO = RTTS + 4*RTTD TCP需要计算一个加权平均往返时间RTTS，反映多次报文传输的整体RTT，每次确认报文到达发送端，发送端都会更新一次RTTS。 “新的RTT样本”可以使用发送方接收到ACK分组的时间戳 - 分组头部时间戳选项计算得到。 α由系统和协议栈开发者决定，α接近0表示新的RTT影响不大，更新较慢；建议标准的RFC规范推荐α=1/8。 RTTD是RTT的加权平均标准差，反应了多次RTT的抖动程度。β推荐为 1/4。 另一个问题：假设发生了重传，并且收到了确认报文，如何确定该ACK报文是对先发送的报文的确认还是重传报文的确认？ 这个问题对RTTS的计算很重要（假设时间戳选项没开启）。如果该ACK是对重传报文的确认，却被误认为是对原来报文的确认，则更新后的RTTS和RTO会偏大。该问题不解决，RTO会因为重传越变越大。 答：Karn算法提出，如果报文段重传收到ACK，无需判断ACK是之前报文的确认还是重传报文的确认，直接不更新本次的RTTS和RTO即可。 这会带来新的问题：如果一段时间内TCP会重传很多报文，采用上述做法会导致RTO失去多次更新，变得不准确。 修正的Karn算法提出，报文段每重传一次就把RTO增大一点： 重传时新RTO = γ * 旧的RTO 举例 停止等待协议下的通信会出现如下情况 无差错的情况 有差错的情况（分组丢失或差错） 在接收⽅ B 会出现两种情况： B 接收 M1 时检测出了差错，就丢弃 M1，其他什么也不做； M1 在传输过程中丢失了，这时 B 当然什么都不知道，也什么都不做。 解决⽅法：超时重传（ARQ） A 为每⼀个已发送的分组都设置超时计时器，A由于重传时间RTO内（略大于RTT）没收到M1的确认因此会重发M1；A 在超时计时器到期之前收到了相应的确认，撤销该超时计时器，继续发送下⼀个分组 M2 。 确认报文丢失或迟到 假设B收到了报文（发送端的报文没丢失）而且报文没有错误，但是B的确认报文丢失或过了很久才到达A。 子情况1：确认报文丢失。 由于A收不到 M1 的确认报文，A 在超时计时器到期后重传 M1。 B ⼜收到了重传的分组 M1，所以丢弃这个重复的分组 M1（通过序号和时间戳选项判断分组是否重复），不向上层交付，并向 A 发送确认。 子情况2：确认报文迟到 由于A收不到 M1 的确认报文，A 在超时计时器到期后重传 M1。 B ⼜收到了重传的分组 M1，所以丢弃这个重复的分组 M1，并向 A 发送确认。A由于会收到2个ACK号相同的确认报文，A会丢弃其中晚到达的一个。 结论：停等协议可以保证可靠传输的实现，但通信效率不高 连续ARQ协议 在一定限度k内，发送方连续发出k个包后停下，如果ACK报文返回则可以继续发送；如果一定时间内没有返回ack则重传； 连续ARQ协议在停止等待协议的基础之上提升了信道利用率，它除了要遵循确认应答机制和自动超时重传机制外，还包括以下机制： 流水线传输机制 指发送方可以连续发送多个分组不必每发一个分组就等待对方的确认，与停止等待协议对立，可以提高信道利用率和链路的吞吐量。当然连续发送的分组数量是有限的，这取决于滑动窗口的大小。 累积确认机制 指接收方不必为每个到达的分组都发送确认，而是收到若干个分组后对按序到达的最后一个分组发送确认。当然TCP协议既可能出现累积确认，也可能出现对单个分组确认。 确认的时机（这里也是TCP确认的时机）： 收到 2*MSS 长度的数据就做确认应答（有些系统是不管数据长度，而是收到2个报文就确认）； 最大延迟0.5秒发送确认应答，即使即使只收到一个分组也要确认（多数系统是0.2秒），该延迟时间由延迟应答计时器来计时； 当接收方接收到失序的报文段时就立刻发出确认（对最后一个有序分组的确认），以便快速重传（当发送方收到连续3个ack号相同的ack报文时就会重传）。 若干分组到达后，累计确认 累积确认是为了提高信道的利用率，提升系统性能，能少发报文就少发报文。当然如果延迟确认的时间长了可能引发发送端重传，也会降低传输效率。 捎带确认机制 如果接收方发送确认时刚好也要发送自己的数据报文，那么这个ACK确认可能会捎带到这个数据报文中，此时就减少了一个报文头部的开销，这叫做捎带确认。 回退N机制 指当报文乱序到达接收端，接收端收到的分组不是连续的，而是缺了某些分组，此时接收端只确认第一个空缺分组之前的分组，空缺分组以及其之后的n个分组都要发送方重传，这就是回退N。 B收到1 2 4 5号分组但由于3号分组未收到就到达确认时机，只能选择确认1和2分组，发送端A需要重发 3~5 号分组。 为了避免回退N机制重复发送已经发过的报文，可以使用TCP选项中的“选择确认SACK功能”。其机制如下： 上图空缺的序号是 1000~1500 和 3001~3501。 SACK的原理是，把乱序到达的分组先暂存在接收缓冲区，并且把空缺分组相邻分组的左右边界序号（必须是成对边界，在本例子中有3个边界0~999 / 1501~3000 / 3501~4500，共 6 * 4字节=24字节）放到头部的SACK选项告诉发送方，发送方就会只重传空缺的数据给接收方，而无需回退N步。 ⾸部选项的⻓度最⼤有 40 字节，指明⼀个对序号⽤掉 8 字节，因此在选项中最多只能指明 4 对序号的边界信息，也就是指明最多3个空缺范围（4对序号用掉 32字节，还需要2个字节指明选项类型和长度）。 SACK文档并未有指明发送方应该怎样响应SACK，所以大多数的实现还是会回退N，重传所有未确认的数据块1000~4500。 对比 TCP可靠传输的实现 TCP的可靠传输以上述连续ARQ协议为基础做出了一些变动，并研究更多的细节如滑动窗口的实现、超时计时器如何设置超时时间、选择确认、流量控制和拥塞控制。 TCP可靠传输基于4点：窗口、序号、确认和重传。其中后3点实现了可靠传输，第1点提高TCP传输效率。 ","date":"2023-07-11","objectID":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E8%BF%90%E8%BE%93%E5%B1%82/:3:3","tags":[],"title":"计算机网络之运输层","uri":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E8%BF%90%E8%BE%93%E5%B1%82/"},{"categories":["计算机网络"],"content":"TCP的发送时机 为了保证TCP传输效率，TCP不会在发送缓冲区一有数据就立刻发送，而是会遵循一些发送的时机，当满足以下3个条件中的一个才会发送数据： TCP维持一个MSS变量，当缓存中的数据到达MSS字节时，就组装成一个报文段发送； 当发送方进程指明要求推送报文段（PSH=1）或者是发送紧急数据（URG=1） 发送方设置一个TCP发送报文计时器，如果到时了，即便发送缓存中的数据量不够MSS也要发送出去。 当然情况1和3要在发送方的可用窗口大于0的情况下才能发送出去。 ","date":"2023-07-11","objectID":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E8%BF%90%E8%BE%93%E5%B1%82/:3:4","tags":[],"title":"计算机网络之运输层","uri":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E8%BF%90%E8%BE%93%E5%B1%82/"},{"categories":["计算机网络"],"content":"Nagle算法 Nagle算法用于发送方比较空闲，没有什么数据要发送的情况下，为提高传输效率的一种算法。假设发送方的应用层是逐字节发送数据给协议栈的缓冲区，则Nagle会这样处理： 若进程要把发送的数据逐个字节的发送到TCP的发送缓存，则发送⽅先发送第⼀个数据字节，缓存后⾯到达的数据字节； 发送⽅收到对第⼀个数据字符的确认后，把发送缓存中的所有数据组装成⼀个报⽂段（不超过MSS）发送出去，继续对随后到达的数据进⾏缓存； 只有在收到对前⼀个报⽂段的确认后继续发送下⼀个报⽂段（相当于退化成停等协议）； 当到达缓冲区的数据已达到发送窗⼝⼤⼩的⼀半或已达到报⽂段的最⼤⻓度时，（即使上一个报文的确认没到达）也⽴即发送⼀个报⽂段。 接收方此时应该适当延迟回发确认报文，并尽量使用捎带确认。 该算法总结一下就是，在应用进程的数据到达发送缓冲区的速度比较慢的时候就退化成停等协议，比较快且快到满足上述第4点的时候就立刻发送报文。 ","date":"2023-07-11","objectID":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E8%BF%90%E8%BE%93%E5%B1%82/:3:5","tags":[],"title":"计算机网络之运输层","uri":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E8%BF%90%E8%BE%93%E5%B1%82/"},{"categories":["计算机网络"],"content":"重传机制 TCP 实现可靠传输的方式之一，是通过序列号与确认应答。 在 TCP 中，当发送端的数据到达接收主机时，接收端主机会返回一个确认应答消息，表示已收到消息。 但在错综复杂的网络，并不一定能如上图那么顺利能正常的数据传输，万一数据在传输过程中丢失了呢？ 所以 TCP 针对数据包丢失的情况，会用重传机制解决。 接下来说说常见的重传机制： 超时重传 快速重传 SACK D-SACK 超时重传 重传机制的其中一个方式，就是在发送数据时，设定一个定时器，当超过指定的时间后，没有收到对方的 ACK 确认应答报文，就会重发该数据，也就是我们常说的超时重传。 TCP 会在以下两种情况发生超时重传： 数据包丢失 确认应答丢失 超时时间应该设置为多少呢？ 我们先来了解一下什么是 RTT（Round-Trip Time 往返时延），从下图我们就可以知道： RTT 指的是数据发送时刻到接收到确认的时刻的差值，也就是包的往返时间。 超时重传时间是以 RTO （Retransmission Timeout 超时重传时间）表示。 假设在重传的情况下，超时时间 RTO 「较长或较短」时，会发生什么事情呢？ 上图中有两种超时时间不同的情况： 当超时时间 RTO 较大时，重发就慢，丢了老半天才重发，没有效率，性能差； 当超时时间 RTO 较小时，会导致可能并没有丢就重发，于是重发的就快，会增加网络拥塞，导致更多的超时，更多的超时导致更多的重发。 精确的测量超时时间 RTO 的值是非常重要的，这可让我们的重传机制更高效。 根据上述的两种情况，我们可以得知，超时重传时间 RTO 的值应该略大于报文往返 RTT 的值。 至此，可能大家觉得超时重传时间 RTO 的值计算，也不是很复杂嘛。 好像就是在发送端发包时记下 t0 ，然后接收端再把这个 ack 回来时再记一个 t1，于是 RTT = t1 – t0。没那么简单，这只是一个采样，不能代表普遍情况。 实际上「报文往返 RTT 的值」是经常变化的，因为我们的网络也是时常变化的。也就因为「报文往返 RTT 的值」 是经常波动变化的，所以「超时重传时间 RTO 的值」应该是一个动态变化的值。 我们来看看 Linux 是如何计算 RTO 的呢？ 估计往返时间，通常需要采样以下两个： 需要 TCP 通过采样 RTT 的时间，然后进行加权平均，算出一个平滑 RTT 的值，而且这个值还是要不断变化的，因为网络状况不断地变化。 除了采样 RTT，还要采样 RTT 的波动范围，这样就避免如果 RTT 有一个大的波动的话，很难被发现的情况。 RFC6289 建议使用以下的公式计算 RTO： 其中 SRTT 是计算平滑的RTT ，DevRTR 是计算平滑的RTT 与 最新 RTT 的差距。 在 Linux 下，α = 0.125，β = 0.25， μ = 1，∂ = 4。别问怎么来的，问就是大量实验中调出来的。 如果超时重发的数据，再次超时的时候，又需要重传的时候，TCP 的策略是超时间隔加倍。 也就是每当遇到一次超时重传的时候，都会将下一次超时时间间隔设为先前值的两倍。两次超时，就说明网络环境差，不宜频繁反复发送。 超时触发重传存在的问题是，超时周期可能相对较长。那是不是可以有更快的方式呢？ 于是就可以用「快速重传」机制来解决超时重发的时间等待。 快速重传 TCP 还有另外一种快速重传（Fast Retransmit）机制，它不以时间为驱动，而是以数据驱动重传。 快速重传机制，是如何工作的呢？其实很简单，一图胜千言。 在上图，发送方发出了 1，2，3，4，5 份数据： 第一份 Seq1 先送到了，于是就 Ack 回 2； 结果 Seq2 因为某些原因没收到，Seq3 到达了，于是还是 Ack 回 2；（因为数据包乱序到达接收端，所以不能给乱序的到达的数据包发送ACK，只能给最后一个有序到达的数据包发ACK，因此也就重发ACK2） 后面的 Seq4 和 Seq5 都到了，但还是 Ack 回 2，因为 Seq2 还是没有收到； 发送端收到了三个 Ack = 2 的确认，知道了 Seq2 还没有收到，就会在定时器过期之前，重传丢失的 Seq2。（至于为什么会是三次，而不是两次、一次。如果次数较少，可能会频繁触发重传） 最后，收到了 Seq2，此时因为 Seq3，Seq4，Seq5 都收到了，于是 Ack 回 6 。 所以，快速重传的工作方式是当收到三个相同的 ACK 报文时，会在定时器过期之前，重传丢失的报文段。 快速重传机制只解决了一个问题，就是超时时间的问题，但是它依然面临着另外一个问题。就是重传的时候，是重传一个，还是重传所有的问题。 举个例子，假设发送方发了 6 个数据，编号的顺序是 Seq1 ~ Seq6 ，但是 Seq2、Seq3 都丢失了，那么接收方在收到 Seq4、Seq5、Seq6 时，都是回复 ACK2 给发送方，但是发送方并不清楚这连续的 ACK2 是接收方收到哪个报文而回复的， 那是选择重传 Seq2 一个报文，还是重传 Seq2 之后已发送的所有报文呢（Seq2、Seq3、 Seq4、Seq5、 Seq6） 呢？ 如果只选择重传 Seq2 一个报文，那么重传的效率很低。因为对于丢失的 Seq3 报文，还得在后续收到三个重复的 ACK3 才能触发重传。 如果选择重传 Seq2 之后已发送的所有报文，虽然能同时重传已丢失的 Seq2 和 Seq3 报文，但是 Seq4、Seq5、Seq6 的报文是已经被接收过了，对于重传 Seq4 ～Seq6 折部分数据相当于做了一次无用功，浪费资源。 可以看到，不管是重传一个报文，还是重传已发送的报文，都存在问题。 为了解决不知道该重传哪些 TCP 报文，于是就有 SACK 方法。 SACK 方法 还有一种实现重传机制的方式叫：SACK（ Selective Acknowledgment）， 选择性确认。 这种方式需要在 TCP 头部「选项」字段里加一个 SACK 的东西，它可以将已收到的数据的信息发送给「发送方」，这样发送方就可以知道哪些数据收到了，哪些数据没收到，知道了这些信息，就可以只重传丢失的数据。 如下图，发送方收到了三次同样的 ACK 确认报文，于是就会触发快速重发机制，通过 SACK 信息发现只有 200~299 这段数据丢失，则重发时，就只选择了这个 TCP 段进行重复。 如果要支持 SACK，必须双方都要支持。在 Linux 下，可以通过 net.ipv4.tcp_sack 参数打开这个功能（Linux 2.4 后默认打开）。 Duplicate SACK Duplicate SACK 又称 D-SACK，其主要使用了 SACK 来告诉「发送方」有哪些数据被重复接收了。 下面举例两个栗子，来说明 D-SACK 的作用。 栗子一号：ACK 丢包 「接收方」发给「发送方」的两个 ACK 确认应答都丢失了，所以发送方超时后，重传第一个数据包（3000 ~ 3499） 于是「接收方」发现数据是重复收到的，于是回了一个 SACK = 3000~3500，告诉「发送方」 3000~3500 的数据早已被接收了，因为 ACK 都到了 4000 了，已经意味着 4000 之前的所有数据都已收到，所以这个 SACK 就代表着 D-SACK。 这样「发送方」就知道了，数据没有丢，是「接收方」的 ACK 确认报文丢了。 栗子二号：网络延时 数据包（1000~1499） 被网络延迟了，导致「发送方」没有收到 Ack 1500 的确认报文。 而后面报文到达的三个相同的 ACK 确认报文，就触发了快速重传机制，但是在重传后，被延迟的数据包（1000~1499）又到了「接收方」； 所以「接收方」回了一个 SACK=1000~1500，因为 ACK 已经到了 3000，所以这个 SACK 是 D-SACK，表示收到了重复的包。 这样发送方就知道快速重传触发的原因不是发出去的包丢了，也不是因为回应的 ACK 包丢了，而是因为网络延迟了。 可见，D-SACK 有这么几个好处： 可以让「发送方」知道，是发出去的包丢了，还是接收方回应的 ACK 包丢了; 可以知道是不是「发送方」的数据包被网络延迟了; 可以知道网络中是不是把「发送方」的数据包给复制了; 在 Linux 下可以通过 net.ipv4.tcp_dsack 参数开启/关闭这个功能（Linux 2.4 后默认打开）。 ","date":"2023-07-11","objectID":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E8%BF%90%E8%BE%93%E5%B1%82/:3:6","tags":[],"title":"计算机网络之运输层","uri":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E8%BF%90%E8%BE%93%E5%B1%82/"},{"categories":["计算机网络"],"content":"滑动窗口 通信过程中一个传输方向上所有的字节的序号（seq）可以看做一个序列（这些序列在socket的缓冲区中），而窗口则是序列中的一个子集。该窗口用于做流量控制以及拥塞控制。 滑动窗口的单位是字节而不是分组。一个连接的两端都各有一对窗口分别是发送窗口和接收窗口，因此一个连接有4个窗口，且是动态变化的。 发送窗口包含 已发送但未确认的数据 和 准备发送的数据。 已发送但未确认的数据是为了方便超时重传的实现。 接收窗口包含 按序到达但未被应⽤程序接收的数据 和 不按序到达的数据。 接收窗口缓存一下未被接收的数据；缓存一下不按序到达的数据，以避免大量数据的重传。 窗口指针保存在套接字中。在不考虑拥塞的情况，发送端A的发送窗口和接收方B的接收窗口大小从整个传输过程来看是一致的（但不是强一致，并不总是一样大）。 发送窗口越大，发送方在收到确认前能连续发送的数据越多，在不考虑网络拥塞因素下传输效率越高。 滑动窗口的工作过程 只观察发送方的发送缓冲区和接收方的接收缓冲区: 1、某一时刻，A 收到了 B 的确认报⽂段：报文携带的窗口值 20 字节，确认号为 31。A 可以把落⼊发送窗⼝中的序号字节⼀次连续性全部发送出去：边发送边接收确认。 2、下一刻A发送了31~41号字节，在确认前会保留在窗口中以便超时重传时使用。 B的接收窗口显示，B没有收到31，32~33是未按需到达的数据，要临时存放在接收窗口，不能上交给应用进程。B的接收窗口也不能移动。 3、下一刻B收到了31，B将31~33字节交付应用层，并从接收缓冲区中删除，且B接收窗口右移3个字节，发送ack=33的确认报文（假设确认报文的窗口大小字段仍是20）。A收到确认后窗口右移3字节。 4、当P2=P3时，可用窗口为0，会停止发送。 缓冲区与窗口的关联 发送方的发送缓存与发送窗口 发送窗⼝通常只是 发送缓存的⼀部分，具体来说 发送缓存 = 发送应用程序最后写入发送缓冲区的最后一个字节 - 发送窗口P1字节。缓冲区中，p1指针之前的数据由于已经发送和收到确认，因此被释放出缓冲区。 接受方的接收缓存与接收窗口 ","date":"2023-07-11","objectID":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E8%BF%90%E8%BE%93%E5%B1%82/:3:7","tags":[],"title":"计算机网络之运输层","uri":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E8%BF%90%E8%BE%93%E5%B1%82/"},{"categories":["计算机网络"],"content":"流量控制 流量控制是指动态控制滑动窗口的大小使得发送方发送数据的速率略小于或等于接收方接收的速率（接收速率其实又由应用程序取走接收缓冲区的速率决定），防止接收方的接收缓存溢出造成分组丢失。 流量控制的实现是通过在接收方的ACK报文携带窗口大小（假设大小X，X=接收方的接收缓冲区的空闲空间大小）同步给发送方，使发送方调整自己的可用窗口（P3-P2部分）为X。 需要注意的是发送窗口的p2-p1取决于ack号，P3-P2取决于ACK报文中的窗口大小。 下面是一个流量控制的过程（不考虑拥塞的情况下）： 图中rwnd(receiver window)表示容许的接收方窗口。 持续计时器 考虑一种情况，如果B向A发送了一个零窗口报文后，A停止向B发数据，后来B又向A发送了一个rwnd=400的ACK报文M。但M丢失了，A一直在等B的非零窗口通知，B也在等A发过来的数据，陷入死锁局面。 解决方法： TCP为每个连接设有一个持续计时器，只要一端A收到零窗口通知就启动该计时器，时间到期，A就发送一个仅携带1字节的“零窗口探测报文”。对端B就会确认这个报文的时候携带新的rwnd值。如果rwnd仍为0则重新启动持续计时器，否则A开始发送数据。 ","date":"2023-07-11","objectID":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E8%BF%90%E8%BE%93%E5%B1%82/:3:8","tags":[],"title":"计算机网络之运输层","uri":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E8%BF%90%E8%BE%93%E5%B1%82/"},{"categories":["计算机网络"],"content":"拥塞控制 什么情况下叫做拥塞？ 网络中，链路容量（带宽）、交换机和路由器中的缓存和处理机都是网络的资源，在某段时间，若对网络中某一资源的需求超过了该资源能提供的部分，导致分组在链路中丢失，这种情况就叫拥塞。 有了TCP的窗口控制后，使计算机网络中两个主机之间不再是以单个数据段的形式发送了，而是能够连续发送大量的数据包。然而，大量数据包同时夜伴随着其他问题，比如说网络负载、网络拥堵等问题。TCP因此使用了拥塞控制机制，使得在面临网络拥塞时遏制发送方的数据发送。 拥塞控制主要有两种方法 端到端的拥塞控制： 因为网络层没有为运输层拥塞控制提供显示支持。所以即使网络中存在拥塞情况，端系统也要通过对网络行为的观察来推断。TCP 就是使用了端到端的拥塞控制方式。IP 层不会向端系统提供有关网络拥塞的反馈信息。那么 TCP 如何推断网络拥塞呢？如果超时或者三次冗余确认就被认为是网络拥塞，TCP 会减小窗口的大小，或者增加往返时延来避免。 网络辅助的拥塞控制: 在网络辅助的拥塞控制中，路由器会向发送方提供关于网络中拥塞状态的反馈。这种反馈信息就是一个比特信息，它指示链路中的拥塞情况。 拥塞的特点： 1、网络拥塞是由网络资源中的短板资源所决定的，只有所有类型的网络资源同时提高供给才会真正改善网络性能（例如你提高了带宽，但是路由器的缓存较小，瓶颈就转移到了缓存那里）。 2、拥塞趋于恶化，例如某个路由器没有足够的缓存，缓存溢出导致丢包和端系统重传，一旦重传又会加重网络拥塞。 3、拥塞的直接表现就是丢包和重传，当端系统的重传次数明显增加，就表明网络很可能发生了拥塞。举个例子：如果是带宽出现瓶颈，则RTT会增加，导致超时重传；如果是路由器缓存瓶颈，分组到达路由器后因缓存溢出而丢包，又会导致超时重传。因此重传就是拥塞的表现。 拥塞的其他指标（了解即可）： • 由于缺少缓存空间⽽被丢弃的分组的百分数； • 平均队列⻓度； • 超时重传的分组数； • 平均分组时延； • 分组时延的标准差，等等。 简单的记就是：丢包率、重传率和时延。拥塞控制是防⽌过多的数据注⼊到⽹络中，使⽹络中的路由器或 链路不致过载。 拥塞控制和流量控制的区别 流量控制是解决端与端的发送与接收速率不匹配的问题，需要发送方同步接收方的接收速度； 拥塞控制是解决端系统的通信量与网络链路资源不匹配引起的路由器和链路过载问题，需要控制端系统注入到网络的数据量和速度。 拥塞控制的方法 拥塞控制需要解决的三个问题 TCP 发送方如何限制它向其他连接发送报文段的速率呢？ TCP是由接收缓存、发送缓存等组成。发送方的TCP拥塞控制机制会跟踪一个变量，即拥塞窗口的变量，拥塞窗口表示为cwnd, 用于限制TCP在接收到ACK之前可以发送到网络的数据量，而接收窗口是用来告诉接收方能够接受的数据量 一般来说，发送方未确认的数据量不得超过 cwnd 和 rwnd 的最小值，也就是LastByteSent−LastByteAcked\u003c=min(cwnd,rwnd) 由于每个数据包的往返时间是RTT，我们假设接收端有足够的缓存空间用于接收数据，我们就不用考虑rwnd了，只用专注于cwnd，那么，该发送方的 发送速率 = cwnd/RTT 字节/秒 . 通过调节cwnd，发送方因此能调整它向连接发送数据的速率。 一个 TCP 发送方是如何感知到网络拥塞的呢？ TCP 根据超时或者 3 个冗余 ACK(丢包了) 来感知的。这就是TCP的快速重传机制。 当发送方感知到端到端的拥塞时，采用何种算法来改变其发送速率呢？ 慢开始、拥塞避免、快重传和快恢复。这4种方法的基本思路是，只要网络没有拥塞，拥塞窗口就可以增大些，出现拥塞就减小些。 慢开始算法（慢启动） 慢开始的思路是从小到大以指数方式增加拥塞窗口的数值。慢开始发生在刚建立连接后的数据收发。 一开始发送方并不清楚网络的拥塞情况，就先将cwnd初始值设置为1~2个SMSS（发送方MSS），新的RFC标准则把初始cwnd设置为2~4，至于取2还是3还是4，取决于SMSS有多大。 在每收到⼀个对新的报⽂段的确认（重传的确认不算）后，发送方的拥塞窗⼝就增加⼀个 SMSS 的数值，因此cwnd会呈指数级别增长。 慢启动的窗口增长速度其实不慢（指数级别），之所以叫慢启动是因为它的初始cwnd值很小。 顺带一提 ，新建立的连接会用到慢启动，TCP 还实现了 慢启动重启 （SSR） 机制。这种机制会在持久连接空闲一定时间后重置拥塞窗口为初始cwnd值。道理很简单， 在连接空闲的同时，网络状况也可能发生了变化，为了避免拥塞，理应将拥塞窗 口重置回“安全的”默认值。 为了不让窗口无限的指数增长，提出了慢开始门限，当窗口大小超过了慢开始门限 ssthresh 则使用拥塞避免算法线性增长窗口。 慢开始⻔限 ssthresh ： 当 cwnd \u003c ssthresh 时，使⽤慢开始算法； 当 cwnd \u003e= ssthresh 时，停⽌使⽤慢开始算法⽽改⽤拥塞避免算法； 拥塞避免算法 该算法是指：当cwnd超过慢开始门限后，每经过一个RTT，拥塞窗口就线性增长 cwnd = cwnd + 1。 快速重传算法 该算法是指：如果发送方连续收到3个重复ack号的确认，说明接收方收到了乱序的报文（某个中间报文丢失或者迟到），发送方会立即进行重传，而不是等到超时时间用完才重传，避免发送方误认为发生了网络拥塞。 中间报文的丢失或迟到极可能是意外丢失或迟到，而不是因为网络拥塞导致的丢失，但不排除拥塞的可能性。快速重传可以使网络的吞吐量提高20%。 快恢复算法 拥塞惩罚是指端系统检测到网络拥塞时（即发生重传时），降低自己cwnd窗口的行为。拥塞惩罚按超时重传和快速重传分为两种惩罚方式： 当发生超时重传时，发送方会认为网络出现拥塞，拥塞窗口cwnd会变成1。 当发生快速重传时，该分组很可能是意外丢失或迟到，但不排除拥塞的可能，因此cwnd会变为 cwnd/2。 快恢复算法是指当发生快速重传时，当前拥塞窗口大小减小一半，之后直接执行拥塞避免算法线性增长cwnd，而不是执行慢开始算法指数增长cwnd。 下面是TCP拥塞控制流程图： 拥塞惩罚机制 整个拥塞惩罚机制逻辑如下： 超时重传的情况下： • 慢开始⻔限 ssthresh = max(cwnd/2，2)； • cwnd = 1； • 执⾏慢开始算法。 快速重传的情况下（快速恢复）： • 慢开始⻔限 ssthresh = 当前拥塞窗⼝ cwnd / 2 ； • 新拥塞窗⼝ cwnd = 慢开始⻔限 ssthresh ； • 开始执⾏拥塞避免算法，使拥塞窗⼝缓慢地线性增⼤。 无论是超时重传还是快速重传，都会导致慢开始门限减半，这会导致多次惩罚后，不再会执行指数增长，而是全变成线性增长。 TCP拥塞控制动态流程图 主动队列管理 AQM 对TCP拥塞控制影响最大的网络层策略是分组丢弃策略。该策略的内容为，到达路由器的分组会按先进先出原则放入到缓存队列中，一旦队列已满，后到达的分组会被丢弃。 这种丢弃策略会导致一连串分组的丢失和超时重传，这一方向的所有TCP连接都进入慢开始状态，这种情况叫做全局同步，全局同步会导致通信量突然下降，不一会儿通信量又突然增大（因为报文指数增长）。 为了避免全局同步，我们可以在队列长度到达某个警戒线时主动丢弃部分分组，而不是在分组数量达到最大队列长度时被动丢弃所有分组，这就是主动队列管理AQM。 AQM有不同的实现方式，比较主流的是随机早期检测RED。 RED规定路由器维持一个最小门限THmin和最大门限THmax。 队列⻓度L ⼩于最⼩⻔限 THmin，将新到达的分组放⼊队 列进⾏排队； 队列⻓度L 超过最⼤⻔限 THmax，将新到达的分组丢弃； 队列⻓度L 在最⼩⻔限 THmin 和最⼤⻔限 THmax 之间，按 照概率 p 丢弃新到达的分组。而且随着队列长度L的增加，p也会变大。 ","date":"2023-07-11","objectID":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E8%BF%90%E8%BE%93%E5%B1%82/:3:9","tags":[],"title":"计算机网络之运输层","uri":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E8%BF%90%E8%BE%93%E5%B1%82/"},{"categories":["计算机网络"],"content":"连接管理 TCP连接要解决3个问题： 使对方知道自己的存在，且确认双方能发送能接收； 允许双方协商一些参数（如最大窗口值，是否用窗口扩大选项和时间戳选项等）； 分配运输资源（缓存大小，连接表中的项目）； 三次握手建立连接 需要3个报文： 下面是连接的状态变化 在连接之前，A和B会先创建传输控制块TCB，存储了连接相关的重要信息如 TCP连接表，指向发送和接收缓存的指针，指向重传队列的指针，seq和ack等。 SYN不能携带数据，但需要消耗一个序号。ACK报文是可以携带数据的。 为什么建立连接是三次握手而不是两次？ 是为了防止已失效的连接请求报文段传到了B产生错误。具体情境如下： A 发送的SYN报文丢失，A又重发了一个SYN报文，并建立连接成功，后来关闭了连接，通信结束。但丢失的SYN报文此时到达B，B发送第二次握手的报文（ACK报文）后直接进入连接状态，而A没有发起建立连接的请求，不会理睬这个ACK报文，但B会一直等待A发送数据报文段过来，B的资源白白浪费。 四次挥手断开连接 需要注意： 通信双方都可以主动发起关闭连接的请求报文。 FIN可以携带数据，但如果不携带数据也会消耗一个序号。 当被动关闭者B进入CLOSE-WAIT状态时，TCP连接处于半关闭状态，此时B可以发送数据，A无法发送数据，但可以接收数据。因此这个状态下，B可能还会继续发送消息给A。 A进入TIME-WAIT状态后，必须经过时间等待计时器设置的时间2MSL后才能进入CLOSED状态。 一个问题：A 为什么必须等待 2MSL （MSL是最长报文段寿命）的时间后才真正关闭连接？ 1、防止第四次挥手的ACK丢失后B无法进入CLOSED状态。 假设A在第三次挥手之后直接进入CLOSED，而且最后一个ACK丢失，B会重发第三次挥手，假设A之前的端口是X，这时有两个情况：一个是A之前的端口又开始建立新的连接，那么A收到该FIN报文之后，会回应一个RST报文给B；一个是A之前的端口没有再开启过了，那么B的FIN报文不会得到ACK回应，B会不停的重传。 2、保证本次连接产生的所有报文（FIN、SYN和数据报文）在这2MSL内从网络中消失，不会和新连接的报文发生混淆（尤其是新连接和旧连接的客户端端口是相同的情况下）。 TCP半关闭 半关闭是指建立连接的两端只有其中一端发送FIN报文，关闭双向连接的某一个方向。主动发送FIN的一端之后就无法向对端发送数据，只能接受对端发送的数据和发送ACK报文段。 一端发送FIN报文之后，另一端发送FIN报文之前的连接状态称为“半关闭状态”。 套接字的close()提供了全关闭操作，而shutdown()则提供了半关闭操作，实际应用中半关闭很少用到。 TCP同时打开与关闭 同时打开是指通信双方A和B，A发送SYN报文给B，并在报文段到达B之前，B也发送SYN报文给A。同时打开只会出现在A和B都是服务器端的情况下。 连接建立超时 如果一个客户端发起连接请求时，服务器是关闭的，那么客户端会在连接等待超时后再重新发送SYN报文，并且每次超时，超时时间都会翻倍。这一行为被称为指数回退。 在Linux中net.ipv4.tcp_syn_retries参数可以配置重发SYN的次数，而net.ipv4.tcp_synack_retries则是第二次握手的SYN报文的重发次数。这两个参数通常选择一个较小值5。 TCP有限状态机 ","date":"2023-07-11","objectID":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E8%BF%90%E8%BE%93%E5%B1%82/:3:10","tags":[],"title":"计算机网络之运输层","uri":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E8%BF%90%E8%BE%93%E5%B1%82/"},{"categories":["计算机网络"],"content":"计时器 超时重传计时器：略； 零窗口持续计时器：零窗口时发送探测报文的计时器； Time-Wait计时器：time-wait等待2MSL的计时器； 保活计时器：防止TCP连接长时间空闲； 发送报文计时器：防止发送方长时间没有发送报文； PS: 保活计时器⽤来防⽌在TCP连接出现⻓时期的空闲以及判断对方是否故障下线。 保活计时器 通常设置为2⼩时 。若服务器过了2⼩时还没 有收到客户的信息，它就发送探测报⽂段。若发送了10个 探测报⽂段（每⼀个相隔75秒）还没有响应，就假定客户 出了故障，因⽽就终⽌该连接。 ","date":"2023-07-11","objectID":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E8%BF%90%E8%BE%93%E5%B1%82/:3:11","tags":[],"title":"计算机网络之运输层","uri":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E8%BF%90%E8%BE%93%E5%B1%82/"},{"categories":["计算机网络"],"content":"[toc] 4.1、网络层概述 ","date":"2023-06-23","objectID":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E7%BD%91%E7%BB%9C%E5%B1%82/:0:0","tags":[],"title":"计算机网络之网络层","uri":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E7%BD%91%E7%BB%9C%E5%B1%82/"},{"categories":["计算机网络"],"content":"简介 网络层的主要任务是实现网络互连，进而实现数据包在各网络之间的传输 这些异构型网络N1~N7如果只是需要各自内部通信，他们只要实现各自的物理层和数据链路层即可 但是如果要将这些异构型网络互连起来，形成一个更大的互联网，就需要实现网络层设备路由器 有时为了简单起见，可以不用画出这些网络，图中N1~N7，而将他们看做是一条链路即可 要实现网络层任务，需要解决以下主要问题： 网络层向运输层提供怎样的服务（“可靠传输”还是“不可靠传输”） 在数据链路层那课讲过的可靠传输，详情可以看那边的笔记：网络层对以下的分组丢失、分组失序、分组重复的传输错误采取措施，使得接收方能正确接受发送方发送的数据，就是可靠传输，反之，如果什么措施也不采取，则是不可靠传输 网络层寻址问题 如何对不同的主机或设备进行唯一的标识和寻址。 路由选择问题 路由器收到数据后，是依据什么来决定将数据包从自己的哪个接口转发出去？ 依据数据包的目的地址和路由器中的路由表 但在实际当中，路由器是怎样知道这些路由记录？ 由用户或网络管理员进行人工配置，这种方法只适用于规模较小且网络拓扑不改变的小型互联网 另一种是实现各种路由选择协议，由路由器执行路由选择协议中所规定的路由选择算法，而自动得出路由表中的路由记录，这种方法更适合规模较大且网络拓扑经常改变的大型互联网 补充 网络层（网际层）除了 IP协议外，还有之前介绍过的地址解析协议ARP，还有网际控制报文协议ICMP，网际组管理协议IGMP ARP协议应该不是网际层，而是属于网络接口层（相当于OSI七层参考模型中的数据链路层和物理层，而ARP协议属于数据链路层） ","date":"2023-06-23","objectID":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E7%BD%91%E7%BB%9C%E5%B1%82/:1:0","tags":[],"title":"计算机网络之网络层","uri":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E7%BD%91%E7%BB%9C%E5%B1%82/"},{"categories":["计算机网络"],"content":"总结 4.2、网络层提供的两种服务 在计算机网络领域，网络层应该向运输层提供怎样的服务（“面向连接”还是“无连接”）曾引起了长期的争论。 争论焦点的实质就是：在计算机通信中，可靠交付应当由谁来负责？是网络还是端系统？ ","date":"2023-06-23","objectID":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E7%BD%91%E7%BB%9C%E5%B1%82/:2:0","tags":[],"title":"计算机网络之网络层","uri":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E7%BD%91%E7%BB%9C%E5%B1%82/"},{"categories":["计算机网络"],"content":"面向连接的虚电路服务 一种观点：让网络负责可靠交付 这种观点认为，应借助于电信网的成功经验，让网络负责可靠交付，计算机网络应模仿电信网络，使用面向连接的通信方式。 通信之前先建立虚电路 (Virtual Circuit)，以保证双方通信所需的一切网络资源。 如果再使用可靠传输的网络协议，就可使所发送的分组无差错按序到达终点，不丢失、不重复。 发送方 发送给 接收方 的所有分组都沿着同一条虚电路传送 虚电路表示这只是一条逻辑上的连接，分组都沿着这条逻辑连接按照存储转发方式传送，而并不是真正建立了一条物理连接。 请注意，电路交换的电话通信是先建立了一条真正的物理连接。 因此分组交换的虚连接和电路交换的连接只是类似，但并不完全一样： 虚电路则是一种建立逻辑通信通道的方式，通信双方在通信前需要建立一条虚电路，它会预留一定的资源，但这些资源在通信过程中并不一直被占用，而是根据实际通信需要进行动态分配。因此，虚电路的通信质量较电路交换略低，但资源利用率更高，且适应通信量变化较大的场景。 ","date":"2023-06-23","objectID":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E7%BD%91%E7%BB%9C%E5%B1%82/:3:0","tags":[],"title":"计算机网络之网络层","uri":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E7%BD%91%E7%BB%9C%E5%B1%82/"},{"categories":["计算机网络"],"content":"无连接的数据报服务 另一种观点：网络提供数据报服务 如果网络层提供可靠的传输服务，也即采用第一种观点（建立面向连接的虚电路服务），会导致线路被占用（尽管虚电路建立的逻辑电路连接，不一定一直占用线路资源，但还是存在占用情况），资源利用率下降。互联网的先驱者提出了一种崭新的网络设计思路。 网络层向上只提供简单灵活的、无连接的、尽最大努力交付的数据报服务。 网络在发送分组时不需要先建立连接。每一个分组（即 IP 数据报）独立发送，与其前后的分组无关（不进行编号）。 网络层不提供服务质量的承诺。即所传送的分组可能出错、丢失、重复和失序（不按序到达终点），当然也不保证分组传送的时限。 发送方 发送给 接收方 的分组可能沿着不同路径传送 尽最大努力交付 如果主机（即端系统）中的进程之间的通信需要是可靠的，那么就由网络的主机中的运输层负责可靠交付（包括差错处理、流量控制等） 。 采用这种设计思路的好处是：网络的造价大大降低，运行方式灵活，能够适应多种应用。 互联网能够发展到今日的规模，充分证明了当初采用这种设计思路的正确性。 ","date":"2023-06-23","objectID":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E7%BD%91%E7%BB%9C%E5%B1%82/:4:0","tags":[],"title":"计算机网络之网络层","uri":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E7%BD%91%E7%BB%9C%E5%B1%82/"},{"categories":["计算机网络"],"content":"虚电路服务与数据报服务的对比 对比的方面 虚电路服务 数据报服务 思路 可靠通信应当由网络来保证 可靠通信应当由用户主机（即端系统）来保证 连接的建立 必须有 不需要 终点地址 仅在连接建立阶段使用，后续发送的每个分组使用短的虚电路号 每个分组都有终点的完整地址 分组的转发 属于同一条虚电路的分组均按照同一路由进行转发 每个分组可以选择不同的路由进行转发 当结点出故障时 所有含有故障结点的虚电路均不能工作，因为虚电路断掉了 出故障的结点可能会丢失分组。可以选择其他节点继续转发路由 分组的顺序 总是按发送顺序到达终点 到达终点时不一定按发送顺序 端到端的差错处理和流量控制 可以由网络负责，也可以由用户主机负责 由用户主机负责，网络只是负责简单快捷的路由 4.3、IPv4 ","date":"2023-06-23","objectID":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E7%BD%91%E7%BB%9C%E5%B1%82/:5:0","tags":[],"title":"计算机网络之网络层","uri":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E7%BD%91%E7%BB%9C%E5%B1%82/"},{"categories":["计算机网络"],"content":"概述 ","date":"2023-06-23","objectID":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E7%BD%91%E7%BB%9C%E5%B1%82/:6:0","tags":[],"title":"计算机网络之网络层","uri":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E7%BD%91%E7%BB%9C%E5%B1%82/"},{"categories":["计算机网络"],"content":"分类编制的IPv4地址 ","date":"2023-06-23","objectID":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E7%BD%91%E7%BB%9C%E5%B1%82/:7:0","tags":[],"title":"计算机网络之网络层","uri":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E7%BD%91%E7%BB%9C%E5%B1%82/"},{"categories":["计算机网络"],"content":"简介 每一类地址都由两个固定长度的字段组成，其中一个字段是网络号 net-id，它标志主机（或路由器）所连接到的网络，而另一个字段则是主机号 host-id，它标志该主机（或路由器）。 主机号为全0的地址表示该子网的网络地址，不特指某个主机IP地址，而是指该子网的地址 主机号为全1的地址和MAC地址为全1的地址一样都是属于广播地址，不能分配给主机或路由器 主机号在它前面的网络号所指明的网络范围内必须是唯一的。 由此可见，一个 IP 地址在整个互联网范围内是唯一的。 A类地址 本地回环地址不仅仅只是127.0.0.1，是一个范围127.0.0.1到127.255.255.254。这些本地回环地址可以被视为是代表自身主机的某个永远不会宕掉的虚拟接口，常常用来测试本机的IP协议安装和网卡是否有问题。 所以，如果需要对本机的某些资源进行访问，我们可以直接访问127.0.0.1到127.255.255.254中的任何一个地址都可以。 如果本机是服务器，当然也可以通过公网IP进行外部访问，这样做虽然可以，但是多了互联网上的路由成本。 B类地址 C类地址 练习 ","date":"2023-06-23","objectID":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E7%BD%91%E7%BB%9C%E5%B1%82/:7:1","tags":[],"title":"计算机网络之网络层","uri":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E7%BD%91%E7%BB%9C%E5%B1%82/"},{"categories":["计算机网络"],"content":"总结 IP 地址的指派范围 实际上，128.0和192.0.0都可以指派了，这才是第一个可以指派的网络号 一般不使用的特殊的 IP 地址 IP 地址的一些重要特点 (1) IP 地址是一种分等级的地址结构。分两个等级的好处是： 第一，IP 地址管理机构在分配 IP 地址时只分配网络号，而剩下的主机号则由得到该网络号的单位自行分配。这样就方便了 IP 地址的管理。 第二，路由器仅根据目的主机所连接的网络号来转发分组（而不考虑目的主机号），这样就可以使路由表中的项目数大幅度减少，从而减小了路由表所占的存储空间。 (2) 实际上 IP 地址是标志一个主机（或路由器）和一条链路的接口。 当一个主机同时连接到两个网络上时，该主机就必须同时具有两个相应的 IP 地址，其网络号 net-id 必须是不同的。这种主机称为多归属主机 (multihomed host)。 由于一个路由器至少应当连接到两个网络（这样它才能将 IP 数据报从一个网络转发到另一个网络），因此一个路由器至少应当有两个不同的 IP 地址。 (3) 用转发器或网桥连接起来的若干个局域网仍为一个网络，因此这些局域网都具有同样的网络号 net-id。 (4) 所有分配到网络号 net-id 的网络，无论是范围很小的局域网，还是可能覆盖很大地理范围的广域网，都是平等的。 ","date":"2023-06-23","objectID":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E7%BD%91%E7%BB%9C%E5%B1%82/:7:2","tags":[],"title":"计算机网络之网络层","uri":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E7%BD%91%E7%BB%9C%E5%B1%82/"},{"categories":["计算机网络"],"content":"划分子网的IPv4地址 ","date":"2023-06-23","objectID":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E7%BD%91%E7%BB%9C%E5%B1%82/:8:0","tags":[],"title":"计算机网络之网络层","uri":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E7%BD%91%E7%BB%9C%E5%B1%82/"},{"categories":["计算机网络"],"content":"为什么要划分子网 在 ARPANET 的早期，IP 地址的设计确实不够合理： IP 地址空间的利用率有时很低，而且划分的网络号可能支持的主机很多 一个网络里的主机太多，就会导致广播的流量压力增大（ARP协议就需要使用广播） 给每一个物理网络分配一个网络号会使路由表变得太大因而使网络性能变坏 一个网络号能提供寻址的主机可能会很多，那么路由表就会较大 两级的 IP 地址不够灵活。 如果想要将原来的网络划分成三个独立的网路 所以是否可以从主机号部分借用一部分作为子网号 但是如果未在图中标记子网号部分，那么我们和计算机又如何知道分类地址中主机号有多少比特被用作子网号了呢？ 所以就有了划分子网的工具：子网掩码 从 1985 年起在 IP 地址中又增加了一个“子网号字段”，使两级的 IP 地址变成为三级的 IP 地址。 这种做法叫做划分子网 (subnetting) 。 划分子网已成为互联网的正式标准协议。 ","date":"2023-06-23","objectID":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E7%BD%91%E7%BB%9C%E5%B1%82/:8:1","tags":[],"title":"计算机网络之网络层","uri":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E7%BD%91%E7%BB%9C%E5%B1%82/"},{"categories":["计算机网络"],"content":"如何划分子网 基本思路 划分子网纯属一个单位内部的事情。单位对外仍然表现为没有划分子网的网络。 从主机号借用若干个位作为子网号 subnet-id，而主机号 host-id 也就相应减少了若干个位。 凡是从其他网络发送给本单位某个主机的 IP 数据报，仍然是根据 IP 数据报的目的网络号 net-id，先找到连接在本单位网络上的路由器。 然后此路由器在收到 IP 数据报后，再按目的网络号 net-id 和子网号 subnet-id 找到目的子网。 最后就将 IP 数据报直接交付目的主机。 划分为三个子网后对外仍是一个网络 优点 划分子网时，减少了 IP 地址的浪费 减少广播域 控制流量 优化路由 提高安全性 使网络的组织更加灵活 更便于维护和管理 划分子网纯属一个单位内部的事情，对外部网络透明，对外仍然表现为没有划分子网的一个网络。 ","date":"2023-06-23","objectID":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E7%BD%91%E7%BB%9C%E5%B1%82/:8:2","tags":[],"title":"计算机网络之网络层","uri":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E7%BD%91%E7%BB%9C%E5%B1%82/"},{"categories":["计算机网络"],"content":"子网掩码 (IP 地址) AND (子网掩码) = 网络地址 重要，下面很多相关知识都会用到 举例 例子1 例子2 默认子网掩码 可以看得出：使用默认子网掩码时，也就是说没有划分子网。 ","date":"2023-06-23","objectID":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E7%BD%91%E7%BB%9C%E5%B1%82/:8:3","tags":[],"title":"计算机网络之网络层","uri":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E7%BD%91%E7%BB%9C%E5%B1%82/"},{"categories":["计算机网络"],"content":"总结 子网掩码是一个网络或一个子网的重要属性。 路由器在和相邻路由器交换路由信息时，必须把自己所在网络（或子网）的子网掩码告诉相邻路由器。 路由器的路由表中的每一个项目，除了要给出目的网络地址外，还必须同时给出该网络的子网掩码。 若一个路由器连接在两个子网上，就拥有两个网络地址和两个子网掩码。 ","date":"2023-06-23","objectID":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E7%BD%91%E7%BB%9C%E5%B1%82/:8:4","tags":[],"title":"计算机网络之网络层","uri":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E7%BD%91%E7%BB%9C%E5%B1%82/"},{"categories":["计算机网络"],"content":"无分类编址的IPv4地址 ","date":"2023-06-23","objectID":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E7%BD%91%E7%BB%9C%E5%B1%82/:9:0","tags":[],"title":"计算机网络之网络层","uri":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E7%BD%91%E7%BB%9C%E5%B1%82/"},{"categories":["计算机网络"],"content":"为什么使用无分类编址 无分类域间路由选择 CIDR (Classless Inter-Domain Routing)。 CIDR 最主要的特点 CIDR使用各种长度的“网络前缀”(network-prefix)来代替分类地址中的网络号和子网号。 IP 地址从三级编址（使用子网掩码）又回到了两级编址。 ","date":"2023-06-23","objectID":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E7%BD%91%E7%BB%9C%E5%B1%82/:9:1","tags":[],"title":"计算机网络之网络层","uri":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E7%BD%91%E7%BB%9C%E5%B1%82/"},{"categories":["计算机网络"],"content":"如何使用无分类编址 举例 ","date":"2023-06-23","objectID":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E7%BD%91%E7%BB%9C%E5%B1%82/:9:2","tags":[],"title":"计算机网络之网络层","uri":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E7%BD%91%E7%BB%9C%E5%B1%82/"},{"categories":["计算机网络"],"content":"路由聚合（构造超网） ","date":"2023-06-23","objectID":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E7%BD%91%E7%BB%9C%E5%B1%82/:9:3","tags":[],"title":"计算机网络之网络层","uri":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E7%BD%91%E7%BB%9C%E5%B1%82/"},{"categories":["计算机网络"],"content":"总结 ","date":"2023-06-23","objectID":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E7%BD%91%E7%BB%9C%E5%B1%82/:9:4","tags":[],"title":"计算机网络之网络层","uri":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E7%BD%91%E7%BB%9C%E5%B1%82/"},{"categories":["计算机网络"],"content":"IPv4地址的应用规划 给定一个IPv4地址块，如何将其划分成几个更小的地址块，并将这些地址块分配给互联网中不同网络，进而可以给各网络中的主机和路由器接口分配IPv4地址 ","date":"2023-06-23","objectID":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E7%BD%91%E7%BB%9C%E5%B1%82/:10:0","tags":[],"title":"计算机网络之网络层","uri":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E7%BD%91%E7%BB%9C%E5%B1%82/"},{"categories":["计算机网络"],"content":"定长的子网掩码FLSM（Fixed Length Subnet Mask） 划分子网的IPv4就是定长的子网掩码 举例 通过上面步骤分析，就可以从子网1~8中任选5个分配给左图中的N1~N5 采用定长的子网掩码划分，只能划分出2^n^个子网，其中n是从主机号部分借用的用来作为子网号的比特数量，每个子网所分配的IP地址数量相同 但是也因为每个子网所分配的IP地址数量相同，不够灵活，容易造成IP地址的浪费 ","date":"2023-06-23","objectID":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E7%BD%91%E7%BB%9C%E5%B1%82/:10:1","tags":[],"title":"计算机网络之网络层","uri":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E7%BD%91%E7%BB%9C%E5%B1%82/"},{"categories":["计算机网络"],"content":"变长的子网掩码VLSM（Variable Length Subnet Mask） 无分类编址的IPv4就是变长的子网掩码 举例 IPv6 4.4、IP数据报的发送和转发过程 举例 源主机如何知道目的主机是否与自己在同一个网络中，是直接交付，还是间接交付？ 可以通过目的IP地址和目的地址的子网掩码进行逻辑与运算得到目的网络地址 如果目的网络地址和源网络地址 相同，就是在同一个网络中，属于直接交付 如果目的网络地址和源网络地址 不相同，就不在同一个网络中，属于间接交付，传输给主机所在网络的默认网关（路由器——下图会讲解）,由默认网关帮忙转发 主机C如何知道路由器R的存在？ 用户为了让本网络中的主机能和其他网络中的主机进行通信，就必须给其指定本网络的一个路由器的接口，由该路由器帮忙进行转发，所指定的路由器，也被称为默认网关 例如：路由器的接口0的IP地址192.168.0.126做为左边网络的默认网关 主机A会将该IP数据报传输给自己的默认网关，也就是图中所示的路由器接口0 路由器收到IP数据报后如何转发？ 检查IP数据报首部是否出错： 若出错，则直接丢弃该IP数据报并通告源主机 若没有出错，则进行转发 根据IP数据报的目的地址在路由表中查找匹配的条目： 若找到匹配的条目，则转发给条目中指示的下一跳（也就是找到从路由器的哪个接口出去） 若找不到，则丢弃该数据报并通告源主机 假设IP数据报首部没有出错，路由器取出IP数据报首部各地址字段的值 接下来路由器对该IP数据报进行查表转发 逐条检查路由条目，将目的地址与路由条目中的地址掩码进行逻辑与运算得到目的网络地址，然后与路由条目中的目的网络进行比较，如果相同，则这条路由条目就是匹配的路由条目，按照它的下一条指示，图中所示的也就是接口1转发该IP数据报 路由器是隔离广播域的 4.5、静态路由配置及其可能产生的路由环路问题 ","date":"2023-06-23","objectID":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E7%BD%91%E7%BB%9C%E5%B1%82/:10:2","tags":[],"title":"计算机网络之网络层","uri":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E7%BD%91%E7%BB%9C%E5%B1%82/"},{"categories":["计算机网络"],"content":"概念 ","date":"2023-06-23","objectID":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E7%BD%91%E7%BB%9C%E5%B1%82/:11:0","tags":[],"title":"计算机网络之网络层","uri":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E7%BD%91%E7%BB%9C%E5%B1%82/"},{"categories":["计算机网络"],"content":"多种情况举例 ","date":"2023-06-23","objectID":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E7%BD%91%E7%BB%9C%E5%B1%82/:12:0","tags":[],"title":"计算机网络之网络层","uri":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E7%BD%91%E7%BB%9C%E5%B1%82/"},{"categories":["计算机网络"],"content":"静态路由配置 ","date":"2023-06-23","objectID":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E7%BD%91%E7%BB%9C%E5%B1%82/:12:1","tags":[],"title":"计算机网络之网络层","uri":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E7%BD%91%E7%BB%9C%E5%B1%82/"},{"categories":["计算机网络"],"content":"默认路由 IPv4地址0.0.0.0/0在路由器中的目的网络指的就是所有网络。默认路由可以被所有网络匹配，但路由匹配有优先级，默认路由是优先级最低的 ","date":"2023-06-23","objectID":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E7%BD%91%E7%BB%9C%E5%B1%82/:12:2","tags":[],"title":"计算机网络之网络层","uri":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E7%BD%91%E7%BB%9C%E5%B1%82/"},{"categories":["计算机网络"],"content":"特定主机路由 有时候，我们可以给路由器添加针对某个主机的特定主机路由条目，一般用于网络管理人员对网络的管理和测试 多条路由可选，匹配路由最具体的 ","date":"2023-06-23","objectID":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E7%BD%91%E7%BB%9C%E5%B1%82/:12:3","tags":[],"title":"计算机网络之网络层","uri":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E7%BD%91%E7%BB%9C%E5%B1%82/"},{"categories":["计算机网络"],"content":"静态路由配置错误导致路由环路 假设将R2的路由表中第三条目录配置错了下一跳。这导致R2和R3之间产生了路由环路 ","date":"2023-06-23","objectID":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E7%BD%91%E7%BB%9C%E5%B1%82/:12:4","tags":[],"title":"计算机网络之网络层","uri":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E7%BD%91%E7%BB%9C%E5%B1%82/"},{"categories":["计算机网络"],"content":"聚合了不存在的网络而导致路由环路 正常情况 错误情况 解决方法 黑洞路由的下一跳为null0，这是路由器内部的虚拟接口，IP数据报进入它后就被丢弃 ","date":"2023-06-23","objectID":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E7%BD%91%E7%BB%9C%E5%B1%82/:12:5","tags":[],"title":"计算机网络之网络层","uri":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E7%BD%91%E7%BB%9C%E5%B1%82/"},{"categories":["计算机网络"],"content":"网络故障而导致路由环路 解决方法 添加故障的网络为黑洞路由 假设。一段时间后故障网络恢复了 R1又自动地得出了其接口0的直连网络的路由条目 针对该网络的黑洞网络会自动失效 如果又故障 则生效该网络的黑洞网络 ","date":"2023-06-23","objectID":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E7%BD%91%E7%BB%9C%E5%B1%82/:12:6","tags":[],"title":"计算机网络之网络层","uri":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E7%BD%91%E7%BB%9C%E5%B1%82/"},{"categories":["计算机网络"],"content":"总结 4.6、路由选择协议 ","date":"2023-06-23","objectID":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E7%BD%91%E7%BB%9C%E5%B1%82/:13:0","tags":[],"title":"计算机网络之网络层","uri":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E7%BD%91%E7%BB%9C%E5%B1%82/"},{"categories":["计算机网络"],"content":"概述 因特网所采用的路由选择协议的主要特点 因特网采用分层次的路由选择协议 ","date":"2023-06-23","objectID":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E7%BD%91%E7%BB%9C%E5%B1%82/:14:0","tags":[],"title":"计算机网络之网络层","uri":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E7%BD%91%E7%BB%9C%E5%B1%82/"},{"categories":["计算机网络"],"content":"自治系统 AS 在单一的技术管理下的一组路由器，而这些路由器使用一种 AS 内部的路由选择协议和共同的度量以确定分组在该 AS 内的路由； 同时还使用一种 AS 之间的路由选择协议用以确定分组在 AS 之间的路由。 自治系统之间的路由选择简称为域间路由选择，自治系统内部的路由选择简称为域内路由选择 域间路由选择使用外部网关协议EGP这个类别的路由选择协议 域内路由选择使用内部网关协议IGP这个类别的路由选择协议 网关协议的名称可称为路由协议 ","date":"2023-06-23","objectID":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E7%BD%91%E7%BB%9C%E5%B1%82/:14:1","tags":[],"title":"计算机网络之网络层","uri":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E7%BD%91%E7%BB%9C%E5%B1%82/"},{"categories":["计算机网络"],"content":"常见的路由选择协议 ","date":"2023-06-23","objectID":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E7%BD%91%E7%BB%9C%E5%B1%82/:14:2","tags":[],"title":"计算机网络之网络层","uri":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E7%BD%91%E7%BB%9C%E5%B1%82/"},{"categories":["计算机网络"],"content":"路由器的基本结构 路由器是一种具有多个输入输出端口的专用计算机，其任务是转发分组 路由器结构可划分为两大部分： 分组转发部分 由三部分构成：交换 交换结构 一组输入端口： 1、信号从某个输入端口进入路由器 2、物理层将信号转换成比特流，送交数据链路层处理 3、数据链路层识别从比特流中识别出帧，去掉帧头和帧尾后，送交网络层处理 4、如果送交网络层的分组是普通待转发的数据分组 5、则根据分组首部中的目的地址进行查表转发 6、若找不到匹配的转发条目，则丢弃该分组；否则，按照匹配条目中所指示的端口进行转发 一组输出端口 1、网络层更新数据分组首部中某些字段的值，例如将数据分组的生存时间减1，然后送交数据链路层进行封装 2、数据链路层将数据分组封装成帧，交给物理层处理 3、物理层将帧看成比特流将其变换成相应的电信号进行发送 路由器的各端口还会有输入缓冲区和输出缓冲区 输入缓冲区用来暂存新进入路由器但还来不及处理的分组 输出缓冲区用来暂存已经处理完毕但还来不及发送的分组 路由器的端口一般都具有输入和输出功能，这些实例分出了输入端口和输出端口是更好演示路由基本工作过程 路由选择部分 路由选择部分的核心构件是路由选择处理机，它的任务是根据所使用的路由选择协议周期性地与其他路由器进行路由信息的交互，来更新路由表 ，如果送交给输入端口的网络层的分组是路由器之间交换路由信息的路由报文，则把这种分组送交给路由选择处理机 路由选择处理机根据分组的内容来更新自己的路由表 路由选择处理机还会周期性地给其他路由器发送自己所知道的路由信息 ","date":"2023-06-23","objectID":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E7%BD%91%E7%BB%9C%E5%B1%82/:14:3","tags":[],"title":"计算机网络之网络层","uri":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E7%BD%91%E7%BB%9C%E5%B1%82/"},{"categories":["计算机网络"],"content":"路由信息协议RIP ","date":"2023-06-23","objectID":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E7%BD%91%E7%BB%9C%E5%B1%82/:15:0","tags":[],"title":"计算机网络之网络层","uri":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E7%BD%91%E7%BB%9C%E5%B1%82/"},{"categories":["计算机网络"],"content":"距离向量 ","date":"2023-06-23","objectID":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E7%BD%91%E7%BB%9C%E5%B1%82/:15:1","tags":[],"title":"计算机网络之网络层","uri":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E7%BD%91%E7%BB%9C%E5%B1%82/"},{"categories":["计算机网络"],"content":"RIP的基本工作过程 ","date":"2023-06-23","objectID":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E7%BD%91%E7%BB%9C%E5%B1%82/:15:2","tags":[],"title":"计算机网络之网络层","uri":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E7%BD%91%E7%BB%9C%E5%B1%82/"},{"categories":["计算机网络"],"content":"RIP的路由条目的更新规则 举例1 路由器C的表 到达各目的网络的下一条都记为问号，可以理解为路由器D并不需要关心路由器C的这些内容 假设路由器C的RIP更新报文发送周期到了，则路由器C将自己路由表中的相关路由信息封装到RIP更新报文中发送给路由器D 路由器C能到达这些网络，说明路由器C的相邻路由器也能到达，只是比路由器C的距离大1，于是根据距离的对比，路由器D更新自己的路由表 举例2 ","date":"2023-06-23","objectID":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E7%BD%91%E7%BB%9C%E5%B1%82/:15:3","tags":[],"title":"计算机网络之网络层","uri":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E7%BD%91%E7%BB%9C%E5%B1%82/"},{"categories":["计算机网络"],"content":"RIP存在“坏消息传播得慢”的问题 解决方法 但是，这些方法也不能完全解决“坏消息传播得慢”的问题，这是距离向量的本质决定 总结 RIP 协议的优缺点 优点： 实现简单，开销较小。 缺点： RIP 限制了网络的规模，它能使用的最大距离为 15（16 表示不可达）。 路由器之间交换的路由信息是路由器中的完整路由表，因而随着网络规模的扩大，开销也就增加。 “坏消息传播得慢”，使更新过程的收敛时间过长。 ","date":"2023-06-23","objectID":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E7%BD%91%E7%BB%9C%E5%B1%82/:15:4","tags":[],"title":"计算机网络之网络层","uri":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E7%BD%91%E7%BB%9C%E5%B1%82/"},{"categories":["计算机网络"],"content":"开放最短路径优先OSPF 开放最短路径优先 OSPF (Open Shortest Path First) 注意：OSPF 只是一个协议的名字，它并不表示其他的路由选择协议不是“最短路径优先”。 ","date":"2023-06-23","objectID":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E7%BD%91%E7%BB%9C%E5%B1%82/:16:0","tags":[],"title":"计算机网络之网络层","uri":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E7%BD%91%E7%BB%9C%E5%B1%82/"},{"categories":["计算机网络"],"content":"概念 ","date":"2023-06-23","objectID":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E7%BD%91%E7%BB%9C%E5%B1%82/:16:1","tags":[],"title":"计算机网络之网络层","uri":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E7%BD%91%E7%BB%9C%E5%B1%82/"},{"categories":["计算机网络"],"content":"问候（Hello）分组 IP数据报首部中协议号字段的取值应为89，来表明IP数据报的数据载荷为OSPF分组. ","date":"2023-06-23","objectID":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E7%BD%91%E7%BB%9C%E5%B1%82/:16:2","tags":[],"title":"计算机网络之网络层","uri":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E7%BD%91%E7%BB%9C%E5%B1%82/"},{"categories":["计算机网络"],"content":"发送链路状态通告LSA 洪泛法有点类似于广播，就是从一个接口进来，从其他剩余所有接口出去 ","date":"2023-06-23","objectID":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E7%BD%91%E7%BB%9C%E5%B1%82/:16:3","tags":[],"title":"计算机网络之网络层","uri":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E7%BD%91%E7%BB%9C%E5%B1%82/"},{"categories":["计算机网络"],"content":"链路状态数据库同步 ","date":"2023-06-23","objectID":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E7%BD%91%E7%BB%9C%E5%B1%82/:16:4","tags":[],"title":"计算机网络之网络层","uri":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E7%BD%91%E7%BB%9C%E5%B1%82/"},{"categories":["计算机网络"],"content":"使用SPF算法计算出各自路由器到达其他路由器的最短路径 ","date":"2023-06-23","objectID":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E7%BD%91%E7%BB%9C%E5%B1%82/:16:5","tags":[],"title":"计算机网络之网络层","uri":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E7%BD%91%E7%BB%9C%E5%B1%82/"},{"categories":["计算机网络"],"content":"OSPF五种分组类型 ","date":"2023-06-23","objectID":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E7%BD%91%E7%BB%9C%E5%B1%82/:16:6","tags":[],"title":"计算机网络之网络层","uri":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E7%BD%91%E7%BB%9C%E5%B1%82/"},{"categories":["计算机网络"],"content":"OSPF的基本工作过程 ","date":"2023-06-23","objectID":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E7%BD%91%E7%BB%9C%E5%B1%82/:16:7","tags":[],"title":"计算机网络之网络层","uri":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E7%BD%91%E7%BB%9C%E5%B1%82/"},{"categories":["计算机网络"],"content":"OSPF在多点接入网络中路由器邻居关系建立 如果不采用其他机制，将会产生大量的多播分组 若DR出现问题，则由BDR顶替DR 为了使OSPF能够用于规模很大的网络，OSPF把一个自治系统再划分为若干个更小的范围，叫做区域（Area） 在该自治系统内，所有路由器都使用OSPF协议，OSPF将该自治系统再划分成4个更小的区域 每个区域都有一个32比特的区域标识符 主干区域的区域标识符必须为0，主干区域用于连通其他区域 其他区域的区域标识符不能为0且不相同 每个区域一般不应包含路由器超过200个 划分区域的好处就是，利用洪泛法交换链路状态信息局限于每一个区域而不是自治系统，这样减少整个网络上的通信量 总结 ","date":"2023-06-23","objectID":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E7%BD%91%E7%BB%9C%E5%B1%82/:16:8","tags":[],"title":"计算机网络之网络层","uri":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E7%BD%91%E7%BB%9C%E5%B1%82/"},{"categories":["计算机网络"],"content":"边界网关协议BGP BGP（Border Gateway Protocol） 是不同自治系统的路由器之间交换路由信息的协议 总结 ","date":"2023-06-23","objectID":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E7%BD%91%E7%BB%9C%E5%B1%82/:17:0","tags":[],"title":"计算机网络之网络层","uri":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E7%BD%91%E7%BB%9C%E5%B1%82/"},{"categories":["计算机网络"],"content":"直接封装RIP、OSPF和BGP报文的协议 4.7、IPv4数据报的首部格式 下面只是说了IPv4的首部格式，数据载荷就是上层交付的数据 ","date":"2023-06-23","objectID":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E7%BD%91%E7%BB%9C%E5%B1%82/:18:0","tags":[],"title":"计算机网络之网络层","uri":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E7%BD%91%E7%BB%9C%E5%B1%82/"},{"categories":["计算机网络"],"content":"各字段的作用 一个 IP 数据报由首部和数据两部分组成。 首部的前一部分是固定长度，共 20 字节，是所有 IP 数据报必须具有的。 在首部的固定部分的后面是一些可选部分，其长度是可变的。 图中的每一行都由32个比特（也就是4个字节）构成，每个小格子称为字段或者域，每个字段或某些字段的组合用来表达IP协议的相关功能 IP数据报的首部长度一定是4字节的整数倍 因为首部中的可选字段的长度从1个字节到40个字节不等，那么，当20字节的固定部分加上1到40个字节长度不等的可变部分，会造成首部长度不是4字节整数倍时，就用取值为全0的填充字段填充相应个字节，以确保IP数据报的首部长度是4字节的整数倍 对IPv4数据报进行分片 现在假定分片2的IP数据报经过某个网络时还需要进行分片 ","date":"2023-06-23","objectID":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E7%BD%91%E7%BB%9C%E5%B1%82/:19:0","tags":[],"title":"计算机网络之网络层","uri":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E7%BD%91%E7%BB%9C%E5%B1%82/"},{"categories":["计算机网络"],"content":"总结 IPv6数据报首部格式 4.8、网际控制报文协议ICMP ","date":"2023-06-23","objectID":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E7%BD%91%E7%BB%9C%E5%B1%82/:20:0","tags":[],"title":"计算机网络之网络层","uri":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E7%BD%91%E7%BB%9C%E5%B1%82/"},{"categories":["计算机网络"],"content":"概念 架构IP网络时需要特别注意两点： 确认网络是否正常工作 遇到异常时进行问题诊断 而ICMP就是解决这些问题的协议，ICMP的主要功能包括： 确认IP包是否成功送达目标地址 通知在发送过程当中IP包被废弃的具体原因 改善网络设置等 有了这些功能以后，就可以获得网络是否正常，设置是否有误以及设备有何异常等信息，从而便于进行网络上的问题诊断 ","date":"2023-06-23","objectID":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E7%BD%91%E7%BB%9C%E5%B1%82/:21:0","tags":[],"title":"计算机网络之网络层","uri":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E7%BD%91%E7%BB%9C%E5%B1%82/"},{"categories":["计算机网络"],"content":"ICMP报文的分类 ","date":"2023-06-23","objectID":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E7%BD%91%E7%BB%9C%E5%B1%82/:21:1","tags":[],"title":"计算机网络之网络层","uri":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E7%BD%91%E7%BB%9C%E5%B1%82/"},{"categories":["计算机网络"],"content":"ICMP报文的格式 ICMP 不是高层协议（看起来好像是高层协议，因为 ICMP 报文是装在 IP 数据报中，作为其中的数据部分），而是 IP 层的协议 ","date":"2023-06-23","objectID":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E7%BD%91%E7%BB%9C%E5%B1%82/:21:2","tags":[],"title":"计算机网络之网络层","uri":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E7%BD%91%E7%BB%9C%E5%B1%82/"},{"categories":["计算机网络"],"content":"ICMP差错报告报文 ","date":"2023-06-23","objectID":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E7%BD%91%E7%BB%9C%E5%B1%82/:22:0","tags":[],"title":"计算机网络之网络层","uri":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E7%BD%91%E7%BB%9C%E5%B1%82/"},{"categories":["计算机网络"],"content":"终点不可达 ","date":"2023-06-23","objectID":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E7%BD%91%E7%BB%9C%E5%B1%82/:22:1","tags":[],"title":"计算机网络之网络层","uri":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E7%BD%91%E7%BB%9C%E5%B1%82/"},{"categories":["计算机网络"],"content":"源点抑制 ","date":"2023-06-23","objectID":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E7%BD%91%E7%BB%9C%E5%B1%82/:22:2","tags":[],"title":"计算机网络之网络层","uri":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E7%BD%91%E7%BB%9C%E5%B1%82/"},{"categories":["计算机网络"],"content":"时间超过 ","date":"2023-06-23","objectID":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E7%BD%91%E7%BB%9C%E5%B1%82/:22:3","tags":[],"title":"计算机网络之网络层","uri":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E7%BD%91%E7%BB%9C%E5%B1%82/"},{"categories":["计算机网络"],"content":"参数问题 ","date":"2023-06-23","objectID":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E7%BD%91%E7%BB%9C%E5%B1%82/:22:4","tags":[],"title":"计算机网络之网络层","uri":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E7%BD%91%E7%BB%9C%E5%B1%82/"},{"categories":["计算机网络"],"content":"改变路由（重定向） ","date":"2023-06-23","objectID":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E7%BD%91%E7%BB%9C%E5%B1%82/:22:5","tags":[],"title":"计算机网络之网络层","uri":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E7%BD%91%E7%BB%9C%E5%B1%82/"},{"categories":["计算机网络"],"content":"不应发送ICMP差错报告报文情况 ","date":"2023-06-23","objectID":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E7%BD%91%E7%BB%9C%E5%B1%82/:22:6","tags":[],"title":"计算机网络之网络层","uri":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E7%BD%91%E7%BB%9C%E5%B1%82/"},{"categories":["计算机网络"],"content":"ICMP应用举例 ","date":"2023-06-23","objectID":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E7%BD%91%E7%BB%9C%E5%B1%82/:23:0","tags":[],"title":"计算机网络之网络层","uri":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E7%BD%91%E7%BB%9C%E5%B1%82/"},{"categories":["计算机网络"],"content":"分组网间探测PING（Packet InterNet Groper） ","date":"2023-06-23","objectID":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E7%BD%91%E7%BB%9C%E5%B1%82/:23:1","tags":[],"title":"计算机网络之网络层","uri":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E7%BD%91%E7%BB%9C%E5%B1%82/"},{"categories":["计算机网络"],"content":"跟踪路由（traceroute） tracert命令的实现原理 ","date":"2023-06-23","objectID":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E7%BD%91%E7%BB%9C%E5%B1%82/:23:2","tags":[],"title":"计算机网络之网络层","uri":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E7%BD%91%E7%BB%9C%E5%B1%82/"},{"categories":["计算机网络"],"content":"总结 4.9、虚拟专用网VPN与网络地址转换NAT ","date":"2023-06-23","objectID":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E7%BD%91%E7%BB%9C%E5%B1%82/:24:0","tags":[],"title":"计算机网络之网络层","uri":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E7%BD%91%E7%BB%9C%E5%B1%82/"},{"categories":["计算机网络"],"content":"虚拟专用网VPN（Virtual Private Network） 由于 IP 地址的紧缺，一个机构能够申请到的IP地址数往往远小于本机构所拥有的主机数。 考虑到互联网并不很安全，一个机构内也并不需要把所有的主机接入到外部的互联网。 假定在一个机构内部的计算机通信也是采用 TCP/IP 协议，那么从原则上讲，对于这些仅在机构内部使用的计算机就可以由本机构自行分配其 IP 地址。 上图是因特网数字分配机构IANA官网查看IPv4地址空间中特殊地址的分配方案 用粉红色标出来的地址就是无需申请的、可自由分配的专用地址或称私有地址 私有地址只能用于一个机构的内部通信，而不能用于和因特网上的主机通信 私有地址只能用作本地地址而不能用作全球地址 因特网中所有路由器对目的地址是私有地址的IP数据报一律不进行转发 本地地址与全球地址 本地地址——仅在机构内部使用的 IP 地址，可以由本机构自行分配，而不需要向互联网的管理机构申请。 全球地址——全球唯一的 IP 地址，必须向互联网的管理机构申请。 问题：在内部使用的本地地址就有可能和互联网中某个 IP 地址重合，这样就会出现地址的二义性问题。 所以部门A和部门B至少需要一个 路由器具有合法的全球IP地址，这样各自的专用网才能利用公用的因特网进行通信 部门A向部门B发送数据流程 两个专用网内的主机间发送的数据报是通过了公用的因特网，但在效果上就好像是在本机构的专用网上传送一样 数据报在因特网中可能要经过多个网络和路由器，但从逻辑上看，R1和R2之间好像是一条直通的点对点链路 因此也被称为IP隧道技术 ","date":"2023-06-23","objectID":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E7%BD%91%E7%BB%9C%E5%B1%82/:25:0","tags":[],"title":"计算机网络之网络层","uri":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E7%BD%91%E7%BB%9C%E5%B1%82/"},{"categories":["计算机网络"],"content":"网络地址转换NAT（Network Address Translation） 举例 使用私有地址的主机，如何才能与因特网上使用全球IP地址的主机进行通信？ 这需要在专用网络连接到因特网的路由器上安装NAT软件 装有NAT软件的路由器叫做NAT路由器。它至少有一个有效的外部全球IP地址 这样，所有使用私有地址的主机在和外界通信时，都要在NAT路由器上将其私有地址转换为全球IP地址 假设，使用私有地址的主机要给因特网上使用全球IP地址的另一台主机发送IP数据报 因特网上的这台主机给源主机发回数据报 当专用网中的这两台使用私有地址的主机都要给因特网使用全球地址的另一台主机发送数据报时，在NAT路由器的NAT转换表中就会产生两条记录，分别记录两个私有地址与全球地址的对应关系 这种基本转换存在一个问题 解决方法 我们现在用的很多家用路由器都是这种NART路由器 内网主机与外网主机的通信，是否能由外网主机首先发起？ 否定 ","date":"2023-06-23","objectID":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E7%BD%91%E7%BB%9C%E5%B1%82/:26:0","tags":[],"title":"计算机网络之网络层","uri":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E7%BD%91%E7%BB%9C%E5%B1%82/"},{"categories":["计算机网络"],"content":"总结 ","date":"2023-06-23","objectID":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E7%BD%91%E7%BB%9C%E5%B1%82/:27:0","tags":[],"title":"计算机网络之网络层","uri":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E7%BD%91%E7%BB%9C%E5%B1%82/"},{"categories":["计算机网络"],"content":"[toc] 3.1、数据链路层概述 ","date":"2023-06-08","objectID":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E6%95%B0%E6%8D%AE%E9%93%BE%E8%B7%AF%E5%B1%82/:0:0","tags":[],"title":"计算机网络之数据链路层","uri":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E6%95%B0%E6%8D%AE%E9%93%BE%E8%B7%AF%E5%B1%82/"},{"categories":["计算机网络"],"content":"概述 ","date":"2023-06-08","objectID":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E6%95%B0%E6%8D%AE%E9%93%BE%E8%B7%AF%E5%B1%82/:1:0","tags":[],"title":"计算机网络之数据链路层","uri":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E6%95%B0%E6%8D%AE%E9%93%BE%E8%B7%AF%E5%B1%82/"},{"categories":["计算机网络"],"content":"链路与数据链路 链路是从一个结点到相邻结点的一段物理线路，数据链路则是在链路的基础上增加了一些必要的硬件（如网络适配器）和软件（如协议的实现）。 网络中的主机、路由器等都必须实现数据链路层 局域网中的主机、交换机等都必须实现数据链路层 从层次上来看数据的流动： 仅从数据链路层观察帧的流动： 主机H1 到主机H2 所经过的网络可以是多种不同类型的 注意：不同的链路层可能采用不同的数据链路层协议 ","date":"2023-06-08","objectID":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E6%95%B0%E6%8D%AE%E9%93%BE%E8%B7%AF%E5%B1%82/:1:1","tags":[],"title":"计算机网络之数据链路层","uri":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E6%95%B0%E6%8D%AE%E9%93%BE%E8%B7%AF%E5%B1%82/"},{"categories":["计算机网络"],"content":"数据链路层使用的信道 数据链路层属于计算机网路的低层。数据链路层使用的信道主要有以下两种类型： 点对点信道 广播信道 局域网属于数据链路层 局域网虽然是个网络。但我们并不把局域网放在网络层中讨论。这是因为在网络层要讨论的是多个网络互连的问题，是讨论分组怎么从一个网络，通过路由器，转发到另一个网络。 而在同一个局域网中，分组怎么从一台主机传送到另一台主机，但并不经过路由器转发。从整个互联网来看，局域网仍属于数据链路层的范围 ","date":"2023-06-08","objectID":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E6%95%B0%E6%8D%AE%E9%93%BE%E8%B7%AF%E5%B1%82/:1:2","tags":[],"title":"计算机网络之数据链路层","uri":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E6%95%B0%E6%8D%AE%E9%93%BE%E8%B7%AF%E5%B1%82/"},{"categories":["计算机网络"],"content":"三个重要问题 数据链路层传送的协议数据单元是帧 ","date":"2023-06-08","objectID":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E6%95%B0%E6%8D%AE%E9%93%BE%E8%B7%AF%E5%B1%82/:2:0","tags":[],"title":"计算机网络之数据链路层","uri":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E6%95%B0%E6%8D%AE%E9%93%BE%E8%B7%AF%E5%B1%82/"},{"categories":["计算机网络"],"content":"封装成帧 封装成帧 (framing) 就是在一段数据的前后分别添加首部和尾部，然后就构成了一个帧。 首部和尾部的一个重要作用就是进行帧定界。 数据链路层采用的点对点信道的PPP协议封装的PPP帧和采用以太网协议的MAC帧 当发送端出问题，帧没有发送完毕，那么发送端只能重发此帧；此时接收端接收到不完整的帧，会丢弃这个帧。 ","date":"2023-06-08","objectID":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E6%95%B0%E6%8D%AE%E9%93%BE%E8%B7%AF%E5%B1%82/:2:1","tags":[],"title":"计算机网络之数据链路层","uri":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E6%95%B0%E6%8D%AE%E9%93%BE%E8%B7%AF%E5%B1%82/"},{"categories":["计算机网络"],"content":"差错控制 在传输过程中可能会产生比特差错：1 可能会变成 0， 而 0 也可能变成 1。 为了保证数据传输的可靠性，在计算机网络传输数据时，必须采用各种差错检测措施。 循环冗余检验CRC 缺点：可能会检测不出错误，从而导致纠错能力不足 ","date":"2023-06-08","objectID":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E6%95%B0%E6%8D%AE%E9%93%BE%E8%B7%AF%E5%B1%82/:2:2","tags":[],"title":"计算机网络之数据链路层","uri":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E6%95%B0%E6%8D%AE%E9%93%BE%E8%B7%AF%E5%B1%82/"},{"categories":["计算机网络"],"content":"透明传输 帧定界符：可以选用ASCII码表中的SOH(0x01)作为帧开始定界符，EOT(0x04)为帧结束定界符。 如果数据部分出现“EOT”或“SOH”时，那么会使得数据部分拆分出帧，使得帧的解析出现问题，所以要进行字节填充。 具体方法：发送端的数据链路层在数据中出现控制字符“EOT”、“SOH”以及“ESC”，则在前面插入一个转义字符“ESC”的编码。接收端的数据链路层在收到删除这个插入的转义字符。这样用字节填充法解决透明传输的问题。 以上三个问题都是使用点对点信道的数据链路层来举例的 如果使用广播信道的数据链路层除了包含上面三个问题外，还有一些问题要解决 如图所示，主机A，B，C，D，E通过一根总线进行互连，主机A要给主机C发送数据，代表帧的信号会通过总线传输到总线上的其他各主机，那么主机B，D，E如何知道所收到的帧不是发送给她们的，主机C如何知道发送的帧是发送给自己的 可以用编址（地址）的来解决，将帧的目的地址添加在帧中一起传输 ","date":"2023-06-08","objectID":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E6%95%B0%E6%8D%AE%E9%93%BE%E8%B7%AF%E5%B1%82/:2:3","tags":[],"title":"计算机网络之数据链路层","uri":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E6%95%B0%E6%8D%AE%E9%93%BE%E8%B7%AF%E5%B1%82/"},{"categories":["计算机网络"],"content":"数据碰撞 除了上面三个问题，还有数据碰撞问题 随着技术的发展，交换技术的成熟，在 有线（局域网）领域 使用点对点链路和链路层交换机的交换式局域网取代了共享式局域网；在无线局域网中仍然使用的是共享信道技术 3.2、封装成帧 ","date":"2023-06-08","objectID":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E6%95%B0%E6%8D%AE%E9%93%BE%E8%B7%AF%E5%B1%82/:2:4","tags":[],"title":"计算机网络之数据链路层","uri":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E6%95%B0%E6%8D%AE%E9%93%BE%E8%B7%AF%E5%B1%82/"},{"categories":["计算机网络"],"content":"介绍 封装成帧是指数据链路层给上层交付的协议数据单元添加帧头和帧尾使之成为帧 帧头和帧尾中包含有重要的控制信息 发送方的数据链路层将上层交付下来的协议数据单元封装成帧后，还要通过物理层，将构成帧的各比特，转换成电信号交给传输媒体，那么接收方的数据链路层如何从物理层交付的比特流中提取出一个个的帧？ 答：需要帧头和帧尾来做帧定界 但并不是每一种数据链路层协议的帧都包含有帧定界标志，例如下面例子 前导码 前同步码：作用是使接收方的时钟同步 帧开始定界符：表明其后面紧跟着的就是MAC帧 另外以太网还规定了帧间间隔为96比特时间，因此，MAC帧不需要帧结束定界符，只需要检测当前比特是否 ","date":"2023-06-08","objectID":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E6%95%B0%E6%8D%AE%E9%93%BE%E8%B7%AF%E5%B1%82/:3:0","tags":[],"title":"计算机网络之数据链路层","uri":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E6%95%B0%E6%8D%AE%E9%93%BE%E8%B7%AF%E5%B1%82/"},{"categories":["计算机网络"],"content":"透明传输 透明 指某一个实际存在的事物看起来却好像不存在一样。 透明传输是指数据链路层对上层交付的传输数据没有任何限制，好像数据链路层不存在一样 帧界定标志也就是个特定数据值，如果在上层交付的协议数据单元中，恰好也包含这个特定数值，接收方就不能正确接收 所以数据链路层应该对上层交付的数据有限制，其内容不能包含帧定界符的值 解决透明传输问题 解决方法：面向字节的物理链路使用字节填充 (byte stuffing) 或字符填充 (character stuffing)，面向比特的物理链路使用比特填充的方法实现透明传输 发送端的数据链路层在数据中出现控制字符“SOH”或“EOT”的前面插入一个转义字符“ESC”(其十六进制编码是1B)。 接收端的数据链路层在将数据送往网络层之前删除插入的转义字符。 如果转义字符也出现在数据当中，那么应在转义字符前面插入一个转义字符 ESC。当接收端收到连续的两个转义字符时，就删除其中前面的一个。 帧的数据部分长度 ","date":"2023-06-08","objectID":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E6%95%B0%E6%8D%AE%E9%93%BE%E8%B7%AF%E5%B1%82/:4:0","tags":[],"title":"计算机网络之数据链路层","uri":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E6%95%B0%E6%8D%AE%E9%93%BE%E8%B7%AF%E5%B1%82/"},{"categories":["计算机网络"],"content":"总结 3.3、差错检测 ","date":"2023-06-08","objectID":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E6%95%B0%E6%8D%AE%E9%93%BE%E8%B7%AF%E5%B1%82/:5:0","tags":[],"title":"计算机网络之数据链路层","uri":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E6%95%B0%E6%8D%AE%E9%93%BE%E8%B7%AF%E5%B1%82/"},{"categories":["计算机网络"],"content":"介绍 ","date":"2023-06-08","objectID":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E6%95%B0%E6%8D%AE%E9%93%BE%E8%B7%AF%E5%B1%82/:6:0","tags":[],"title":"计算机网络之数据链路层","uri":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E6%95%B0%E6%8D%AE%E9%93%BE%E8%B7%AF%E5%B1%82/"},{"categories":["计算机网络"],"content":"奇偶校验 奇偶检验法只能检测出奇数个错误，不能检测出所有的错误 ","date":"2023-06-08","objectID":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E6%95%B0%E6%8D%AE%E9%93%BE%E8%B7%AF%E5%B1%82/:7:0","tags":[],"title":"计算机网络之数据链路层","uri":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E6%95%B0%E6%8D%AE%E9%93%BE%E8%B7%AF%E5%B1%82/"},{"categories":["计算机网络"],"content":"循环冗余校验CRC(Cyclic Redundancy Check) 例题 总结 循环冗余校验 CRC 是一种检错方法，而帧校验序列 FCS 是添加在数据后面的冗余码 3.4、可靠传输 ","date":"2023-06-08","objectID":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E6%95%B0%E6%8D%AE%E9%93%BE%E8%B7%AF%E5%B1%82/:8:0","tags":[],"title":"计算机网络之数据链路层","uri":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E6%95%B0%E6%8D%AE%E9%93%BE%E8%B7%AF%E5%B1%82/"},{"categories":["计算机网络"],"content":"基本概念 下面是比特差错 其他传输差错 分组丢失 路由器输入队列快满了，主动丢弃收到的分组 分组失序 数据并未按照发送顺序依次到达接收端 分组重复 由于某些原因，有些分组在网络中滞留了，没有及时到达接收端，这可能会造成发送端对该分组的重发，重发的分组到达接收端，但一段时间后，滞留在网络的分组也到达了接收端，这就造成分组重复的传输差错 ","date":"2023-06-08","objectID":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E6%95%B0%E6%8D%AE%E9%93%BE%E8%B7%AF%E5%B1%82/:9:0","tags":[],"title":"计算机网络之数据链路层","uri":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E6%95%B0%E6%8D%AE%E9%93%BE%E8%B7%AF%E5%B1%82/"},{"categories":["计算机网络"],"content":"三种可靠协议 一般情况下，有线链路的不由数据链路层提供可靠传输，而交由上层解决可靠传输问题；但是无线链路的误码率较高，需要数据链路层提供可靠传输服务。有三种可靠协议实现： 停止-等待协议SW 回退N帧协议GBN 选择重传协议SR 这三种可靠传输实现机制的基本原理并不仅限于数据链路层，可以应用到计算机网络体系结构的各层协议中 ","date":"2023-06-08","objectID":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E6%95%B0%E6%8D%AE%E9%93%BE%E8%B7%AF%E5%B1%82/:10:0","tags":[],"title":"计算机网络之数据链路层","uri":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E6%95%B0%E6%8D%AE%E9%93%BE%E8%B7%AF%E5%B1%82/"},{"categories":["计算机网络"],"content":"停止-等待协议 ","date":"2023-06-08","objectID":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E6%95%B0%E6%8D%AE%E9%93%BE%E8%B7%AF%E5%B1%82/:11:0","tags":[],"title":"计算机网络之数据链路层","uri":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E6%95%B0%E6%8D%AE%E9%93%BE%E8%B7%AF%E5%B1%82/"},{"categories":["计算机网络"],"content":"定义 停止等待协议中，发送方每发送一个分组都必须等待接收方的确认分组ACK（Acknowled Character）才能发送下一个分组，如果是否认分组NAK（Negative Acknowledge)则重传上次发送的分组 ","date":"2023-06-08","objectID":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E6%95%B0%E6%8D%AE%E9%93%BE%E8%B7%AF%E5%B1%82/:11:1","tags":[],"title":"计算机网络之数据链路层","uri":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E6%95%B0%E6%8D%AE%E9%93%BE%E8%B7%AF%E5%B1%82/"},{"categories":["计算机网络"],"content":"停止-等待协议可能遇到的问题 超时重传 如果发送方分组丢失，那么接收方就不会发送ACK，此时发送方就会一直等待ACK而不能发送数据。为了避免这种情况，发送方主动设置一个时钟，如果分组发出后超过了一定的时间没有收到ACK就自动重发分组，这称为超时重传，超时时间一般设置为略大于一个分组往返的时间 确认丢失 接收方的确认（否认）分组也是可能丢失的，假设发送方发送了分组并且被接收方接收，接收方的确认分组在传输过程中丢失，那么发送方因为没收到确认分组就会触发超时重传把相同的分组再发送一遍，此时接收方会收到一个重复的分组 为了避免分组重复，发送方需要给每个分组编号，因为发送方每次只发送一个分组就停下来等待，所以发送方只需使用一个比特进行编号就能使分组区分开来 当接收方检查编号发现是重复的分组时就知道上个分组的确认分组丢失了，此时接收方只需丢弃收到的分组并发送ACK即可，发送方收到ACK后也不会再重发分组了 既然数据分组需要编号，确认分组是否需要编号？ 要。如下图所示 确认迟到 接收方的ACK也有可能迟到，发送方发送分组0后因为ACK迟到触发超时重传再次发送分组0，再次发送分组0后收到了迟到的ACK然后发送方发送分组1。假设接收方先收到重传的分组0然后发送ACK再收到分组1，但是发送方收到重传的分组0的ACK时有可能误认为是分组1的ACK。为了避免上述情况，ACK也需要编号以便发送方确认，同样的ACK编号也只需要一个比特 注意，图中最下面那个数据分组与之前序号为0的那个数据分组不是同一个数据分组 注意事项 ","date":"2023-06-08","objectID":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E6%95%B0%E6%8D%AE%E9%93%BE%E8%B7%AF%E5%B1%82/:11:2","tags":[],"title":"计算机网络之数据链路层","uri":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E6%95%B0%E6%8D%AE%E9%93%BE%E8%B7%AF%E5%B1%82/"},{"categories":["计算机网络"],"content":"停止-等待协议的信道利用率 假设收发双方之间是一条直通的信道 TD：是发送方发送数据分组所耗费的发送时延 RTT：是收发双方之间的往返时间 TA：是接收方发送确认分组所耗费的发送时延 TA一般都远小于TD，可以忽略，当RTT远大于TD时，信道利用率会非常低 像停止-等待协议这样通过确认和重传机制实现的可靠传输协议，常称为自动请求重传协议ARQ(Automatic Repeat reQuest)，意思是重传的请求是自动进行，因为不需要接收方显式地请求，发送方重传某个发送的分组 由以上的种种情况分析，发送方每发送一个分组都需要等待接收方的回应再做下一步，因此停止等待协议信道利用率很低，大多数时间都用来等待了 ","date":"2023-06-08","objectID":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E6%95%B0%E6%8D%AE%E9%93%BE%E8%B7%AF%E5%B1%82/:11:3","tags":[],"title":"计算机网络之数据链路层","uri":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E6%95%B0%E6%8D%AE%E9%93%BE%E8%B7%AF%E5%B1%82/"},{"categories":["计算机网络"],"content":"回退N帧协议GBN ","date":"2023-06-08","objectID":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E6%95%B0%E6%8D%AE%E9%93%BE%E8%B7%AF%E5%B1%82/:12:0","tags":[],"title":"计算机网络之数据链路层","uri":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E6%95%B0%E6%8D%AE%E9%93%BE%E8%B7%AF%E5%B1%82/"},{"categories":["计算机网络"],"content":"为什么用回退N帧协议 在相同的时间内，使用停止-等待协议的发送方只能发送一个数据分组，而采用流水线传输的发送方，可以发送多个数据分组 回退N帧协议在流水线传输的基础上，利用发送窗口来限制发送方可连续发送数据分组的个数 ","date":"2023-06-08","objectID":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E6%95%B0%E6%8D%AE%E9%93%BE%E8%B7%AF%E5%B1%82/:12:1","tags":[],"title":"计算机网络之数据链路层","uri":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E6%95%B0%E6%8D%AE%E9%93%BE%E8%B7%AF%E5%B1%82/"},{"categories":["计算机网络"],"content":"无差错情况流程 发送方将序号落在发送窗口内的0~4号数据分组，依次连续发送出去 他们经过互联网传输正确到达接收方，就是没有乱序和误码，接收方按序接收它们，每接收一个，接收窗口就向前滑动一个位置，并给发送方发送针对所接收分组的确认分组，在通过互联网的传输正确到达了发送方 发送方每接收一个确认分组，发送窗口就向前滑动一个位置，这样就有新的序号落入发送窗口，发送方可以将收到确认的数据分组从缓存中删除了，而接收方可以择机将已接收的数据分组交付上层处理 ","date":"2023-06-08","objectID":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E6%95%B0%E6%8D%AE%E9%93%BE%E8%B7%AF%E5%B1%82/:12:2","tags":[],"title":"计算机网络之数据链路层","uri":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E6%95%B0%E6%8D%AE%E9%93%BE%E8%B7%AF%E5%B1%82/"},{"categories":["计算机网络"],"content":"累计确认 累计确认 优点: 即使确认分组丢失，发送方也可能不必重传 减小接收方的开销 减小对网络资源的占用 缺点： 不能向发送方及时反映出接收方已经正确接收的数据分组信息 ","date":"2023-06-08","objectID":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E6%95%B0%E6%8D%AE%E9%93%BE%E8%B7%AF%E5%B1%82/:12:3","tags":[],"title":"计算机网络之数据链路层","uri":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E6%95%B0%E6%8D%AE%E9%93%BE%E8%B7%AF%E5%B1%82/"},{"categories":["计算机网络"],"content":"有差错情况 例如 在传输数据分组时，5号数据分组出现误码，接收方通过数据分组中的检错码发现了错误 于是丢弃该分组，而后续到达的这剩下四个分组与接收窗口的序号不匹配 接收同样也不能接收它们，将它们丢弃，并对之前按序接收的最后一个数据分组进行确认，发送ACK4，每丢弃一个数据分组，就发送一个ACK4 当收到重复的ACK4时，就知道之前所发送的数据分组出现了差错，于是可以不等超时计时器超时就立刻开始重传，具体收到几个重复确认就立刻重传，根据具体实现决定 如果收到这4个重复的确认并不会触发发送立刻重传，一段时间后。超时计时器超时，也会将发送窗口内以发送过的这些数据分组全部重传 若WT超过取值范围，例如WT=8，会出现什么情况？ 习题 总结 回退N帧协议在流水线传输的基础上利用发送窗口来限制发送方连续发送数据分组的数量，是一种连续ARQ协议 在协议的工作过程中发送窗口和接收窗口不断向前滑动，因此这类协议又称为滑动窗口协议 由于回退N帧协议的特性，当通信线路质量不好时，其信道利用率并不比停止-等待协议高 ","date":"2023-06-08","objectID":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E6%95%B0%E6%8D%AE%E9%93%BE%E8%B7%AF%E5%B1%82/:12:4","tags":[],"title":"计算机网络之数据链路层","uri":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E6%95%B0%E6%8D%AE%E9%93%BE%E8%B7%AF%E5%B1%82/"},{"categories":["计算机网络"],"content":"选择重传协议SR 具体流程请看视频 习题 总结 3.5、点对点协议PPP 点对点协议PPP（Point-to-Point Protocol）是目前使用最广泛的点对点数据链路层协议 PPP协议是因特网工程任务组IEIF在1992年制定的。经过1993年和1994年的修订，现在的PPP协议已成为因特网的正式标准[RFC1661，RFC1662] 数据链路层使用的一种协议，它的特点是： 简单； 只检测差错，而不是纠正差错（因为在差错检验时，选用的是CRC差错检验技术，CRC差错检验技术并没有纠错能力） 不使用序号，也不进行流量控制； 可同时支持多种网络层协议，例如ip、ipx等协议 PPPoE 是为宽带上网的主机使用的链路层协议 ","date":"2023-06-08","objectID":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E6%95%B0%E6%8D%AE%E9%93%BE%E8%B7%AF%E5%B1%82/:13:0","tags":[],"title":"计算机网络之数据链路层","uri":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E6%95%B0%E6%8D%AE%E9%93%BE%E8%B7%AF%E5%B1%82/"},{"categories":["计算机网络"],"content":"帧格式 必须规定特殊的字符作为帧定界符 ","date":"2023-06-08","objectID":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E6%95%B0%E6%8D%AE%E9%93%BE%E8%B7%AF%E5%B1%82/:14:0","tags":[],"title":"计算机网络之数据链路层","uri":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E6%95%B0%E6%8D%AE%E9%93%BE%E8%B7%AF%E5%B1%82/"},{"categories":["计算机网络"],"content":"透明传输 必须保证数据传输的透明性 实现透明传输的方法 面向字节的异步链路：字节填充法（插入“转义字符”） 面向比特的同步链路：比特填充法（插入“比特0”） ","date":"2023-06-08","objectID":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E6%95%B0%E6%8D%AE%E9%93%BE%E8%B7%AF%E5%B1%82/:15:0","tags":[],"title":"计算机网络之数据链路层","uri":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E6%95%B0%E6%8D%AE%E9%93%BE%E8%B7%AF%E5%B1%82/"},{"categories":["计算机网络"],"content":"差错检测 能够对接收端收到的帧进行检测，并立即丢弃有差错的帧。 ","date":"2023-06-08","objectID":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E6%95%B0%E6%8D%AE%E9%93%BE%E8%B7%AF%E5%B1%82/:16:0","tags":[],"title":"计算机网络之数据链路层","uri":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E6%95%B0%E6%8D%AE%E9%93%BE%E8%B7%AF%E5%B1%82/"},{"categories":["计算机网络"],"content":"工作状态 当用户拨号接入 ISP 时，路由器的调制解调器对拨号做出确认，并建立一条物理连接。 PC 机向路由器发送一系列的 LCP 分组（封装成多个 PPP 帧）。 这些分组及其响应选择一些 PPP 参数，并进行网络层配置，NCP 给新接入的 PC 机分配一个临时的 IP 地址，使 PC 机成为因特网上的一个主机。 通信完毕时，NCP 释放网络层连接，收回原来分配出去的 IP 地址。接着，LCP 释放数据链路层连接。最后释放的是物理层的连接。 可见，PPP 协议已不是纯粹的数据链路层的协议，它还包含了物理层和网络层的内容。 3.6、媒体接入控制（介质访问控制）——广播信道 媒体接入控制（介质访问控制）使用一对多的广播通信方式 Medium Access Control翻译成媒体接入控制，有些翻译成介质访问控制 局域网的数据链路层 局域网最主要的特点是： 网络为一个单位所拥有； 地理范围和站点数目均有限。 局域网具有如下主要优点： 具有广播功能，从一个站点可很方便地访问全网。局域网上的主机可共享连接在局域网上的各种硬件和软件资源。 便于系统的扩展和逐渐地演变，各设备的位置可灵活调整和改变。 提高了系统的可靠性、可用性和残存性。 数据链路层的两个子层: 为了使数据链路层能更好地适应多种局域网标准，IEEE 802 委员会就将局域网的数据链路层拆成两个子层： 逻辑链路控制 LLC (Logical Link Control)子层； 媒体接入控制 MAC (Medium Access Control)子层。 与接入到传输媒体有关的内容都放在 MAC子层，而 LLC 子层则与传输媒体无关。不管采用何种协议的局域网，对 LLC 子层来说都是透明的。 ","date":"2023-06-08","objectID":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E6%95%B0%E6%8D%AE%E9%93%BE%E8%B7%AF%E5%B1%82/:17:0","tags":[],"title":"计算机网络之数据链路层","uri":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E6%95%B0%E6%8D%AE%E9%93%BE%E8%B7%AF%E5%B1%82/"},{"categories":["计算机网络"],"content":"基本概念 为什么要媒体接入控制（介质访问控制）？ 共享信道带来的问题 若多个设备在共享信道上同时发送数据，则会造成彼此干扰，导致发送失败。 随着技术的发展，交换技术的成熟和成本的降低，具有更高性能的使用点对点链路和链路层交换机的交换式局域网在有线领域已完全取代了共享式局域网，但由于无线信道的广播天性，无线局域网仍然使用的是共享媒体技术 ","date":"2023-06-08","objectID":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E6%95%B0%E6%8D%AE%E9%93%BE%E8%B7%AF%E5%B1%82/:18:0","tags":[],"title":"计算机网络之数据链路层","uri":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E6%95%B0%E6%8D%AE%E9%93%BE%E8%B7%AF%E5%B1%82/"},{"categories":["计算机网络"],"content":"静态划分信道 信道复用 频分复用FDM (Frequency Division Multiplexing) 将整个带宽分为多份，用户在分配到一定的频带后，在通信过程中自始至终都占用这个频带。 频分复用的所有用户在同样的时间占用不同的带宽资源（请注意，这里的“带宽”是频率带宽而不是数据的发送速率）。 时分复用TDM (Time Division Multiplexing) 时分复用则是将时间划分为一段段等长的时分复用帧（TDM帧）。每一个时分复用的用户在每一个 TDM 帧中占用固定序号的时隙。 每一个用户所占用的时隙是周期性地出现（其周期就是TDM帧的长度）的。 TDM 信号也称为等时 (isochronous) 信号。 时分复用的所有用户在不同的时间占用同样的频带宽度。 波分复用 WDM(Wavelength Division Multiplexing) 波分复用就是光的频分复用，使用一根光纤来同时传输多个光载波信号 光信号传输一段距离后悔衰减，所以要用 掺铒光纤放大器 放大光信号 码分复用 CDM (Code Division Multiplexing) ","date":"2023-06-08","objectID":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E6%95%B0%E6%8D%AE%E9%93%BE%E8%B7%AF%E5%B1%82/:19:0","tags":[],"title":"计算机网络之数据链路层","uri":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E6%95%B0%E6%8D%AE%E9%93%BE%E8%B7%AF%E5%B1%82/"},{"categories":["计算机网络"],"content":"动态接入控制 受控接入 受控接入在局域网中使用得较少，本书不再讨论 随机接入 重点 ","date":"2023-06-08","objectID":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E6%95%B0%E6%8D%AE%E9%93%BE%E8%B7%AF%E5%B1%82/:20:0","tags":[],"title":"计算机网络之数据链路层","uri":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E6%95%B0%E6%8D%AE%E9%93%BE%E8%B7%AF%E5%B1%82/"},{"categories":["计算机网络"],"content":"随机接入（CSMA/CD协议） 总线局域网使用协议：CSMA/CD媒体接入控制协议 CSMA/CD协议曾经用于各种总线结构以太网和双绞线以太网的早起版本中。 现在的以太网基于交换机和全双工连接，不会有碰撞，因此没有必要使用CSMA/CD协议 ","date":"2023-06-08","objectID":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E6%95%B0%E6%8D%AE%E9%93%BE%E8%B7%AF%E5%B1%82/:21:0","tags":[],"title":"计算机网络之数据链路层","uri":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E6%95%B0%E6%8D%AE%E9%93%BE%E8%B7%AF%E5%B1%82/"},{"categories":["计算机网络"],"content":"基本概念 最初的以太网是将许多计算机都连接到一根总线上。易于实现广播通信。当初认为这样的连接方法既简单又可靠，因为总线上没有有源器件。 以太网（Ethernet）是一种计算机局域网技术。IEEE组织的IEEE 802.3标准制定了以太网（Ethernet）的技术标准 以太网采用无连接的工作方式，对发送的数据帧不进行编号，也不要求对方发回确认。目的站收到有差错帧就把它丢弃，其他什么也不做。 ","date":"2023-06-08","objectID":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E6%95%B0%E6%8D%AE%E9%93%BE%E8%B7%AF%E5%B1%82/:21:1","tags":[],"title":"计算机网络之数据链路层","uri":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E6%95%B0%E6%8D%AE%E9%93%BE%E8%B7%AF%E5%B1%82/"},{"categories":["计算机网络"],"content":"多址接入MA 表示许多主机以多点接入的方式连接在一根总线上。 ","date":"2023-06-08","objectID":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E6%95%B0%E6%8D%AE%E9%93%BE%E8%B7%AF%E5%B1%82/:21:2","tags":[],"title":"计算机网络之数据链路层","uri":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E6%95%B0%E6%8D%AE%E9%93%BE%E8%B7%AF%E5%B1%82/"},{"categories":["计算机网络"],"content":"载波监听CS 是指每一个站在发送数据之前先要检测一下总线上是否有其他计算机在发送数据，如果有，则暂时不要发送数据，以免发生碰撞。以太网的mac帧每隔96比特时间就会传输一个帧，所以，监听的时候只需要监听96比特时间就知道当前信道是否空闲了。 总线上并没有什么“载波”。因此， “载波监听”就是用电子技术检测总线上有没有其他计算机发送的数据信号。 ","date":"2023-06-08","objectID":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E6%95%B0%E6%8D%AE%E9%93%BE%E8%B7%AF%E5%B1%82/:21:3","tags":[],"title":"计算机网络之数据链路层","uri":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E6%95%B0%E6%8D%AE%E9%93%BE%E8%B7%AF%E5%B1%82/"},{"categories":["计算机网络"],"content":"碰撞检测CD “碰撞检测”就是计算机边发送数据边检测信道上的信号电压大小。 当几个站同时在总线上发送数据时，总线上的信号电压摆动值将会增大（互相叠加）。 当一个站检测到的信号电压摆动值超过一定的门限值时，就认为总线上至少有两个站同时在发送数据，表明产生了碰撞。 所谓“碰撞”就是发生了冲突。因此“碰撞检测”也称为“冲突检测”。 在发生碰撞时，总线上传输的信号产生了严重的失真，无法从中恢复出有用的信息来。 每一个正在发送数据的站，一旦发现总线上出现了碰撞，就要立即停止发送，免得继续浪费网络资源，然后等待一段随机时间后再次发送。 为什么要进行碰撞检测？ 因为信号传播时延对载波监听产生了影响 A 需要单程传播时延的 2 倍的时间，才能检测到与 B 的发送产生了冲突 ","date":"2023-06-08","objectID":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E6%95%B0%E6%8D%AE%E9%93%BE%E8%B7%AF%E5%B1%82/:21:4","tags":[],"title":"计算机网络之数据链路层","uri":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E6%95%B0%E6%8D%AE%E9%93%BE%E8%B7%AF%E5%B1%82/"},{"categories":["计算机网络"],"content":"CSMA/CD 协议工作流程 ","date":"2023-06-08","objectID":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E6%95%B0%E6%8D%AE%E9%93%BE%E8%B7%AF%E5%B1%82/:21:5","tags":[],"title":"计算机网络之数据链路层","uri":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E6%95%B0%E6%8D%AE%E9%93%BE%E8%B7%AF%E5%B1%82/"},{"categories":["计算机网络"],"content":"CSMA/CD 协议工作——争用期（碰撞窗口） ","date":"2023-06-08","objectID":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E6%95%B0%E6%8D%AE%E9%93%BE%E8%B7%AF%E5%B1%82/:21:6","tags":[],"title":"计算机网络之数据链路层","uri":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E6%95%B0%E6%8D%AE%E9%93%BE%E8%B7%AF%E5%B1%82/"},{"categories":["计算机网络"],"content":"CSMA/CD 协议工作——最小帧长 ","date":"2023-06-08","objectID":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E6%95%B0%E6%8D%AE%E9%93%BE%E8%B7%AF%E5%B1%82/:21:7","tags":[],"title":"计算机网络之数据链路层","uri":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E6%95%B0%E6%8D%AE%E9%93%BE%E8%B7%AF%E5%B1%82/"},{"categories":["计算机网络"],"content":"CSMA/CD 协议工作——最大帧长 ","date":"2023-06-08","objectID":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E6%95%B0%E6%8D%AE%E9%93%BE%E8%B7%AF%E5%B1%82/:21:8","tags":[],"title":"计算机网络之数据链路层","uri":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E6%95%B0%E6%8D%AE%E9%93%BE%E8%B7%AF%E5%B1%82/"},{"categories":["计算机网络"],"content":"CSMA/CD 协议工作——截断二进制指数退避算法 该算法可以用来解决以太网数据发送和传输过程中，站点间的碰撞问题。该算法是在发生碰撞过后确定随机等待时间的时候起作用。随着重传次数增加，那么，再次等待的随机时间就会更长。 ","date":"2023-06-08","objectID":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E6%95%B0%E6%8D%AE%E9%93%BE%E8%B7%AF%E5%B1%82/:21:9","tags":[],"title":"计算机网络之数据链路层","uri":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E6%95%B0%E6%8D%AE%E9%93%BE%E8%B7%AF%E5%B1%82/"},{"categories":["计算机网络"],"content":"CSMA/CD 协议工作——信道利用率 在理想情况下，以太网上的各个站点数据发送与传输不发生冲突，那么计算信道利用率时，就不用考虑争用期了。 ","date":"2023-06-08","objectID":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E6%95%B0%E6%8D%AE%E9%93%BE%E8%B7%AF%E5%B1%82/:21:10","tags":[],"title":"计算机网络之数据链路层","uri":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E6%95%B0%E6%8D%AE%E9%93%BE%E8%B7%AF%E5%B1%82/"},{"categories":["计算机网络"],"content":"CSMA/CD 协议工作——帧接收流程 ","date":"2023-06-08","objectID":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E6%95%B0%E6%8D%AE%E9%93%BE%E8%B7%AF%E5%B1%82/:21:11","tags":[],"title":"计算机网络之数据链路层","uri":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E6%95%B0%E6%8D%AE%E9%93%BE%E8%B7%AF%E5%B1%82/"},{"categories":["计算机网络"],"content":"CSMA/CD 协议的重要特性 使用 CSMA/CD 协议的以太网不能进行全双工通信而只能进行双向交替通信（半双工通信）。 每个站在发送数据之后的一小段时间内，存在着遭遇碰撞的可能性。 这种发送的不确定性使整个以太网的平均通信量远小于以太网的最高数据率。 CSMA/CD协议曾经用于各种总线结构以太网和双绞线以太网的早起版本中。 现在的以太网基于交换机和全双工连接，不会有碰撞，因此没有必要使用CSMA/CS协议 ","date":"2023-06-08","objectID":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E6%95%B0%E6%8D%AE%E9%93%BE%E8%B7%AF%E5%B1%82/:21:12","tags":[],"title":"计算机网络之数据链路层","uri":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E6%95%B0%E6%8D%AE%E9%93%BE%E8%B7%AF%E5%B1%82/"},{"categories":["计算机网络"],"content":"随机接入（CSMA/CA协议） 无线局域网使用的协议：CSMA/CA ","date":"2023-06-08","objectID":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E6%95%B0%E6%8D%AE%E9%93%BE%E8%B7%AF%E5%B1%82/:22:0","tags":[],"title":"计算机网络之数据链路层","uri":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E6%95%B0%E6%8D%AE%E9%93%BE%E8%B7%AF%E5%B1%82/"},{"categories":["计算机网络"],"content":"为什么无线局域网要使用CSMA/CA协议 ","date":"2023-06-08","objectID":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E6%95%B0%E6%8D%AE%E9%93%BE%E8%B7%AF%E5%B1%82/:22:1","tags":[],"title":"计算机网络之数据链路层","uri":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E6%95%B0%E6%8D%AE%E9%93%BE%E8%B7%AF%E5%B1%82/"},{"categories":["计算机网络"],"content":"帧间间隔IFS（Inter Frame Space） ","date":"2023-06-08","objectID":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E6%95%B0%E6%8D%AE%E9%93%BE%E8%B7%AF%E5%B1%82/:22:2","tags":[],"title":"计算机网络之数据链路层","uri":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E6%95%B0%E6%8D%AE%E9%93%BE%E8%B7%AF%E5%B1%82/"},{"categories":["计算机网络"],"content":"CSMA/CA协议的工作原理 源站为什么在检测到信道空闲后还要再等待一段时间DIFS？ 考虑到可能有其他的站有高优先级的帧要发送。若有，就要让高优先级帧先发送 目的站为什么正确接收数据帧后还要等待一段时间SIFS才能发送ACK帧？ SIFS是最短的帧间间隔，用来分隔开属于一次对话的各帧，在这段时间内，一个站点应当能够从发送方式切换到接收方式，以便做好接受ACK帧或其他帧的准备。 信道由忙转为空闲且经过DIFS时间后，其他站还要退避一段随机时间才能使用信道？ 防止多个站点同时发送数据而产生碰撞 使用退避算法的时机 ","date":"2023-06-08","objectID":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E6%95%B0%E6%8D%AE%E9%93%BE%E8%B7%AF%E5%B1%82/:22:3","tags":[],"title":"计算机网络之数据链路层","uri":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E6%95%B0%E6%8D%AE%E9%93%BE%E8%B7%AF%E5%B1%82/"},{"categories":["计算机网络"],"content":"CSMA/CA协议的退避算法 退避算法的示例 当A正在发送数据时，B, C和D都有数据要发送(用向上的箭头表示)。由于这三个站都检测到信道忙，因此都要执行退避算法，各自随机退避一段时间再发送数据。802. I 1标准规定，退避时间必须是整数倍的时隙时间。前面己经讲过，第i次退避是在时隙{0, 1, …, 2^2^+i}中随机地选择一个。这样做是为了使不同站点选择相同退避时间的概率减少。因此，第1次退避(i=1)要推迟发送的时间是在时隙{0, 1,…，7}中(共8个时隙)随机选择一个，而第2次退避是在时隙{0, 1,…，15}中(共16个时隙)随机选择一个。当时隙编号达到255时(这对应于第6次退避)就不再增加了。这半决定退避时间的变量，称为退避变量。 退避时间选定后，就相当于设置了一个退避计时器(backoff timer)。站点每经历一个时隙的时间就检测一次信道。这可能发生两种情况: 若检测到信道空闲，退避计时器就继续倒计时; 若检测到信道忙，就冻结退避计时器的剩余时间，重新等待信道变为空闲并再经过时间DIFS后，从剩余时间开始继续倒计时。如果退避计时器的时间减小到零时，就开始发送整个数据帧。 C的退避计时器最先减到零，于是C立即把整个数据帧发送出去。请注意，A发送完数据后信道就变为空闲。C的退避计时器一直在倒计时。当C在发送数据的过程中，B和D检测到信道忙，就冻结各自的退避计时器的数值，重新期待信道变为空闲。正在这时E也想发送数据。由于E检测到信道忙，因此E就执行退避算法和设置退避计时器。 当C发送完数据并经过了时间DIFS后，B和D的退避计时器又从各自的剩余时间开始倒计时。现在争用信道的除B和D外，还有E、D的退避计时器最先减到零，于是D得到了发送权。在D发送数据时，B和E都冻结其退避计时器。 以后E的退避计时器比B先减少到零。当E发送数据时，B再次冻结其退避计时器。等到E发送完数据并经过时间DIFS后，B的退避计时器才继续工作，一直到把最后剩余的时间用完，然后就发送数据。 冻结退避计时器剩余时间的做法是为了使协议对所有站点更加公平。 ","date":"2023-06-08","objectID":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E6%95%B0%E6%8D%AE%E9%93%BE%E8%B7%AF%E5%B1%82/:22:4","tags":[],"title":"计算机网络之数据链路层","uri":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E6%95%B0%E6%8D%AE%E9%93%BE%E8%B7%AF%E5%B1%82/"},{"categories":["计算机网络"],"content":"CSMA/CA协议的信道预约和虚拟载波监听 虚拟载波监听机制能减少隐蔽站带来的碰撞问题的示例 3.7、MAC地址、IP地址以及ARP协议 ","date":"2023-06-08","objectID":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E6%95%B0%E6%8D%AE%E9%93%BE%E8%B7%AF%E5%B1%82/:22:5","tags":[],"title":"计算机网络之数据链路层","uri":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E6%95%B0%E6%8D%AE%E9%93%BE%E8%B7%AF%E5%B1%82/"},{"categories":["计算机网络"],"content":"MAC地址 使用点对点信道的数据链路层不需要使用地址 使用广播信道的数据链路层必须使用地址来区分各主机 ","date":"2023-06-08","objectID":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E6%95%B0%E6%8D%AE%E9%93%BE%E8%B7%AF%E5%B1%82/:23:0","tags":[],"title":"计算机网络之数据链路层","uri":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E6%95%B0%E6%8D%AE%E9%93%BE%E8%B7%AF%E5%B1%82/"},{"categories":["计算机网络"],"content":"广播信道的数据链路层必须使用地址（MAC） MAC地址又称为硬件地址或物理地址。请注意：不要被 “物理” 二字误导认为物理地址属于物理层范畴，物理地址属于数据链路层范畴 ","date":"2023-06-08","objectID":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E6%95%B0%E6%8D%AE%E9%93%BE%E8%B7%AF%E5%B1%82/:23:1","tags":[],"title":"计算机网络之数据链路层","uri":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E6%95%B0%E6%8D%AE%E9%93%BE%E8%B7%AF%E5%B1%82/"},{"categories":["计算机网络"],"content":"IEEE 802局域网的MAC地址格式 组织唯一标识符OUI 生产网络设备的厂商，需要向IEEE的注册管理机构申请一个或多个OUI 网络接口标识符 由获得OUI的厂商自行随意分配 EUI-48 48是这个MAC地址的位数 对于使用EUI-48空间的应用程序，IEEE的目标寿命为100年（直到2080年），但是鼓励采用EUI-64作为替代 关于无效的 MAC 帧 数据字段的长度与长度字段的值不一致； 帧的长度不是整数个字节； 用收到的帧检验序列 FCS 查出有差错； 数据字段的长度不在 46 ~ 1500 字节之间。局域网中，MAC帧会根据以太网协议进行传输，所以最少需要64个字节，然后减去非数据字段字节数得到46 有效的 MAC 帧长度为 64 ~ 1518 字节之间。 对于检查出的无效 MAC 帧就简单地丢弃。以太网不负责重传丢弃的帧。 ","date":"2023-06-08","objectID":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E6%95%B0%E6%8D%AE%E9%93%BE%E8%B7%AF%E5%B1%82/:23:2","tags":[],"title":"计算机网络之数据链路层","uri":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E6%95%B0%E6%8D%AE%E9%93%BE%E8%B7%AF%E5%B1%82/"},{"categories":["计算机网络"],"content":"IEEE 802局域网的MAC地址发送顺序 ","date":"2023-06-08","objectID":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E6%95%B0%E6%8D%AE%E9%93%BE%E8%B7%AF%E5%B1%82/:23:3","tags":[],"title":"计算机网络之数据链路层","uri":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E6%95%B0%E6%8D%AE%E9%93%BE%E8%B7%AF%E5%B1%82/"},{"categories":["计算机网络"],"content":"单播MAC地址举例 主机B给主机C发送单播帧，主机B首先要构建该单播帧，在帧首部中的目的地址字段填入主机C的MAC地址，源地址字段填入自己的MAC地址，再加上帧首部的其他字段、数据载荷以及帧尾部，就构成了该单播帧 主机B将该单播帧发送出去，主机A和C都会收到该单播帧 主机A的网卡发现该单播帧的目的MAC地址与自己的MAC地址不匹配，丢弃该帧 主机C的网卡发现该单播帧的目的MAC地址与自己的MAC地址匹配，接受该帧 并将该帧交给其上层处理 ","date":"2023-06-08","objectID":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E6%95%B0%E6%8D%AE%E9%93%BE%E8%B7%AF%E5%B1%82/:23:4","tags":[],"title":"计算机网络之数据链路层","uri":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E6%95%B0%E6%8D%AE%E9%93%BE%E8%B7%AF%E5%B1%82/"},{"categories":["计算机网络"],"content":"广播MAC地址举例 假设主机B要发送一个广播帧，主机B首先要构建该广播帧，在帧首部中的目的地址字段填入广播地址，也就是十六进制的全F，源地址字段填入自己的MAC地址，再加上帧首部中的其他字段、数据载荷以及帧尾部，就构成了该广播帧 主机B讲该广播帧发送出去，主机A和C都会收到该广播帧，发现该帧首部中的目的地址字段的内容是广播地址，就知道该帧是广播帧，主机A和主机C都接受该帧，并将该帧交给上层处理 ","date":"2023-06-08","objectID":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E6%95%B0%E6%8D%AE%E9%93%BE%E8%B7%AF%E5%B1%82/:23:5","tags":[],"title":"计算机网络之数据链路层","uri":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E6%95%B0%E6%8D%AE%E9%93%BE%E8%B7%AF%E5%B1%82/"},{"categories":["计算机网络"],"content":"多播MAC地址举例 假设主机A要发送多播帧给该多播地址。将该多播地址的左起第一个字节写成8个比特，第一个字节的最低比特位是1，这就表明该地址是多播地址。 快速判断地址是不是多播地址，就是上图所示箭头所指的第十六进制数不能整除2（1,3,5,7,9,B,D,F），则该地址是多播地址。理解：最低位为1表明一定是个奇数。 假设主机B，C和D支持多播，各用户给自己的主机配置多播组列表如下所示 主机B属于两个多播组，主机C也属于两个多播组，而主机D不属于任何多播组 主机A首先要构建该多播帧，在帧首部中的目的地址字段填入该多播地址，源地址点填入自己的MAC地址，再加上帧首部中的其他字段、数据载荷以及帧尾部，就构成了该多播帧 主机A将该多播帧发送出去，主机B、C、D都会收到该多播帧 主机B和C发现该多播帧的目的MAC地址在自己的多播组列表中，主机B和C都会接受该帧 主机D发现该多播帧的目的MAC地址不在自己的多播组列表中，则丢弃该多播帧 给主机配置多播组列表进行私有应用时，不得使用公有的标准多播地址 ","date":"2023-06-08","objectID":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E6%95%B0%E6%8D%AE%E9%93%BE%E8%B7%AF%E5%B1%82/:23:6","tags":[],"title":"计算机网络之数据链路层","uri":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E6%95%B0%E6%8D%AE%E9%93%BE%E8%B7%AF%E5%B1%82/"},{"categories":["计算机网络"],"content":"IP地址 IP地址属于网络层的范畴，不属于数据链路层的范畴 下面内容讲的是IP地址的使用，详细的IP地址内容在网络层中介绍 ","date":"2023-06-08","objectID":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E6%95%B0%E6%8D%AE%E9%93%BE%E8%B7%AF%E5%B1%82/:24:0","tags":[],"title":"计算机网络之数据链路层","uri":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E6%95%B0%E6%8D%AE%E9%93%BE%E8%B7%AF%E5%B1%82/"},{"categories":["计算机网络"],"content":"基本概念 ","date":"2023-06-08","objectID":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E6%95%B0%E6%8D%AE%E9%93%BE%E8%B7%AF%E5%B1%82/:24:1","tags":[],"title":"计算机网络之数据链路层","uri":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E6%95%B0%E6%8D%AE%E9%93%BE%E8%B7%AF%E5%B1%82/"},{"categories":["计算机网络"],"content":"从网络体系结构看IP地址与MAC地址 ","date":"2023-06-08","objectID":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E6%95%B0%E6%8D%AE%E9%93%BE%E8%B7%AF%E5%B1%82/:24:2","tags":[],"title":"计算机网络之数据链路层","uri":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E6%95%B0%E6%8D%AE%E9%93%BE%E8%B7%AF%E5%B1%82/"},{"categories":["计算机网络"],"content":"数据包转发过程中IP地址与MAC地址的变化情况 图上各主机和路由器各接口的IP地址和MAC地址用简单的标识符来表示 如何从IP地址找出其对应的MAC地址？ ARP协议 ","date":"2023-06-08","objectID":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E6%95%B0%E6%8D%AE%E9%93%BE%E8%B7%AF%E5%B1%82/:24:3","tags":[],"title":"计算机网络之数据链路层","uri":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E6%95%B0%E6%8D%AE%E9%93%BE%E8%B7%AF%E5%B1%82/"},{"categories":["计算机网络"],"content":"ARP协议 如何从IP地址找出其对应的MAC地址？ ARP（地址解析协议） ","date":"2023-06-08","objectID":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E6%95%B0%E6%8D%AE%E9%93%BE%E8%B7%AF%E5%B1%82/:25:0","tags":[],"title":"计算机网络之数据链路层","uri":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E6%95%B0%E6%8D%AE%E9%93%BE%E8%B7%AF%E5%B1%82/"},{"categories":["计算机网络"],"content":"流程 ARP高速缓存表 当主机B要给主机C发送数据包时，会首先在自己的ARP高速缓存表中查找主机C的IP地址所对应的MAC地址，但未找到，因此，主机B需要发送ARP请求报文，来获取主机C的MAC地址 ARP请求报文有具体的格式，上图的只是简单描述 ARP请求报文被封装在MAC帧中发送，目的地址为广播地址 主机B发送封装有ARP请求报文的广播帧，总线上的其他主机都能收到该广播帧 收到ARP请求报文的主机A和主机C会把ARP请求报文交给上层的ARP进程 主机A发现所询问的IP地址不是自己的IP地址，因此不用理会 主机C的发现所询问的IP地址是自己的IP地址，需要进行相应 动态与静态的区别 ARP协议只能在一段链路或一个网络上使用，而不能跨网络使用 ARP协议的使用是逐段链路进行的 ","date":"2023-06-08","objectID":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E6%95%B0%E6%8D%AE%E9%93%BE%E8%B7%AF%E5%B1%82/:25:1","tags":[],"title":"计算机网络之数据链路层","uri":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E6%95%B0%E6%8D%AE%E9%93%BE%E8%B7%AF%E5%B1%82/"},{"categories":["计算机网络"],"content":"总结 ARP表中的IP地址与MAC地址的对应关系记录，是会定期自动删除的，因为IP地址与MAC地址的对应关系不是永久性的 3.8、集线器与交换机的区别 ","date":"2023-06-08","objectID":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E6%95%B0%E6%8D%AE%E9%93%BE%E8%B7%AF%E5%B1%82/:25:2","tags":[],"title":"计算机网络之数据链路层","uri":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E6%95%B0%E6%8D%AE%E9%93%BE%E8%B7%AF%E5%B1%82/"},{"categories":["计算机网络"],"content":"集线器-在物理层扩展以太网 ","date":"2023-06-08","objectID":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E6%95%B0%E6%8D%AE%E9%93%BE%E8%B7%AF%E5%B1%82/:26:0","tags":[],"title":"计算机网络之数据链路层","uri":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E6%95%B0%E6%8D%AE%E9%93%BE%E8%B7%AF%E5%B1%82/"},{"categories":["计算机网络"],"content":"概念 传统以太网最初是使用粗同轴电缆，后来演进到使用比较便宜的细同轴电缆，最后发展为使用更便宜和更灵活的双绞线。 采用双绞线的以太网采用星形拓扑，在星形的中心则增加了一种可靠性非常高的设备，叫做集线器 (hub)。 集线器是也可以看做多口中继器，每个端口都可以成为一个中继器，中继器是对减弱的信号进行放大和发送的设备 集线器的以太网在逻辑上仍是个总线网，需要使用CSMA/CD协议来协调各主机争用总线，只能工作在半双工模式，收发帧不能同时进行 ","date":"2023-06-08","objectID":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E6%95%B0%E6%8D%AE%E9%93%BE%E8%B7%AF%E5%B1%82/:26:1","tags":[],"title":"计算机网络之数据链路层","uri":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E6%95%B0%E6%8D%AE%E9%93%BE%E8%B7%AF%E5%B1%82/"},{"categories":["计算机网络"],"content":"集线器HUB在物理层扩展以太网 使用集线器扩展：将多个以太网段连成更大的、多级星形结构的以太网 优点 使原来属于不同碰撞域的以太网上的计算机能够进行跨碰撞域的通信。 扩大了以太网覆盖的地理范围。 缺点 碰撞域增大了，但总的吞吐量并未提高，反而降低。 假设两个主机进行数据交换，那么在整个扩展以太网下的所有主机都会收到数据，所以那么其他主机发送数据或传输数据就会发生冲突。显然，这个冲突的可能性反而提高了 如果不同的碰撞域使用不同的数据率，那么就不能用集线器将它们互连起来。 碰撞域 碰撞域（collision domain）又称为冲突域，是指网络中一个站点发出的帧会与其他站点发出的帧产生碰撞或冲突的那部分网络。 例如：几个主机接到一个集线器上，形成星型拓扑结构，那么一个主机给另外一个主机进行数据交换时，其他主机都能收到数据，因此，其他主机不能发送再发送数据了，否则就会冲突。 所以，这也是集线器的最大问题，容易导致“广播风暴”。 碰撞域越大，发生碰撞的概率越高。 为了解决集线器组装时，冲突越来越大的问题，后面又出现了网桥和交换机来对以太网进行扩展的技术 ","date":"2023-06-08","objectID":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E6%95%B0%E6%8D%AE%E9%93%BE%E8%B7%AF%E5%B1%82/:26:2","tags":[],"title":"计算机网络之数据链路层","uri":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E6%95%B0%E6%8D%AE%E9%93%BE%E8%B7%AF%E5%B1%82/"},{"categories":["计算机网络"],"content":"以太网交换机和网桥-在数据链路层扩展以太网 ","date":"2023-06-08","objectID":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E6%95%B0%E6%8D%AE%E9%93%BE%E8%B7%AF%E5%B1%82/:27:0","tags":[],"title":"计算机网络之数据链路层","uri":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E6%95%B0%E6%8D%AE%E9%93%BE%E8%B7%AF%E5%B1%82/"},{"categories":["计算机网络"],"content":"概念 扩展以太网更常用的方法是在数据链路层进行。 早期使用网桥，现在使用以太网交换机。 网桥 网桥工作在数据链路层。 它根据 MAC 帧的目的地址对收到的帧进行转发和过滤。 当网桥收到一个帧时，并不是向所有的接口转发此帧，而是先检查此帧的目的MAC 地址，然后再确定将该帧转发到哪一个接口，或把它丢弃。 交换机 1990 年问世的交换式集线器 (switching hub) 可明显地提高以太网的性能。 交换式集线器常称为以太网交换机 (switch) 或第二层交换机 (L2 switch)，强调这种交换机工作在数据链路层。 以太网交换机实质上就是一个多接口的网桥 网桥和交换机用户分割冲突域，就是网桥和交换机可以较少被逼的广播(hub导致的)，但不能分割广播域。不严格地说，交换机可以看作网桥的高度集成。 ","date":"2023-06-08","objectID":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E6%95%B0%E6%8D%AE%E9%93%BE%E8%B7%AF%E5%B1%82/:27:1","tags":[],"title":"计算机网络之数据链路层","uri":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E6%95%B0%E6%8D%AE%E9%93%BE%E8%B7%AF%E5%B1%82/"},{"categories":["计算机网络"],"content":"集线器HUB与交换机SWITCH区别 使用集线器互连而成的共享总线式以太网上的某个主机，要给另一个主机发送单播帧，该单播帧会通过共享总线传输到总线上的其他各个主机 使用交换机互连而成的交换式以太网上的某个主机，要给另一个主机发送单播帧，该单播帧进入交换机后，交换机会将该单播帧转发给目的主机，而不是网络中的其他各个主机 这个例子的前提条件是忽略ARP过程，并假设交换机的帧交换表已经学习或配置好了 以太网交换机的交换方式 存储转发方式 把整个数据帧先缓存后再进行处理。 直通 (cut-through) 方式 接收数据帧的同时就立即按数据帧的目的 MAC 地址决定该帧的转发接口，因而提高了帧的转发速度。 缺点是它不检查差错就直接将帧转发出去，因此有可能也将一些无效帧转发给其他的站。 这个例子的前提条件是忽略ARP过程，并假设交换机的帧交换表已经学习或配置好了 对比集线器和交换机 多台主机同时给另一台主机发送单播帧 集线器以太网：会产生碰撞，遭遇碰撞的帧会传播到总线上的各主机 交换机以太网：会将它们缓存起来，然后逐个转发给目的主机，不会产生碰撞 这个例子的前提条件是忽略ARP过程，并假设交换机的帧交换表已经学习或配置好了 集线器扩展以太网和交换机扩展以太网区别 单播 广播 多个单播 广播域（broadcast domain）：指这样一部分网络，其中任何一台设备发出的广播通信都能被该部分网络中的所有其他设备所接收。 ","date":"2023-06-08","objectID":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E6%95%B0%E6%8D%AE%E9%93%BE%E8%B7%AF%E5%B1%82/:27:2","tags":[],"title":"计算机网络之数据链路层","uri":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E6%95%B0%E6%8D%AE%E9%93%BE%E8%B7%AF%E5%B1%82/"},{"categories":["计算机网络"],"content":"总结 工作在数据链路层的以太网交换机，其性能远远超过工作在物理层的集线器，而且价格并不贵，这就使得集线器逐渐被市场淘汰 3.9、以太网交换机自学习和转发帧的流程 ","date":"2023-06-08","objectID":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E6%95%B0%E6%8D%AE%E9%93%BE%E8%B7%AF%E5%B1%82/:28:0","tags":[],"title":"计算机网络之数据链路层","uri":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E6%95%B0%E6%8D%AE%E9%93%BE%E8%B7%AF%E5%B1%82/"},{"categories":["计算机网络"],"content":"概念 ","date":"2023-06-08","objectID":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E6%95%B0%E6%8D%AE%E9%93%BE%E8%B7%AF%E5%B1%82/:29:0","tags":[],"title":"计算机网络之数据链路层","uri":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E6%95%B0%E6%8D%AE%E9%93%BE%E8%B7%AF%E5%B1%82/"},{"categories":["计算机网络"],"content":"自学习和转发帧的例子 以下例子假设各主机知道网络中其他各主机的MAC地址（无需进行ARP） A -\u003e B A 先向 B 发送一帧。该帧从接口 1 进入到交换机 交换机收到帧后，先查找（图中左边）交换表。没有查到应从哪个接口转发这个帧给 B 交换机把这个帧的源地址 A 和接口 1 写入（图中左边）交换表中 交换机向除接口 1 以外的所有的接口广播这个帧 接口 4到接口 2，先查找（图中右边）交换表。没有查到应从哪个接口转发这个帧给 B 交换机把这个帧的源地址 A 和接口 2 写入（图中右边）交换表中 除B主机之外与该帧的目的地址不相符，将丢弃该帧 主机B发现是给自己的帧，接受该帧 可以看到在这个过程中，没有找到目标MAC地址对应的接口时，就将源MAC地址和源主机接接口号记录在表中，之后再将帧进行广播，如果目的MAC地址与源主机相通，则可以将帧接收到。故此，这个过程学习到了源主机MAC地址和接口。 B -\u003e A B 向 A 发送一帧。该帧从接口 3 进入到交换机 交换机收到帧后，先查找（图中左边）交换表。发现（图中左边）交换表中的 MAC 地址有 A，表明要发送给A的帧应从接口1转发出去。于是就把这个帧传送到接口 1 转发给 A。 主机 A 发现目的地址是它，就接受该帧 交换机把这个帧的源地址 B 和接口 3 写入（图中左边）交换表中 如果交换机的帧交换表中含有目的主机MAC地址对应的接口号，那么就不需要进行广播了，直接就将帧传给指定接口。当然，源主机MAC和接口地址没有再帧交换表时，也应该学习到。 E -\u003e A E 向 A发送一帧 交换机收到帧后，先查找（图中右边）交换表。发现（图中右边）交换表中的 MAC 地址有 A，表明要发送给A的帧应从接口2转发出去。于是就把这个帧传送到接口 2 转发给 接口 4。 交换机把这个帧的源地址 E 和接口 3 写入（图中右边）交换表中 接口 4 到 左边的交换机，先查找（图中左边）交换表。发现（图中左边）交换表中的 MAC 地址有 A，表明要发送给A的帧应从接口1转发出去。于是就把这个帧传送到接口 1 转发给 A。 交换机把这个帧的源地址 E 和接口 4 写入（图中左边）交换表中 主机 A 发现目的地址是它，就接受该帧 这个过程主要是在帧的传播过程中，学习到接口和目的源主机MAC地址的关系。 G -\u003e A 主机 A、主机 G、交换机 1的接口 1就共享同一条总线（相当于总线式网络，可以想象成用集线器连接了） 主机 G 发送给 主机 A 一个帧 主机 A 和 交换机接口 1都能接收到 主机 A 的网卡收到后，根据帧的目的MAC地址A，就知道是发送给自己的帧，就接受该帧 交换机 1收到该帧后，首先进行登记工作 然后交换机 1对该帧进行转发，该帧的MAC地址是A，在（图中左边）交换表查找MAC 地址有 A MAC 地址为 A的接口号是1，但是该帧正是从接口 1 进入交换机的，交换机不会再从该接口 1 将帧转发出去，因为这是没有必要，于是丢弃该帧 随着网络中各主机都发送了帧后，网络中的各交换机就可以学习到各主机的MAC地址，以及它们与自己各接口的对应关系 考虑到可能有时要在交换机的接口更换主机，或者主机要更换其网络适配器，这就需要更改交换表中的项目。为此，在交换表中每个项目都设有一定的有效时间。过期的项目就自动被删除。 以太网交换机的这种自学习方法使得以太网交换机能够即插即用，不必人工进行配置，因此非常方便。 ","date":"2023-06-08","objectID":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E6%95%B0%E6%8D%AE%E9%93%BE%E8%B7%AF%E5%B1%82/:30:0","tags":[],"title":"计算机网络之数据链路层","uri":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E6%95%B0%E6%8D%AE%E9%93%BE%E8%B7%AF%E5%B1%82/"},{"categories":["计算机网络"],"content":"总结 交换机自学习和转发帧的步骤归纳 3.10、以太网交换机的生成树协议STP ","date":"2023-06-08","objectID":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E6%95%B0%E6%8D%AE%E9%93%BE%E8%B7%AF%E5%B1%82/:31:0","tags":[],"title":"计算机网络之数据链路层","uri":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E6%95%B0%E6%8D%AE%E9%93%BE%E8%B7%AF%E5%B1%82/"},{"categories":["计算机网络"],"content":"如何提高以太网的可靠性 ","date":"2023-06-08","objectID":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E6%95%B0%E6%8D%AE%E9%93%BE%E8%B7%AF%E5%B1%82/:32:0","tags":[],"title":"计算机网络之数据链路层","uri":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E6%95%B0%E6%8D%AE%E9%93%BE%E8%B7%AF%E5%B1%82/"},{"categories":["计算机网络"],"content":"生成树协议STP 为什么要用STP协议？ 为了解决第二层网络环路问题而又要保证网络的稳定和健壮性，引入了链路动态管理的策略。 首先通过阻塞某些链路避免环路的产生（避免环路） 再次当正常工作的链路由于故障断开时，阻塞的链路立刻激活，迅速取代故障链路的位置，保证网络的正常运行。 IEEE 802.1D 标准制定了一个生成树协议 STP (Spanning Tree Protocol)。 其要点是：不改变网络的实际拓扑，但在逻辑上则切断某些链路，使得从一台主机到所有其他主机的路径是无环路的树状结构，从而消除了兜圈子现象。 ","date":"2023-06-08","objectID":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E6%95%B0%E6%8D%AE%E9%93%BE%E8%B7%AF%E5%B1%82/:33:0","tags":[],"title":"计算机网络之数据链路层","uri":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E6%95%B0%E6%8D%AE%E9%93%BE%E8%B7%AF%E5%B1%82/"},{"categories":["计算机网络"],"content":"STP中根桥，根端口，指定端口的选举规则 https://developer.aliyun.com/article/911748 3.11、虚拟局域网VLAN ","date":"2023-06-08","objectID":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E6%95%B0%E6%8D%AE%E9%93%BE%E8%B7%AF%E5%B1%82/:33:1","tags":[],"title":"计算机网络之数据链路层","uri":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E6%95%B0%E6%8D%AE%E9%93%BE%E8%B7%AF%E5%B1%82/"},{"categories":["计算机网络"],"content":"LAN与VLAN LAN 表示 Local Area Network，本地局域网。 一个 LAN 表示一个广播域，含义是：LAN 中的所有成员都会收到任意一个成员发出的广播包。 上图为最基本的LAN布局。如果设备间想要通讯，必须要获取到对方的MAC地址。 举例：A 发信息给 C，A 并不知道 C 的 MAC 地址。此时通过 ARP 协议（Address Resolution Protocol；地址解析协议；）获取 C 的 MAC 地址，A 先要广播一个包含目标 IP 地址的 ARP 请求到链接在集线器上的所有设备上，C 接受到广播后返回 MAC 地址给 A，其他设备则丢弃信息。至此已经建立设备间通信的准备条件。 虚拟局域网（VLAN）是在局域网（LAN）的逻辑上划分成多个广播域，每一个广播域就是一个 VLAN。 下图为交换机划分虚拟局域网。交换机把一个广播域划分成了3个广播域，物理上这些设备在一个交换机上，但是逻辑上已经分别划分到三个交换机上，所以会有三个局域网（虚拟局域网），三个广播域。 ","date":"2023-06-08","objectID":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E6%95%B0%E6%8D%AE%E9%93%BE%E8%B7%AF%E5%B1%82/:34:0","tags":[],"title":"计算机网络之数据链路层","uri":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E6%95%B0%E6%8D%AE%E9%93%BE%E8%B7%AF%E5%B1%82/"},{"categories":["计算机网络"],"content":"为什么要虚拟局域网VLAN 广播风暴 分割广播域的方法 为了分割广播域，所以虚拟局域网VLAN技术应运而生 ","date":"2023-06-08","objectID":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E6%95%B0%E6%8D%AE%E9%93%BE%E8%B7%AF%E5%B1%82/:35:0","tags":[],"title":"计算机网络之数据链路层","uri":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E6%95%B0%E6%8D%AE%E9%93%BE%E8%B7%AF%E5%B1%82/"},{"categories":["计算机网络"],"content":"概念 利用以太网交换机可以很方便地实现虚拟局域网 VLAN (Virtual LAN)。 IEEE 802.1Q 对虚拟局域网 VLAN 的定义： 虚拟局域网 VLAN 是由一些局域网网段构成的与物理位置无关的逻辑组，而这些网段具有某些共同的需求。每一个 VLAN 的帧都有一个明确的标识符，指明发送这个帧的计算机是属于哪一个 VLAN。 同一个VLAN内部可以广播通信，不同VLAN不可以广播通信 虚拟局域网其实只是局域网给用户提供的一种服务，而并不是一种新型局域网。 由于虚拟局域网是用户和网络资源的逻辑组合，因此可按照需要将有关设备和资源非常方便地重新组合，使用户从不同的服务器或数据库中存取所需的资源。 ","date":"2023-06-08","objectID":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E6%95%B0%E6%8D%AE%E9%93%BE%E8%B7%AF%E5%B1%82/:36:0","tags":[],"title":"计算机网络之数据链路层","uri":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E6%95%B0%E6%8D%AE%E9%93%BE%E8%B7%AF%E5%B1%82/"},{"categories":["计算机网络"],"content":"虚拟局域网VLAN的实现机制 虚拟局域网VLAN技术是在交换机上实现的，需要交换机能够实现以下功能 能够处理带有VLAN标记的帧——IEEE 802.1 Q帧 交换机的各端口可以支持不同的端口类型，不同端口类型的端口对帧的处理方式有所不同 Access端口 交换机与用户计算机之间的互连 同一个VLAN内部可以广播通信，不同VLAN不可以广播通信。 简单来说就是：VLAN相较于LAN的实现上，就是在广播以太网MAC帧的时候，再在这个帧上添加一个4字节的VLAN标记，指明只有满足PVID==VID条件才能将广播的以太网MAC帧发送到相同VLAN的交换机的access类型端口连着的主机；其他不满足PVID==VID的条件的主机也就是不在一个VLAN下，自然就不能接收到帧。 Truck端口 交换机之间 或 交换机与路由器之间的互连 小例题 华为交换机私有的Hybrid端口类型 ","date":"2023-06-08","objectID":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E6%95%B0%E6%8D%AE%E9%93%BE%E8%B7%AF%E5%B1%82/:37:0","tags":[],"title":"计算机网络之数据链路层","uri":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E6%95%B0%E6%8D%AE%E9%93%BE%E8%B7%AF%E5%B1%82/"},{"categories":["计算机网络"],"content":"总结 虚拟局域网优点 虚拟局域网（VLAN）技术具有以下主要优点： 改善了性能 简化了管理 降低了成本 改善了安全性 ","date":"2023-06-08","objectID":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E6%95%B0%E6%8D%AE%E9%93%BE%E8%B7%AF%E5%B1%82/:38:0","tags":[],"title":"计算机网络之数据链路层","uri":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E6%95%B0%E6%8D%AE%E9%93%BE%E8%B7%AF%E5%B1%82/"},{"categories":["计算机网络"],"content":"[toc] 2.1、物理层的基本概念 2.2、物理层下面的传输媒体 传输媒体也称为传输介质或传输媒介，他就是数据传输系统中在发送器和接收器之间的物理通路。传输媒体可分为两大类，即导引型传输媒体和非导引型传输媒体 传输媒体不属于计算机网络体系结构的任何一层。如果非要将它添加到体系结构中，那只能将其放置到物理层之下。 ","date":"2023-06-08","objectID":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E7%89%A9%E7%90%86%E5%B1%82/:0:0","tags":[],"title":"计算机网络之物理层","uri":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E7%89%A9%E7%90%86%E5%B1%82/"},{"categories":["计算机网络"],"content":"导引型传输媒体 在导引型传输媒体中，电磁波被导引沿着固体媒体传播。 ","date":"2023-06-08","objectID":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E7%89%A9%E7%90%86%E5%B1%82/:1:0","tags":[],"title":"计算机网络之物理层","uri":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E7%89%A9%E7%90%86%E5%B1%82/"},{"categories":["计算机网络"],"content":"同轴电缆 ","date":"2023-06-08","objectID":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E7%89%A9%E7%90%86%E5%B1%82/:1:1","tags":[],"title":"计算机网络之物理层","uri":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E7%89%A9%E7%90%86%E5%B1%82/"},{"categories":["计算机网络"],"content":"双绞线 ","date":"2023-06-08","objectID":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E7%89%A9%E7%90%86%E5%B1%82/:1:2","tags":[],"title":"计算机网络之物理层","uri":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E7%89%A9%E7%90%86%E5%B1%82/"},{"categories":["计算机网络"],"content":"光纤 多模光纤 可以存在多条不同角度入射的光线在一条光纤中传输。这种光纤就称为多模光纤。 单模光纤 若光纤的直径减小到只有一个光的波长，则光纤就像一根波导那样，它可使光线一直向前传播，而不会产生多次反射。这样的光纤称为单模光纤。 ","date":"2023-06-08","objectID":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E7%89%A9%E7%90%86%E5%B1%82/:1:3","tags":[],"title":"计算机网络之物理层","uri":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E7%89%A9%E7%90%86%E5%B1%82/"},{"categories":["计算机网络"],"content":"电力线 ","date":"2023-06-08","objectID":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E7%89%A9%E7%90%86%E5%B1%82/:1:4","tags":[],"title":"计算机网络之物理层","uri":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E7%89%A9%E7%90%86%E5%B1%82/"},{"categories":["计算机网络"],"content":"非导引型传输媒体 非导引型传输媒体是指自由空间。 ","date":"2023-06-08","objectID":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E7%89%A9%E7%90%86%E5%B1%82/:2:0","tags":[],"title":"计算机网络之物理层","uri":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E7%89%A9%E7%90%86%E5%B1%82/"},{"categories":["计算机网络"],"content":"无线电波 ","date":"2023-06-08","objectID":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E7%89%A9%E7%90%86%E5%B1%82/:2:1","tags":[],"title":"计算机网络之物理层","uri":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E7%89%A9%E7%90%86%E5%B1%82/"},{"categories":["计算机网络"],"content":"微波 ","date":"2023-06-08","objectID":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E7%89%A9%E7%90%86%E5%B1%82/:2:2","tags":[],"title":"计算机网络之物理层","uri":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E7%89%A9%E7%90%86%E5%B1%82/"},{"categories":["计算机网络"],"content":"红外线 ","date":"2023-06-08","objectID":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E7%89%A9%E7%90%86%E5%B1%82/:2:3","tags":[],"title":"计算机网络之物理层","uri":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E7%89%A9%E7%90%86%E5%B1%82/"},{"categories":["计算机网络"],"content":"可见光 LIFI 2.3、传输方式 ","date":"2023-06-08","objectID":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E7%89%A9%E7%90%86%E5%B1%82/:2:4","tags":[],"title":"计算机网络之物理层","uri":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E7%89%A9%E7%90%86%E5%B1%82/"},{"categories":["计算机网络"],"content":"串行传输和并行传输 串行传输： 数据是一个比特一个比特依次发送的，因此在发送端与接收端之间，只需要一条数据传输线路即可 并行传输： 一次发送n个比特，因此，在发送端和接收端之间需要有n条传输线路 并行传输的优点是比串行传输的速度n倍，但成本高 数据在传输线路上的传输采用是串行传输，计算机内部的数据传输常用并行传输 计算机内部往往采用一次性传输一个字长的数据。例如64位计算机，可以一次性传输64位数据。 ","date":"2023-06-08","objectID":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E7%89%A9%E7%90%86%E5%B1%82/:3:0","tags":[],"title":"计算机网络之物理层","uri":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E7%89%A9%E7%90%86%E5%B1%82/"},{"categories":["计算机网络"],"content":"同步传输和异步传输 同步传输： 数据块以稳定的比特流的形式传输。字节之间没有间隔 接收端在每个比特信号的中间时刻进行检测，以判别接收到的是比特0还是比特1 由于不同设备的时钟频率存在一定差异，不可能做到完全相同，在传输大量数据的过程中，所产生的判别时刻的累计误差，会导致接收端对比特信号的判别错位 所以要使收发双发时钟保持同步 异步传输： 以字节为独立的传输单位，字节之间的时间间隔不是固定 接收端仅在每个字节的起始处对字节内的比特实现同步 通常在每个字节前后分别加上起始位和结束位 ","date":"2023-06-08","objectID":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E7%89%A9%E7%90%86%E5%B1%82/:4:0","tags":[],"title":"计算机网络之物理层","uri":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E7%89%A9%E7%90%86%E5%B1%82/"},{"categories":["计算机网络"],"content":"单向通信（单工）、双向交替通信（半双工）和双向同时通信（全双工） 在许多情况下，我们要使用“信道（channel）”这一名词。信道和电路并不等同。 信道一般都是用来表示向某一个方向传送信息的媒体。因此，一条通信电路往往包含一条发送信道和一条接收信道。 从通信的双方信息交互的方式来看，可以有以下三种基本方式： 单向通信： 又称为单工通信，即只能有一个方向的通信而没有反方向的交互。无线电广播或有线电以及电视广播就属于这种类型 双向交替通信： 又称为半双工通信，即通信的双方可以发送信息，但不能双方同时发送（当然也就不能同时接收）。这种通信方式使一方发送另一方接收，过一段时间后可以再反过来 双向同时通信： 又称为全双工通信，即通信的双发可以同时发送和接收信息。 单向通信只需要一条信道，而双向交替通信或双向同时通信则需要两条信道（每个方向各一条） 双向同时通信的传输效率最高 2.4、编码与调制 常用术语 数据 (data) —— 运送消息的实体。 信号 (signal) —— 数据的电气的或电磁的表现。 模拟信号 (analogous signal) —— 代表消息的参数的取值是连续的。 数字信号 (digital signal) —— 代表消息的参数的取值是离散的。 码元 (code) —— 在使用时间域（或简称为时域）的波形表示数字信号时，代表不同离散数值的基本波形。 基带信号（即基本频带信号）—— 来自信源的信号。像计算机输出的代表各种文字或图像文件的数据信号都属于基带信号。基带信号就是发出的直接表达了要传输信息的信号。 带通信号——把基带信号经过载波调制后，把信号的频率范围搬移到较高的频段以便在信道中传输。 因此在传输距离较近时，基带信号衰减不大，所以传输距离较近时的计算机网络都采用基带传输方式。例如：计算机到监视器、打印机，我们直接以数据线相连，无需经过调制解调器。 但是，如果距离较远时，显然就需要调制解调器了，也就是采用带通信号。 在计算机网络中，常见的是将数字基带信号通过编码或调制的方法在相应信道进行传输 ","date":"2023-06-08","objectID":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E7%89%A9%E7%90%86%E5%B1%82/:5:0","tags":[],"title":"计算机网络之物理层","uri":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E7%89%A9%E7%90%86%E5%B1%82/"},{"categories":["计算机网络"],"content":"传输媒体与信道的关系 信道的几个基本概念 信道 —— 一般用来表示向某一个方向传送信息的媒体。 单向通信（单工通信）——只能有一个方向的通信而没有反方向的交互。 双向交替通信（半双工通信）——通信的双方都可以发送信息，但不能双方同时发送(当然也就不能同时接收)。 双向同时通信（全双工通信）——通信的双方可以同时发送和接收信息。 严格来说，传输媒体不能和信道划等号 对于单工传输，传输媒体只包含一个信道，要么是发送信道，要么是接收信道 对于半双工和全双工，传输媒体中要包含两个信道，一个发送信道，另一个是接收信道 如果使用信道复用技术，一条传输媒体还可以包含多个信道 ","date":"2023-06-08","objectID":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E7%89%A9%E7%90%86%E5%B1%82/:6:0","tags":[],"title":"计算机网络之物理层","uri":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E7%89%A9%E7%90%86%E5%B1%82/"},{"categories":["计算机网络"],"content":"常用编码 不归零编码 正电平表示比特1/0 负电平表示比特0/1 中间的虚线是零电平，所谓不归零编码，就是指在整个码元时间内，电平不会出现零电平 实际比特1和比特0的表示要看现实怎么规定 这需要发送方的发送与接收方的接收做到严格的同步 需要额外一根传输线来传输时钟信号，使发送方和接收方同步，接收方按时钟信号的节拍来逐个接收码元 但是对于计算机网络，宁愿利用这根传输线传输数据信号，而不是传输时钟信号 由于不归零编码存在同步问题，因此计算机网络中的数据传输不采用这类编码！ 归零编码 归零编码虽然自同步，但编码效率低 曼彻斯特编码 在每个码元时间的中间时刻，信号都会发生跳变 负跳变表示比特1/0 正跳变表示比特0/1 码元中间时刻的跳变即表示时钟，又表示数据 实际比特1和比特0的表示要看现实怎么规定 传统以太网使用的就是曼切斯特编码 差分曼彻斯特编码 在每个码元时间的中间时刻，信号都会发送跳变，但与曼彻斯特不同 跳变仅表示时钟 码元开始处电平是否变换表示数据 变化表示比特1/0 不变化表示比特0/1 实际比特1和比特0的表示要看现实怎么规定 比曼彻斯特编码变化少，更适合较高的传输速率 总结 ","date":"2023-06-08","objectID":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E7%89%A9%E7%90%86%E5%B1%82/:7:0","tags":[],"title":"计算机网络之物理层","uri":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E7%89%A9%E7%90%86%E5%B1%82/"},{"categories":["计算机网络"],"content":"调制 数字信号转换为模拟信号，在模拟信道中传输，例如WiFi，采用补码键控CCK/直接序列扩频DSSS/正交频分复用OFDM等调制方式。 模拟信号转换为另一种模拟信号，在模拟信道中传输，例如，语音数据加载到模拟的载波信号中传输。频分复用FDM技术，充分利用带宽资源。 基本调制方法 调幅AM：所调制的信号由两种不同振幅的基本波形构成。每个基本波形只能表示1比特信息量。 调频FM：所调制的信号由两种不同频率的基本波形构成。每个基本波形只能表示1比特信息量。 调相PM：所调制的信号由两种不同初相位的基本波形构成。每个基本波形只能表示1比特信息量。 但是使用基本调制方法，1个码元只能包含1个比特信息 混合调制 上图码元所对应的4个比特是错误的，码元不能随便对应4个比特 ","date":"2023-06-08","objectID":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E7%89%A9%E7%90%86%E5%B1%82/:8:0","tags":[],"title":"计算机网络之物理层","uri":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E7%89%A9%E7%90%86%E5%B1%82/"},{"categories":["计算机网络"],"content":"码元 在使用时间域的波形表示数字信号时，代表不同离散数值的基本波形。 2.5、信道的极限容量 任何实际的信道都不是理想的，在传输信号时会产生各种失真以及带来多种干扰。 码元传输的速率越高，或信号传输的距离越远，或传输媒体质量越差，在信道的输出端的波形的失真就越严重。 失真的原因： 码元传输的速率越高 信号传输的距离越远 噪声干扰越大 传输媒体质量越差 ","date":"2023-06-08","objectID":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E7%89%A9%E7%90%86%E5%B1%82/:9:0","tags":[],"title":"计算机网络之物理层","uri":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E7%89%A9%E7%90%86%E5%B1%82/"},{"categories":["计算机网络"],"content":"奈氏准则 ","date":"2023-06-08","objectID":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E7%89%A9%E7%90%86%E5%B1%82/:10:0","tags":[],"title":"计算机网络之物理层","uri":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E7%89%A9%E7%90%86%E5%B1%82/"},{"categories":["计算机网络"],"content":"香农公式 ","date":"2023-06-08","objectID":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E7%89%A9%E7%90%86%E5%B1%82/:11:0","tags":[],"title":"计算机网络之物理层","uri":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E7%89%A9%E7%90%86%E5%B1%82/"},{"categories":["计算机网络"],"content":"对比 补充：信道复用技术 本节内容视频未讲到，是《计算机网络（第7版）谢希仁》物理层的内容 ","date":"2023-06-08","objectID":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E7%89%A9%E7%90%86%E5%B1%82/:12:0","tags":[],"title":"计算机网络之物理层","uri":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E7%89%A9%E7%90%86%E5%B1%82/"},{"categories":["计算机网络"],"content":"频分复用、时分复用和统计时分复用 复用 (multiplexing) 是通信技术中的基本概念。 它允许用户使用一个共享信道进行通信，降低成本，提高利用率。 频分复用 FDM (Frequency Division Multiplexing) 将整个带宽分为多份，用户在分配到一定的频带后，在通信过程中自始至终都占用这个频带。 频分复用的所有用户在同样的时间占用不同的带宽资源（请注意，这里的“带宽”是频率带宽而不是数据的发送速率）。 时分复用TDM (Time Division Multiplexing) 时分复用则是将时间划分为一段段等长的时分复用帧（TDM帧）。每一个时分复用的用户在每一个 TDM 帧中占用固定序号的时隙。 每一个用户所占用的时隙是周期性地出现（其周期就是TDM帧的长度）的。 TDM 信号也称为等时 (isochronous) 信号。 时分复用的所有用户在不同的时间占用同样的频带宽度。 时分复用可能会造成线路资源的浪费 使用时分复用系统传送计算机数据时，由于计算机数据的突发性质，用户对分配到的子信道的利用率一般是不高的。 统计时分复用 STDM (Statistic TDM) ","date":"2023-06-08","objectID":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E7%89%A9%E7%90%86%E5%B1%82/:13:0","tags":[],"title":"计算机网络之物理层","uri":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E7%89%A9%E7%90%86%E5%B1%82/"},{"categories":["计算机网络"],"content":"波分复用 波分复用 WDM(Wavelength Division Multiplexing) ","date":"2023-06-08","objectID":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E7%89%A9%E7%90%86%E5%B1%82/:14:0","tags":[],"title":"计算机网络之物理层","uri":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E7%89%A9%E7%90%86%E5%B1%82/"},{"categories":["计算机网络"],"content":"码分复用 码分复用 CDM (Code Division Multiplexing) 常用的名词是码分多址 CDMA (Code Division Multiple Access)。 各用户使用经过特殊挑选的不同码型，因此彼此不会造成干扰。 这种系统发送的信号有很强的抗干扰能力，其频谱类似于白噪声，不易被敌人发现。 ","date":"2023-06-08","objectID":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E7%89%A9%E7%90%86%E5%B1%82/:15:0","tags":[],"title":"计算机网络之物理层","uri":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E7%89%A9%E7%90%86%E5%B1%82/"},{"categories":["计算机网络"],"content":"[toc] 图示说明 代表着主机 代表服务器 代表着路由器 代表着网络 1.1、计算机网络在信息时代的作用 计算机网络已由一种通信基础设施发展成为一种重要的信息服务基础设施 计算机网络已经像水，电，煤气这些基础设施一样，成为我们生活中不可或缺的一部分 ","date":"2023-06-03","objectID":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E6%A6%82%E8%BF%B0/:0:0","tags":[],"title":"计算机网络之概述","uri":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E6%A6%82%E8%BF%B0/"},{"categories":["计算机网络"],"content":"数字化 将现实世界的信息存储到计算机中，也就是01二进制来表示所有的现实世界信息。例如，我们可以将个人的身份信息存放为计算机中的一堆二进制位。 数字化的目的是将现实世界中的事物转化为数字化的形式，以便于计算机和互联网的处理和传输。 ","date":"2023-06-03","objectID":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E6%A6%82%E8%BF%B0/:0:1","tags":[],"title":"计算机网络之概述","uri":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E6%A6%82%E8%BF%B0/"},{"categories":["计算机网络"],"content":"网络化 网络化中有三网：电信网络、计算机网络、有线电视网络。 电信网络 电信网络是指通过电信技术实现的通信网络，它包括了固定电话网络、移动通信网络、互联网等。 电信网络通过无线电波、光纤、铜线等传输介质，将声音、图像、数据等信息传输到用户之间。电信网络的优点是覆盖面广、传输速度快、信息容量大、可靠性高等。 在现代社会中，电信网络已经成为人们生活、工作、学习和娱乐中不可或缺的一部分。 计算机网络 计算机网络是指将多台计算机通过通信设备连接起来，以便它们之间可以进行数据交换和资源共享的网络系统。 计算机网络可以通过有线或无线的方式连接计算机，使得计算机之间可以相互通信，共享数据和设备。计算机网络的目的是实现信息的共享和传递，提高计算机资源的利用率，同时也可以提高工作效率和降低成本。 计算机网络的应用非常广泛，包括互联网（因特网）、局域网、广域网、数据中心网络等。 有线电视网络 有线电视网络是一种通过有线电缆传输电视信号的网络。它使用同轴电缆、光纤或双绞线等传输介质，将电视信号传输到用户家中的电视机上。 有线电视网络通常提供更多的电视频道和更高的画质，同时也可以提供互联网和电话服务。 优点：信号稳定、画质高、可靠性强，但需要铺设大量的电缆，成本较高。 ","date":"2023-06-03","objectID":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E6%A6%82%E8%BF%B0/:0:2","tags":[],"title":"计算机网络之概述","uri":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E6%A6%82%E8%BF%B0/"},{"categories":["计算机网络"],"content":"信息化 利用信息技术和通信技术来改善生产、管理和服务等方面的工作，使得信息的获取、处理、传递和利用更加高效和便捷。 数字化是信息化的基础和前提，而信息化则是数字化的高级应用和发展。 ","date":"2023-06-03","objectID":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E6%A6%82%E8%BF%B0/:0:3","tags":[],"title":"计算机网络之概述","uri":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E6%A6%82%E8%BF%B0/"},{"categories":["计算机网络"],"content":"我国互联网发展状况 1.2、因特网概述 ","date":"2023-06-03","objectID":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E6%A6%82%E8%BF%B0/:1:0","tags":[],"title":"计算机网络之概述","uri":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E6%A6%82%E8%BF%B0/"},{"categories":["计算机网络"],"content":"1、网络、互连网（互联网）和因特网 网络：网络（Network）由若干**结点（Node）和连接这些结点的链路（Link）**组成。 互连网（互联网）：多个网络通过路由器互连起来，这样就构成了一个覆盖范围更大的网络，即互连网（互联网）。因此，互联网又称为“网络的网络（Network of Networks）”。 因特网：因特网（Internet）是世界上最大的互连网络（用户数以亿计，互连的网络数以百万计）。 internet与Internet的区别 internet(互联网或互连网)是一个通用名词，它泛指多个计算机网络互连而成的网络。在这些网络之间的通信协议可以是任意的。 Internet（因特网）则是一个专用名词，它指当前全球最大的、开放的、由众多网络互连而成的特定计算机网络，它采用TCP/IP协议族作为通信的规则，其前身是美国的ARPANET。 任意把几个计算机网络互连起来（不管采用什么协议），并能够相互通信，这样构成的是一个互连网(internet) ，而不是互联网(Internet)。 ","date":"2023-06-03","objectID":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E6%A6%82%E8%BF%B0/:2:0","tags":[],"title":"计算机网络之概述","uri":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E6%A6%82%E8%BF%B0/"},{"categories":["计算机网络"],"content":"2、因特网发展的三个阶段 因特网服务提供者ISP(Internet Service Provider) 普通用户是如何接入到因特网的呢？ 答：通过ISP接入因特网 ISP可以从因特网管理机构申请到成块的IP地址，同时拥有通信线路以及路由器等联网设备。任何机构和个人只需缴纳费用，就可从ISP的得到所需要的IP地址。 因为因特网上的主机都必须有IP地址才能进行通信，这样就可以通过该ISP接入到因特网 中国的三大ISP：中国电信，中国联通和中国移动 基于ISP的三层结构的因特网 一旦某个用户能够接入到因特网，那么他也可以成为一个ISP，所需要做的就是购买一些如调制解调器或路由器这样的设备，让其他用户可以和他相连。 ","date":"2023-06-03","objectID":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E6%A6%82%E8%BF%B0/:3:0","tags":[],"title":"计算机网络之概述","uri":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E6%A6%82%E8%BF%B0/"},{"categories":["计算机网络"],"content":"3、因特网的标准化工作 因特网的标准化工作对因特网的发展起到了非常重要的作用。 因特网在指定其标准上的一个很大的特点是面向公众。 因特网所有的RFC(Request For Comments)技术文档都可从因特网上免费下载； 任何人都可以随时用电子邮件发表对某个文档的意见或建议。 因特网协会ISOC是一个国际性组织，它负责对因特网进行全面管理，以及在世界范围内促进其发展和使用。 因特网体系结构委员会IAB，负责管理因特网有关协议的开发； 因特网工程部IETF，负责研究中短期工程问题，主要针对协议的开发和标准化； 因特网研究部IRTF，从事理论方面的研究和开发一些需要长期考虑的问题。 制订因特网的正式标准要经过一下4个阶段： 1、因特网草案（在这个阶段还不是RFC文档） 2、建议标准（从这个阶段开始就成为RFC文档） 3、草案标准 4、因特网标准 ","date":"2023-06-03","objectID":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E6%A6%82%E8%BF%B0/:4:0","tags":[],"title":"计算机网络之概述","uri":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E6%A6%82%E8%BF%B0/"},{"categories":["计算机网络"],"content":"4、因特网的组成 边缘部分 由所有连接在因特网上的主机组成（台式电脑，大型服务器，笔记本电脑，平板，智能手机等）。这部分是用户直接使用的，用来进行通信（传送数据、音频或视频）和资源共享。 核心部分 由大量网络和连接这些网络的路由器组成。这部分是为边缘部分提供服务的（提供连通性和交换）。 路由器是一种专用计算机，但我们不称它为主机，路由器是实现分组交换的关键构建，其任务是转发收到的分组，这是网络核心最重要的部分。 处在互联网边缘的部分就是连接在互联网上的所有的主机，这些主机又称为端系统 (end system)。 端系统在功能上可能有很大的差别： 小的端系统可以是一台普通个人电脑，具有上网功能的智能手机，甚至是一个很小的网络摄像头。 大的端系统则可以是一台非常昂贵的大型计算机。 端系统的拥有者可以是个人，也可以是单位（如学校、企业、政府机关等），当然也可以是某个ISP。 ","date":"2023-06-03","objectID":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E6%A6%82%E8%BF%B0/:5:0","tags":[],"title":"计算机网络之概述","uri":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E6%A6%82%E8%BF%B0/"},{"categories":["计算机网络"],"content":"补充： 端系统之间通信的含义 “主机 A 和主机 B 进行通信”实际上是指：运行在主机 A 上的某个程序和运行在主机 B 上的另一个程序进行通信。即“主机 A 的某个进程和主机 B 上的另一个进程进行通信”。简称为“计算机之间通信”。 端系统之间的通信方式通常可划分为两大类： 客户-服务器方式： 客户 (client) 和服务器 (server) 都是指通信中所涉及的两个应用进程。 客户 - 服务器方式所描述的是进程之间服务和被服务的关系。 客户是服务的请求方，服务器是服务的提供方。 服务请求方和服务提供方都要使用网络核心部分所提供的服务。 对等连接方式： 对等连接 (peer-to-peer，简写为 P2P ) 是指两个主机在通信时并不区分哪一个是服务请求方还是服务提供方。 只要两个主机都运行了对等连接软件 ( P2P 软件) ，它们就可以进行平等的、对等连接通信。 双方都可以下载对方已经存储在硬盘中的共享文档。 1.3 三种交换方式 网络核心部分是互联网中最复杂的部分。 网络中的核心部分要向网络边缘中的大量主机提供连通性，使边缘部分中的任何一个主机都能够向其他主机通信（即传送或接收各种形式的数据）。 数据是怎么在网络中进行交换的呢？数据交换的方式有三种： 电路交换 分组交换 报文交换 ","date":"2023-06-03","objectID":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E6%A6%82%E8%BF%B0/:5:1","tags":[],"title":"计算机网络之概述","uri":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E6%A6%82%E8%BF%B0/"},{"categories":["计算机网络"],"content":"1、电路交换（Circuit Switching） 传统两两相连的方式，当电话数量很多时，电话线也很多，就很不方便 所以要使得每一部电话能够很方便地和另一部电话进行通信，就应该使用一个中间设备将这些电话连接起来，这个中间设备就是电话交换机 电话交换机接通电话线的方式称为电路交换； 从通信资源的分配角度来看，交换（Switching）就是按照某种方式动态地分配传输线路的资源； 电路交换的三个步骤： 建立连接：分配通信资源 通话：一直占用通信资源，也就是那些线路资源。如果有其他连接请求则会等当前连接释放。 释放连接：归还通信资源 当使用电路交换来传送计算机数据时，其线路的传输效率往往很低。这是因为计算机数据是突发式地出现在传输线路上的，而传输路线可能已经被上一个电路交换连接占有着，那么这个线路就不能被其他连接所使用。 所以计算机通常采用的是分组交换，而不是电路交换 优点： 特别适合大量数据的实时交换，例如打电话、看直播等 缺点： 缺点也很明显。一旦建立连接后就会一直占有通信资源，使得被占用的那些线路无法被其他连接使用，只能等当前连接释放 ","date":"2023-06-03","objectID":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E6%A6%82%E8%BF%B0/:6:0","tags":[],"title":"计算机网络之概述","uri":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E6%A6%82%E8%BF%B0/"},{"categories":["计算机网络"],"content":"2、分组交换（Packet Switching） 在网络核心部分起特殊作用的是路由器(router)。路由器是实现分组交换 (packet switching) 的关键构件，其任务是转发收到的分组，这是网络核心部分最重要的功能。 通常我们把表示该消息的整块数据成为一个报文。 在发送报文之前，先把较长的报文划分成一个个更小的等长数据段，在每一个数据段前面。加上一些由必要的控制信息组成的首部后，就构成一个分组，也可简称为“包”，相应地，首部也可称为“包头”。 首部包含了分组的目的地址 分组从源主机到目的主机，可走不同的路径。 发送方 构造分组 发送分组 路由器 缓存分组 转发分组 简称为“分组转发” 在路由器中的输入和输出端口之间没有直接连线。 路由器处理分组的过程是： 把收到的分组先放入缓存（暂时存储）； 查找转发表，找出到某个目的地址应从哪个端口转发； 把分组送到适当的端口转发出去。 接收方 接收分组 还原报文 优点： 相较于电路交换、报文交换而言，分组交换类似于多级流水线CPU，可以大幅提高数据交换的效率。 现代计算机网络的数据交换方式就是采用分组交换方式 缺点： 分组交换可能会出现时延，阻塞的情况 ","date":"2023-06-03","objectID":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E6%A6%82%E8%BF%B0/:7:0","tags":[],"title":"计算机网络之概述","uri":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E6%A6%82%E8%BF%B0/"},{"categories":["计算机网络"],"content":"3、报文交换（Message Switching） 报文交换中的交换结点也采用存储转发方式，但报文交换对报文的大小没有限制，这就要求交换结点需要较大的缓存空间。报文交换主要用于早期的电报通信网，现在较少使用，通常被较先进的分组交换方式所取代。 ","date":"2023-06-03","objectID":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E6%A6%82%E8%BF%B0/:8:0","tags":[],"title":"计算机网络之概述","uri":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E6%A6%82%E8%BF%B0/"},{"categories":["计算机网络"],"content":"三种交换方式的对比 假设A，B，C，D是分组传输路径所要经过的4个结点交换机，纵坐标为时间 电路交换： 通信之前首先要建立连接；连接建立好之后，就可以使用已建立好的连接进行数据传送；数据传送后，需释放连接，以归还之前建立连接所占用的通信线路资源。 一旦建立连接，中间的各结点交换机就是直通形式的，比特流可以直达终点； 一旦连接建立，所占用的线路资源不可以被其他连接所复用，只有等待当前连接资源的释放。 报文交换： 可以随时发送报文，而不需要事先建立连接；整个报文先传送到相邻结点交换机，全部存储下来后进行查表转发，转发到下一个结点交换机。 整个报文需要在各结点交换机上进行存储转发，由于不限制报文大小，因此需要各结点交换机都具有较大的缓存空间。 很少使用这种数据交换方式了 分组交换： 可以随时发送分组，而不需要事先建立连接。构成原始报文的一个个分组，依次在各结点交换机上存储转发。各结点交换机在发送分组的同时，还缓存接收到的分组。 构成原始报文的一个个分组，在各结点交换机上进行存储转发，相比报文交换，减少了转发时延，还可以避免过长的报文长时间占用链路，同时也有利于进行差错控制。 某个节点转发完一个分组数据包到下一个节点时，又可以继续转发下一个分组数据包，同时下一个节点也可以转发收到分组数据包。类似于CPU多级流水线原理。 1.4 计算机网络的定义和分类 ","date":"2023-06-03","objectID":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E6%A6%82%E8%BF%B0/:9:0","tags":[],"title":"计算机网络之概述","uri":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E6%A6%82%E8%BF%B0/"},{"categories":["计算机网络"],"content":"定义 计算机网络的精确定义并未统一 计算机网络的最简单的定义是：一些互相连接的、自治的计算机的集合。 互连：是指计算机之间可以通过有线或无线的方式进行数据通信； 自治：是指独立的计算机，他有自己的硬件和软件，可以单独运行使用； 集合：是指至少需要两台计算机； 计算机网络的较好的定义是：计算机网络主要是由一些通用的，可编程的硬件（一定包含有中央处理机CPU）互连而成的，而这些硬件并非专门用来实现某一特定目的（例如，传送数据或视频信号）。这些可编程的硬件能够用来传送多种不同类型的数据，并能支持广泛的和日益增长的应用。 计算机网络所连接的硬件，并不限于一般的计算机，而是包括了智能手机等智能硬件。 计算机网络并非专门用来传送数据，而是能够支持很多种的应用（包括今后可能出现的各种应用）。 ","date":"2023-06-03","objectID":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E6%A6%82%E8%BF%B0/:10:0","tags":[],"title":"计算机网络之概述","uri":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E6%A6%82%E8%BF%B0/"},{"categories":["计算机网络"],"content":"分类 按交换技术分类： 电路交换网络 报文交换网络 分组交换网络 按使用者分类： 公用网 专用网 按传输介质分类： 有线网络 无线网络 按覆盖范围分类： 广域网WAN（Wide Area Network） 作用范围通常为几十到几千公里，因而有时也称为远程网（long haul network）。广域网是互联网的核心部分，其任务是通过长距离（例如跨越不同的国家）运送主机所发送的数据。 城域网MAN 作用范围一般是一个城市，可跨越几个街区甚至整个城市 局域网LAN 一般用微型计算机或工作站通过高速通信线路相连（速率通常在 10 Mbit/s 以上），但地理上作用范围较小（1 km 左右） 个域网PAN 就是在个人工作的地方把个人使用的电子设备用无线技术连接起来的网络。 以上的作用范围或距离的含义指的是：实际数据网络传输距离。 例如：A户和B户主机，距离50m，但是两个通过互联网或者任何广域网技术进行相互地资源访问。那么显然，这属于广域网。但是如果A户和B户通过自己购买交换机搭建网络线路和路由，使得两户互通，然后通过这个线路进行资源访问，那么这就是局域网。 按拓扑结构分类： 总线型网络 星型网络 环形网络 网状型网络 1.5 计算机网络的性能指标 ","date":"2023-06-03","objectID":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E6%A6%82%E8%BF%B0/:11:0","tags":[],"title":"计算机网络之概述","uri":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E6%A6%82%E8%BF%B0/"},{"categories":["计算机网络"],"content":"速率 ","date":"2023-06-03","objectID":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E6%A6%82%E8%BF%B0/:12:0","tags":[],"title":"计算机网络之概述","uri":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E6%A6%82%E8%BF%B0/"},{"categories":["计算机网络"],"content":"带宽 ","date":"2023-06-03","objectID":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E6%A6%82%E8%BF%B0/:13:0","tags":[],"title":"计算机网络之概述","uri":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E6%A6%82%E8%BF%B0/"},{"categories":["计算机网络"],"content":"吞吐量 带宽1 Gb/s的以太网，代表其额定速率是1 Gb/s，这个数值也是该以太网的吞吐量的绝对上限值。因此，对于带宽1 Gb/s的以太网，可能实际吞吐量只有 700 Mb/s，甚至更低。 注意：吞吐量还可以用每秒传送的字节数或帧数表示 ","date":"2023-06-03","objectID":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E6%A6%82%E8%BF%B0/:14:0","tags":[],"title":"计算机网络之概述","uri":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E6%A6%82%E8%BF%B0/"},{"categories":["计算机网络"],"content":"时延 时延时指数据（一个报文或分组，甚至比特）从网络（或链路）的一端传送到另一端所需的时间。 网络时延由几部分组成： 发送时延 主机或路由器发送数据帧所需要的时间，也就是从发送数据帧的第一个比特算起，到该帧的最后一个比特发送完毕所需的时间。 传播时延 电磁波在信道中传播一定的距离需要花费的时间。 处理时延 主机或路由器在收到分组时要花费一定时间进行处理 排队时延 分组在进过网络传输时，要经过许多路由器。但分组在进入路由器后要先在输入队列中排队等待处理。 有时会把排队时延看成处理时延一部分 网络时延 = 发送时延 + 传播时延 + 处理时延 （处理时延包含排队时延） 当处理时延忽略不计时，发送时延 和 传播时延谁占主导，要具体情况具体分析 ","date":"2023-06-03","objectID":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E6%A6%82%E8%BF%B0/:15:0","tags":[],"title":"计算机网络之概述","uri":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E6%A6%82%E8%BF%B0/"},{"categories":["计算机网络"],"content":"时延带宽积 时延带宽积 = 传播时延 * 带宽 ","date":"2023-06-03","objectID":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E6%A6%82%E8%BF%B0/:16:0","tags":[],"title":"计算机网络之概述","uri":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E6%A6%82%E8%BF%B0/"},{"categories":["计算机网络"],"content":"往返时间 互联网上的信息不仅仅单方向传输而是双向交互的。因此，我们有时很需要知道双向交互一次所需的时间。 ","date":"2023-06-03","objectID":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E6%A6%82%E8%BF%B0/:17:0","tags":[],"title":"计算机网络之概述","uri":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E6%A6%82%E8%BF%B0/"},{"categories":["计算机网络"],"content":"利用率 利用率有信道利用率和网络利用率两种。 为什么信道利用率越高反而网络时延就会越大呢？ 答案：当信道利用率越高时，网络中传输的数据量也就越大，但同时也会带来更多的数据包冲突和重传，导致网络时延增加。当网络中的数据包冲突和重传达到一定程度时，就会出现网络拥塞，导致数据传输的时延和丢包率增加。因此，在设计网络时，需要综合考虑信道利用率和网络时延之间的关系，以达到最优的性能。 ","date":"2023-06-03","objectID":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E6%A6%82%E8%BF%B0/:18:0","tags":[],"title":"计算机网络之概述","uri":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E6%A6%82%E8%BF%B0/"},{"categories":["计算机网络"],"content":"丢包率 1.6 计算机网络体系结构 ","date":"2023-06-03","objectID":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E6%A6%82%E8%BF%B0/:19:0","tags":[],"title":"计算机网络之概述","uri":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E6%A6%82%E8%BF%B0/"},{"categories":["计算机网络"],"content":"1、常见的计算机网络体系结构 ","date":"2023-06-03","objectID":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E6%A6%82%E8%BF%B0/:20:0","tags":[],"title":"计算机网络之概述","uri":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E6%A6%82%E8%BF%B0/"},{"categories":["计算机网络"],"content":"OSI七层模型 第1层：物理层 从OSI模型的最底层开始是物理层。物理层规定了在载体上发送和接收数据的硬件方法，包括定义电缆，网卡和物理方面。快速以太网，RS232和ATM是具有物理层组件的协议。 它解决了网络的物理特征。这包括用于将所有物体连接在一起的电缆类型。所使用的连接器的类型，电缆的长度等。例如，用于100BaseT电缆的以太网标准规定了双绞线电缆的电气特性，连接器的尺寸和形状，电缆的最大长度。 **物理层还规定了用于通过电缆将数据从一个网络节点传输到另一个网络的信号的电气特性。**除了‘0’或‘1’的二进制特征外，信号没有任何特殊的含义。OSI模型上层将为在物理层传输的比特分配含义。 网络中使用的一种非常重要的物理层设备是网络TAP。网络TAP是一种硬件设备，用于复制网络链路上的流量并将副本重定向到故障排除和分析工具，即使TAP断电也不会中断流量或引入故障点。 从图中可以看出，东向流量被引向Monitor端口A，西向流量被引向Monitor端口B。 ▣第1层物理实例包括以太网，FDDI，B8ZS，V.35，V.24，RJ45。 第2层：数据链路层 数据链路层是我们开始对要通过网络发送的内容赋予意义或智能的地方。数据链路层上的协议解决了以下问题，例如要发送的数据包的大小，要传送的每个数据包的寻址方式，使其到达预定的接收方，以及一种确保不超过一个节点尝试同时向接收方发送数据包的方法。 数据链路层提供了错误检测和纠正功能，以确保发送的数据与接收的数据相同。如果错误无法纠正，数据链接标准需要规定如何将错误告知节点，以便它可以重新发送出错的数据。 每个节点(网络接口卡–NIC)在数据链路层有一个地址，称为媒体访问控制地址，通常称为MAC地址。这是实际的硬件地址，是由设备制造商分配的。您可以通过打开命令窗口并运行ipconfig / all命令来找到设备的MAC地址。 ▣ 第2层数据链路示例包括PPP，FDDI，ATM，IEEE 802.5 / 802.2，IEEE 802.3 / 802.2，HDLC，帧中继。 第3层：网络层 **第3层负责在网络中进行网络消息的路由。网络层的一个重要功能是逻辑寻址。**每个网络设备都有一个物理地址，称为MAC地址（见第2层）。当你为电脑买了一块网卡时，该网卡的MAC地址是不能改变的。但是，如果你想使用一些其他的寻址系统，来引用你的计算机和其他设备，第3层网络层就是你可以设置所谓的 “逻辑地址 “的地方。逻辑地址为网络设备提供了一个位置，可以使用您分配的地址在网络上对其进行访问。 逻辑地址可以由IP或IPX等网络层协议创建和使用。网络层协议将逻辑地址转换为MAC地址。 例如，如果您使用IP作为网络层协议，则会为网络上的设备分配IP地址，例如107.210.76.30。由于IP协议在第3层上运行以实际发送数据包，因此IP需要将设备的IP地址转换为正确的MAC地址。您可以使用ipconfig / all命令查找计算机或其他设备的IP地址。 解析IP地址后，我们现在需要设置路由，将数据包移动到目的地。当一个网络上的数据包需要发送到另一个网络上的计算机时，路由就会发挥作用。 ▣第3层网络示例包括AppleTalk DDP，IP，IPX。 第4层：传输层 传输层是一台网络计算机与另一台网络计算机进行通信的基本层。传输层是最流行的网络协议之一传输控制协议（TCP）的地方。**传输层的主要目的是确保数据包在网络中可靠无误地移动。**传输层通过在网络设备之间建立连接，确认数据包的接收并重新发送未收到的或到达时已损坏的数据包来实现此目的。 在许多情况下，传输层协议将大的消息分成较小的数据包，可以有效地在网络上发送（分组交换技术）。传输层协议在接收端重组消息，确保一次传输中包含的所有数据包都能收到，并且没有数据丢失。 ▣第4层传输示例包括SPX，TCP，UDP。 第5层：会话层 会话层建立、管理和终止网络节点之间的连接。在网络上传输数据之前，必须先建立会话。会话层确保正确建立和维护这些会话。它提供全双工，半双工或单工操作，并建立检查点、延期、终止和重新启动过程。OSI模型使该层负责会话的正常关闭（这是TCP协议的一个属性），同时还负责会话检查点和恢复，这在Internet协议套件中通常不使用。会话层通常在使用远程过程调用的应用环境中显式实现。 ▣第5层会话示例包括NFS，NetBios Name，RPC，SQL。 第6层：表示层 表示层负责将网络发送的数据从一种表示形式转换为另一种表示形式。例如，表示层可以应用复杂的压缩技术，以便在网络上发送时，需要较少的数据字节来表示信息。在传输的另一端，传输层则对数据进行解压缩。 这一层通过从应用到网络格式的转换，提供了不受数据表示差异（如加密）影响的独立性，反之亦然。表示层可以通过压缩、编码、加密等手段将数据转换为应用层可以接受的形式。该层对要在网络上发送的数据进行格式化和加密，使数据不受兼容性问题的影响。它有时被称为语法层。 ▣第6层演示示例包括encryption，ASCII，EBCDIC，TIFF，GIF，PICT，JPEG，MPEG，MIDI。 第7层：应用层 OSI模型的最高层-应用层，它处理应用程序用于与网络通信的技术。该层的名称是有点令人困惑，因为**应用程序（如Excel或Word）实际上不是该层的一部分。而是，应用层表示应用程序与网络交互的级别，使用编程接口请求网络服务。**HTTP是最常用的应用程序层协议之一，它代表超文本传输协议。HTTP是万维网的基础。 ▣第7层应用示例包括WWW browsers，NFS，SNMP，Telnet，HTTP，FTP。 ","date":"2023-06-03","objectID":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E6%A6%82%E8%BF%B0/:20:1","tags":[],"title":"计算机网络之概述","uri":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E6%A6%82%E8%BF%B0/"},{"categories":["计算机网络"],"content":"TCP/IP四层模型 如今用的最多的是TCP/IP体系结构，现今规模最大的、覆盖全球的、基于TCP/IP的互联网并未使用OSI标准。 TCP/IP体系结构相当于将OSI体系结构的物理层和数据链路层合并为了网络接口层，并去掉了会话层和表示层。 TCP/IP在网络层使用的协议是IP协议，IP协议的意思是网际协议，因此TCP/IP体系结构的网络层称为网际层 在用户主机的操作系统中，通常都带有符合TCP/IP体系结构标准的TCP/IP协议族。 而用于网络互连的路由器中，也带有符合TCP/IP体系结构标准的TCP/IP协议族。 只不过路由器一般只包含网络接口层和网际层。 网络接口层：并没有规定具体内容，这样做的目的是可以互连全世界各种不同的网络接口，例如：有线的以太网接口，无线局域网的WIFI接口等。 网际层：它的核心协议是IP协议。 运输层：TCP和UDP是这层的两个重要协议。 应用层：这层包含了大量的应用层协议，如 HTTP , DNS 等。 **IP协议（网际层）可以将不同的网络接口（网络接口层）进行互连，并向其上的TCP协议和UDP协议（运输层）**提供网络互连服务 而TCP协议在享受IP协议提供的网络互连服务的基础上，可向应用层的相应协议提供可靠的传输服务。 UDP协议在享受IP协议提供的网络互连服务的基础上，可向应用层的相应协议提供不可靠的传输服务。 TCP/IP体系结构中最重要的是IP协议和TCP协议，因此用TCP和IP来表示整个协议大家族。 教学时把TCP/IP体系结构的网络接口层分成了物理层和数据链路层 ","date":"2023-06-03","objectID":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E6%A6%82%E8%BF%B0/:20:2","tags":[],"title":"计算机网络之概述","uri":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E6%A6%82%E8%BF%B0/"},{"categories":["计算机网络"],"content":"2、计算机网络体系结构分层的必要性 物理层问题 这图说明 第一，严格来说，传输媒体并不属于物理层 计算机传输的信号，并不是图示的方波信号 这样举例只是让初学者容易理解 数据链路层问题 网络层问题 运输层问题 如何标识与网络通信相关的应用进程：一个分组到来，我们应该交给哪个进程处理呢？浏览器进程还是QQ进程 应用层问题 应用层该用什么方法（应用层协议）去解析数据 总结 ","date":"2023-06-03","objectID":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E6%A6%82%E8%BF%B0/:21:0","tags":[],"title":"计算机网络之概述","uri":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E6%A6%82%E8%BF%B0/"},{"categories":["计算机网络"],"content":"3、计算机网络体系结构分层思想举例 例子：主机的浏览器如何与Web服务器进行通信 解析： 主机和Web服务器之间基于网络的通信，实际上是主机中的浏览器应用进程与Web服务器中的Web服务器应用进程之间基于网络的通信 体系结构的各层在整个过程中起到怎样的作用？ 1、发送方发送 第一步： 应用层按照HTTP协议的规定构建一个HTTP请求报文 应用层将HTTP请求报文交付给运输层处理 第二步： 运输层给HTTP请求报文添加一个TCP首部，使之成为TCP报文段 TCP报文段的首部格式作用是区分应用进程以及实现可靠传输 运输层将TCP报文段交付给网络层处理 第三步： 网络层给TCP报文段添加一个IP首部，使之成为IP数据报 IP数据报的首部格式作用是使IP数据报可以在互联网传输，也就是被路由器转发 网络层将IP数据报交付给数据链路层处理 第四步： 数据链路层给IP数据报添加一个首部和一个尾部，使之成为帧（图示右边为首部，左边为尾部） 该首部的作用主要是为了让帧能够在一段链路上或一个网络上传输，能够被相应的目的主机接收 该尾部的作用是让目的主机检查所接收到的帧是否有误码 数据链路层将帧交付给物理层 第五步： 物理层先将帧看做是比特流，这里的网络N1假设是以太网，所以物理层还会给该比特流前面添加前导码 前导码的作用是为了让目的主机做好接收帧的准备 物理层将装有前导码的比特流变换成相应的信号发送给传输媒体 第六步： 信号通过传输媒体到达路由器 2、路由器转发 在路由器中 物理层将信号变为比特流，然后去掉前导码后，将其交付给数据链路层 数据链路层将帧的首部和尾部去掉后，将其交付给网络层，这实际交付的是IP数据报 网络层解析IP数据报的首部，从中提取目的网络地址 在路由器中 提取目的网络地址后查找自身路由表。确定转发端口，以便进行转发 网络层将IP数据报交付给数据链路层 数据链路层给IP数据报添加一个首部和一个尾部，使之成为帧 数据链路层将帧交付给物理层 物理层先将帧看成比特流，这里的网络N2假设是以太网，所以物理层还会给该比特流前面添加前导码 物理层将装有前导码的比特流变换成相应的信号发送给传输媒体，信号通过传输媒体到达Web服务器 可以看得出来：路由器的工作主要是将在物理线路里的信号，按照OSI七层模型进行解析，直到得到网络层的IP数据报的目的网络地址，这样就可以根据路由器的路由表和当前数据包需要转发的目的网络地址，选择一个最佳的路径将数据报发送出去，直到数据报到达目的主机。 3、接收方接收 和发送方（主机）发送过程的封装正好是反着来 在Web 服务器上 物理层将信号变换为比特流，然后去掉前导码后成为帧，交付给数据链路层 数据链路层将帧的首部和尾部去掉后成为IP数据报，将其交付给网络层 网络层将IP数据报的首部去掉后成为TCP报文段，将其交付给运输层 运输层将TCP报文段的首部去掉后成为HTTP请求报文，将其交付给应用层 应用层对HTTP请求报文进行解析，然后给主机发回响应报文 发回响应报文的步骤和之前过程类似 ","date":"2023-06-03","objectID":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E6%A6%82%E8%BF%B0/:22:0","tags":[],"title":"计算机网络之概述","uri":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E6%A6%82%E8%BF%B0/"},{"categories":["计算机网络"],"content":"4、计算机网络体系结构中的专用术语 以下介绍的专用术语来源于OSI的七层协议体系结构，但也适用于TCP/IP的四层体系结构和五层协议体系结构 实体 协议 协议：控制两个对等实体进行逻辑通信的规则的集合 协议三要素： 语法：定义所交换信息的格式 语义：定义收发双方所要完成的操作 同步：定义收发双发的时序关系 服务 简单来说，PDU是在网络协议中传输的数据单元，而SDU是上层协议向下层协议传递的数据单元。在数据传输过程中，SDU会被分割成多个PDU进行传输，而接收方会将接收到的PDU重新组装成SDU。 ","date":"2023-06-03","objectID":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E6%A6%82%E8%BF%B0/:23:0","tags":[],"title":"计算机网络之概述","uri":"/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B9%8B%E6%A6%82%E8%BF%B0/"},{"categories":["缓存系统"],"content":"[toc] 缓存淘汰 ","date":"2023-05-31","objectID":"/%E7%BC%93%E5%AD%98%E6%B7%98%E6%B1%B0%E7%AD%96%E7%95%A5/:0:0","tags":[],"title":"缓存淘汰策略","uri":"/%E7%BC%93%E5%AD%98%E6%B7%98%E6%B1%B0%E7%AD%96%E7%95%A5/"},{"categories":["缓存系统"],"content":"背景 不论是进程内缓存，还是分布式缓存，都无法避免这样一个问题：当我们需要缓存的数据大于物理内存时，那么就需要通过操作系统虚拟内存管理不断地在硬盘和内存中进行页面置换。设想一下：缓存数据越大，那么缺页中断的就越多，导致页面置换就越多，从而磁盘IO增加，从而大大拖垮缓存的性能。所以，虽然操作系统允许我们运行比物理内存更大更多的进程，但是为了兼具缓存性能，我们依然需要适当的控制，来保证我们缓存数据的大小不能超过物理内存，一旦超过就可能会发生页面置换中的磁盘IO 所以，我们可以采取如下一些策略。 内存足够时，我们也可以进一步减轻内存负担，例如过期删除、主动清理等 内存不足够时，需要通过一些好的策略来做内存数据的淘汰，避免物理内存不足后发生页面置换现象；或者我们直接加内存 在缓存数据的淘汰中，我们需要将数据分为两类：高频热数据和低频冷数据。我们应该尽可能地避免高频热数据被缓存淘汰掉，这样会降低缓存的命中率。所以，我们需要采取一些好的缓存策略来保证我们的缓存命中率。 ","date":"2023-05-31","objectID":"/%E7%BC%93%E5%AD%98%E6%B7%98%E6%B1%B0%E7%AD%96%E7%95%A5/:1:0","tags":[],"title":"缓存淘汰策略","uri":"/%E7%BC%93%E5%AD%98%E6%B7%98%E6%B1%B0%E7%AD%96%E7%95%A5/"},{"categories":["缓存系统"],"content":"几种缓存淘汰策略及实现 ","date":"2023-05-31","objectID":"/%E7%BC%93%E5%AD%98%E6%B7%98%E6%B1%B0%E7%AD%96%E7%95%A5/:2:0","tags":[],"title":"缓存淘汰策略","uri":"/%E7%BC%93%E5%AD%98%E6%B7%98%E6%B1%B0%E7%AD%96%E7%95%A5/"},{"categories":["缓存系统"],"content":"FIFO先进先出淘汰 缓存满了之后，自动删除较早放入的缓存数据。 // @author cold bin // @date 2023/5/18 package cache import ( \"container/list\" \"errors\" \"sync\" ) type fifoCache struct { maxCap uint32 // 缓存允许的最大容量，这里就暂时用个粗略地数字单表允许地容量 list *list.List // 保证fifo cacheMap map[string]*list.Element // 缓存 lock sync.RWMutex } var _ Cache = \u0026fifoCache{} func NewFifoCache(maxCap uint32) Cache { return \u0026fifoCache{ maxCap: maxCap, list: \u0026list.List{}, cacheMap: make(map[string]*list.Element, maxCap), } } func (c *fifoCache) Get(key string) (any, error) { if v, ok := c.cacheMap[key]; ok { return v, nil } return nil, errors.New(\"key不存在\") } // Set 当容量足够时，就可以继续放；当容量不足够时，就按照fifo策略淘汰 func (c *fifoCache) Set(key string, value any) error { c.lock.Lock() defer c.lock.Unlock() // 存在即更新 if v, ok := c.cacheMap[key]; ok { v.Value = value c.cacheMap[key] = v return nil } // 是否触发缓存淘汰，这里容量判断（实际上精确地内存占用判断较为困难，因为操作系统不只有物理内存，还有虚拟的内存，只能粗略估计） if c.list.Len()*10 \u003e= int(c.maxCap) \u0026\u0026 c.list.Len() \u003e 0 { // 淘汰最早节点 k := c.list.Back() delete(c.cacheMap, k.Value.(string)) c.list.Remove(k) } // 不存在即添加 c.cacheMap[key] = \u0026list.Element{Value: value} c.list.PushFront(key) return nil } func (c *fifoCache) Del(key string) error { c.lock.Lock() defer c.lock.Unlock() if e, ok := c.cacheMap[key]; ok { delete(c.cacheMap, key) c.list.Remove(e) return nil } return errors.New(\"key不存在\") } 优点：实现很简单 缺点：先进来的数据可能是热点数据，淘汰掉热点数据，会导致缓存的命中率降低。 ","date":"2023-05-31","objectID":"/%E7%BC%93%E5%AD%98%E6%B7%98%E6%B1%B0%E7%AD%96%E7%95%A5/:2:1","tags":[],"title":"缓存淘汰策略","uri":"/%E7%BC%93%E5%AD%98%E6%B7%98%E6%B1%B0%E7%AD%96%E7%95%A5/"},{"categories":["缓存系统"],"content":"LFU最不经常使用 LFU最不经常使用算法，是基于数据的访问频次来对缓存数据进行淘汰的算法。 其淘汰数据依据两点：访问频次 核心：如果数据过去被访问多次，那么将来被访问的频率也就更高，当内存超过最大容量时，淘汰掉访问次数较少的数据。 依然使用队列实现，淘汰部分淘汰掉访问次数最低即可 // @author cold bin // @date 2023/5/18 package cache import ( \"errors\" \"sort\" \"sync\" ) type lfuCache struct { maxCap uint32 // 缓存允许的最大容量 counts counts // 记录访问频次的队列 cacheMap map[string]*entry // 缓存 lock sync.RWMutex } var _ Cache = \u0026lfuCache{} type counts []*entry func (c counts) find(key string) int { for i, e := range c { if e.k == key { return i } } return -1 } // 更新频次，需要重新调整序列 func (c *counts) update(en *entry, v any, weight int) { en.v = v en.weight = weight // 调整排序 sort.Sort(c) } func (c counts) Len() int { return len(c) } func (c counts) Less(i, j int) bool { return c[i].weight \u003c c[j].weight } func (c counts) Swap(i, j int) { c[i], c[j] = c[j], c[i] c[i].idx = i c[j].idx = j } type entry struct { k string v any weight int idx int } func NewLfuCache(maxCap uint32) Cache { return \u0026lfuCache{ maxCap: maxCap, counts: make([]*entry, 0, 1024), cacheMap: make(map[string]*entry, maxCap), } } func (c *lfuCache) Get(key string) (any, error) { c.lock.Lock() defer c.lock.Unlock() if v, ok := c.cacheMap[key]; ok { // 找key并更新访问频次 c.counts.update(v, v.v, v.weight+1) return v, nil } return nil, errors.New(\"key不存在\") } func (c *lfuCache) Set(key string, value any) error { c.lock.Lock() defer c.lock.Unlock() // 存在即更新 if e, ok := c.cacheMap[key]; ok { c.counts.update(e, value, e.weight+1) return nil } // 不存在即新增 e := \u0026entry{ k: key, v: value, weight: 0, idx: -1, } c.cacheMap[key] = e c.counts = append(c.counts, e) // 容量满了，触发内存淘汰 if c.counts.Len()*10 \u003e= int(c.maxCap) \u0026\u0026 len(c.counts) \u003e 0 { idx := len(c.counts) - 1 e := c.counts[idx] c.counts = c.counts[:idx] delete(c.cacheMap, e.k) } return nil } func (c *lfuCache) Del(key string) error { c.lock.Lock() defer c.lock.Unlock() if _, ok := c.cacheMap[key]; ok { i := c.counts.find(key) c.counts = append(c.counts[:i], c.counts[i+1:]...) delete(c.cacheMap, key) return nil } return errors.New(\"key不存在\") } 优点：命中率较高 缺点： 实现较复杂，内存占用较高 另一种缓存污染问题 越到后期，数据就越难缓存，因为前期缓存数据积累的访问次数太大，会导致后期热点数据存不到缓存里 ","date":"2023-05-31","objectID":"/%E7%BC%93%E5%AD%98%E6%B7%98%E6%B1%B0%E7%AD%96%E7%95%A5/:2:2","tags":[],"title":"缓存淘汰策略","uri":"/%E7%BC%93%E5%AD%98%E6%B7%98%E6%B1%B0%E7%AD%96%E7%95%A5/"},{"categories":["缓存系统"],"content":"LRU最近最少使用 最近最少使用缓存淘汰算法，其淘汰最近一段时间最少被访问的缓存数据。我们使用队列可以实现这种淘汰算法，对于访问的元素，移动到链表尾，这样链表头为较旧的元素，当容量满时，淘汰掉链表头元素即可。 // @author cold bin // @date 2023/5/18 package cache import ( \"container/list\" \"errors\" \"sync\" ) type lruCache struct { maxCap uint32 // 缓存允许的最大容量 list []*list.Element // 记录访问顺序 cacheMap map[string]*list.Element // 缓存 lock sync.RWMutex } var _ Cache = \u0026lruCache{} func NewLruCache(maxCap uint32) Cache { return \u0026lruCache{ maxCap: maxCap, list: make([]*list.Element, 0, 1024), cacheMap: make(map[string]*list.Element, maxCap), } } func (c *lruCache) find(key string) int { for i, k := range c.list { if k.Value == key { return i } } return -1 } // lru核心 func (c *lruCache) swapLast(key string) { i := c.find(key) if i == -1 { panic(\"i==-1\") } e := c.list[i] c.list = append(c.list[:i], c.list[i+1:]...) c.list = append(c.list, e) } func (c *lruCache) Get(key string) (any, error) { c.lock.Lock() defer c.lock.Unlock() if v, ok := c.cacheMap[key]; ok { // 元素key移到尾部 c.swapLast(key) return v, nil } return nil, errors.New(\"not found\") } func (c *lruCache) Set(key string, value any) error { c.lock.Lock() defer c.lock.Unlock() // 存在则更新 if v, ok := c.cacheMap[key]; ok { v.Value = value // c.cacheMap[key] = v c.swapLast(key) return nil } // 容量满时则淘汰头部元素即可 if len(c.list)*10 \u003e= int(c.maxCap) \u0026\u0026 len(c.list) \u003e 0 { tmpKey := c.list[0] c.list = c.list[1:] delete(c.cacheMap, tmpKey.Value.(string)) } // 不存在则添加 v := \u0026list.Element{Value: value} k := \u0026list.Element{Value: key} c.cacheMap[key] = v c.list = append(c.list, k) return nil } func (c *lruCache) Del(key string) error { c.lock.Lock() defer c.lock.Unlock() i := c.find(key) c.list = append(c.list[:i], c.list[i+1:]...) delete(c.cacheMap, key) return nil } 上面lru实现的时间复杂度较高。其实可以采用双向链表+哈希表的方式实现O(1)级别的时间复杂度 优点：缓存命中率较高 缺点： 内存占用高 缓存污染 当我们在批量读取数据的时候，由于数据被访问了一次，这些大量数据都会被加入到「活跃 LRU 链表」里，然后之前缓存在活跃 LRU 链表（或者 young 区域）里的热点数据全部都被淘汰了，如果这些大量的数据在很长一段时间都不会被访问的话，那么整个 LRU 链表就被污染了。 预读失效 操作系统在访问内存数据的时候，如果数据不在内存里，会从磁盘里去加载数据，又因为空间局部性原理，会加载连续的多个块，此时，只有一个块里有热点数据，其余块都没有热点数据，然而还会把lru链表最后的热点数据挤出去。另外几个没有热点数据的块，当然也就预读失效了。 如何解决lru的缓存污染和预读污染问题？ ","date":"2023-05-31","objectID":"/%E7%BC%93%E5%AD%98%E6%B7%98%E6%B1%B0%E7%AD%96%E7%95%A5/:2:3","tags":[],"title":"缓存淘汰策略","uri":"/%E7%BC%93%E5%AD%98%E6%B7%98%E6%B1%B0%E7%AD%96%E7%95%A5/"},{"categories":["缓存系统"],"content":"LRU-K lru-1 数据第一次被访问，加入到访问历史列表； 如果数据在访问历史列表里后没有达到K次访问，则按照LRU淘汰； 当访问历史队列中的数据访问次数达到K次后，将数据索引从历史队列删除，将数据移到缓存队列中，并缓存此数据，缓存队列重新按照时间排序； 缓存数据队列中被再次访问后，重新排序； 需要淘汰数据时，淘汰缓存队列中排在末尾的数据，即淘汰\"倒数第K次访问离现在最久\"的数据。 // @author cold bin // @date 2023/5/19 package cache import ( \"container/list\" \"errors\" \"sync\" ) type lrukCache struct { maxCap uint32 // 缓存允许的最大容量 k int // 指定多少次访问后移入缓存队列，一般推荐为两次 history []*entry1 // 访问历史记录，只有访问次数大于等于k的数据才会被缓存，其余数据按照lru淘汰。采用链表更好点 list []*list.Element // 缓存里的key顺序，最近访问的元素放到队列的头部，越往后就是越久的元素，按照lru方式淘汰 cacheMap map[string]*list.Element // 缓存 lock sync.RWMutex } var _ Cache = \u0026lrukCache{} type entry1 struct { k string v any count int // 访问次数 } // 判断key在哪个队列里：-1-\u003ehistory;1-\u003elist func (c *lrukCache) judge(key string) (idx int, ans int) { for i, e := range c.history { if e.k == key { return i, -1 } } for i, e := range c.list { if e.Value.(string) == key { return i, 1 } } return -1, 0 } // list里最近访问的元素放到list后面 // 切片这里时间复杂度较高，采用链表的话，更好点 func (c *lrukCache) placeLruk(idx int) { e := c.list[idx] c.list = append(c.list[:idx], c.list[idx+1:]...) c.list = append(c.list, e) } // history里最近访问的元素放到history后面 func (c *lrukCache) placeHistory(idx int) { e := c.history[idx] c.history = append(c.history[:idx], c.history[idx+1:]...) c.history = append(c.history, e) } // 访问次数达到k的元素从history移到list，并将存入缓存 func (c *lrukCache) historyMoveToList(hidx int) { e := c.history[hidx] // 删除 c.history = append(c.history[:hidx], c.history[hidx+1:]...) // 放到list末尾 c.list = append(c.list, \u0026list.Element{Value: e.k}) // 放入缓存 c.cacheMap[e.k] = \u0026list.Element{Value: e.v} } func (c *lrukCache) Get(key string) (any, error) { c.lock.Lock() defer c.lock.Unlock() idx, ans := c.judge(key) switch ans { case 1: // list // 将新访问的缓存移到lru后面 c.placeLruk(idx) return c.cacheMap[key].Value, nil case -1: // history // 访问history，如果访问次数达到k，移到list；否则按照lru规则 c.history[idx].count++ // 先记录值 v := c.history[idx].v if c.history[idx].count == c.k { // 可以放list和缓存了 c.historyMoveToList(idx) } else { // lru规则 c.placeHistory(idx) } return v, nil case 0: return nil, errors.New(\"not found\") default: return nil, errors.New(\"not known error\") } } func (c *lrukCache) Set(key string, value any) error { c.lock.Lock() defer c.lock.Unlock() idx, ans := c.judge(key) switch ans { case 1: // list // 更新缓存和list c.cacheMap[key].Value = value c.placeLruk(idx) return nil case -1: // history // 更新history c.history[idx].count++ if c.history[idx].count == c.k { // 可以放list和缓存了 c.historyMoveToList(idx) } else { // lru规则 c.placeHistory(idx) } return nil case 0: // 不存在即添加到history后，并按照lru淘汰 // 是否触发缓存淘汰 if (len(c.history)+len(c.list))*10 \u003e= int(c.maxCap) { c.history = c.history[1:] // 没有缓存 } // 添加 e := \u0026entry1{k: key, v: value} c.history = append(c.history, e) return nil default: return errors.New(\"not known error\") } } func (c *lrukCache) Del(key string) error { c.lock.Lock() defer c.lock.Unlock() idx, ans := c.judge(key) switch ans { case 1: // list // 删除list c.list = append(c.list[:idx], c.list[idx+1:]...) // 删除缓存 delete(c.cacheMap, key) return nil case -1: // history c.history = append(c.history[:idx], c.history[idx+1:]...) return nil default: return errors.New(\"not found\") } } LRU其实就是LRU-1，意思就是最近使用过1次的数据，就加入缓存。这样做可能会导致这样的情况发生：近期前有大量访问，但是近期没有访问的数据，那么这些数据就可能会被LRU规则淘汰掉，但这些数据还是有可能在近期后被访问。这样就造成某些偶尔发生的缓存命中率低的现象。 LRU-K策略指的是，只有数据被访问k次后，才能被加入缓存里，这样可以降低缓存污染（减少冷数据加入缓存的情况），从而提高缓存命中率。 其实linux操作系统中page cache的缓存淘汰策略里也是通过将将lru算法改进为lru-2算法提高进入active list的lru缓存的门槛，从而降低了缓存污染严重性。 ","date":"2023-05-31","objectID":"/%E7%BC%93%E5%AD%98%E6%B7%98%E6%B1%B0%E7%AD%96%E7%95%A5/:2:4","tags":[],"title":"缓存淘汰策略","uri":"/%E7%BC%93%E5%AD%98%E6%B7%98%E6%B1%B0%E7%AD%96%E7%95%A5/"},{"categories":["缓存系统"],"content":"2Q 2Q（Two Queue）缓存淘汰算法是一种基于 LRU（Least Recently Used）和 FIFO（First In First Out）算法的混合算法。其主要思想是将缓存分为两个队列：Q1 和 Q2。 当数据第一次访问时，2Q算法将数据缓存在FIFO队列里面，当数据第二次被访问时，则将数据从FIFO队列移到LRU队列里面，两个队列各自按照自己的方法淘汰数据。 // @author cold bin // @date 2023/5/19 package cache import ( \"container/list\" \"errors\" \"sync\" ) type twoQueue struct { maxCap uint32 // 缓存允许的最大容量 fifoList []*list.Element // fifo lruList []*list.Element // lru cacheMap map[string]*list.Element // 缓存 lock sync.RWMutex } var _ Cache = \u0026twoQueue{} func NewTwoQueue(maxCap uint32) Cache { return \u0026twoQueue{ maxCap: maxCap, fifoList: make([]*list.Element, 0, 1024), lruList: make([]*list.Element, 0, 1024), cacheMap: make(map[string]*list.Element, 1024), } } func (c *twoQueue) placeLru(idx int) { e := c.lruList[idx] c.lruList = append(c.lruList[:idx], c.lruList[idx+1:]...) c.lruList = append(c.lruList, e) } // 判断key存在与哪个队列。-1-\u003efifo;1-\u003elru func (c *twoQueue) judge(key string) (idx int, ans int) { for i, k := range c.fifoList { if k.Value.(string) == key { return i, -1 } } for i, k := range c.lruList { if k.Value.(string) == key { return i, 1 } } return -1, 0 } func (c *twoQueue) Get(key string) (any, error) { c.lock.Lock() defer c.lock.Unlock() if _, ok := c.cacheMap[key]; ok { // 如果访问数据在fifo队列里，就移到lru队列；如果在lfu队列里，就移到lru尾部 idx, ans := c.judge(key) switch ans { case 1: // lru c.placeLru(idx) case -1: // fifo // 删除 k := c.fifoList[idx] c.fifoList[idx] = nil c.fifoList = append(c.fifoList[:idx], c.fifoList[idx+1:]...) // 移动到lru尾部 c.lruList = append(c.lruList, k) // 是否触发lru内存淘汰 if (len(c.lruList)+len(c.fifoList))*10 \u003e= int(c.maxCap) \u0026\u0026 len(c.lruList) \u003e 0 { tmpKey := c.lruList[0] c.lruList = c.lruList[1:] delete(c.cacheMap, tmpKey.Value.(string)) } case 0: return nil, errors.New(\"not exist\") default: return nil, errors.New(\"not known error\") } } return nil, errors.New(\"not exist\") } func (c *twoQueue) Set(key string, value any) error { c.lock.Lock() defer c.lock.Unlock() if _, ok := c.cacheMap[key]; ok { // 存在就更新：如果访问数据在fifo队列里，就移到lru队列；如果在lru队列里，就移到lru尾部 idx, ans := c.judge(key) switch ans { case 1: // lru c.placeLru(idx) case -1: // fifo // 删除 k := c.fifoList[idx] c.fifoList[idx] = nil c.fifoList = append(c.fifoList[:idx], c.fifoList[idx+1:]...) // 是否触发lru内存淘汰 if (len(c.lruList)+len(c.fifoList))*10 \u003e= int(c.maxCap) \u0026\u0026 len(c.lruList) \u003e 0 { tmpKey := c.lruList[0] c.lruList = c.lruList[1:] delete(c.cacheMap, tmpKey.Value.(string)) } // 移动到lru尾部 c.lruList = append(c.lruList, k) case 0: return errors.New(\"not exist\") default: return errors.New(\"not known error\") } } // fifo是否触发缓存淘汰 if (len(c.fifoList)+len(c.lruList))*10 \u003e= int(c.maxCap) \u0026\u0026 len(c.fifoList) \u003e 0 { // 淘汰最早节点 k := c.fifoList[0] delete(c.cacheMap, k.Value.(string)) c.fifoList = c.fifoList[1:] } // 新增则加入fifo队列 c.cacheMap[key] = \u0026list.Element{Value: value} c.fifoList = append([]*list.Element{\u0026list.Element{Value: key}}, c.fifoList...) return nil } func (c *twoQueue) Del(key string) error { //TODO implement me panic(\"implement me\") } 优点：更快，一般来说可以实现O(1)级别的淘汰 缺点：belady现象：物理内存增加反而导致缓存命中率下降 ","date":"2023-05-31","objectID":"/%E7%BC%93%E5%AD%98%E6%B7%98%E6%B1%B0%E7%AD%96%E7%95%A5/:2:5","tags":[],"title":"缓存淘汰策略","uri":"/%E7%BC%93%E5%AD%98%E6%B7%98%E6%B1%B0%E7%AD%96%E7%95%A5/"},{"categories":["缓存系统"],"content":"应用与举例 ","date":"2023-05-31","objectID":"/%E7%BC%93%E5%AD%98%E6%B7%98%E6%B1%B0%E7%AD%96%E7%95%A5/:3:0","tags":[],"title":"缓存淘汰策略","uri":"/%E7%BC%93%E5%AD%98%E6%B7%98%E6%B1%B0%E7%AD%96%E7%95%A5/"},{"categories":["缓存系统"],"content":"redis中的近似LRU策略 当实际内存超出 maxmemory 时，就会执行一次内存淘汰策略，具体何种内存淘汰策略，可在maxmemory-policy 中进行配置。Redis 提供了如下几种策略： noeviction：不会继续服务写请求 (DEL 请求可以继续服务)，读请求可以继续进行。这样可以保证不会丢失数据，但是会让线上的业务不能持续进行。这是默认的淘汰策略。 volatile-lru：尝试淘汰设置了过期时间的 key，最少使用的 key 优先被淘汰。没有设置过期时间的 key 不会被淘汰，这样可以保证需要持久化的数据不会突然丢失。 volatile-ttl：跟上面一样，除了淘汰的策略不是 LRU，而是 key 的剩余寿命 ttl 的值，ttl 越小越优先被淘汰。 volatile-random：跟上面一样，不过淘汰的 key 是过期 key 集合中随机的 key。 allkeys-lru：区别于 volatile-lru，这个策略要淘汰的 key 对象是全体的 key 集合，而不只是过期的 key 集合。这意味着没有设置过期时间的 key 也会被淘汰。 allkeys-random：跟上面一样，不过淘汰的策略是随机的 key。 volatile-xxx 策略只会针对带过期时间的 key 进行淘汰，allkeys-xxx 策略会对所有的 key 进行淘汰。 如果你只是拿 Redis 做缓存，那应该使用 allkeys-xxx，客户端写缓存时不必携带过期时间。 如果你还想同时使用 Redis 的持久化功能，那就使用 volatile-xxx 策略，这样可以保留没有设置过期时间的 key，它们是永久的 key 不会被 LRU 算法淘汰。 redis中的lru： 在 volatile-lru 策略中，Redis 并没有采用经典的 LRU 算法，而是采用了一种近似 LRU 算法。至于为什么会不是。主要有以下几点： 内存占用高（除了缓存数据，需要额外引入链表来记录key的访问顺序，以实现lru策略） 大量节点的访问，会带来链表或队列中节点频繁的移动，较为损耗性能 传统lru会出现缓存污染问题和预读失效问题（当然，redis并没有预读缓存） 那么针对以上两个问题，近似lru策略这样解决： 近似 LRU 算法中，是在现有数据结构的基础上，使用随机采样法来淘汰元素，避免额外的内存开销。（就是每次淘汰的数据并不是全部的key，而是采取的样本） 近似 LRU 算法中，Redis 给每个 key 增加了一个额外的小字段（长度为 24 个 bit），存储了元素最后一次次被访问的时间戳。Redis 会对少量的 key 进行采样，并淘汰采样的数据中最久没被访问过的 key。这也就意味着 Redis 无法淘汰数据库最久访问的数据。 Redis LRU 算法有一个重要的点在于，它可以更改样本数量（修改 maxmemory-samples 属性值）来调整算法的精度，使其近似接近真实的 LRU 算法，同时又避免了内存的消耗，因为每次只需要采样少量样本，而不是全部数据。 ","date":"2023-05-31","objectID":"/%E7%BC%93%E5%AD%98%E6%B7%98%E6%B1%B0%E7%AD%96%E7%95%A5/:3:1","tags":[],"title":"缓存淘汰策略","uri":"/%E7%BC%93%E5%AD%98%E6%B7%98%E6%B1%B0%E7%AD%96%E7%95%A5/"},{"categories":["缓存系统"],"content":"bigcache中的fifo策略 完完全全的fifo策略，淘汰的时候会淘汰掉最早的数据，但是容易出现缓存命中率不高的问题。总而言之，fifo只考虑了时间维度因素，并没有考虑到访问频率因素。 当然选取fifo策略，相较于lru、lfu等策略实现起来更简单，而且也更快，内存占用也更低 ","date":"2023-05-31","objectID":"/%E7%BC%93%E5%AD%98%E6%B7%98%E6%B1%B0%E7%AD%96%E7%95%A5/:3:2","tags":[],"title":"缓存淘汰策略","uri":"/%E7%BC%93%E5%AD%98%E6%B7%98%E6%B1%B0%E7%AD%96%E7%95%A5/"},{"categories":["缓存系统"],"content":"memcached中的lru策略 memcached的缓存淘汰策略是受2Q设计影响的修改LRU（Segmented LRU，分段LRU）。主要是借鉴了2q策略中的数据分类，memcached将数据分为以下几类： HOT queue：如果一个 item 的过期时间（TTL）很短，会进入该队列，在 HOT queue 中不会发生 bump，如果一个 item 到达了 queue 的 tail，那么会进入到 WARM 队列（如果 item 是 ACTIVE 状态）或者 COLD 队列（如果 item 处于不活跃状态）。 WARM queue：如果一个 item 不是 FETCHED，永远不会进入这个队列，该队列里面的 item TTL 时间相对较长，这个队列的 lock 竞争会很少。该队列 tail 处的一个 item 如果再一次被访问，会 bump 回到 head，否则移动到 COLD 队列。 COLD queue：包含了最不活跃的 item，一旦该队列内存满了，该队列 tail 处的 item 会被 清除。如果一个 item 被激活了，那么会异步移动到 WARM 队列。 TEMP queue：该队列中的 item TTL 通常只有几秒，该列队中的 item 永远不会发生 bump，也不会进入其他队列，节省了 CPU 时间，也避免了 lock 竞争。 “bump”是memcached中的一个标记，用于标记某个键值对不应该被淘汰。 ","date":"2023-05-31","objectID":"/%E7%BC%93%E5%AD%98%E6%B7%98%E6%B1%B0%E7%AD%96%E7%95%A5/:3:3","tags":[],"title":"缓存淘汰策略","uri":"/%E7%BC%93%E5%AD%98%E6%B7%98%E6%B1%B0%E7%AD%96%E7%95%A5/"},{"categories":["缓存系统"],"content":"总结 根据业务需求选择合适的算法。不同的业务场景对缓存的需求不同，因此需要根据实际情况选择合适的缓存淘汰算法。 避免频繁清除缓存。频繁清除缓存会导致缓存命中率降低，从而影响系统的性能。因此，在选择缓存淘汰算法时，需要考虑清除缓存的频率。 配置合适的缓存容量。缓存容量过小会导致缓存命中率降低，而缓存容量过大会浪费系统资源。因此，需要根据实际情况配置合适的缓存容量。 考虑多级缓存。多级缓存可以提高缓存命中率，从而提高系统的性能。例如，可以比如bigcache+redis的方式构建多级缓存。 使用缓存预热技术。缓存预热技术可以在系统启动时将常用的数据预先加载到缓存中，从而提高缓存命中率。 ","date":"2023-05-31","objectID":"/%E7%BC%93%E5%AD%98%E6%B7%98%E6%B1%B0%E7%AD%96%E7%95%A5/:4:0","tags":[],"title":"缓存淘汰策略","uri":"/%E7%BC%93%E5%AD%98%E6%B7%98%E6%B1%B0%E7%AD%96%E7%95%A5/"},{"categories":["操作系统"],"content":"[toc] ","date":"2023-05-30","objectID":"/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E4%B9%8B%E8%BE%93%E5%85%A5%E8%BE%93%E5%87%BA%E7%AE%A1%E7%90%86/:0:0","tags":[],"title":"操作系统之输入输出管理","uri":"/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E4%B9%8B%E8%BE%93%E5%85%A5%E8%BE%93%E5%87%BA%E7%AE%A1%E7%90%86/"},{"categories":["操作系统"],"content":"IO管理概述 ","date":"2023-05-30","objectID":"/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E4%B9%8B%E8%BE%93%E5%85%A5%E8%BE%93%E5%87%BA%E7%AE%A1%E7%90%86/:1:0","tags":[],"title":"操作系统之输入输出管理","uri":"/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E4%B9%8B%E8%BE%93%E5%85%A5%E8%BE%93%E5%87%BA%E7%AE%A1%E7%90%86/"},{"categories":["操作系统"],"content":"IO设备基本概念 IO设备管理是操作系统设计中最凌乱也最具挑战性的部分。由于它包含了很多领域的不同设备以及与设备相关的应用程序，因此很难有一个通用且一直的设计方案。所以在理解设备管理之前，应该先了解具体的IO设备类型。 IO设备可以根据不同的方式进行分类。 按使用特性分类IO设备 人机交互类外部设备，又称慢速IO设备，用于桶计算机用户之间交互的设备，如打印机、显示器、鼠标、键盘等。这类设备数据交换速度相对较慢，通常是以字节为单位进行数据交换。 存储设备，用于存储程序和数据的设备，如磁盘、磁带、光盘等。这类设备用于数据交换，速度较快，通常以多字节组成的块为单位进行数据交换。 网络通信设备，用于与远程设备通信的设备，如各种网络接口、调制解调器等。其数据交换速度介于外部设备与存储设备之间。网络通信设备在使用和管理上与前两者设备有很大的不同。 按传输速率分类IO设备 低速设备，传输速率仅为每秒钟几个字节至数百个字节的一类设备，如键盘、鼠标等。 中速设备，传输速率在每秒数千个字节至数万个字节的一类设备，如行式打印机、激光打印机等。 高速设备，传输速率在数百个千字节至千兆字节的一类设备，如磁带机、磁盘机、光盘机等。 按信息交换单位分类IO设备 块设备 由于信息的存取总是以数据块为单位，所以存储信息的设备称为块设备。它属于有结构设备，如磁盘等。磁盘设备的基本特征是传输速率高，以及可寻址，即对他可随机地读写任意块。 字符设备 用于数据输入输出的设备为字符设备，因为其传输的基本单位是字符。它属于无结构类型，如交互式终端机、打印机等。他们的传输速率低、不可寻址、并且在输入输出时常采用中断驱动方式。 IO设备的使用方式 对于IO设备，有以下三种不同类型的使用方式：独占式使用设备。独占式使用设备是指在申请设备是，如果设备空闲，就将其独占，不再允许其他进程申请使用，一直等到该设备被释放才允许其他进程申请使用。例如：打印机。 分时式共享使用设备。独占式使用设备时，设备利用率低，当设备没有独占使用的要求时，可以通过分时共享使用，提高利用率。例如：对磁盘设备的IO操作，各进程每次IO操作请求可以通过分时来交替进行。 以SPOOLing方式使用外部设备。SPOOLing技术是在批处理操作系统时代引入的，即假脱机IO技术。这种技术用于对设备的操作，实质上就是对IO操作进行批处理。具体的内容后面有单独讲解。 采用上面三种使用方式的设备分别称为独占设备、共享设备和虚拟设备。 IO设备的构成 IO设备通常包括一个机械部件和一个电子部件。为了达到设计的模块性和通用性，一般将其分开。 电子部件成为IO控制器，在个人计算机中，通常是一块插入主板扩充槽的印制电路板。 机械部件即设备本身。例如鼠标、键盘等。 ","date":"2023-05-30","objectID":"/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E4%B9%8B%E8%BE%93%E5%85%A5%E8%BE%93%E5%87%BA%E7%AE%A1%E7%90%86/:1:1","tags":[],"title":"操作系统之输入输出管理","uri":"/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E4%B9%8B%E8%BE%93%E5%85%A5%E8%BE%93%E5%87%BA%E7%AE%A1%E7%90%86/"},{"categories":["操作系统"],"content":"IO控制器 由于具体的设备操作涉及硬件接口，且不同的设备有不同的硬件特性和参数，所以这些复杂的操作交由操作系统用户编写程序来操作是不实际的。引入控制器后，系统可以通过几个简单的参数完成对控制器的操作，而具体的硬件操作则由控制器调用相应的设备接口完成。 设备控制器的引入大大简化了操作系统的设计，特别是有利于计算机系统和操作系统对各类控制器和设备的兼容；同时也实现了主存和设备之间的数据传输操作，使CPU从繁重的设备控制操作中解放出来。 IO控制器优点： 简化了操作系统的设计 将CPU从复杂的设备控制操作中解放出来 设备（IO）控制器的主要功能 接收和识别CPU或通道发来的命令，如磁盘控制器能接收读、写、查找、搜索等命令。 数据交换。CPU和IO设备之间的数据交换，IO控制器为了和高速CPU进行数据交换，往往需要数据寄存器来做缓存层。 向CPU报告设备状态。 设备地址识别。 IO控制器的组成 IO控制方式 设备管理的主要任务之一是控制设备和内存或处理器之间的数据传送，外围设备和内存之间的输入输出控制方式有四种 程序直接控制方式 计算机从外部设备读取数据到存储器，每次读一个字的数据。对读入的每个字，CPU需要对状态循环检查，知道确定该字已经在IO控制器的数据寄存器中。在程序IO方式中，由于CPU的高速型和IO设备的低速性，致使CPU的绝大部分时间都处于等待IO设备完成数据IO的循环测试中，造成CPU的极大浪费。CPU之所以要不断地测试IO设备的状态，就是因为在CPU中无中断机构，使IO设备无法向CPU报告它已完成了一个字符的输入操作。 缺点 采用这种IO控制方式会导致CPU和IO设备只能串行工作，导致CPU的利用率降低。 中断驱动方式 中断驱动方式的思想是：允许IO设备主动打断CPU的运行并请求服务，从而“解放”CPU，使得其向IO控制器发送命令后可以继续做其他有用的工作。 我们从IO控制器和CPU两个角度分别来看中断驱动方式的工作过程： 从IO控制器的角度来看，IO控制器从CPU接受一个读命令，然后从外围设备读数据。一旦数据读入到该IO控制器的数据寄存器，便通过控制线给CPU发出一个中断信号，表示数据已准备好，然后等待CPU请求该数据。IO控制器收到CPU发出的取数据请求后，将数据放到数据总线上，传到CPU的寄存器中。至此，本次IO操作完成，IO控制器又可以开始下一次IO操作。 从CPU的角度来看，CPU发送读命令，然后保存当前运行程序的上下文（现场，包括程序计数器及处理器寄存器），转去执行其他程序。在每个指令周期的末尾，CPU检查中断。当有来自IO控制器的中断时，CPU保存当前正在运行程序的上下文，转去执行中断处理程序处理该中断。这时，CPU从IO控制器读一个字的数据传送到寄存器，并存入主存。接着，CPU恢复发出IO命令的程序（或其他程序）的上下文，然后继续运行。 缺点： 中断驱动方式依然需要花费较多的CPU时间。因为数据中的每个字在存储器与IO控制器之间的传输都必须通过CPU处理。 优点： 相比程序直接控制方式，中断驱动方式可以很好的会减少CPU等待IO设备完成IO的时间。 DMA方式 中断驱动方式中，CPU仍然需要主动处理在存储器和IO设备之间的数据传送，所以速度还是受限，而直接内存存取（DMA）方式的基本思想是在外围设备和内存之间开辟直接的数据交换通路，彻底解放CPU。 该方式的特点是： 基本单位是数据块 所传诵的数据，是从设备直接送入内存的或者相反，中间不需要经过CPU的内存拷贝 仅在传送一个或多个数据块的开始和结束时，才需CPU干预，整块数据的传送是在DMA控制器的控制下完成的。 DMA的工作过程是： CPU读写数据时，他给IO控制器发出一条命令，启动DMA控制器，然后继续其他工作。 之后CPU就把这个操作委托给DMA控制器，由该控制器负责处理。DMA控制器直接与存储器交互，传送整个数据块，这个过程不需要CPU参与。 当传送完成后，DMA控制器发送一个中断信号给处理器。因此，只有在传送开始和结束时才需要CPU的参与。 通道控制方式 DMA方式在工作流程上大幅度释放CPU，但是这种方式还需要CPU来控制传输的数据块大小、传输的内存位置，为了将CPU从这些繁琐的工作中释放处理，有了通道方式。 IO通道是专门负责输入输出的处理机，当CPU要完成一组读写操作时，只需要给I/O通道发出一条I/O指令，给出要执行的通道程序的首地址和要访问的I/O设备，通道接收到该指令后，执行完I/O任务，也就是完成数据传输，才给CPU发送中断请求。 ","date":"2023-05-30","objectID":"/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E4%B9%8B%E8%BE%93%E5%85%A5%E8%BE%93%E5%87%BA%E7%AE%A1%E7%90%86/:1:2","tags":[],"title":"操作系统之输入输出管理","uri":"/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E4%B9%8B%E8%BE%93%E5%85%A5%E8%BE%93%E5%87%BA%E7%AE%A1%E7%90%86/"},{"categories":["操作系统"],"content":"IO软件层次结构 IO软件层次中，其中设备独立性软件、设备驱动程序、中断处理程序这三部分属于操作系统内核部分，即“IO系统”或称“IO核心子系统” 用户层软件 用户层软件实现了与用户交互的接口，用户可直接使用该层提供的、与I/O操作相关的库函数对设备进行操作。 Eg：printf(“hello, world!\"); ==用户层软件将用户请求翻译成格式化的I/O请求，并通过“系统调用”请求操作系统内核的服务。== Eg：printf(“hello, world!”); 会被翻译成等价的 write 系统调用，当然，用户层软件也会在系统调用时填入相应参数。 Windows 操作系统向外提供的一系列系统调用，但是由于系统调用的格式严格，使用麻烦，因此在用户层上封装了一系列更方便的库函数接口供用户使用（Windows API）。 设备独立性软件 设备独立性软件，又称设备无关性软件。==与设备的硬件特性无关的功能几乎都在这一层实现。== 主要实现的功能： 向上层提供统一的调用接口（如 read/write 系统调用） 设备的保护 原理类似与文件保护。==设备被看做是一种特殊的文件==，不同用户对各个文件的访问权限是不一样的，同理，对设备的访问权限也不一样。 差错处理 设备独立性软件需要对一些设备的错误进行处理 设备的分配与回收 数据缓冲区管理 可以通过缓冲技术屏蔽设备之间数据交换单位大小和传输速度的差异 建立逻辑设备名到物理设备名的映射关系；根据设备类型选择调用相应的驱动程序 用户或用户层软件发出I/O操作相关系统调用的系统调用时，需要指明此次要操作的I/O设备的逻辑设备名（eg：去学校打印店打印时，需要选择 打印机1/打印机2/打印机3 ，其实这些都是逻辑设备名） 设备独立性软件需要通过“逻辑设备表（LUT，Logical UnitTable）”来确定逻辑设备对应的物理设备，并找到该设备对应的设备驱动程序 操作系统系统可以采用两种方式管理逻辑设备表（LUT）： 第一种方式，整个系统只设置一张LUT，这就意味着所有用户不能使用相同的逻辑设备名，因此这种方式只适用于单用户操作系统。 第二种方式，为每个用户设置一张LUT，各个用户使用的逻辑设备名可以重复，适用于多用户操作系统。系统会在用户登录时为其建立一个用户管理进程，而LUT就存放在用户管理进程的PCB中 设备驱动程序 各式各样的设备，外形不同，其内部的电子部件（I/O控制器）也有可能不同。驱动程序是一个进程。 ==为何不同的设备需要不同的设备驱动程序？== 佳能打印机的厂家规定状态寄存器为 0 代表空闲，1代表忙碌。有两个数据寄存器 惠普打印机的厂家规定状态寄存器为 1代表空闲，0代表忙碌。有一个数据寄存器 不同设备的内部硬件特性也不同，这些特性只有厂家才知道，因此厂家须提供与设备相对应的驱动程序，CPU执行驱动程序的指令序列，来完成设置设备寄存器，检查设备状态等工作 ==主要负责对硬件设备的具体控制，将上层发出的一系列命令（如read/write）转化成特定设备“能听得懂”的一系列操作。包括设置设备寄存器；检查设备状态等。== 不同的I/O设备有不同的硬件特性，具体细节只有设备的厂家才知道。因此厂家需要根据设备的硬件特性设计并提供相应的驱动程序。 中断处理程序 当I/O任务完成时，I/O控制器会发送一个中断信号，系统会根据中断信号类型找到相应的中断处理程序并执行。中断处理程序的处理流程如下： 总结 ","date":"2023-05-30","objectID":"/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E4%B9%8B%E8%BE%93%E5%85%A5%E8%BE%93%E5%87%BA%E7%AE%A1%E7%90%86/:1:3","tags":[],"title":"操作系统之输入输出管理","uri":"/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E4%B9%8B%E8%BE%93%E5%85%A5%E8%BE%93%E5%87%BA%E7%AE%A1%E7%90%86/"},{"categories":["操作系统"],"content":"IO核心子系统 ","date":"2023-05-30","objectID":"/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E4%B9%8B%E8%BE%93%E5%85%A5%E8%BE%93%E5%87%BA%E7%AE%A1%E7%90%86/:2:0","tags":[],"title":"操作系统之输入输出管理","uri":"/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E4%B9%8B%E8%BE%93%E5%85%A5%E8%BE%93%E5%87%BA%E7%AE%A1%E7%90%86/"},{"categories":["操作系统"],"content":"IO核心子系统概述 概念 设备独立性软件 已在前文说明 设备驱动软件 已在前文说明 中断处理程序 已在前文说明 IO调度与设备保护 IO调度 调度一组IO请求就是确定确定一个好的顺序来执行这些请求。应用程序所发布的系统调用的顺序不一定总是最佳选择，所以需要调度来改善系统整体性能，是进程之间公平的共享设备访问，减少IO完成所需要的平均等待时间。 操作系统开发人员通过为每个设备维护一个请求队列来实现调度。当一个应用程序执行阻塞IO系统调用时，该请求就加到相应设备的队列上。IO调度会重新安排队列顺序以改善系统总体效率和应用程序的平均响应时间。 IO子系统还可以使用主存或磁盘上的存储空间的技术，如缓冲、高速缓冲、假脱机等。 设备保护 操作系统中，设备被视为文件。文件保护机制也就是设备保护机制，例如口令或密码保护、访问控制等机制。 详见文件管理章节。 ","date":"2023-05-30","objectID":"/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E4%B9%8B%E8%BE%93%E5%85%A5%E8%BE%93%E5%87%BA%E7%AE%A1%E7%90%86/:2:1","tags":[],"title":"操作系统之输入输出管理","uri":"/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E4%B9%8B%E8%BE%93%E5%85%A5%E8%BE%93%E5%87%BA%E7%AE%A1%E7%90%86/"},{"categories":["操作系统"],"content":"SPOOLing技术 为了缓和CPU的高速型与IO设备低速性之间的矛盾而引入了脱机输入、脱机输出技术。该技术是利用专门的外围控制机，将低速IO设备上的数据传送到高速磁盘上；或者相反。 SPOOLing的意思是外部设备同时联机操作，又称为假脱机输入输出操作，是操作系统中采用的一项将独占设备改造成共享设备的技术。 在磁盘上开辟出的两个存储区域： 输入井模拟脱机输入时的磁盘，用于收容IO设备输入的数据。 输出井模拟脱机输出的磁盘，用于收容用户程序的输出数据。 在内存中开辟的两个缓冲区: 输入缓冲区用于暂存由输入设备送来的数据，以后再传送到输入井。 输出缓冲区用于暂存从输出井送来的数据，以后再传送到输出设备。 过程： 输入：输入进程模拟脱机输入时的外围控制机，将用户要求的数据从输入设备通过输入缓冲区再送到输入井。当CPU需要输入数据时，直接将数据从输入井读入内存。 输出：输出进程模拟脱机输出时的外围控制机，把用户要求输出的数据先从内存送到输出井，待输出设备空闲时，再将输出井中的数据经过输出缓冲区送到输出设备。 综上，实质上就是利用输入井作为输入时的缓存区；输出井作为输出时的缓存区。 为什么说是缓存区呢？其实，输入输出井就是在磁盘的缓存中开辟的区域，属于高速辅存。 SPOOLing技术举例 共享打印机是使用SPOOLing技术的一个实例，这项技术已被广泛的用于多用户系统和局域网络中。当用户进程请求打印输出时，SPOOLing系统同意为它打印输出，但并不真正立即把打印机分配给该用户进程，而只为她做两件事： 1）由输出进程在输出井中为之申请一个空闲磁盘块区，并将要打印的数据送入其中。 2）输出进程再为用户进程申请一张空白的用户请求打印表，并将用户的打印要求填入其中，再将该表挂到请求打印队列中。 SPOOLing系统的特点是：提高了IO速度；将独占设备改造为共享设备；实现了虚拟设备功能。 ","date":"2023-05-30","objectID":"/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E4%B9%8B%E8%BE%93%E5%85%A5%E8%BE%93%E5%87%BA%E7%AE%A1%E7%90%86/:2:2","tags":[],"title":"操作系统之输入输出管理","uri":"/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E4%B9%8B%E8%BE%93%E5%85%A5%E8%BE%93%E5%87%BA%E7%AE%A1%E7%90%86/"},{"categories":["操作系统"],"content":"设备分配与回收 设备分配的基本任务是根据用户的IO请求，为他们分配所需的设备。设备分配的总原则是充分发挥设备的使用效率，尽可能地让设备忙碌，又要避免由于不合理的分配方法造成进程死锁。 从设备的特性来看，可以把设备分成独占设备、共享设备和虚拟设备三类。 对于独立设备，将一个设备分配给某进程后，便有该进程独占，直至该进程完成或释放该设备。 对于共享设备，可以同时分配给多个进程使用，但需要对这些进程访问该设备的先后次序进行合理的调度。 虚拟设备属于可共享设备，可以将它同时分配给多个进程使用。 设备管理的数据结构 设备分配依据的主要数据结构有设备控制表（DCT）、控制器控制表（COCT）、通道控制表（CHCT）和系统设备表（SDT），各数据结构功能如下： 设备控制表：系统为每一个设备配置一张DCT，它用于记录设备的特性以及与IO控制器连接的情况。DCT包括设备标示符、设备类型、设备状态、指向COUCT的指针等。其中，设备队列指针指向等待使用该设备的进程组成的等待队列，控制器表指针指向于该设备相连接的设备控制器。 控制器控制表：每个控制器都配有一张COCT，它反应设备控制器的使用状态以及和通道的连接情况等。 通道控制表：每个通道配有一张CHCT。 系统设备表：整个系统只有一张SDT，它记录已连接到系统中的所有物理设备的情况，每个物理设备占一个表目。 由于在多道程序系统中，进程数多于资源数，会引起资源的竞争。因此，要有一套合理的分配原则，主要考虑的因素有：IO设备的固有属性，IO设备的分配算法，设备分配的安全性，以及设备独立性。 设备分配原则与算法 设备分配原则。设备的分配原则应根据设备特性、用户要求和系统配置的情况来决定。设备分配的总原则既要充分发挥设备的使用效率，又要避免造成进程死锁，还要将用户程序和具体设备隔离开。 设备分配方式。设备分配方式有静态分配和动态分配两种。 静态分配 主要用于对独占设备的分配，它在用户作业开始执行前，有系统一次性分配该作业所要求的全部设备、控制器（和通道）。一旦分配后，这些设备、控制器（和通道）就一直为高作业所占用，直到该作业被撤销。静态分配方式不会出现死锁，但设备的使用效率较低。因此，静态分配方式并不符合分配的总原则。 动态分配 是在进程执行过程中，根据执行需要进行分配。当进程需要设备时，通过系统调用命令向系统提出设备请求，由系统按照事先规定的策略给进程分配所需要的设备、IO控制器，一旦用完之后，便立即释放。动态分配方式有利于提高设备的利用率，但如果分配算法使用不当，则有可能造成进程死锁。 设备分配算法。常用的动态设备分配算法有先请求先分配、优先级高者优先等。 对于独占设备，即可以采用动态分配方式也可以静态分配方式，往往采用静态分配方式，即在作业执行前，将作业所要用到的这一类设备分配给它。 共享设备可被多个进程所共享，一般采用动态分配方式，但在每个IO传输的单位时间内只被一个进程所占有，通常采用先请求先分配和优先级高者先分的分配算法。 设备分配安全性和独立性 设备分配的安全性是指设备分配中应防止发生进程死锁。 安全分配方式。每当进程发出IO请求后便进入阻塞状态，直到其IO操作完成时才被唤醒。这样，一旦进程已经获得某种设备后便阻塞，不能再请求任何资源，而且在它阻塞时也不保持任何资源。 优点是设备分配安全；缺点是CPU和IO设备是串行工作的。 不安全分配方式。进程在发出IO请求后继续运行，需要时发出第二个、第三个IO请求等。仅当进程所请求的设备已被另一进程占用时，才进入阻塞状态。 优点是一个进程可以同时操作几个设备，从而使进程推进迅速；缺点是这种设备分配有可能产生死锁。 为了提高设备分配的灵活性和设备的利用率、方便实现IO重定向，因此引入了设备独立性。设备独立性是指应用程序独立于具体使用的物理设备。 为了实现设备独立性，在应用程序中使用逻辑设备名来请求使用某类设备，在系统中设置一张逻辑设备表（LUT），用于将逻辑设备名映射为物理设备名。LUT表项包括逻辑设备名、物理设备名和设备驱动程序入口地址；当进程用逻辑设备名来请求分配设备时，系统为他分配相应的物理设备，并在LUT中建立一个表项，以后进程再利用逻辑设备名请求IO操作时，系统通过查找LUT来寻找相应的物理设备和驱动程序。 在系统中可采取两种方式建立逻辑设备表： 在整个系统中只设置一张LUT表。这样，所有进程的设备分配情况都记录在这张表中，故不允许有相同的逻辑设备名，主要适用于单用户系统中。 为每个用户设置一张LUT。当用户登录时，系统便为用户建立一个进程，同时也位置建立一张LUT，并肩改变放入进程的PCB中。 设备分配步骤 根据进程请求的逻辑设备名查找SDT；（用户编程时提供的逻辑设备名就是“设备名”） 查找SDT，找到用户进程指定类型的、并且空闲的设备，将其分配给该进程。操作系统在逻辑设备表LUT中新增一个表项。 根据DCT找到COCT，若控制器忙碌则将进程PCB挂到控制器等待队列，不忙碌则将控制器分配给进程； 根据COCT找到CHCT，若通道忙碌则将进程挂到通道等待队列中，不忙碌则将通道分配给进程。 设备回收步骤 设备回收就是设备分配的逆操作。 ","date":"2023-05-30","objectID":"/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E4%B9%8B%E8%BE%93%E5%85%A5%E8%BE%93%E5%87%BA%E7%AE%A1%E7%90%86/:2:3","tags":[],"title":"操作系统之输入输出管理","uri":"/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E4%B9%8B%E8%BE%93%E5%85%A5%E8%BE%93%E5%87%BA%E7%AE%A1%E7%90%86/"},{"categories":["操作系统"],"content":"缓冲区的管理 操作系统总是用磁盘高速缓存技术来提高磁盘的IO速度，对高速缓存复制的访问要比原始数据访问更为高效。例如，正在运行的进程的指令即存储在磁盘上，也存储在物理内存上，也被复制到CPU的二级和一级高速缓存中。 不过，磁盘高速缓存技术不同于通常意义下的介于CPU与内存之间的小容量高速存储器，而是利用内存中的存储空间来暂存从磁盘中读出的一系列盘块中的信息，因此，磁盘高速缓存在逻辑上属于磁盘，物理上则是驻留在内存中的盘块。 高速缓存在内存中分为两种形式：一种是在内存中开辟一个单独的存储空间作为磁盘高速缓存，大小固定；另一种是把未利用的内存空间作为一个缓冲池，共请求分页系统和磁盘IO时共享。 在设备管理子系统中，引入缓冲区的目的有： 1）缓和CPU与IO设备间速度不匹配的矛盾。 2）减少对CPU的中断频率，放宽对CPU 中断响应时间的限制。 3）解决基本数据单元大小不匹配的问题。 4）提高CPU和IO设备之间的并行性。 其实现方法有： 1）采用硬件缓冲器，但由于成本太高，出一些关键部位外，一般情况下不采用硬件缓冲器。 2）采用缓冲区（位于内存区域） 根据系统设置缓冲器的个数，缓冲技术可以分为以下几种 单缓冲 在设备和处理器之间设置一个缓冲区。设备和处理器交换数据时，先把被交换数据写入缓冲区，然后把需要数据的设备或处理器从缓冲区取走数据。 写入数据和取走数据不能同时进行，只能串行执行，所以原则是 非空不写 未满不读 在块设备输入时，假定从磁盘把一块数据输入到缓冲区的时间为T，操作系统将该缓冲区中的数据局传送到用户区的时间为M，而CPU对这一块数据处理的时间为C。由于T和C是可以并行的，所以可把系统对每一块数据的处理时间表示为Max（C,T）+M。 双缓冲 双缓冲区机制又称缓冲对换。IO设备输入数据时先输入到缓冲区1，直到缓冲区1满后才输入到缓冲区2，此时操作系统可以从缓冲区1中取出数据放入用户进程，并由CPU计算。 双缓冲的使用提高了处理器和输入设备的并行操作的程度。 系统处理一块数据的时间可以粗略地认为是Max（C,T）。如果CT，则可使CPU不必等待设备输入。对于字符设备，若采用行输入方式，则采用双缓冲可使用户再输入完第一行之后，在CPU执行第一行中的命令的同事，用户可继续向第二缓冲区输入下一行数据。而单缓冲情况下则必须等待一行数据被提取完毕才可输入下一行的数据。 如果两台机器之间通信仅配置了单缓冲，那么，他们在任意时刻都只能实现单方向的数据传输。为了实现双向数据传输，必须在两台机器中都设置两个缓冲区，一个用作发送缓冲区，另一个用作接收缓冲区。 循环缓冲 包含多个大小相等的缓冲区，每个缓冲区中有一个缓冲区，最后一个缓冲区指针指向第一个缓冲区，多个缓冲区构成一个环形。用于输入输出时，还需要有两个指针in和out。对输入而言，首先要从设备接收数据到缓冲区中，in指针指向可以输入数据的第一个空缓冲区；当运行进程需要数据时，从循环缓冲去中去一个装满数据的缓冲区，并从此缓冲区中提取数据，out指针指向可以提取数据的第一个满缓冲区。输出正好相反。 缓冲池 由多个系统共用的缓冲区组成，缓冲区按其使用状况可以形成三个队列：空缓冲队列、装满输入数据的缓冲队列（输入队列）和装满输出数据的缓冲队列（输出队列）。还应具有四种缓冲区：用于收容输入数据的工作缓冲区、用于提取输入数据的工作缓冲区、用于收容输出数据的工作缓冲区、用于提取输出数据的工作缓冲区。 收容输入。在输入进程需要输入数据时，便调用Getbuf(emq)过程，从空缓冲队列emq的队首摘下一空缓冲区，把它作为收容输入工作缓冲区hin。然后，把数据输入其中，装满后再调用Putbuf(inq，hin)过程，将该缓冲区挂在输入队列 inq 上。 提取输入。当计算进程需要输入数据时，调用 Getbuf(inq)过程，从输入队列 inq 的队首取得一个缓冲区，作为提取输入工作缓冲区(sin)，计算进程从中提取数据。计算进程用完该数据后，再调用 Putbuf(emq，sin)过程，将该缓冲区挂到空缓冲队列 emq 上。 收容输出。当计算进程需要输出时，调用 Getbuf(emq)过程从空缓冲队列 emq 的队首取得一个空缓冲区，作为收容输出工作缓冲区 hout。当其中装满输出数据后，又调用Putbuf(outq，hout)过程，将该缓冲区挂在 outq 末尾。 提取输出。由输出进程调用 Getbuf(outq)过程，从输出队列的队首取得一装满输出数据的缓冲区，作为提取输出工作缓冲区 sout。在数据提取完后，再调用 Putbuf(emq，sout)过程，将该缓冲区挂在空缓冲队列末尾。 缓存（cache）与缓冲（buffer）的对比 缓存是指在计算机系统中，为了提高数据读写效率而设置的高速缓存存储器。缓存通常被用来存储经常被访问的数据，以减少对主存储器的访问次数，从而提高读写速度。缓存的数据可以是来自外部存储器的数据，也可以是来自CPU内部的数据。 缓冲是指在计算机系统中，为了解决数据传输速度不匹配而设置的临时存储区。缓冲通常被用来存储数据，以平衡数据的输入和输出速度。缓冲的数据通常是来自输入输出设备的数据，例如磁盘、网络等，缓冲可以将这些数据暂时存储起来，等待处理器或者其他设备的处理。 简单来说，缓存是为了提高数据读写效率而设置的高速存储器，缓冲是为了解决数据传输速度不匹配而设置的临时存储区。 ","date":"2023-05-30","objectID":"/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E4%B9%8B%E8%BE%93%E5%85%A5%E8%BE%93%E5%87%BA%E7%AE%A1%E7%90%86/:2:4","tags":[],"title":"操作系统之输入输出管理","uri":"/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E4%B9%8B%E8%BE%93%E5%85%A5%E8%BE%93%E5%87%BA%E7%AE%A1%E7%90%86/"},{"categories":["操作系统"],"content":"出错处理 操作系统可以采用内存保护，这样一来就可以预防许多硬件和应用程序的错误，即便有一些设备硬件上的适龄也不回导致系统的完全崩溃。 IO设备传输中出现的错误很多，如网络上的堵塞和传输过载等。操作系统可以对一些短暂的出错进行处理，比如读取磁盘出错，那么可以选择重新对磁盘进行read操作；再比如在网络上发送数据出错，那么只要网络通信协议允许，就可以做resend操作。但是，如果计算机系统中的重要组件出现了永久性错误，那么操作系统将无法恢复。 作为一个规则，IO系统调用通常返回一位调用状态信息，以表示成功或失败。在UNIX系统中，用一个名为errno的全局变量来表示出错代码，以表示出错原因。 注意：read、send和resend都是操作系统的基本输入输出命令，分别用来读、发送和重发数据。 ","date":"2023-05-30","objectID":"/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E4%B9%8B%E8%BE%93%E5%85%A5%E8%BE%93%E5%87%BA%E7%AE%A1%E7%90%86/:2:5","tags":[],"title":"操作系统之输入输出管理","uri":"/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E4%B9%8B%E8%BE%93%E5%85%A5%E8%BE%93%E5%87%BA%E7%AE%A1%E7%90%86/"},{"categories":["操作系统"],"content":"[toc] ","date":"2023-05-21","objectID":"/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E4%B9%8B%E6%96%87%E4%BB%B6%E7%AE%A1%E7%90%86/:0:0","tags":[],"title":"操作系统之文件管理","uri":"/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E4%B9%8B%E6%96%87%E4%BB%B6%E7%AE%A1%E7%90%86/"},{"categories":["操作系统"],"content":"文件系统基础 ","date":"2023-05-21","objectID":"/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E4%B9%8B%E6%96%87%E4%BB%B6%E7%AE%A1%E7%90%86/:1:0","tags":[],"title":"操作系统之文件管理","uri":"/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E4%B9%8B%E6%96%87%E4%BB%B6%E7%AE%A1%E7%90%86/"},{"categories":["操作系统"],"content":"文件的概念 由于系统的内存有限并且不能长期保存，故平时总是把它们以文件的形式存放在外存中，需要时再将它们调入内存。如何高效的对文件进行管理是操作系统实现的目标。 文件和文件系统 现代OS几乎都是通过文件系统来组织和管理在计算机中所存储的大量程序和数据的。文件系统的管理功能是通过把它所管理的程序和数据组织成一系列文件的方法来实现的。而文件则是指具有文件名的若干相关元素的集合。元素通常是记录，而记录是一组有意义的数据项的集合。可以把数据组成分为数据项、记录、文件。 数据项，数据项是最低级数据组织形式。分为基本数据项（用于描述一个对象某种属性的字符集，是数据组织中可以明明的最小逻辑数据单位，即原子数据，又称为数据元素或字段）和组合数据项（由若干个基本数据项组成） 记录，是一组相关数据项的集合，用于描述一个对象在某方面的属性，为了能够唯一标识一个记录，需要在记录中确定一个或集合数据项，把他们的集合称为关键字，关键字是能够唯一标识一个记录的数据项。 文件，文件是具有文件名的一组相关元素的集合，分为有结构文件和无结构文件。有结构文件由若干个相关记录组成，无结构文件则被看成一个字符流。文件是文件系统的最大数据单位。文件应该具有自己的属性，包括文件类型（如源文件、目标文件、可执行文件等），文件长度（文件的当前长度，也可能是最大允许长度），文件的物理位置（指示文件在哪一个设备上及在该设备的哪个位置的指针），文件的建立时间（文件最后一次修改时间）。 一个文件可对应若干个记录，一个记录可对应若干个数据项。 文件系统管理的对象有：文件（作为文件管理的直接对象），目录（为了方便用户对文件的存取和检索，在文件系统中配置目录，每个目录项中，必须含有文件名及该文件所在的物理地址，对目录的组织和管理是方便和提高对文件存取速度的关键），磁盘（磁盘）存储空间（文件和目录必定占用存储空间，对这部分空间的有效管理，不仅能提高外存的利用率，而且能提高对文件的存取速度）。 文件操作 创建文件，在创建一个新文件时，系统首先要为新文件分配必要的外存空间，并在文件系统的目录中，为之建立一个目录项，目录项中应该记录新文件的文件名及其在外存的地址等属性。 删除文件，当已不再需要某文件时，可将其从文件系统中删除。在删除时，系统应先从目录中找到要删除文件的目录项，使之成为空项，然后回收该文件所占用的存储空间。 读文件，读文件时，须在相应系统调用中给出文件名和应读入的内存目标地址。此时，系统要查找目录，找到指定目录项，从中得到被读文件在外存中的位置。在目录项中，还有一个指针用于对文件进行读/写。 写文件，写文件时，须在相应系统调用中给出文件名和其在内存源地址。此时，系统要查找目录，找到指定目录项，从再利用目录中的写指针进行写操作。 截断文件，如果一个文件的内容已经陈旧而需要全部更新时，一种方法是将此文件删除，再重新创建一个新文件，但如果文件名和属性均无改变，则可采取截断文件的方法，其将原有的文件长度设置为0，放弃原有文件的内容，再将新内容读入。 设置文件的读/写位置，用于设置文件读/写指针的位置，以便每次读/写文件时，不需要从始端开始而是从所设置的位置开始操作。可以改顺序存取为随机存取。 当前OS所提供的大多数对文件的操作，其过程大致都是这样两步： 首先，检索文件目录来找到指定文件的属性及其在外存上的位置； 然后，对文件实施相应的操作，如读/写文件等。 当用户要求对一个文件实施多次读/写或其他操作时，每次都要从检索目录开始，为了避免多次重复地检索目录，在大多数OS中都引入了打开这一文件系统调用，当用户第一次请求对某文件系统进行操作时，先利用open系统调用将该文件打开。打开是指系统将指名文件的属性（包括该文件在外存上的物理位置）从外存拷贝到内存打开文件表的一个表目中，并将该表目的编号（索引号）返回给用户，以后，当用户再要求对该文件进行操作时，便可利用系统所返回的索引号向系统提出操作请求，系统便可直接利用该索引号到打开文件表中去查找，从而避免了对该文件的再次检索，如果用户不再需要对该文件实施操作，可利用关闭系统调用来关闭此文件，OS将会把该文件从打开文件表中的表目上删除掉。 小tips：如果没有调用close系统调用来关闭文件资源，操作系统会在程序结束时自动关闭文件资源。但是，如果程序在关闭文件之前崩溃或意外退出，文件资源可能会一直处于打开状态，导致资源泄漏。此外，如果程序打开了大量文件而没有关闭它们，可能会导致系统资源不足，从而影响其他程序的运行。因此，为了保证程序的稳定性和性能，建议在读写文件完毕后及时调用close系统调用来关闭文件资源。 ","date":"2023-05-21","objectID":"/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E4%B9%8B%E6%96%87%E4%BB%B6%E7%AE%A1%E7%90%86/:1:1","tags":[],"title":"操作系统之文件管理","uri":"/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E4%B9%8B%E6%96%87%E4%BB%B6%E7%AE%A1%E7%90%86/"},{"categories":["操作系统"],"content":"文件的结构 对任何的文件，都存在以下两种形式的结构 文件的逻辑结构，这是从用户观点出发所观察到的文件组织形式，是用户可以直接处理的数据及其结构，独立于文件的物理特性，又称为文件组织。 文件的物理结构，又称为文件的存储结构，是指文件在外存上的存储组织形式，不仅与存储介质有关，还与外存分配方式有关。 文件物理结构涉及硬件，不是操作系统关注的重点 文件逻辑结构的类型 文件的逻辑结构可分为两大类，一类是有结构文件，这是指由一个以上的记录构成的文件，故把他称为记录式文件，另一类是无结构文件，这是指由字符流构成的文件，又称为流式文件。 有结构文件（记录式文件） 每个记录都用于描述实体集中的一个实体，各记录有着相同或不同数目的数据项，记录分为定长记录（文件中所有记录的长度都是相同的，所有记录中的各数据项都处在记录中相同的位置，具有相同的顺序和长度）和变长记录（文件中个记录的长度不相同，可能由于一个记录中所包含的数据项目并不相同）。 根据用户和系统的需要，可采用多种方式来组织这些记录，如顺序文件（记录按照某种顺序排列所形成的文件，记录通常是定长的，能较快查找到文件中的记录），索引文件（记录为可变长度时，通常建立一张索引表，并为每个记录设置一个表项，加快对记录检索的速度），索引顺序文件（为文件建立一张索引表，为每一组记录中的第一个记录设置一个表项）。 无结构文件（流式文件） 对于源程序、可执行文件、库函数等通常采用的是无结构文件形式，即流式文件，其长度以字节为单位。 库函数文件通常是库函数文件通常是二进制文件，其文件类型取决于库函数的编译方式和目标平台。在Unix/Linux系统中，常用的库函数文件扩展名为.so（共享对象）或.a（静态库），在Windows系统中则通常是.dll（动态链接库）或.lib（静态库）。这些库文件包含了编译好的函数和代码，可以被程序调用和链接，以实现特定的功能。库函数文件可以由开发者自己编写，也可以使用第三方库，如标准C库或开源库等。 顺序文件 顺序文件是有结构文件。文件是记录的集合，它可以按照各种不同的顺序进行排列，一般地，可归纳为以下两种情况。 链表结构（变长），各记录之间的顺序与关键字无关，通常按照时间先后排序，最先存入的记录作为第一个记录，其次，为第二个记录，以此类推。链表结构存储检索效率低下，所以定位删除、更新、查询的效率都较为低下。 顺序结构（定长），文件中所有记录按照关键字排列，可以按照关键词长度从大到小排列。顺序结构的检索效率更高，但增加删除效率低下，需要删除原分配空间，另外分配一个更小，将所有没有删除的数据拷贝到新分配好的空间中去，可以类比二维数组实现。 顺序文件的最佳应用场合是在对诸记录进行批量存取时，即每次要读或写一大批记录时，此时，对顺序文件的存取效率是所有逻辑文件中最高的，此外，只有顺序文件才能存储在磁带上，并能有效工作。但是想要增加或删除一个文件比较困难。 索引文件 对于定长记录文件，顺序文件的实现，可以方便的实现顺序存取和直接存取。然而，顺序文件对于变长记录就很难实现。为了解决变长记录检索问题，可为变长记录文件建立一张索引表，对主文件中的每个记录，在索引表中设有一个相应的表项，用于记录该记录的长度L及指向该记录的指针（指向该记录在逻辑地址空间的首址），由于索引表示按记录键排序的，因此，索引表本身是一个定长记录的顺序文件。从而可以方便实现直接存取。 在对索引文件进行检索时，首先根据用户（程序)提供的关键字，并利用折半查找检索索引表，从中找到相应的表项，再利用该表项给出的指向记录的指针值，去访问所需的记录。每当要向索引文件中增加一个新纪录时，便须对索引表进行修改。索引表的问题在于除了有主文件外，还需要配置一张索引表，每个记录需要有一个索引项，因此提高了存储费用。 索引顺序文件 其有效克服了变长记录不便于直接存取的缺点，而且所付出的代价也不算太大，它是顺序文件和索引文件相结合的产物，它将顺序文件中的所有记录分为若干个组，为顺序文件建立一张索引表，在索引表中为每组中的第一个记录建立一个索引项，其中含有该记录的键值和指向记录的指针。 在对索引顺序文件进行检索时，首先利用用户（程序）所提供的关键字及某种查找算法去检索索引表，找到该记录组中的第一个记录的表项，从中得到该记录组第一个记录在主文件中的位置，然后，再利用顺序查找法去查找主文件，从中找出所要求的记录。 类似于字典查找，先找拼音或部首缩小范围，再找具体的字。 直接文件 对于直接文件，则根据给定的记录键值，直接获得指定记录的物理地址，换言之，记录键值本身就决定了记录的物理地址，这种由记录键值到记录物理地址的转换被称为键值转换。 哈希文件 利用Hash函数可将记录键值转换为相应记录的地址，为了能实现文件存储空间的动态分配，通常由Hash函数所求得的并非是相应记录的地址，而是指向一目录表相应表目的指针，该表目的内容指向相应记录所在的物理块。 文件的目录结构 为了能够对文件实施有效的管理，必须对它们加以妥善组织，这主要是通过文件目录实现的，文件目录也是一种数据结构，用于标识系统中的文件及其物理地址，供检索时使用，对目录的管理要求如下： 实现按名存取，即用户只须向系统提供所需访问的文件的名字，便能够快速准确地找到指定文件在外存上的存储位置，这是目录管理中最基本的功能。 提高对目录检索速度，通过合理地组织目录结构的方法，可加快对目录的检索速度，从而提高对文件的存取速度。 文件共享，在多用户系统中，应该允许用户共享一个文件。 允许文件重名，系统应允许不同用户对不同文件采用相同的名字，以便用户按照自己的习惯给文件命名和使用文件。 文件控制块（FCB） 一个FCB条目可能是文件，也可能是目录，但不能即是文件，又是目录。因为目录也是文件的一种。FCB的有序集合称为文件目录，一个FCB就是一个文件目录项。为了创建一个新文件，系统将分配一个FCB并存放在文件目录中，称为目录项。 值得注意的是，FCB是存储在目录中，而并不是目录项中。也就是说，目录文件会存储当前目录里的FCB，而当前目录的FCB是在上级目录中存放 基本信息：文件名、类型、物理位置 存取控制权限信息：文件读写权限 使用信息：创建时间等 索引节点index node 理解inode，要从文件储存说起。文件储存在硬盘上，硬盘的最小存储单位叫做”扇区”（Sector）。每个扇区储存512字节（相当于0.5KB）。操作系统读取硬盘的时候，不会一个个扇区地读取，这样效率太低，而是一次性连续读取多个扇区，即一次性读取一个”块”（block）。这种由多个扇区组成的”块”，是文件存取的最小单位。”块”的大小，最常见的是4KB，即连续八个 sector组成一个 block。文件数据都储存在”块”中，那么很显然，我们还必须找到一个地方储存文件的元信息，比如文件的创建者、文件的创建日期、文件的大小等等。这种储存文件元信息的区域就叫做inode，中文译名为”索引节点” 。 inode包含内容 Linux中目录的数据块中的每一项中都包含了文件名和其对应的inode。inode记录了文件的属性以及该文件实际存储位置，即数据块号（block number），每一个block（常见大小4KB），通过inode可以实现文件的查找定位。inode是Linux中的，Unix中是vnode。 基本上，inode包含的信息至少有如下这些：（1）文件的类型 （2）文件访问权限； （3）文件的所有者与组； （4）文件的大小； （5）链接数，即指向该inode的文件名总数； （6）文件的状态改变时间（ctime）、最近访问时间（atime）和最近修改时间（mtime）； （7）文件特殊属性，SUID、SGID和SBIT； （8）文件内容的真正指向（pointer）。 可以用stat命令，查看某个文件的inode信息。 每个文件都只占用一个inode。因此，文件系统能够建立的文件数量与inode数量有关。系统读取档案时需要先找到inode，并分析inode所记录的权限与用户是否符合，若符合才能够开始实际读取block的内容。 操作系统读取磁盘文件的流程 根据给定的文件的所在目录，获取该目录的数据实体。再根据数据实体中的数据项，找到对应文件的inode（拿到了inode地址）； 根据文件inode，找到inodeTable； 根据inodeTable中的对应关系，找到对应的block； 读取文件。 inode的优点 对于有些无法删除的文件可以通过删除inode节点来删除； 移动或者重命名文件，只是改变了目录下的文件名到inode的映射，并不需要实际对硬盘操作； 删除文件的时候，只需要删除inode，不需要实际清空那块硬盘，只需要在下次写入的时候覆盖即可（这也是为什么删除了数据可以进行数据恢复的原因之一）； 打开一个文件后，只需要通过inode来识别文件。 inode和FCB的关系 FCB = 文件名 + inode的所有内容 FCB是在文件打开时创建的，而inode是在文件创建时就被创建的 FCB随着文件关闭而释放，inode并不会 寻找文件时，需要通过inode包含的文件物理地址查询 目录结构 目录结构的组织，关系到文件系统的存取速度，也关系到文件的共享性和安全性，目前常用的目录结构形式有单级目录、两级目录、多级目录。 单级目录 在整个系统中只建立一张目录表，每个文件占一个目录项，目录项中含文件名、文件扩展名、文件长度、文件类型、文件物理地址、状态位（表示目录项是否空闲）等。 缺点： 查找速度慢：所有文件都在同一个目录下，文件越多，查询越慢 无法实现文件重名：在同一目录下，没","date":"2023-05-21","objectID":"/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E4%B9%8B%E6%96%87%E4%BB%B6%E7%AE%A1%E7%90%86/:1:2","tags":[],"title":"操作系统之文件管理","uri":"/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E4%B9%8B%E6%96%87%E4%BB%B6%E7%AE%A1%E7%90%86/"},{"categories":["操作系统"],"content":"文件的共享 文件共享使多个用户（进程）共享同一份文件，系统中只需保留该文件的一份副本。如果系统不能提供共享功能，那么每个需要该文件的用户都要有各自的副本，会造成对存储空间的极大浪费。随着计算机技术的发展，文件共享的范围已由单机系统发展到多机系统，进而通过网络扩展到全球。 这些文件的分享是通过分布式文件系统、远程文件系统、分布式信息系统实现的。这些系统允许多个客户通过C/S模型共享网络中的服务器文件。 现代常用的两种文件共享方法有：硬链接和软链接 硬链接：基于索引结点的共享方式 在树形结构的目录中，当有两个或多个用户要共享一个子目录或文件时，必须将共享文件或子目录链接到两个或多个用户的目录中，才能方便地找到该文件，如图所示 在这种共享方式中引用索引结点，即诸如文件的物理地址及其他的文件属性等信息，不再是放在目录项中，而是放在索引结点中。 在文件目录中只设置文件名及指向相应索引结点的指针。在索引结点中还应有一个链接计数count,用于表示链接到本索引结点（亦即文件）上的用户目录项的数目。只有当count=0时，也就是文件没有其他用户被硬链接，此时可以删除掉文件；count\u003e0时，不能删除文件。 优点： 读取效率高：多个用户访问共享文件时，无需经过路径名，只需要找到当前用户的共享文件名，即可拿到索引节点； 存储开销小：共享文件只有一个索引节点，没有额外存储开销。 缺点： 可能会出现访问共享文件异常空指针地情况：如果删除掉索引节点时，还有用户保留了共享文件。那么这个共享文件的inode指针将会为空，访问会出错。 不好实现网络文件地共享 示例说明 当count=2时，表示有两个用户目录项链接到本文件上，或者说是有两个用户共享此文件。 当用户A创建一个新文件时，它便是该文件的所有者，此时将count置为1。 当有用户 B要共享此文件时，在用户B的目录中增加一个目录项，并设置一指针指向该文件的索引结点。 此时，文件主仍然是用户A，count=2。如果用户A不再需要此文件，不能将文件直接删除。 因为，若删除了该文件，也必然删除了该文件的索引结点，这样便会便用户B的指针悬空，而用户B则可能正在此文件上执行写操作，此时用户B会无法访问到文件。 因此用户A不能删除此文件，只是将该文件的count减1，然后删除自己目录中的相应目录项。用户B仍可以使用该文件。 当count=0时，表示没有用户使用该文件，系统将负责删除该文件。 软链接：利用符号链实现文件共享 为使用户B能共享用户A的一个文件F,可以由系统创建一个LINK类型的新文件，也取名为F，并将文件F写入用户B的目录中，以实现用户B的目录与文件F的链接。 在新文件中只包含被链接文件F的路径名或URL。这样的链接方法被称为符号链接。 值得注意的是：在利用符号链方式实现文件共享时，只有文件的拥有者才拥有指向其索引结点的指针；而共享该文件的其他用户则只有该文件的路径名，并不拥有指向其索引结点的指针，也就是说，通过符号链的方式共享文件时，需要通过路径名来查找索引节点，显然比较慢。而且，每个软链接文件都需要创建一个link类型文件的索引节点，增加了存储开销。 优点： 不存在异常悬空指针的情况：当文件的拥有者把一个共享文件删除后，其他用户通过符号链去访问它时，会出现访问失败，于是将符号链删除，此时不会产生任何影响。 方便地获取网络文件：网络共享只需提供该文件所在机器的网络地址以及该机器中的文件路径 缺点： 读取效率较低：当其他用户读共享文件时，需要根据文件路径名逐个地查找目录，直至找到该文件的索引结点。因此，每次访问时，都可能要多次地读盘，使得访问文件的开销变大并增加了启动磁盘的频率； 存储开销更大：符号链文件共享就是创建一个link类型的文件，所以也需要创建一个索引节点来保存link类型文件的元信息； ","date":"2023-05-21","objectID":"/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E4%B9%8B%E6%96%87%E4%BB%B6%E7%AE%A1%E7%90%86/:1:3","tags":[],"title":"操作系统之文件管理","uri":"/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E4%B9%8B%E6%96%87%E4%BB%B6%E7%AE%A1%E7%90%86/"},{"categories":["操作系统"],"content":"文件的保护 为了防止文件共享可能会导致文件被破坏或未经核准的用户修改文件，文件系统必须控制用户对文件的存取，即解决对文件的读、写、执行的许可问题。 为此，必须在文件系统中建立相应的文件保护机制。 文件保护通过口令保护、加密保护和访问控制等方式实现。其中，口令保护和加密保护是为了防止用户文件被他人存取或窃取，而访问控制则用于控制用户对文件的访问方式。 口令保护与加密保护 口令保护：用户需要输入对的口令，才能拿到索引节点，之后再通过索引节点中文件物理地址去访问磁盘上的文件。但是口令保护有个无法忽视的缺点，那就是如果某用户越过了索引节点，直接拿到文件物理地址，口令保护就会失去作用； 加密保护：对存储的文件进行可靠加密，只有解密才能拿到文件内容，可以很好的起到文件保护作用。 访问控制 控制的含义就是控制不同文件的操作权限。解决访问控制最常用的方法是根据用户身份进行文件操作控制。 而实现基于身份访问的最为普通的方法是为每个文件和目录增加一个访问控制列表(Access-Control List, ACL)，以规定每个用户名及其所允许的访问类型。 这种方法的优点是可以使用复杂的访同方法。其缺点是长度无法预期并且可能导致复杂的空间管理，使用精简的访问列表可以解决这个问题。 精简的访问列表釆用拥有者、组和其他三种用户类型。 拥有者：创建文件的用户。 组：一组需要共享文件且具有类似访问的用户。 其他：系统内的所有其他用户。 这样只需用三个域列出访问表中这三类用户的访问权限即可。 文件拥有者在创建文件时，说明创建者用户名及所在的组名，系统在创建文件时也将文件主的名字、所属组名列在该文件的FCB中。 用户访问该文件时，按照拥有者所拥有的权限访问文件，如果用户和拥有者在同一个用户组则按照同组权限访问，否则只能按其他用户权限访问。UNIX操作系统即釆用此种方法。 ","date":"2023-05-21","objectID":"/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E4%B9%8B%E6%96%87%E4%BB%B6%E7%AE%A1%E7%90%86/:1:4","tags":[],"title":"操作系统之文件管理","uri":"/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E4%B9%8B%E6%96%87%E4%BB%B6%E7%AE%A1%E7%90%86/"},{"categories":["操作系统"],"content":"文件系统实现 ","date":"2023-05-21","objectID":"/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E4%B9%8B%E6%96%87%E4%BB%B6%E7%AE%A1%E7%90%86/:2:0","tags":[],"title":"操作系统之文件管理","uri":"/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E4%B9%8B%E6%96%87%E4%BB%B6%E7%AE%A1%E7%90%86/"},{"categories":["操作系统"],"content":"文件系统层次结构 文件系统接口。文件系统为用户提供与文件及目录有关的调用，如新建、打开、读写、关闭、删除文件，建立、删除目录等。此层由若干程序模块组成，每一模块对应一条系统调用，用户发出系统调用时，控制即转入相应的模块。 文件目录系统的主要功能是管理文件目录，其任务有管理活跃文件目录表、管理读写状态信息表、管理用户进程的打开文件表、管理与组织在存储设备上的文件目录结构、调用下一级存取控制模块。 实现文件保护主要由存取控制模块完成，它把用户的访问要求与FCB中指示的访问控制权限进行比较，以确认访问的合法性。 逻辑文件系统与文件信息缓冲区的主要功能是根据文件的逻辑结构将用户要读写的逻辑记录转换成文件的逻辑结构内的相应块号。 物理文件系统的主要功能是把逻辑记录所在的相对块号转换成实际的物理地址。 辅助分配模块的主要功能是管理辅存空间，即负责分配辅存空闲空间和回收辅存空间。 设备管理程序模块的主要功能是分配设备、分配设备读写缓冲区、磁盘调度、启动设备、处理设备中断、释放设备读写缓冲区、释放设备等。 ","date":"2023-05-21","objectID":"/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E4%B9%8B%E6%96%87%E4%BB%B6%E7%AE%A1%E7%90%86/:2:1","tags":[],"title":"操作系统之文件管理","uri":"/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E4%B9%8B%E6%96%87%E4%BB%B6%E7%AE%A1%E7%90%86/"},{"categories":["操作系统"],"content":"目录实现 在读文件前，必须先打开文件。打开文件时，操作系统利用路径名找到相应目录项，目录项中提供了查找文件磁盘块所需要的信息，目录实现的基本方法有线性列表和哈希表两种方法。 目录就是FCB的集合 线性表 线性表的项是由文件名和数据块指针组成。（数据块指针指向的可能是子目录，也可能是一个具体的文件） 下图的画法是链表实现，当然，也可以是数组实现。 **优点：**实现较为简单 **缺点：**线性表的增删操作较为复杂耗时，而且查询也比较耗时(O(n)) 哈希表 哈希表根据文件名得到一个值，并返回一个指向线性列表中元素的指针。 **优点：**查询目录速度更快 **缺点：**可能会发生哈希冲突，导致文件可能会被覆盖；增删文件时，会触发重哈希，较为耗损性能。 目录查询必须通过在磁盘上反复搜索完成，需要不断的进行IO操作，开销较大。所以如前面所述，为了减少IO操作，把当前使用的文件目录复制到内存，以后要使用该文件时只要在内存中操作，从而降低了磁盘操作次数。 ","date":"2023-05-21","objectID":"/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E4%B9%8B%E6%96%87%E4%BB%B6%E7%AE%A1%E7%90%86/:2:2","tags":[],"title":"操作系统之文件管理","uri":"/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E4%B9%8B%E6%96%87%E4%BB%B6%E7%AE%A1%E7%90%86/"},{"categories":["操作系统"],"content":"文件实现 文件分配对应于文件的物理结构，是指如何为文件分配磁盘块。常用的磁盘空间分配方式有三种：连续分配、链接分配和索引分配。有的系统对三种方式都支持，但是更普遍的是一个系统只提供一种方法支持。 文件分配方式 连续分配 如果将块分配给文件，使得文件的所有逻辑块都得到硬盘中的连续物理块，则这种分配方案被称为连续分配。 优点： 实现简单。只需要在目录项中记录文件名、起始地址和文件长度即可。 可以获得优秀的读取性能。通过连续分配磁盘空间块的方式，作业访问磁盘时需要的寻道数和寻道时间最小。 支持随机访问文件。可以通过指针指定当前的读写位置，后续就可以直接再这个指针上进行偏移读取文件任何部分内容。（因为文件在磁盘上的数据是连续的，所以可以不断地偏移指针） 缺点： 文件长度不能随便动态增加。因为一个文件末尾后的盘块可能已经分配给其他文件，一旦需要增加，就需要大量移动盘块。 外部碎片增加。反复增删文件后会产生外部碎片。所以给文件连续分配磁盘空间块时，比较适合文件长度固定的文件。 链接分配 隐式链接分配 链接分配解决了连续分配的所有问题。也就是说，链接分配可以实现文件长度动态增加，而且不存在外部碎片。 在链接分配中，分配给特定文件的磁盘块不需要在磁盘上连续存在。分配给文件的每个磁盘块都包含一个指向分配给同一文件的下一个磁盘块的指针。 优点 链接分配没有外部碎片。通过链接的方式，所有的磁盘空闲块都能得到利用，所以也就没有外部碎片。 可以使用任何空闲块来满足文件块请求。只要是空闲块都可以被链接写入文件。 只要空闲块可用，文件可以继续增长。 目录条目将仅包含起始块地址。 缺点 随机（直接）访问不提供。因为每个块直接并不是地址连续的，而是一个块包含下一个块的指针。所以，扫描一块后，就需要取出下一个块的地址，无法通过指针直接偏移到文件的任何位置。这个过程都是在磁盘里的。 额外的空间占用。指针在磁盘块中需要一些空间。 会有数据丢失风险。链接列表中的任何指针都不能被破坏，否则文件将被损坏。 文件访问性能较低。需要遍历每个块。 显式链接分配 链接分配的显式实现——文件分配表FAT。 隐式链表分配的主要缺点是它不提供对特定块的随机访问。要访问一个块，我们还需要访问它之前的所有块。 文件分配表克服了链表分配的缺点。在这个方案中，维护一个文件分配表，它收集所有的磁盘块链接。该表对每个磁盘块都有一个条目，并按块编号进行索引。 优点 使用整个磁盘块获取数据。 坏磁盘块不会导致所有连续的块丢失。FAT将指向下一个磁盘的指针和磁盘数据分离了。 提供随机访问，尽管它不太快。为什么说隐式链接分配提供了随机访问呢？因为通过FAT的记录，我们也可以做到指针确切偏移到文件存储磁盘的任意一个位置，只是相比于连续分配方式，隐式链接分配的指针偏移要更加复杂一点，需要在FAT中取出下一个要偏移的块。这个过程都是在内存进行的。 每个文件操作中只需要遍历FAT。 缺点 每个磁盘块都需要一个FAT条目。 根据FAT条目的数量，FAT大小可能非常大。 可以通过增加块大小来减少FAT条目的数量，但也会增加内部碎片。 区别 显式链接分配方式，就是数据块和指向下一个数据块的指针都存于同一个磁盘块； 而隐式连接分配方式，就是将数据块和指向下一个数据块的指针进行分离，所有的指针存于FAT中，而FAT需要加载到内存里。 索引分配 FAT的限制： 文件分配表尽量解决尽可能多的问题，但会导致一个缺点。 块的数量越多，FAT的大小就越大。 因此，我们需要为文件分配表分配更多空间。 由于文件分配表需要被缓存，因此不可能在缓存中具有尽可能多的空间。 在这里我们需要一种可以解决这些问题的新技术。 索引分配方案不是维护所有磁盘指针的文件分配表，而是将一个文件中所有磁盘指针存储在一个称为索引块的磁盘块中。 索引块不包含文件数据，但它保存指向分配给该特定文件的所有磁盘块的指针。目录条目将只包含索引块地址。 优点 支持直接访问。 坏数据块会导致只有该块的丢失。 缺点 坏索引块可能导致整个文件丢失。 文件的大小取决于数据块的数量，索引块可以容纳。 文件较小时，会造成索引块的浪费。 更多的指针开销 如果文件太大，一个索引块无法存下时，那么就需要多个索引块进行索引，如何组织多个索引块呢？ 单级索引分配 在索引分配中，文件大小取决于磁盘块的大小。要允许大文件，我们必须将几个索引块链接在一起。在链接索引分配中， 提供文件名称的小标题 前100个块地址的集合 指向另一个索引块的指针 对于较大的文件，索引块的最后一个条目是一个指向另一个索引块的指针。 这也被称为链接模式。 优点: 它消除了文件大小限制 缺点: 随机访问变得有点困难 多级索引分配 在多级指数分配中，有各种索引级别。 有外层索引块包含指向内层索引块的指针，内层索引块包含指向文件数据的指针。 外层索引用于查找内层索引。 内层索引用于查找所需的数据块。 优点: 随机访问变得更好，更高效。 缺点: 文件的访问时间会更长；文件最大大小有限。 值得注意的是，多级索引的索引层数，将决定能存储文件的容量上限，不能无休止的增加文件长度。 文件存储空间管理 由于文件存储设备是分成若干个大小相等的物理块，并以块为单位来交换信息的，因此，文件存储空间的管理实质上是一个空闲块的组织和管理问题，它包括空闲块组织，空闲块的分配和空闲块的回收等几个问题。 空闲表法 空闲表法就是为所有空闲空间建立一张表，表内容包括空闲区的第一个块号和该空闲区的块个数，注意，这个方式是连续分配的。如下图： 当请求分配磁盘空间时，系统依次扫描空闲表里的内容，直到找到一个合适的空闲区域为止。当用户撤销一个文件时，系统回收文件空间。这时，也需顺序扫描空闲表，寻找一个空闲表条目并将释放空间的第一个物理块号及它占用的块数填到这个条目中。 这种方法仅当有少量的空闲区时才有较好的效果。因为，如果存储空间中有着大量的小的空闲区，则空闲表变得很大，这样查询效率会很低。另外，这种分配技术适用于建立连续文件。 空闲链表法 我们也可以使用「链表」的方式来管理空闲空间，每一个空闲块里有一个指针指向下一个空闲块，这样也能很方便的找到空闲块并管理起来。如下图： 当创建文件需要一块或几块时，就从链头上依次取下一块或几块。反之，当回收空间时，把这些空闲块依次接到链头上。 这种技术只要在主存中保存一个指针，令它指向第一个空闲块。其特点是简单，但不能随机访问，工作效率低，因为每当在链上增加或移动空闲块时需要做很多 I/O 操作，同时数据块的指针消耗了一定的存储空间。 空闲表法和空闲链表法都不适合用于大型文件系统，因为这会使空闲表或空闲链表太大。 成组链表法 在 UNIX 系统中采用的是成组链接法，这是将上述两种方法相结合而形成的一种空闲盘块管理方法，它兼备了上述两种方法的优点而克服了两种方法均有的表太长的缺点。 其大致思想是：把顺序的n个空闲扇区地址保存在第一个空闲扇区内，其后一个空闲扇区则保存另一顺序空闲扇区的地址和空闲块数，如此继续直至所有空闲扇区均予以链接。系统只需要保存一个指向第一个空闲扇区的指针。 位示图法 位图是利用二进制的一位来表示磁盘中一个盘块的使用情况，磁盘上所有的盘块都有一个二进制位与之对应。 当值为 0 时，表示对应的盘块空闲，值为 1 时，表示对应的盘块已分配。它形式如下： 1111110011111110001110110111111100111 ... 盘块分配与回收过程如下： 在 Linux 文件系统就采用了位图的方式来管理空闲空间，不仅用于数据空闲块的管理，还用于 inode 空闲块的管理，因为 inode 也是存储在磁盘的，自然也要有对其管理。 ","date":"2023-05-21","objectID":"/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E4%B9%8B%E6%96%87%E4%BB%B6%E7%AE%A1%E7%90%86/:2:3","tags":[],"title":"操作系统之文件管理","uri":"/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E4%B9%8B%E6%96%87%E4%BB%B6%E7%AE%A1%E7%90%86/"},{"categories":["操作系统"],"content":"[toc] ","date":"2023-05-11","objectID":"/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E4%B9%8B%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86/:0:0","tags":[],"title":"操作系统之内存管理","uri":"/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E4%B9%8B%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86/"},{"categories":["操作系统"],"content":"内存管理概念 ","date":"2023-05-11","objectID":"/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E4%B9%8B%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86/:1:0","tags":[],"title":"操作系统之内存管理","uri":"/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E4%B9%8B%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86/"},{"categories":["操作系统"],"content":"内存管理的基本原理和要求 存储器结构 上图展示了一个典型的存储器层次结构。一般而言，从高层往底层走，存储设备变得更慢、更便宜和更大。在最高层是少量快速的CPU 寄存器，CPU 可以再一个时钟周期内访问它们。接下来是一个或者多个小型到中型的基于 SRAM 的高速缓存存储器，可以再几个 CPU 时钟周期内访问它们。然后是一个大的基于 DRAM 的主存，可以在几十或者几百个时钟周期内访问它们。接下来是慢速但是容量很大的本地磁盘。最后有些系统甚至包括了一层附加的远程服务器上的磁盘，要通过网络来访问它们，例如网络文件系统（Network File System,NFS）这样的分布式文件系统，允许程序访问存储在远程的网络服务器上的文件。 存储器层次结构的核心是，对于每个 k , 位于 k 层的更快更小的存储设备作为位于 k+1 层的更大更慢的存储设备的缓存。也就是说，层次结构中的每一层都缓存来自较低一层的数据对象。例如，本地磁盘作为通过网络从远程磁盘取出文件的缓存，以此类推知道 CPU 寄存器作为L1的缓存。 进程运行原理 用户程序-\u003e进程的过程 创建进程首先要将程序和数据装入内存。将用户源程序变为可在内存中执行的程序，通常需要以下几个步骤： 编译：由编译程序将用户源代码编译成若干个目标模块（也就是机器码）。 注意！！！ 编译并不能生成可执行的程序，仅仅只是将用户源代码编译成了若干个目标模块（机器码），需要链接后才能拿到可执行文件。 链接：由链接程序将编译后形成的一组目标模块以及所需库函数链接在一起，形成一个完整的装入模块。 生成了可执行文件，也就是装入模块. 显然实际开发中引入他人编写好的库文件可以省略某些功能的开发环节，提高项目的开发效率。但遗憾的是，“开源”的库文件很难找到，多数程序员并不会直接分享源代码，他们更愿意分享库文件的二进制版本——链接库。 所谓链接库（库函数文件），其实就是将开源的库文件进行编译、打包操作后得到的二进制文件。 库函数文件通常是库函数文件通常是二进制文件，其文件类型取决于库函数的编译方式和目标平台。在Unix/Linux系统中，常用的库函数文件扩展名为.so（共享对象）或.a（静态库），在Windows系统中则通常是.dll（动态链接库）或.lib（静态库）。这些库文件包含了编译好的函数和代码，可以被程序调用和链接，以实现特定的功能。库函数文件可以由开发者自己编写，也可以使用第三方库，如标准C库或开源库等。 虽然链接库是二进制文件，但无法独立运行，必须等待其它程序调用，才会被载入内存，但是已经编译成机器码了，只需等待调用即可运行了。 装入：由装入程序将装入模块装入内存运行。 编译 这部分并不是操作系统的工作，是由编程语言编译器完成。 链接 程序的链接有以下三种方式： 静态链接：在程序运行之前，先将各目标模块及它们所需的库函数链接成一个完整的可执行程序，以后不再拆开。 缺点 首先，可执行文件内部拷贝了所有目标文件和静态链接库的指令和数据，文件本身的体积会很大。 当系统中存在多个链接同一个静态库的可执行文件时，每个可执行文件中都存有一份静态库的指令和数据，就会造成内存空间的极大浪费。 优点 动态链接库形成的可执行文件，可以放到其他机子上运行，前提是目标机器上有与该可执行文件所需的动态链接库版本兼容的库文件并且目标操作系统与编译时使用的操作系统兼容。而静态链接库，只有编译时使用的操作系统兼容这一条限制。 装入时动态链接：将用户源程序编译后所得到的一组目标模块，在装入内存时，釆用边装入边链接的链接方式。 运行时动态链接：对某些目标模块的链接，是在程序执行中需要该目标模块时，才对它进行的链接。其优点是便于修改和更新，便于实现对目标模块的共享。 装入 绝对装入 在编译时，如果知道程序将驻留在内存的某个位置，编译程序将产生绝对地址的目标代码。绝对装入程序按照装入模块中的地址，将程序和数据装入内存。由于程序中的逻辑地址与实际内存地址完全相同，故不需对程序和数据的地址进行修改。 绝对装入方式只适用于单道程序环境。另外，程序中所使用的绝对地址,可在编译或汇编时给出，也可由程序员直接赋予。而通常情况下在程序中釆用的是符号地址，编译或汇编时再转换为绝对地址。 可重定位装入 在多道程序环境下，多个目标模块的起始地址通常都是从0开始，程序中的其他地址都是相对于起始地址的,此时应釆用可重定位装入方式。根据内存的当前情况，将装入模块装入到内存的适当位置。装入时对目标程序中指令和数据的修改过程称为重定位，地址变换通常是在装入时一次完成的，所以又称为静态重定位，如下图（a）所示。 静态重定位的特点是在一个作业装入内存时，必须分配其要求的全部内存空间，如果没有足够的内存，就不能装入该作业。此外，作业一旦进入内存后，在整个运行期间不能在内存中移动，也不能再申请内存空间。 动态运行时装入，也称为动态重定位 程序在内存中如果发生移动，就需要釆用动态的装入方式。装入程序在把装入模块装入内存后，并不立即把装入模块中的相对地址转换为绝对地址，而是把这种地址转换推迟到程序真正要执行时才进行。因此，装入内存后的所有地址均为相对地址。这种方式需要一个重定位寄存器的支持，如上图（b）所示。 动态重定位的特点是可以将程序分配到不连续的存储区中；在程序运行之前可以只装入它的部分代码即可投入运行，然后在程序运行期间，根据需要动态申请分配内存；便于程序段的共享，可以向用户提供一个比存储空间大得多的地址空间。 逻辑地址空间与物理地址空间 编译后，每个目标模块都是从0号单元开始编址，称为该目标模块的相对地址（或逻辑地址)。 当链接程序将各个模块链接成一个完整的可执行目标程序时，链接程序顺序依次按各个模块的相对地址构成统一的从0号单元开始编址的逻辑地址空间。用户程序和程序员只需知道逻辑地址，而内存管理的具体机制则是完全透明的，它们只有系统编程人员才会涉及。不同进程可以有相同的逻辑地址，因为这些相同的逻辑地址可以映射到主存的不同位置。 物理地址空间是指内存中物理单元的集合，它是地址转换的最终地址，进程在运行时执行指令和访问数据最后都要通过物理地址从主存中存取。当装入程序将可执行代码装入内存时，必须通过地址转换将逻辑地址转换成物理地址，这个过程称为地址重定位。 内存保护 内存分配前，需要保护操作系统不受用户进程的影响，同时保护用户进程不受其他用户进程的影响。通过釆用重定位寄存器和界地址寄存器来实现这种保护。重定位寄存器含最小的物理地址值，界地址寄存器含逻辑地址值。每个逻辑地址值必须小于界地址寄存器；内存管理机构动态地将逻辑地址与界地址寄存器进行比较，如果未发生地址越界，则加上重定位寄存器的值后映射成物理地址，再送交内存单元，如图所示。 当CPU调度程序选择进程执行时，派遣程序会初始化重定位寄存器和界地址寄存器。每一个逻辑地址都需要与这两个寄存器进行核对，以保证操作系统和其他用户程序及数据不被该进程的运行所影响。 ","date":"2023-05-11","objectID":"/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E4%B9%8B%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86/:1:1","tags":[],"title":"操作系统之内存管理","uri":"/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E4%B9%8B%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86/"},{"categories":["操作系统"],"content":"内存扩充 覆盖与交换技术是在多道程序环境下用来扩充内存的两种方法。 内存覆盖 早期的计算机系统中，主存容量很小，虽然主存中仅存放一道用户程序，但是存储空间放不下用户进程的现象也经常发生，这一矛盾可以用覆盖技术来解决。 覆盖的基本思想是：由于程序运行时并非任何时候都要访问程序及数据的各个部分（尤其是大程序），因此可以把用户空间分成一个固定区和若干个覆盖区。将经常活跃的部分放在固定区，其余部分按调用关系分段。首先将那些即将要访问的段放入覆盖区，其他段放在外存中，在需要调用前，系统再将其调入覆盖区，替换覆盖区中原有的段。 外存就是磁盘缓存，进程被suspend之后存储的地方 覆盖技术的特点是打破了必须将一个进程的全部信息装入主存后才能运行的限制，但当同时运行程序的代码量大于主存时仍不能运行。 内存交换 交换（对换）的基本思想是：把处于等待状态（或在CPU调度原则下被剥夺运行权利）的程序从内存移到辅（外）存，把内存空间腾出来，这一过程又叫换出；把准备好竞争CPU运行的程序从辅（外）存移到内存，这一过程又称为换入。中级调度就是釆用交换技术。 例如，有一个CPU釆用时间片轮转调度算法的多道程序环境。时间片到，内存管理器将刚刚执行过的进程换出，将另一进程换入到刚刚释放的内存空间中。同时，CPU调度器可以将时间片分配给其他已在内存中的进程。每个进程用完时间片都与另一进程交换。理想情况下，内存管理器的交换过程速度足够快，总有进程在内存中可以执行。 有关交换需要注意以下几个问题： 交换需要备份存储，通常是快速磁盘。它必须足够大，并且提供对这些内存映像的直接访问。 为了有效使用CPU，需要每个进程的执行时间比交换时间长，而影响交换时间的主要是转移时间。转移时间与所交换的内存空间成正比。 如果换出进程，必须确保该进程是完全处于空闲状态。 交换空间通常作为磁盘的一整块，且独立于文件系统，因此使用就可能很快。(外存，磁盘的缓存) 交换通常在有许多进程运行且内存空间吃紧时开始启动，而系统负荷降低就暂停。 普通的交换使用不多，但交换策略的某些变种在许多系统中（如UNIX系统）仍发挥作用。 交换技术主要是在不同进程（或作业）之间进行，而覆盖则用于同一个程序或进程中。由于覆盖技术要求给出程序段之间的覆盖结构，使得其对用户和程序员不透明，所以对于主存无法存放用户程序的矛盾，现代操作系统是通过虚拟内存技术来解决的，覆盖技术则已成为历史；而交换技术在现代操作系统中仍具有较强的生命力。 ","date":"2023-05-11","objectID":"/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E4%B9%8B%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86/:1:2","tags":[],"title":"操作系统之内存管理","uri":"/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E4%B9%8B%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86/"},{"categories":["操作系统"],"content":"连续分配管理方式 连续分配方式，是指为一个用户程序分配一个连续的内存空间。它主要包括单一连续分配、固定分区分配和动态分区分配。 单一连续分配 内存在此方式下分为系统区和用户区，系统区仅提供给操作系统使用，通常在低地址部分；用户区是为用户提供的、除系统区之外的内存空间。这种方式无需进行内存保护。 这种方式的优点是实现简单、无外部碎片，可以釆用覆盖技术，不需要额外的技术支持 内存被分为系统区和用户区，没有任何外部的内存空间。 缺点是只能用于单用户、单任务的操作系统中，有内部碎片，存储器的利用率极低 用户区只能存放一个用户进程，但是用户区的剩余内存肯定大于等于0，也就是内存碎片 固定分区分配 固定分区分配是最简单的一种多道程序存储管理方式，它将用户内存空间划分为若干个固定大小的区域，每个分区只装入一道作业。当有空闲分区时，便可以再从外存的后备作业队列中,选择适当大小的作业装入该分区，如此循环。 固定分区分配在划分分区时，有两种不同的方法，如上图所示 分区大小相等：用于利用一台计算机去控制多个相同对象的场合，缺乏灵活性。 分区大小不等：划分为含有多个较小的分区、适量的中等分区及少量的大分区。 为便于内存分配，通常将分区按大小排队，并为之建立一张分区说明表，其中各表项包括每个分区的起始地址、大小及状态（是否已分配），如图(a)所示。当有用户程序要装入时，便检索该表，以找到合适的分区给予分配并将其状态置为”已分配”；未找到合适分区则拒绝为该用户程序分配内存。存储空间的分配情况如图(b)所示。 这种分区方式存在两个问题： 一是程序可能太大而放不进任何一个分区中，这时用户不得不使用覆盖技术来使用内存空间； 二是主存利用率低，当程序小于固定分区大小时，也占用了一个完整的内存分区空间，这样分区内部有空间浪费，这种现象称为内部碎片。 固定分区是可用于多道程序设计最简单的存储分配，无外部碎片，但不能实现多进程共享一个主存区，所以存储空间利用率低。固定分区分配很少用于现在通用的操作系统中，但在某些用于控制多个相同对象的控制系统中仍发挥着一定的作用。 动态分区分配 动态分区分配又称为可变分区分配，是一种动态划分内存的分区方法。这种分区方法不预先将内存划分，而是在进程装入内存时，根据进程的大小动态地建立分区，并使分区的大小正好适合进程的需要。因此系统中分区的大小和数目是可变的。 如上图所示，系统有64MB内存空间，其中低8MB固定分配给操作系统，其余为用户可用内存。开始时装入前三个进程，在它们分别分配到所需空间后，内存只剩下4MB，进程4无法装入。在某个时刻，内存中没有一个就绪进程，CPU出现空闲，操作系统就换出进程2，换入进程4。由于进程4比进程2小，这样在主存中就产生了一个6MB的内存块。之后CPU又出现空闲，而主存无法容纳进程2,操作系统就换出进程1，换入进程2。 动态分区在开始分配时是很好的，但是之后会导致内存中出现许多小的内存块。随着时间的推移，内存中会产生越来越多的碎片（图中最后的4MB和中间的6MB，且随着进程的换入/换出，很可能会出现更多更小的内存块)，内存的利用率随之下降。 这些小的内存块称为外部碎片(指在所有分区外的存储空间会变成越来越多的碎片)，这与固定分区中的内部碎片正好相对。克服外部碎片可以通过紧凑（Compaction)技术来解决，就是操作系统不时地对进程进行移动和整理。但是这需要动态重定位寄存器的支持，且相对费时。紧凑的过程实际上类似于Windows系统中的磁盘整理程序，只不过后者是对外存空间的紧凑。 在进程装入或换入主存时，如果内存中有多个足够大的空闲块，操作系统必须确定分配哪个内存块给进程使用，这就是动态分区的分配策略，考虑以下几种算法： 首次适应(First Fit)算法：空闲分区以地址递增的次序链接。分配内存时顺序查找，找到大小能满足要求的第一个空闲分区。 最佳适应(Best Fit)算法：空闲分区按容量递增形成分区链，找到第一个能满足要求的空闲分区。 最坏适应(Worst Fit)算法：又称最大适应(Largest Fit)算法，空闲分区以容量递减的次序链接。找到第一个能满足要求的空闲分区，也就是挑选出最大的分区。 邻近适应(Next Fit)算法：又称循环首次适应算法，由首次适应算法演变而成。不同之处是分配内存时从上次查找结束的位置开始继续查找。 在这几种方法中，首次适应算法不仅是最简单的，而且通常也是最好和最快的。在UNIX 系统的最初版本中，就是使用首次适应算法为进程分配内存空间，其中使用数组的数据结构 (而非链表）来实现。不过，首次适应算法会使得内存的低地址部分出现很多小的空闲分区，而每次分配查找时，都要经过这些分区，因此也增加了查找的开销。 邻近适应算法试图解决这个问题，但实际上，它常常会导致在内存的末尾分配空间（因为在一遍扫描中，内存前面部分使用后再释放时，不会参与分配)，分裂成小碎片。它通常比首次适应算法的结果要差。 最佳适应算法虽然称为“最佳”，但是性能通常很差，因为每次最佳的分配会留下很小的难以利用的内存块，它会产生最多的外部碎片。 最坏适应算法与最佳适应算法相反，选择最大的可用块，这看起来最不容易产生碎片，但是却把最大的连续内存划分开，会很快导致没有可用的大的内存块，因此性能也非常差。 Kunth和Shore分别就前三种方法对内存空间的利用情况做了模拟实验，结果表明： 首次适应算法可能比最佳适应法效果好，而它们两者一定比最大适应法效果好。另外注意,在算法实现时,分配操作中，最佳适应法和最大适应法需要对可用块进行排序或遍历查找，而首次适应法和邻近适应法只需要简单查找；回收操作中，当回收的块与原来的空闲块相邻时（有三种相邻的情况，比较复杂)，需要将这些块合并。在算法实现时，使用数组或链表进行管理。除了内存的利用率，这里的算法开销也是操作系统设计需要考虑的一个因素。 以上三种内存分区管理方法有一共同特点，即用户进程（或作业）在主存中都是连续存放的。这里对它们进行比较和总结，见上表图。 ","date":"2023-05-11","objectID":"/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E4%B9%8B%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86/:1:3","tags":[],"title":"操作系统之内存管理","uri":"/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E4%B9%8B%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86/"},{"categories":["操作系统"],"content":"非连续分配与管理方式 非连续分配允许一个程序分散地装入到不相邻的内存分区中，根据分区的大小是否固定分为分页存储管理方式和分段存储管理方式。 分页存储管理方式中，又根据运行作业时是否要把作业的所有页面都装入内存才能运行分为基本分页存储管理方式和请求分页存储管理方式。下面介绍基本分页存储管理方式。 基本分页存储管理 固定分区会产生内部碎片，动态分区会产生外部碎片，这两种技术对内存的利用率都比较低。我们希望内存的使用能尽量避免碎片的产生。 这就引入了分页的思想：把主存空间划分为大小相等且固定的块，块相对较小，作为主存的基本单位。每个进程也以块为单位进行划分，进程在执行时，以块为单位逐个申请主存中的块空间。 分页的方法从形式上看，像分区相等的固定分区技术，分页管理不会产生外部碎片。但它又有本质的不同点：块的大小相对分区要小很多，而且进程也按照块进行划分，进程运行时按块申请主存可用空间并执行。这样，进程只会在为最后一个不完整的块申请一个主存块空间时，才产生主存碎片，所以尽管会产生内部碎片，但是这种碎片相对于进程来说也是很小的，每个进程平均只产生半个块大小的内部碎片（也称页内碎片）。 分页存储几个概念 页和页框 进程中的块称为页(Page)，内存中的块称为页框（Page Frame，或页帧）。外存也以同样的单位进行划分，直接称为块(Block)。进程在执行时需要申请主存空间，就是要为每个页分配主存中的可用页框，这就产生了页和页框的一一对应。 地址结构 分页存储管理的逻辑地址结构如图所示 地址结构包含两部分：前一部分为页号P，后一部分为页内偏移量W。地址长度为32 位，其中011位为页内地址，即每页大小为4KB；12-31位为页号，地址空间最多允许有2^20页。 页表 为了便于在内存中找到进程的每个页面所对应的物理块，系统为每个进程建立一张页表，记录页面在内存中对应的物理块号，页表一般存放在内存中。 在配置了页表后，进程执行时，通过查找该表，即可找到每页在内存中的物理块号。 可见，页表的作用是实现从页号到物理块号的地址映射，如图所示 基本地址变换机构 在系统中通常设置一个页表寄存器(PTR)，存放页表在内存的始址F和页表长度M。进程未执行时，页表的始址和长度存放在进程控制块中，当进程执行时，才将页表始址和长度存入页表寄存器。设页面大小为L，逻辑地址A到物理地址E的变换过程如下： 计算页号P(P=A/L)和页内偏移量W (W=A%L)。 比较页号P和页表长度M，若P \u003e= M，则产生越界中断，否则继续执行。 页表中页号P对应的页表项地址 = 页表起始地址F + 页号P * 页表项长度，取出该页表项内容b，即为物理块号。 计算E=b*L+W，用得到的物理地址E去访问内存。 以上整个地址变换过程均是由硬件自动完成的。 例如，若页面大小L为1K字节，页号2对应的物理块为b=8，计算逻辑地址A=2500 的物理地址E的过程如下：P=2500/1K=2，W=2500%1K=452，查找得到页号2对应的物理块的块号为 8，E=8*1024+452=8644。 下面讨论分页管理方式存在的两个主要问题： 每次访存操作都需要进行逻辑地址到物理地址的转换，地址转换过程必须足够快，否则访存速度会降低； 每个进程引入了页表，用于存储映射机制，页表不能太大，否则内存利用率会降低。 为解决以上问题，引出了后面的“具有快表的地址变换机构”和“两级页表” 具有快表的地址变换机构 由上面介绍的地址变换过程可知，若页表全部放在内存中，则存取一个数据或一条指令至少要访问两次内存：一次是访问页表，确定所存取的数据或指令的物理地址，第二次才根据该地址存取数据或指令。显然，这种方法比通常执行指令的速度慢了一半。 为此，在地址变换机构中增设了一个具有并行查找能力的高速缓冲存储器——快表，又称联想寄存器(TLB)，用来存放当前访问的若干页表项，以加速地址变换的过程。与此对应，主存中的页表也常称为慢表，配有快表的地址变换机构如图所示。 在具有快表的分页机制中，地址的变换过程： CPU给出逻辑地址后，由硬件进行地址转换并将页号送入高速缓存寄存器，并将此页号与快表中的所有页号进行比较。 如果找到匹配的页号，说明所要访问的页表项在快表中，则直接从中取出该页对应的页框号，与页内偏移量拼接形成物理地址。这样，存取数据仅一次访存便可实现。 如果没有找到，则需要访问主存中的页表，在读出页表项后，应同时将其存入快表，以便后面可能的再次访问。但若快表已满，则必须按照一定的算法对旧的页表项进行替换。 注意：有些处理机设计为快表和慢表同时查找，如果在快表中查找成功则终止慢表的查找。 一般快表的命中率可以达到90%以上，这样，分页带来的速度损失就降低到10%以下。快表的有效性是基于著名的局部性原理，这在后面的虚拟内存中将会具体讨论。 两级页表 问题一: 根据页号查询页表的方法:K 号页对应的页表项存放位置 = 页表始址 + K * 4 ，页表必须连续存放，因此当页表很大时，需要占用很多个连续的页框； 问题二: 没有必要让整个页表常驻内存，因为进程在一段时间内可能只需要访问某几个特定的页面。 解决办法：把页表再分页并离散存储，然后再建立一张页表记录页表各个部分的存放位置，称为页目录表，或称外层页表，或称顶层页表。 多级页表 基本分段存储管理方式 分页管理方式是从计算机的角度考虑设计的，以提高内存的利用率，提升计算机的性能, 且分页通过硬件机制实现，对用户完全透明；而分段管理方式的提出则是考虑了用户和程序员，以满足方便编程、信息保护和共享、动态增长及动态链接等多方面的需要。 分段 段式管理方式按照用户进程中的自然段划分逻辑空间。例如，用户进程由主程序、两个子程序、栈和一段数据组成，于是可以把这个用户进程划分为5个段，每段从0 开始编址，并分配一段连续的地址空间（段内要求连续，段间不要求连续，因此整个作业的地址空间是二维的）。其逻辑地址由段号S与段内偏移量W两部分组成。 在下图中，段号为16位，段内偏移量为16位，则一个作业最多可有2^16=65536个段，最大段长为64KB。 在页式系统中，逻辑地址的页号和页内偏移量对用户是透明的，用户不可以更改，是由操作系统计算出来的；但在段式系统中，段号和段内偏移量必须由用户显示提供，在髙级程序设计语言中，这个工作由编译程序完成。 段表 每个进程都有一张逻辑空间与内存空间映射的段表，其中每一个段表项对应进程的一个段，段表项记录该段在内存中的起始地址和段的长度。段表的内容如图所示。 在配置了段表后，执行中的进程可通过查找段表，找到每个段所对应的内存区。可见，段表用于实现从逻辑段到物理内存区的映射，如图所示。 地址变换机构 分段系统的地址变换过程如图所示。为了实现进程从逻辑地址到物理地址的变换功能，在系统中设置了段表寄存器，用于存放段表始址F和段表长度M。其从逻辑地址A到物理地址E之间的地址变换过程如下： 从逻辑地址A中取出前几位为段号S，后几位为段内偏移量W。 比较段号S和段表长度M，若S多M，则产生越界中断，否则继续执行。 段表中段号S对应的段表项地址 = 段表起始地址F + 段号S * 段表项长度，取出该段表项的前几位得到段长C。若段内偏移量\u003e=C，则产生越界中断，否则继续执行。 取出段表项中该段的起始地址b，计算 E = b + W，用得到的物理地址E去访问内存。 段的共享与保护 在分段系统中，段的共享是通过两个作业的段表中相应表项指向被共享的段的同一个物理副本来实现的。当一个作业正从共享段中读取数据时，必须防止另一个作业修改此共享段中的数据。不能修改的代码称为纯代码或可重入代码（它不属于临界资源)，这样的代码和不能修改的数据是可以共享的，而可修改的代码和数据则不能共享。 与分页管理类似，分段管理的保护方法主要有两种：一种是存取控制保护，另一种是地址越界保护。地址越界保护是利用段表寄存器中的段表长度与逻辑地址中的段号比较，若段号大于段表长度则产生越界中断；再利用段表项中的段长和逻辑地址中的段内位移进行比较，若段内位移大于段长，也会产生越界中断。 段页管理方式 页式存储管理能有效地提高内存利用率，而分段存储管理能反映程序的逻辑结构并有利于段的共享。如果将这两种存储管理方法结合起来，就形成了段页式存储管理方式。 在段页式系统中，作业的地址空间首先被分成若干个逻辑段，每段都有自己的段号，然后再将每一段分成若干个大小固定的页。对内存空间的管理仍然和分页存储管理一样，将其分成若干个和页面大小相同的存储块，对内存的分配以存储块为单位，如图所示。 在段页式系统中，作业的逻辑地址分为三部分：段号、页号和页内偏移量，如图所示。 为了实现地址变换，系统为每个进程建立一张段表，而每个分段有一张页表。段表表项中至少包括段号、页表长度和页表起始地址，页表表项中至少包括页号和块号。此外，系统中还应有一个段表寄存器，指出作业的段表起始地址和段表长度。 注意：在一个进程中，段表只有一个，而页表可能有多个。 在进行地址变换时，首先通过段表查到页表起始地址，然后通过页表找到页帧号，最后形成物理地址。如图所示，进行一次访问实际需要三次访问主存，这里同样可以使用快表以加快查找速度，其关键字由段号、页号组成，值是对应的页帧号和保护码。 分页 VS 分段 页是信息的物理单位。分页的主要目的是为了实现离散分配，提高内存利用率。分页仅仅是系统管理上的需要，完全是系统行为，对用户是不可见的。 段是信息的逻辑单位。分段的主要目的是更好地满足用户需求。一个段通常包含着一组属于一个逻辑模块的信息。 分段对用户是可见的，用户编程时需要显式地给出段名。 页的","date":"2023-05-11","objectID":"/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E4%B9%8B%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86/:1:4","tags":[],"title":"操作系统之内存管理","uri":"/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E4%B9%8B%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86/"},{"categories":["操作系统"],"content":"内存回收* 操作系统层面的内存回收主要是通过虚拟内存管理来实现的 ","date":"2023-05-11","objectID":"/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E4%B9%8B%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86/:1:5","tags":[],"title":"操作系统之内存管理","uri":"/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E4%B9%8B%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86/"},{"categories":["操作系统"],"content":"虚拟内存管理 ","date":"2023-05-11","objectID":"/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E4%B9%8B%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86/:2:0","tags":[],"title":"操作系统之内存管理","uri":"/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E4%B9%8B%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86/"},{"categories":["操作系统"],"content":"虚拟内存基本概念 虚拟内存：具有请求调入和置换功能，从逻辑上对内存容量加以扩充的一种存储系统，也就是内存+外存 局部性原理： 时间局部性原理：如果执行了程序中的某条指令，那么不久后这条指令很有可能再次执行;如果某个数据被访问过，不久之后该数据很可能再次被访问。(因为程序中存在大量的循环) 空间局部性原理：一旦程序访问了某个存储单元，在不久之后，其附近的存储单元也很有可能被访问。(因为很多数据在内存中都是连续存放的，并且程序的指令也是顺序地在内存中存放的) 虚拟内存的特征： 多次性：前面内存基本管理方式就是一次性将整个进程装入内存运行，但是现在很多应用都远远大于内存大小，显而易见，内存并不能完整装下现在的应用了。一次装不下，我们可以分多次装入内存，每次只运行一部分需要的程序和加载需要的数据，不需要的数据和程序可以换出到硬盘缓存中，从而腾出空间存放要调入内存的信息 对换性：是指无需在作业运行时一直常驻内存，而是允许在作业的运行过程中，进行换进和换出，例如，当进程被suspend后就剋以换出，ready后就换进。 虚拟性：是指从逻辑上扩充内存的容量，使用户所看到的内存容量，远大于实际的内存容量。（不断换入换出，内存就可以容纳比实际内存容量更大的容量） ","date":"2023-05-11","objectID":"/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E4%B9%8B%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86/:2:1","tags":[],"title":"操作系统之内存管理","uri":"/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E4%B9%8B%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86/"},{"categories":["操作系统"],"content":"虚拟内存实现 请求分页存储管理 请求分页系统建立在基本分页系统基础之上，为了支持虚拟存储器功能而增加了请求调页功能和页面置换功能。 请求分页是目前最常用的一种实现虚拟存储器的方法。 在请求分页系统中，只要求将当前需要的一部分页面装入内存，便可以启动作业运行。 在作业执行过程中，当所要访问的页面不在内存时，再通过调页功能将其从外存（硬盘，但不是硬盘缓存）调入，同时还可以通过置换功能将暂时不用的页面换出到外存上，以便腾出内存空间。 为了实现请求分页，系统必须提供一定的硬件支持。除了需要一定容量的内存及外存的计算机系统，还需要有页表机制、缺页中断机构和地址变换机构。 页表机制 请求分页系统的页表机制不同于基本分页系统，请求分页系统在一个作业运行之前不要求全部一次性调入内存，因此在作业的运行过程中，必然会出现要访问的页面不在内存的情况，如何发现和处理这种情况是请求分页系统必须解决的两个基本问题。 为此，在请求页表项中增加了四个字段，如图所示。 相较于基本分页存储管理，页表项新增四列： 状态位P：用于指示该页是否已调入内存，供程序访问时参考。 访问字段A：用于记录本页在一段时间内被访问的次数，或记录本页最近己有多长时间未被访问，供置换算法换出页面时参考。 修改位M：标识该页在调入内存后是否被修改过。 外存地址：用于指出该页在外存上的地址，通常是物理块号，供调入该页时参考。 缺页中断机构 在请求分页系统中，每当所要访问的页面不在内存时，便产生一个缺页中断，请求操作系统将所缺的页调入内存。 此时应将缺页的进程阻塞（调页完成唤醒)，如果内存中有空闲块，则分配一个块，将要调入的页装入该块，并修改页表中相应页表项，若此时内存中没有空闲块，则要淘汰某页（若被淘汰页在内存期间被修改过，则要将其写回外存)。 缺页中断作为中断同样要经历，诸如保存CPU环境、分析中断原因、转入缺页中断处理程序、恢复CPU环境等几个步骤。 但与一般的中断相比，它有以下两个明显的区别： 在指令执行期间产生和处理中断信号，而非一条指令执行完后，属于内部中断。 如果该指令访问的地址仍然没有在内存中，就会再次触发缺页中断，重复上述操作，直到该指令能够正常执行。 因此，一条指令在执行期间可能会产生多次缺页中断，直到它所需要的所有页面都在内存中。 地址变换机构 请求分页系统中的地址变换机构，是在分页系统地址变换机构的基础上，为实现虚拟内存，又增加了某些功能而形成的。 如图所示，在进行地址变换时，先检索快表： 若找到要访问的页，便修改页表项中的访问位（写指令则还须重置修改位)，然后利用页表项中给出的物理块号和页内地址形成物理地址。 若未找到该页的页表项，应到内存中去查找页表，再对比页表项中的状态位P，看该页是否已调入内存，未调入则产生缺页中断，请求从外存把该页调入内存。 可能需要页面置换。存在这种情况：当内存已经满了，但是此时CPU发生缺页中断，需要请求外存的缺页。那么只有将已经满了的内存，换出页，再调入所需的页。 请求分段存储管理 略 请求段页式存储管理 略 ","date":"2023-05-11","objectID":"/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E4%B9%8B%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86/:2:2","tags":[],"title":"操作系统之内存管理","uri":"/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E4%B9%8B%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86/"},{"categories":["操作系统"],"content":"页面置换算法 页面的换入、换出需要磁盘I/O，会有较大的开销，因此好的页面置换算法应该追求更少的缺页率。 最佳置换算法(OPT) 先进先出置换算法(FIFO) 最近最久未使用置换算法(LRU) 时钟置换算法(CLOCK) 当采用简单Clock算法时，只需为每页设置一位访问位，再将内存中的所有页面都通过链接指针链接成一个循环队列。 当某页被访问时，其访问位被置为1。置换算法在选择一页淘汰时，只需检查页的访问位，如果是0，就选择将该页换出；若为1，则重新将它置0，暂不换出，而给该页第二次驻留内存的机会，再按照FIFO算法检查下一个页面。当检查到队列中的最后一个页面时，若其访问位仍为1，则再返回到队首去检查第一个页面。由于该算法是循环地检查各页面的使用情况，故称为Clock算法。 因该算法只有一位访问位，只能用它表示该页是否已经使用过，而置换时是将未使用过的页面换出去，又称为最近未用算法NRU(Not recently used)。 改进型的时钟置换算法 在将一个页面换出时，如果该页已被修改过，需将该页重新写回到磁盘上；但如果该页未被修改过，则不必将它拷回磁盘。 在改进型Clock算法中，除需考虑页面的使用情况外，需再增加一个因素，即置换代价。 这样，在选择页面换出时，既要是未使用过的页面，又要是未被修改过的页面。把同时满足这两个条件的页面作为首选淘汰的页面。 由访问位A和修改位M可以组合成下面四种类型的页面： 1类（A=0,M=0）：表示该页最近既未被访问，又未被修改，是最佳淘汰页，可以换出到外存上。 2类（A=0,M=1）：表示该页最近未被访问，但已被修改，并不是很好的淘汰页，可以考虑换出到外存（硬盘）。 3类（A=1,M=0）：表示该页最近已被访问，但未被修改，该页有可能再被访问，需要保留在内存里。 4类（A=1,M=1）：表示该页最近已被访问且被修改，该页很可能再被访问，需要保留在内存里。 在内存中的每个页必定是这四类页面之一，在进行页面置换时，可采用与简单Clock算法相类似的算法，其差别在于该算法需同时检查访问位与修改位，以确定该页是四类页面中的哪一种。其执行过程可分为以下三步： 第一步：从指针所指示的当前位置开始，扫描循环队列，寻找A=0且M=0的第一类页面，将所遇到的第一个页面作为所选中的淘汰页。在第一次扫描期间不改变访问位A。 第二步：如果第一步失败，即扫描一次后未遇到第一类页面，则开始第二轮扫描，寻找A=0且M=1的第二类页面，将所遇到的第一个这类 页面作为淘汰页。在第二轮扫描期间，将所有扫描过的页面的访问位都置0. 第三步：如果第二步也失败，即也未找到第二类页面，则将指针返回到开始的位置。然后重复第一步，如果仍失败，必要时再重复第二步，此时就一定能找到被淘汰的页。 该算法与简单Clock算法相比，可减少磁盘的I/O操作次数。但为了找到一个可置换的页，可能需经过几轮扫描。换言之，实现该算法本身的开销将有所增加。 上图的NRU算法有错误。 Belady异常：在某些情况下，分配的物理页越多，产生缺页中断的次数反而越多。 Belady现象演示 假定给某进程分为5页（page），但是它在内存中只分配到3个页帧（page frame），现在有一访问串：1,2,3,4,1,2,5,1,2,3,4,5，表示依次访问第1页、第2页…… 刚开始时，进程页还在虚存（磁盘）中，尚未缓存到内存中，所以第一次要访问第1页时发生一次缺页故障，此时调入第1页到内存中，占一个页帧 此时还剩下两个页帧未分配，由于接下来依次访问第2、3页，同理会触发两次缺页故障，在此之后，第1、2、3页都已经缓存在内存中 接下来要访问第四页，由于在此之前第1,2,3页已经缓存在内存中，该进程所分配到的3个页帧已满，为此必须替换掉一页，才能把第四页加载进来，此时又发生一次缺页故障。由于采用FIFO替换算法，因为第一页是最先进来，所以它会被替换出去 接下来又要访问第一页，由于当前缓存页时第4、2、3页，从而根据FIFO，要将第2页替换为第1页，这就又发生一次缺页中断，调入第1页后，此时存在于内存中的是第4、1、3页。同理，接下来要访问第2页，发生一次缺页中断，将第3页替换为第2页，此时存在于内存中的是第4、1、2页。 在接下来的访问中，如果第K页已经存在内存中，则直接使用，所以此时不会发生缺页故障，重复按照上述过程，我们可以得到如下示例图表 红色标识出的是发生缺页故障后调入的页，可以看见共发生9次缺页异常，而从访问串可知访问12次，所以缺页率为9/12=0.75。 现在，该进程在上述3页帧的基础上多分配一页帧，也就是变成四页帧，则仿照上述分析过程，可画出如下图表 红色标识出的是发生缺页故障后调入的页，蓝色标识的是之前调入的页面，可以看见共发生10次缺页异常，而从访问串可知访问12次，所以缺页率为10/12=0.833。 FIFO替换算法产生该现象的原因是它没有考虑到程序执行的动态特征。 ","date":"2023-05-11","objectID":"/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E4%B9%8B%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86/:2:3","tags":[],"title":"操作系统之内存管理","uri":"/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E4%B9%8B%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86/"},{"categories":["操作系统"],"content":"页面分配策略 先简述几个名词： 驻留集：一个进程的驻留集指当前在主存中的这个进程的页的集合。 由于采用虚拟存储技术，驻留集的大小（实际内存大小）一般小于进程的大小（实际占有内存再加上进程存储在外存上的资源）。若驻留集太小，会导致频繁缺页；太大会导致多道程序并发度降低，资源利用率下降。 工作集：一个进程的工作集指这个进程最近被使用过的页的集合。 抖动：又称颠簸，指刚被调出去的页又马上被调回，调回不久后又被调出。 置换策略：固定分配局部置换、可变分配全部置换、可变分配局部置换。 页面分配与置换策略 固定分配：操作系统为每个进程分配一组固定数目大小的物理块。在程序运行过程中，不允许改变。即驻留集大小固定不变。 可变分配：先为每个进程分配一定大小的物理块，在程序运行过程中，可以动态改变物理块的大小。即驻留集大小可变。 局部置换：进程发生缺页时，只能选择当前进程中的物理块进行置换。 全局置换：可以将操作系统进程中保留的空闲物理块分配给缺页进程，还可以将别的进程持有的物理块置换到外存，再将这个物理块分配给缺页的进程。 固定分配局部置换 系统为每个进程分配一定数量的内存块（物理块），在整个运行期都不改变。若进程在运行过程中发生了缺页，则只能在本进程的内存页面中选出一个进行调出，再调回需要的页面。 缺点：不好确定一个进程到底应该分配多大的实际内存才合理。 可变分配全局置换 系统为每个进程分配一定数量的内存块。操作系统还会保持一个空闲物理块的队列。若某个进程发生缺页，可以从空闲物理块中取出一块分配给该进程。如果空闲物理块没有了，那么会选择一个未锁定（不那么重要的，可能是其它进程的）的页面换出到外存，再将物理块分配给缺页的进程。 缺点：在空闲物理块没有的情况下，如果将其它进程的页面调出外存，那么这个进程就会拥有较小的驻留集，如此会导致该进程的缺页率上升。 可变分配局部置换 刚开始为每个进程分配一定数量的物理块。当进程发生缺页时，只允许从当前进程的物理块中选出一个换出内存。如果当前进程在运行的时候频繁缺页，系统会为该进程动态增加一些物理块，直到该进程缺页率趋于适中程度；如果说一个进程在运行过程中缺页率很低或者不缺页，则可以适当减少该进程分配的物理块。通过这些操作可以保持多道程序的并发度较高。 与 可变分配全局置换 的区别： 可变分配全局置换：只要发生缺页，就会分配新的物理块 可变分配局部置换：根据缺页率动态增加或者减少物理块的数量。 ","date":"2023-05-11","objectID":"/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E4%B9%8B%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86/:2:4","tags":[],"title":"操作系统之内存管理","uri":"/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E4%B9%8B%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86/"},{"categories":["操作系统"],"content":"[TOC] ","date":"2023-03-17","objectID":"/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E4%B9%8B%E8%BF%9B%E7%A8%8B%E7%AE%A1%E7%90%86/:0:0","tags":[],"title":"操作系统之进程管理","uri":"/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E4%B9%8B%E8%BF%9B%E7%A8%8B%E7%AE%A1%E7%90%86/"},{"categories":["操作系统"],"content":"什么是进程？ ","date":"2023-03-17","objectID":"/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E4%B9%8B%E8%BF%9B%E7%A8%8B%E7%AE%A1%E7%90%86/:1:0","tags":[],"title":"操作系统之进程管理","uri":"/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E4%B9%8B%E8%BF%9B%E7%A8%8B%E7%AE%A1%E7%90%86/"},{"categories":["操作系统"],"content":"概念 进程（process）：是一个具有一定独立功能的程序，关于某个数据集合的一次运行活动，是操作系统资源分配的基本单位 要点 进程是程序的正在被执行的实例 躺在磁盘里的程序不是进程，而是程序被加载到内存正在运行的过程才是进程 进程是程序在一个数据集合上运行的过程 进程是操作系统进行资源分配的基本单位 ","date":"2023-03-17","objectID":"/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E4%B9%8B%E8%BF%9B%E7%A8%8B%E7%AE%A1%E7%90%86/:1:1","tags":[],"title":"操作系统之进程管理","uri":"/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E4%B9%8B%E8%BF%9B%E7%A8%8B%E7%AE%A1%E7%90%86/"},{"categories":["操作系统"],"content":"结构和特征 进程的结构 控制块（Process Control Block, PCB）：进程的唯一标识，用以操作系统来辨别不同的进程，包括如下几部分： OS根据PCB来对进程进行控制和管理，进程的创建和撤销都是根据PCB进行的 存储：程序计数器、进程状态、CPU暂存器、存储器管理、输入输出状态等信息 PCB在新建进程时创建，并常驻内存，是进程实体的一部分，也是进程的唯一标识 数据段：原始数据及进程开启后产生的各种数据 数据区域（data region）：存储（全局和静态）变量和进程执行期间使用的动态分配的内存 堆栈区域（stack region）：存储活动过程调用的指令和本地变量、中间数据 程序段：存放在文本区域（text region），被多个进程共享 存储处理器执行的代码，二进制格式 程序的一次执行过程就是一个进程，程序实体就是进程的映像 进程的特征（理解） 动态性：由创建而生，由撤销而亡 并发性：多个进程可以同时运行（相互之间互不干扰耶） 独立性：独立资源分配（这样就可以保证更好的保证并发性） 异步性：相互独立，互不干扰，各自以不可预知的速度向前推进或者说实体按异步方式运行 ","date":"2023-03-17","objectID":"/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E4%B9%8B%E8%BF%9B%E7%A8%8B%E7%AE%A1%E7%90%86/:1:2","tags":[],"title":"操作系统之进程管理","uri":"/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E4%B9%8B%E8%BF%9B%E7%A8%8B%E7%AE%A1%E7%90%86/"},{"categories":["操作系统"],"content":"进程与线程 什么是线程？ 线程（thread）是一系列活动按事先设定好的顺序一次执行的过程，是一系列指令的集合 是一条执行路径，不能单独存在，必须包含在进程中 可以并发执行 是程序执行的基本单位，每个线程可以独立执行，并共享进程的资源和内存空间 线程是OS运算调度的最小单位 区别 进程是操作系统资源分配和调度的基本单位，也就是说，进程创建的目的其实就是操作系统向程序运行的那个过程分配资源；而线程是OS运算调度的最小单位，也就是说，线程才是真正使用CPU运算的，才是真正干活的。 而且，进程里的多个线程是共享进程资源的 调度：OS中进程是资源分配的基本单位，线程是运算调度的最小单位 资源：进程才拥有资源，多个线程可以共享进程资源（进程申请资源，线程只有资源的使用权） 地址空间或其他资源：不同进程之间相互独立，但是同一进程里的不同线程是共用进程资源的，如内存等 通信：进程通信需要其他手段辅助，线程通信就更为容易一点 线程相对于进程，大大降低了创建、撤销和切换可执行实体的成本和难度 为什么要引入线程？ 节省资源：线程比进程更容易创建和撤销，引入线程后，降低开销 共享进程资源：线程可以共享进程的资源和内存空间，这使得线程间的通信更加容易 提高响应能力：通过多线程技术，可以在后台执行耗时的任务，同时保持前台界面的响应性能 提高系统的并发性：多个线程可以并行执行，从而提高系统的并发性和吞吐量。如java的多线程编程 线程的实现方式 用户级线程（ULT） 在用户空间实现的线程，叫用户级线程。OS无法感知用户级线程的存在，此时线程的创建、调度、切换、运行、销毁都是用户空间实现的，内核空间并没有参与，OS只是在起初给多个用户级线程所在的进程分配资源。剩下的线程调度和切换等线程的管理工作都是由应用程序完成的，OS并没有参与其中。 CPU分配到程序P，由程序P完成用户线程调度，分配资源 优点： 线程的切换不需要转到内核空间，节省了模式切换的开销 调度算法可以由进程根据需要，对自己的线程选择调度算法进行管理和调度，而与OS的低级调度算法无关 用户级线程的实现与OS平台无关，因为对于线程管理的代码是属于用户程序的一部分，因此用户级线程甚至可以在不支持线程机制的操作平台上实现 缺点： 系统调用阻塞问题，再基于进程机制的OS中，大多数系统调用将使进程阻塞，因此当线程执行一个系统调用时，不仅该线程被阻塞，而且进程内的所有线程会被阻塞。而内核支持线程中的其他线程依然能运行 不能利用多处理机进行多重处理的优点，内核每次分配给一个进程的仅有一个CPU，因此，进程中仅有一个线程能执行，在该线程放弃CPU前，其他线程只能等待 内核级线程（KLT） 在内核空间实现的线程，叫内核级线程。进程里的所有线程的管理工作都交由内核态来完成。当然，每个内核线程都会设置一个内核线程控制块（类似于PCB的作用）用以内核的感知和控制。 进程P通过轻量级进程LWP调用内核KLT，内核线程通过调度器Thread Scheduler调度将任务映射到相应的CPU上 但是，程序一般不会直接去使用内核线程，而是通过去使用内核线程的一种高级接口叫轻量级进程（Light Weight Process）去操作内核线程（官方定义名称，在大多数系统中，LWP与普通进程的区别也在于它只有一个最小的执行上下文和调度程序所需的统计信息，而这也是它之所以被称为轻量级的原因）。并且它们之间的关系是1:1存在。需要注意的是只有先支持内核线程，才能有轻量级进程。 优点： 在多处理机系统中，内核能够同时调度同一进程中的多个线程并行执行 如果进程中的一个线程被阻塞了，内核可以调度该进程中的其他线程占有处理机运行，也可以运行其他进程中的线程。也就是说，不会出现一个线程阻塞，进程内的其他线程无法执行的情况 内核支持线程具有很小的数据结构和堆栈，线程的切换比较快，切换开销小 内核本身也可以采用多线程技术，可以提高系统的执行速度和效率 缺点： 对于用户的线程切换而言，其模式切换的开销比较大，在同一个进程中，从一个线程切换到另一个线程时，需要从用户态转到核心态进行，这是因为用户进程的线程在用户态运行，而线程的调度和管理是在内核实现的，系统开销较大 相较于用户级线程而言，内核对内核级线程的支持是有限的 混合实现 使用用户线程和内核级线程混合实现这种方式，分别使用了用户线程和内核级线程的优点。在这种混合模式中，用户线程与内核级线程的数量比是不定的，即为N:M的关系。 进程P多个用户级线程开启，线程的创建、销毁、切换都是有自己来，但是线程调度、处理器映射通过分配轻量级进程来做系统调用。算是一种折中的做法吧 总结 进程 作为系统资源分配的基本单位 可以包括多个线程，一般来说，至少有一个线程 进程不是一个可执行的实体，真正执行的是线程。进程只是负责应用程序实体的资源管理 线程 是运行和调度的基本单位 目的 引入进程的目的在于，使多道程序并发执行而互不影响，提高系统的资源利用率和吞吐量 引入线程的目的是为了减少程序在开发时的时空开销，提高系统的并发性。相较于进程而言，大大降低 ","date":"2023-03-17","objectID":"/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E4%B9%8B%E8%BF%9B%E7%A8%8B%E7%AE%A1%E7%90%86/:1:3","tags":[],"title":"操作系统之进程管理","uri":"/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E4%B9%8B%E8%BF%9B%E7%A8%8B%E7%AE%A1%E7%90%86/"},{"categories":["操作系统"],"content":"进程是怎么运行的？ ","date":"2023-03-17","objectID":"/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E4%B9%8B%E8%BF%9B%E7%A8%8B%E7%AE%A1%E7%90%86/:2:0","tags":[],"title":"操作系统之进程管理","uri":"/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E4%B9%8B%E8%BF%9B%E7%A8%8B%E7%AE%A1%E7%90%86/"},{"categories":["操作系统"],"content":"进程状态 进程阻塞原因以I/O请求为例 五种基本状态 就绪（Ready） 进程已经准备好，已分配到所需资源，只要分配到CPU就能够立即运行 执行（Running） 进程处于就绪状态被调度后，进程进入执行状态 阻塞（Blocking） 正在执行的进程由于某些事件（I/O请求，申请缓存区失败）而暂时无法继续运行，进程受到阻塞。在满足请求时进入就绪状态等待进程调度 何时会发生？ 例如，应用程序发出I/O请求时，需要等待系统调用的结果返回，那么这段等待时间是阻塞的，就可以将阻塞的进程的CPU使用权夺走，交给已经就绪的进程使用 创建（New） 进程在创建时需要申请一个空白PCB，向其中填写控制和管理进程的信息，完成资源分配。如果创建工作无法完成，比如资源无法满足，就无法被调度运行，把此时进程所处状态称为创建状态；当进程创建完成后，进程就会进入就绪态。 何时会发生？ 例如：用户启动应用，应用开始加载到内存里，创建进程work 终止（Terminated） 进程结束，或出现错误，或被系统终止，进入终止状态，进行进程资源释放和回收。 何时会发生？ 例如：正常结束、异常结束、外界干预 ","date":"2023-03-17","objectID":"/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E4%B9%8B%E8%BF%9B%E7%A8%8B%E7%AE%A1%E7%90%86/:2:1","tags":[],"title":"操作系统之进程管理","uri":"/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E4%B9%8B%E8%BF%9B%E7%A8%8B%E7%AE%A1%E7%90%86/"},{"categories":["操作系统"],"content":"进程控制 即OS对进程实现有效的管理，包括创建新进程、撤销已有进程、挂起、阻塞和唤醒、进程切换等操作。OS通过原语操作实现进程控制 原语 定义：由若干条指令组成，完成特定的功能，是一种原子操作 其实就是将多条指令封装成函数并形成原子操作，用以进程控制 特点 原子性：要么全做，要么都不做，执行过程不会被中断 在内核态（也叫管态、系统态、特权态、核心态）下执行，常驻内存 是内核的三大支撑功能之一（中断处理、时钟管理、原语操作） 原语图 创建原语（create）、阻塞原语（block）、唤醒原语（wakeup）、撤销原语（destroy） 创建 父子进程 在OS中，允许一个进程创建另一个进程，创建者称为父进程，被创建者称为子进程，子进程也能创建更多孙进程。结构像树一样。如在UNIX中，由父子进程组成一个进程组。 子进程可以继承父进程所拥有的的资源。当子进程被撤销时，应将从父进程那里获得的资源归还给父进程。此外，在撤销父进程的时候，也必须同时撤销其所有子进程。 注意：在Windows中不存在任何进程层次结构，所有进程具有相同的地位。但当进程创建了一个进程时，进程创建者会获得一个句柄（令牌），可以用来被控制被创建的进程，但是这个句柄是可以传递的。也就是说，获得了句柄的进程就拥有控制该进程的权利，因此进程之间不是层次关系，而是是否获得句柄，控制与被控制的关系。 引进创建进程的典型事件 用户登录：在分时系统中，用户在终端键入登录命令后，若登录成功，系统将为该用户建立一个进程。 作业调度：在多道批处理系统中，当作业调度程序按一定算法调度到某些作业时，便将它们存入内存，为它们创建进程。 提供服务：当运行中用户程序提出某种请求，系统将专门创建一个进程来提供用户所需要的服务，如需要打印文件时，创建打印进程。 应用请求：用户进程自己创建新进程，与创建者进程以并发的形式完成特定任务。 进程创建过程 OS调用进程创建原语Create，该原语按照以下步骤创建一个新进程： 申请空白PCB，为新进程从申请获得唯一的数字标识符。 为新进程分配其运行所需资源，包括各种物理资源和逻辑资源，如CPU时间、所需内存大小、I/O设备等。 初始化PCB：①初始化标识信息（数字标识符和父进程标识符）②初始化处理机状态（程序计数器指向程序入口地址，栈指针指向栈顶）③初始化处理机控制信息（设置进程状态、优先级等） 如果进程就绪队列能够接纳新进程，便将新进程插入就绪队列。 终止 引起进程终止的事件 正常结束：表示进程的任务已经完成，准备退出运行。在批处理系统中，通常会在程序的最后安排一条Halt指令，用于向OS表示运行已结束。 异常结束：指进程在运行时发生了某种异常事件，使程序无法继续运行。常见的有： ① 越界：程序访问的存储区越出该进程的区域。 ② 保护错：进程视图访问一个不存在或不允许访问的资源或文件。 ③ 非法指令：程序试图去执行一条不存在的指令。 ④ 特权指令错：用户没有执行当前指令的权限。 ⑤ 运行超时：执行时间超过允许执行的最大值。 ⑥ 等待超时：进程等待某事件的时间超过规定最大值。 ⑦ 算术运算错：进程试图执行一个被进制的运算。 ⑧ I/O故障：在I/O过程发送了错误 外界干预：指进程应外界的请求而终止运行。这些干预有：程序员或操作系统干预；父进程请求；父进程终止。 进程终止过程 OS调用进程终止原语，按下述过程终止指定的进程： 根据被终止进程的标识符，从PCB集合中检索出该进程的PCB，读取进程的状态。 若进程处于执行状态，立即终止进程，并置调度标志位真，在进程终止后重新进程CPU调度。 若进程还有子孙进程，将它们也终止，防止它们称为不可控的进程。 将终止进程所拥有的全部资源归还给父进程，或归还给系统。 将终止进程的PCB从所在队列中移除，操作系统记录等待需要搜集信息的其它程序搜集信息。 阻塞和唤醒 在使用阻塞和唤醒时，必须成对使用，如果在某进程中引起了进程阻塞，那么与之合作的、或相关进程中必须安排一条该进程的唤醒原语，保证进程不会永久的处于阻塞状态，无法再运行。 引起进程的阻塞与唤醒的事件 向系统请求共享资源失败，请求者进程只能被阻塞，只有在共享资源空闲时，才会被唤醒。 等待某种操作的完成，在等待过程中应该是阻塞状态，等操作完成后，才会唤醒。 新数据尚未到达，进程只能阻塞，直到数据到达被唤醒。 等待新任务的到达，进程会阻塞，直到新任务到达会被唤醒，处理完成后又把自己阻塞起来。 进程阻塞过程 当需要阻塞进程时，进程调用阻塞原语block将自己阻塞，是一种主动行为。进入block状态后，进程先立即停止执行，然后将PCB的状态改为阻塞状态，并将PCB插入相应的阻塞队列。最后，重新进行CPU调度，切换前在PCB中保留当前进程的CPU状态，然后将CPU状态按新进程的PCB设置。 进程唤醒过程 当被阻塞进程所期待的事情发生，有关进程调用唤醒原语wakeup将等待该事件的进程唤醒。 执行过程：将阻塞进程从阻塞队列中移出，将其PCB的状态改为就绪状态，然后将PCB插入到就绪队列中。 挂起与激活 进程的挂起与激活操作分别通过suspend原语和active原语实现。值得注意的是：静止阻塞和静止就绪不是进程的五大基本状态之一。之后，直接在外存里放着，需要用的时候再加载到内存里 进程的挂起 当系统中出现了引起进程挂起的事件，OS利用挂起原语suspend将指定进程或处于阻塞状态的进程挂起。 执行过程：首先检查被挂起进程的状态，如果是就绪状态，就改为就绪挂起状态；如果是阻塞状态，就改为阻塞挂起状态；为了方便用户或父进程考查该进程的运行情况，把该进程的PCB复制到某指定的内存区域（一般挂起状态的进程存在于外存）。如果被挂起进程正在执行，就重新进行调度。（也就是说，正在执行的进程不能被突然挂起，需要等到该进程调度到活动阻塞或活动就绪状态才行） 进程的激活 当系统中发生激活进程的时间时，OS将利用激活原语active将制定进程激活。 激活过程：先将进程（PCB）从外存调入内存，检查该进程现在的状态，若是就绪挂起，则改为就绪状态；若是阻塞挂起，则改为阻塞状态。加入采用抢占式调度策略，则每当有就绪挂起状态的进程被激活，就要检查是否需要进行重新调度（如使用优先级的话，检查激活进程的优先级是否大于当前正在运行的进程，如果低则不用重新调度；如果高则剥夺当前进程的运行，把CPU分配给刚激活的进程）。 进程切换 状态转换 ","date":"2023-03-17","objectID":"/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E4%B9%8B%E8%BF%9B%E7%A8%8B%E7%AE%A1%E7%90%86/:2:2","tags":[],"title":"操作系统之进程管理","uri":"/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E4%B9%8B%E8%BF%9B%E7%A8%8B%E7%AE%A1%E7%90%86/"},{"categories":["操作系统"],"content":"进程调度 调度的概念 **定义：**根据一定的算法和规则将处理机资源进行重新分配的过程 **前提：**作业/进程数远大于处理机数目 **目的：**提高资源利用率，减少处理机的空闲时间 **调度程序：**一方面要满足特定系统用户的需求（快速响应），另一方面要考虑系统整体效率（系统平均周转时间）和调度算法本身的开销 调度层次： 高级调度（作业调度） 将外存里的后背作业队列调入内存，并筛入就绪队列 只调入一次，调出一次 中级调度（内存调度） 将进程调至外村，条件合适再调回内存 在内、外村对换取进行进程对换 低级调度（进程调度） 从就绪队列选取进程分配给处理机 最基本的调度，频率非常高 进程调度的时机 什么时候会发生进程调度呢？引起进程调度的因素有哪些？这些也就是进程的调度时机。 正在执行的进程执行完毕 执行中的进程因提出I/O请求或发生等事件而暂停执行。 时间片完成 在进程通信或同步过程中执行了某种原语操作，如P操作（wait操作）阻塞 高优先者进入（适用于剥夺式调度的系统） 进程调度的方式 抢占式调度（剥夺式调度） 定义：立即暂停当前进程，分配处理机的时间片给另一个进程 **原则：**优先权/短进程优先/时间片原则 优先级更高的进程出现在就绪队列，那么当前进程直接让出CPU时间片给这个更高优先级的进程； 短进程优先：短进程执行时间很短，那么可以直接去执行这个短进程 时间片优先：当前进程的时间片用完后，需要将处理机的使用权移交给其他就绪态进程 **优点：**具有作业优先级，适合分时/实时操作系统 非抢占式调度（非剥夺式调度） **定义：**若有进程请求执行，需要等待处理机执行完当前进程或当前进程阻塞 **缺点：**适合批处理系统，不适合分时/实时系统 进程调度的过程 保存镜像：记录进程的CPU现场 调度算法：确定分配处理机的原则 进程切换：分配处理机给其他进程 处理机回收：从进程收回处理机 进程调度的切换具体过程 检查是否允许上下文切换，有可能某进程处于原语操作中，不允许切换 保存当前进程的上下文（程序计数器PC指针，寄存器） 更新PCB信息 把此进程的PCB移入队列（可能是就绪队列，也可能是阻塞队列） 选择另一个就绪态队列执行，并更新PCB 更新内存管理的数据结构 恢复所选进程的上下文，将CPU执行权交给所选进程 典型的调度算法 FCFS（先来先服务） First Come First Served 定义：分配处理机给作业队列或就绪队列中队头的作业或就绪态进程 原则：按照作业/就绪态进程到达顺序服务 **调度方式：**非抢占式调度 **适用场景：**作业调度、进程调度 **优点：**有利于CPU繁忙型作业，充分利用CPU资源 **缺点：**I/O繁忙型作业，操作耗时，CPU比较浪费 SJF（短作业优先） Shortest Job First 定义：所需服务时间最短的作业/就绪态进程优先分配处理机使用权 原则：追求最少的平均（带权）周转时间 **调度方式：**SJF/SRTN非抢占式调度 由于短作业优先调度算法里还是存在进程阻塞后，再加入就绪队列，此时进程已经进行了一部分工作，那么，再进行短作业调度时，看的就是剩余执行时间了。这种调度也被称为SRTN（Shortest Remaining Time Next） **适用场景：**作业调度、进程调度 **优点：**平均等待/周转时间最少 **缺点：**长作业周转时间会增加或饥饿；估计时间不准确，不能保证紧迫任务计时处理 HRRN（高响应比优先调度） HRRN（Highest Response Ratio Next）调度算法是介于先来先服务算法与最短进程优先算法之间的一种折中算法。先来先服务算法只考虑进程的等待时间而忽视了进程的执行时间，而最短进程优先调度算法只考虑用户估计的进程的执行时间而忽视了就绪进程的等待时间。 为此需要定义响应比Rp: Rp=（等待时间+预计执行时间）/执行时间=响应时间/执行时间 定义： Rp较大的进程获得处理机使用权 原则：综合考虑作业或进程的等待时间和执行时间 **调度方式：**非抢占式 **适用场景：**作业调度、进程调度 **优点：**优点很明显，权衡了进程等待时间和执行时间，不会使得某些进程等待太长时间 **缺点：**一旦当前进程放弃执行权（完成或阻塞）时，那么就会重新计算所有进程的响应比； PSA（优先级调度） **定义：**又叫优先级调度，按作业/进程的优先级（紧迫程度）进行调度 **原则：**优先级最高（最紧迫）的作业/进程先调度 **调度方式：**抢占式/非抢占式（并不能获得即使执行） 抢占式：高优先级的立即执行 非抢占式：高优先级等待当前进程让出处理机后执行 **适用场景：**作业调度/进程调度 优先级设置原则： 静态/动态优先级 系统\u003e用户；交互型\u003e非交互型;I/O型\u003e计算型 低优先级进程可能会产生”饥饿“，也就是低优先级进程的等待时间会大大延长 RR（时间片轮转调度） **定义：**按进程到达就绪队列的顺序，轮流分配一个时间片去执行，时间用完则剥夺 原则：公平、轮流为每个进程服务，进程在一定时间内都能得到响应 **调度方式：**抢占式，由时钟确定时间，时间一到，就会产生中断 时间片的决定因素 系统的响应时间、就绪队列的进程数量、系统的处理能力 **适用场景：**进程调度 **优点：**公平，响应快，适用于分时系统 **缺点：**时间片太大，相当于FCFS；太小，处理机切换频繁，开销增大 MFQ（多级反馈队列调度） 定义： 设置多个按优先级排序的就绪队列。优先级从高到低，时间片从小到大。新进程采用队列降级法 新来的进程会进入第一季队列，然后分时间片来获得处理机； 没有执行完，将会移动到下一级队列，前面的队列不为空，不执行后续队列进程 原则：集前几种调度算法的有点，相当于PSA+RR **调度方式：**抢占式 **适用场景：**进程调度 优缺点： 对各类型的相对公平：快速响应。 新来的进程都是进入第一级队列，都会得到CPU时间片执行一会儿，没执行完会扔到下一级队列，等到上一级队列进程执行完毕，下一级的队列里的进程再次得到执行 终端型作业用户：短作业优先 作业越短，那么最终完成进程所在队列级数就会越小，所以整体来看，作业越短，进程完成时间就越短 批处理作业用户：周转时间短 当批处理的大量进程都需要执行时，使用MFQ算法调度，让每个进程都可以公平得到CPU时间片，这样算的话 长批处理作业用户：每个进程都会被依次执行，当前进程没执行完毕就会塞入下一级队列等待执行。所以长批处理作业，也是会得到CPU时间片的，不会发生“饥饿”现象 在多级反馈队列进程调度算法里，低优先级队列里的进程会发生”饥饿“吗？ 答：不会。因为MFQ调度算法里前面队列不为空，后面的队列就不会得到执行，所以低优先级的队列一定是被执行过至少一个CPU时间片的 ","date":"2023-03-17","objectID":"/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E4%B9%8B%E8%BF%9B%E7%A8%8B%E7%AE%A1%E7%90%86/:2:3","tags":[],"title":"操作系统之进程管理","uri":"/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E4%B9%8B%E8%BF%9B%E7%A8%8B%E7%AE%A1%E7%90%86/"},{"categories":["操作系统"],"content":"进程之间是怎么协作的？ ","date":"2023-03-17","objectID":"/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E4%B9%8B%E8%BF%9B%E7%A8%8B%E7%AE%A1%E7%90%86/:3:0","tags":[],"title":"操作系统之进程管理","uri":"/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E4%B9%8B%E8%BF%9B%E7%A8%8B%E7%AE%A1%E7%90%86/"},{"categories":["操作系统"],"content":"进程通信 每个进程的用户地址空间都是独立的，一般而言是不能互相访问的，但内核空间是每个进程都共享的，所以进程之间要通信必须通过内核。 进程间通信目的一般有共享数据，数据传输，消息通知，进程控制等。以 Unix/Linux 为例，介绍几种重要的进程间通信方式：共享内存，管道，消息队列，信号量，信号，套接字 定义 进程间通信就是在不同进程之间传播或交换信息，那么不同进程之间存在着什么双方都可以访问的介质呢？进程的用户空间是互相独立的，一般而言是不能互相访问的，唯一的例外是共享内存区。另外，系统空间是“公共场所”，各进程均可以访问，所以内核也可以提供这样的条件。此外，还有双方都可以访问的外设。在这个意义上，两个进程当然也可以通过磁盘上的普通文件交换信息，或者通过“注册表”或其它数据库中的某些表项和记录交换信息。广义上这也是进程间通信的手段，但是一般都不把这算作“进程间通信”。进程间通信（IPC，Inter Process Communication）是一组编程接口，让程序员能够协调不同的进程，使之能在一个操作系统里同时运行，并相互传递、交换信息。IPC方法包括管道（PIPE）、消息队列、信号、共享存储以及套接字（Socket）。 为什么需要进程间通信呢？ 因为有些复杂程序或者是系统需要多个进程或者线程共同完成某个具体的任务，那么也就需要进程之间通信和数据访问。整个系统以进程粒度运行可以进一步提高系统整体并行性能和内存访问安全，每个进程可以有各自的分工。所以多个进程共同完成一个大的系统是比单个进程多线程要有很大的优势。 管道 管道在Linux中就是文件（本质上就是内存里固定大小的缓冲区），而且大小固定，是以流的形式读写的：未满不读，已满不写，未空不写，已空不写。 例如：有个4kb大小的管道，那么写的时候，就是 如果你学过 Linux 命令，那你肯定很熟悉|这个竖线。 ps auxf | grep mysql 上面命令行里的|竖线就是一个管道，它的功能是将前一个命令ps auxf的输出，作为后一个命令grep mysql的输入，从这功能描述，可以看出管道传输数据是单向的，如果想相互通信，我们需要创建两个管道才行. 同时，我们得知上面这种管道是没有名字，所以|表示的管道称为匿名管道，用完了就销毁。 管道还有另外一个类型是命名管道，也被叫做 FIFO，因为数据是先进先出的传输方式。 在使用命名管道前，先需要通过 mkfifo 命令来创建，并且指定管道名字： mkfifo myPipe myPipe就是这个管道的名称，基于 Linux 一切皆文件的理念，所以管道也是以文件的方式存在，我们可以用ls看一下，这个文件的类型是 p，也就是 pipe（管道） 的意思： $ ls -l prw-r--r--. 1 root root 0 Jul 17 02:45 myPipe 接下来，我们往myPipe这个管道写入数据： echo \"hello\" \u003e myPipe # 将数据写进管道 # 停住了 ... 你操作了后，你会发现命令执行后就停在这了，这是因为管道里的内容没有被读取，只有当管道里的数据被读完后，命令才可以正常退出。 于是，我们执行另外一个命令来读取这个管道里的数据： cat \u003c myPipe # 读取管道里的数据 hello 可以看到，管道里的内容被读取出来了，并打印在了终端上，另外一方面，echo 那个命令也正常退出了。 我们可以看出，管道这种通信方式效率低，不适合进程间频繁地交换数据。当然，它的好处，自然就是简单，同时也我们很容易得知管道里的数据已经被另一个进程读取了。 我们可以得知，对于匿名管道，它的通信范围是存在父子关系的进程。因为管道没有实体，也就是没有管道文件，只能通过 fork 来复制父进程 fd 文件描述符，来达到通的目的。 在 shell 里面执行 A | B命令的时候，A 进程和 B 进程都是 shell 创建出来的子进程，A 和 B 之间不存在父子关系，它俩的父进程都是 shell。 另外，对于命名管道，它可以在不相关的进程间也能相互通信。因为命令管道，提前创建了一个类型为管道的设备文件，在进程里只要使用这个设备文件，就可以相互通信。 消息队列 前面说到管道的通信方式是效率低的，因此管道不适合进程间频繁地交换数据。对于这个问题，消息队列的通信模式就可以解决。 比如，A 进程要给 B 进程发送消息，A 进程把数据放在对应的消息队列后就可以正常返回了，B 进程需要的时候再去读取数据就可以了。同理，B 进程要给 A 进程发送消息也是如此。 再来，消息队列是保存在内核中的消息链表，在发送数据时，会分成一个一个独立的数据单元，也就是消息体（数据块），消息体是用户自定义的数据类型，消息的发送方和接收方要约定好消息体的数据类型，所以每个消息体都是固定大小的存储块，不像管道是无格式的字节流数据。如果进程从消息队列中读取了消息体，内核就会把这个消息体删除。 消息队列生命周期随内核，如果没有释放消息队列或者没有关闭操作系统，消息队列会一直存在，而前面提到的匿名管道的生命周期，是随进程的创建而建立，随进程的结束而销毁。 缺点 消息队列通信过程中，存在用户态与内核态之间的数据拷贝开销，因为进程写入数据到内核中的消息队列时，会发生从用户态拷贝数据到内核态的过程，同理另一进程读取内核中的消息数据时，会发生从内核态拷贝数据到用户态的过程。 共享内存 消息队列的读取和写入的过程，都会有发生用户态与内核态之间的消息拷贝过程。那共享内存的方式，就很好的解决了这一问题。 现代操作系统，对于内存管理，采用的是虚拟内存技术，也就是每个进程都有自己独立的虚拟内存空间，不同进程的虚拟内存映射到不同的物理内存中。所以，即使进程 A 和 进程 B 的虚拟地址是一样的，其实访问的是不同的物理内存地址，对于数据的增删查改互不影响。 共享内存的机制，就是拿出一块虚拟地址空间来，映射到相同的物理内存中。这样这个进程写入的东西，另外一个进程马上就能看到了，都不需要拷贝来拷贝去，传来传去，大大提高了进程间通信的速度。 Linux系统mmap的实现机制 信号量 用了共享内存通信方式，带来新的问题，那就是如果多个进程同时修改同一个共享内存，很有可能就冲突了。例如两个进程都同时写一个地址，那先写的那个进程会发现内容被别人覆盖了。（脏写） 为了防止多进程竞争共享资源，而造成的数据错乱，所以需要保护机制，使得共享的资源，在任意时刻只能被一个进程访问。正好，信号量就实现了这一保护机制。 信号量其实是一个整型的计数器，主要用于实现进程间的互斥与同步，而不是用于缓存进程间通信的数据。 信号量表示资源的数量，控制信号量的方式有两种原子操作： 一个是 P 操作，这个操作会把信号量减去 1，相减后如果信号量 \u003c 0，则表明资源已被占用，进程需阻塞等待；相减后如果信号量 \u003e= 0，则表明还有资源可使用，进程可正常继续执行。 另一个是 V 操作，这个操作会把信号量加上 1，相加后如果信号量 \u003c= 0，则表明当前有阻塞中的进程，于是会将该进程唤醒运行；相加后如果信号量 \u003e 0，则表明当前没有阻塞中的进程； P 操作是用在进入共享资源之前，V 操作是用在离开共享资源之后，这两个操作是必须成对出现的。 接下来，举个例子，如果要使得两个进程互斥访问共享内存，我们可以初始化信号量为 1。 具体的过程如下： 进程 A 在访问共享内存前，先执行了 P 操作，由于信号量的初始值为 1，故在进程 A 执行 P 操作后信号量变为 0，表示共享资源可用，于是进程 A 就可以访问共享内存。 若此时，进程 B 也想访问共享内存，执行了 P 操作，结果信号量变为了 -1，这就意味着临界资源已被占用，因此进程 B 被阻塞。 直到进程 A 访问完共享内存，才会执行 V 操作，使得信号量恢复为 0，接着就会唤醒阻塞中的线程 B，使得进程 B 可以访问共享内存，最后完成共享内存的访问后，执行 V 操作，使信号量恢复到初始值 1。 可以发现，信号初始化为 1，就代表着是互斥信号量，它可以保证共享内存在任何时刻只有一个进程在访问，这就很好的保护了共享内存。 另外，在多进程里，每个进程并不一定是顺序执行的，它们基本是以各自独立的、不可预知的速度向前推进，但有时候我们又希望多个进程能密切合作，以实现一个共同的任务。 例如，进程 A 是负责生产数据，而进程 B 是负责读取数据，这两个进程是相互合作、相互依赖的，进程 A 必须先生产了数据，进程 B 才能读取到数据，所以执行是有前后顺序的。 那么这时候，就可以用信号量来实现多进程同步的方式，我们可以初始化信号量为 0。 具体过程： 如果进程 B 比进程 A 先执行了，那么执行到 P 操作时，由于信号量初始值为 0，故信号量会变为 -1，表示进程 A 还没生产数据，于是进程 B 就阻塞等待； 接着，当进程 A 生产完数据后，执行了 V 操作，就会使得信号量变为 0，于是就会唤醒阻塞在 P 操作的进程 B； 最后，进程 B 被唤醒后，意味着进程 A 已经生产了数据，于是进程 B 就可以正常读取数据了。 可以发现，信号初始化为 0，就代表着是同步信号量，它可以保证进程 A 应在进程 B 之前执行。 信号 信号一般用于一些异常情况下的进程间通信，是一种异步通信，它的数据结构一般就是一个数字. 在 Linux 操作系统中， 为了响应各种各样的事件，提供了几十种信号，分别代表不同的意义。我们可以通过 kill -l 命令，查看所有的信号： $ kill -l 1) SIGHUP 2) SIGINT 3) SIGQUIT 4) SIGILL","date":"2023-03-17","objectID":"/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E4%B9%8B%E8%BF%9B%E7%A8%8B%E7%AE%A1%E7%90%86/:3:1","tags":[],"title":"操作系统之进程管理","uri":"/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E4%B9%8B%E8%BF%9B%E7%A8%8B%E7%AE%A1%E7%90%86/"},{"categories":["操作系统"],"content":"进程同步 概念 进程同步概念 主要任务是使并发执行的诸进程之间能有效地共享资源和相互合作，使执行的结果具有可再现性。 进程之间的两种制约关系 间接相互制约关系（互斥）：互斥地访问系统共享资源 直接相互制约关系（同步）：进程间合作，比如进程A、B，进程B是对进程A的数据进行处理，那么进程B就一定要在进程A之后执行。 例如管道，发送者先发满管道后，接收者才可以去读管道。 互斥的访问过程与原则 临界资源 一段时间仅允许一个进程访问的资源 临界资源可能是硬件，也可能是软件：变量，数据，表格，队列等。 并发进程对临界资源的访问必须做某种限制，否则就可能出现与时间有关的错误 临界资源访问过程 **进入区：**尝试进入临界区，成功就上锁（lock） **临界区：**访问共享资源 **退出区：**解锁（unlock），唤醒其他进程 **剩余区：**剩余代码 //一个访问临界资源的循环进程描述伪代码如下： while(true) { 进入区 临界区 退出区 剩余区 } 临界资源访问原则 空闲让进：临界区空闲，允许一个进程进入 忙则等待：临界区已有进程，其他进程进入阻塞状态 有限等待：处于等待的进程，等待的时间有限 让权等待：等待时应让出CPU执行权，防止”忙等待“ 互斥软件实现基本方法* while代码就是自旋锁 单标志法 算法思想：两个进程在访问完临界区后会把临界区的权限转交给另一个进程。也就是说每个进程进入临界区的权限只能被另一个进程赋予。 turn的初始值为0，即刚开始值允许0号进程进入临界区。 若P1先上执行机运行，则会一直卡在⑤。直到P1的时间片用完，发生调度，切换P0上处理机运行。代码①不会卡住P0，P0可以正常访问临界区，在P0访问临界区期间即使切换会P1，P1依然会被卡在⑤。只有P0在退出区将turn改为1后，P1才能进入临界区。 因此，该算法可以实现“同一时刻最多只允许一个进程访问临界区”。 但是只能按照P0 -\u003e P1 -\u003e P0 -\u003e P1…这样轮流访问。如果P0访问临界区完毕后，不再访问临界区，但是P1需要再次访问临界区时turn!=1，进程不能获得锁，一直阻塞，这样就违背“空闲让进”原则。 因此，单标记法存在的主要问题是：违背“空闲让进”原则和“让权等待”。 双标志先检查法 算法思想：设置一个布尔型数组flag[]，数组中各个元素用来标记各进程想进入临界区的意愿，比如“falg[0]=ture”意味着0号进程P0现在向进入临界区。每个进程在进入临界区之前先检查当前有没有别的进程想进入临界区，如果没有，则把自身对应的标志flag[i]设置为true，之后开始访问临界区。 在高并发的情况下，有可能按照①⑤②⑥③…的顺序执行，那么P0和P1将会同时访问临界区。因此，双标志先检查法的主要问题是:违反“忙则等待”原则和“让权等待”。 原因在于进入区的“检查”和“上锁”不是原子的。“检查” 后，“上锁”前可能已经发生进程切换。 双标志后检查法 算法思想:双标志先检查法的改版。前一个算法的问题是先“检查”后“上锁”，但是这两个操作不是原子的，因此导致了两个进程同时进入临界区的问题。因此，人们又想到先“上锁”后“检查”的方法，来避免上述问题。 高并发下，按照①⑤②…的顺序执行，PO 和P1将都无法进入临界区。因此，双标志后检查法虽然解决了“忙则等待”的问题，但是又违背了“空闲让进”、“让权等待”和“有限等待”原则，会因各进程都长期无法访问临界资源而产生“饥饿”现象。两个进程都争着想进入临界区，但是谁也不让谁，最后谁都无法进入临界区。 Peterson 算法 算法思想：结合双标记法、单标记法的思想。如果双方都争着想进入临界区，那可以让进程尝试“孔融让梨”（谦让）。做一个有礼貌的进程。 Peterson算法用软件方法解决了进程互斥问题，遵循了空闲让进、忙则等待、有限等待三个原则，但是依然未遵循让权等待的原则。 Peterson算法相较于之前三种软件解决方案来说，是最好的，但依然不够好。 互斥硬件实现基本方法 中断屏蔽方法：关中断/开中断 禁止一切中断，CPU执行完临界区之前，不会发生进程切换的中断响应。 关中断时间长会影响效率 不适用于多处理机，无法防止其他处理机调度其他进程访问临界区。因为关中断只对当前处理机生效，不会对其他处理机是否响应中断请求造成影响 只是用于内核进程（该指令运行于内核态） Test-And-Set指令 简称TS指令，也有些地方称为TestAndSetLock指令，或者TSL指令。TSL指令是用硬件实现的，执行的过程不允许被中断，属于原子操作。以下是用C语言描述的逻辑。 //bool lock表示当前临界区是否被加锁 //true标识已加锁，false表示未加锁 bool TestAndSet(bool *lock){ bool old; old=*lock; //old用来存放lock原来的值 *lock=true; //无论之前是否加锁，都将lock设为true return old; //返回lock原来的值 } //以下是使用TSL指令实现互斥的算法逻辑 while(TestAndSet(\u0026lock)); //“上锁”并“检查”是一个原子操作 critical section; //临界区代码 lock=false; //解锁 remainder section; //剩余区代码 优点：实现简单，把“上锁”和“检查“通过硬件的方式变成原子操作；适用于多处理机环境。 缺点：仍然不满足让权等待原则（无法进入临界区时释放处理机）。 原因：暂时无法进入临界区的进程会占用CPU并循环执行TSL指令，从而导致忙等。 swap指令 Swap指令，又叫XCHG指令，是用硬件实现的原子操作，执行的过程中不允许被中断。以下是C语言描述的逻辑 Swap(bool *a,bool *b){ bool tmp; tmp=*a; *a=*b; *b=tmp; } //以下是用Swap指令实现互斥的算法逻辑 //lock表示当前临界区是否被加锁 bool old=true; while(old==true) Swap(\u0026lock,\u0026old); //临界区代码... lock=false; //剩余区代码... 逻辑上和TSL一样，优缺点和TSL也一样，不满足让权等待。 信号量 信号量其实就是一个变量(可以是一个整数， 也可以是更复杂的记录型变量)，可以用一个信号量来表示系统中某种资源的数量，比如:系统中只有一台打印机，就可以设置一一个初值为1的信号量。 用户进程可以通过使用操作系统提供的一对原语来对信号量进行操作，从而很方便的实现了进程互斥、进程同步。 原语是一种特殊的程序段，是原子操作。**原语是由关中断/开中断指令实现的。**软件解决方案的主要问题是由“进入区的各种操作不是原子的”，因此如果能把进入区、退出区的操作都用“原语”实现，使这些操作步骤都成为一个原子操作就能避免问题。 wait、signal 原语常简称为P、V操作(来自荷兰语proberen和verhogen)。因此，做题的时候常把wait(S)、signal(S) 两个操作分别写为P(S)、V(S) 整形信号量 简而言之，就是使用整型变量作为信号量，用以表示系统中某种资源的数量。 记录型信号量 整型信号量的缺陷是存在“忙等”问题（自旋），因此人们又提出了“记录型信号量”，即用记录型数据结构表示的信号量。 信号量实现进程的互斥 分析并发进程的关键活动，划定临界区(如:对临界资源打印机的访问就应放在临界区) 设置互斥信号量mutex，初值为1 在临界区之前执行P(mutex) 在临界区之后执行V(mutex) 注意:对不同的临界资源需要设置不同的互斥信号量。 P、V操作必须成对出现。 缺少P(mutex)就不能保证临界资源的互斥访问。缺少V(mutex)会导致资源永不被释放，等待进程永不被唤醒。 信号量实现进程的同步 分析什么地方需要实现“同步关系”，即必须保证“一前一后”执行的两个操作(或两句代码) 设置同步信号量S,初始为0 在“前操作”之后执行V(S) 在“后操作”之前执行P(S) 具体实现 若先执行到V(S)操作，则S++后S=1。之后当执行到P(S)操作时，由于S=1，表示有可用资源，会执行S–， S的值变回0，P2进程不会执行block原语，而是继续往下执行代码4。 若先执行到P(S)操作，由于S=0，S– 后S=-1，表示此时没有可用资源，因此P操作中会执行block原语，主动请求阻塞。之后当执行完代码2，继而执行V(S)操作，S++， 使S变回0。由于此时有进程在该信号量对应的阻塞队列中，因此会在V操作中执行wakeup原语，唤醒P2进程。这样P2就可以继续执行代码4了 信号量机制实现进程的前驱关系（多个进程的同步） 进程P1中有句代码S1，P2中有句代码S2…P6中有句代码S6。这些代码要求按如下前驱图所 示的顺序来执行：其实每一对前驱关系都是一个进程同步问题(需要保证一前一后的操作) 因此： 要为每一对前驱关系各设置一个同步变量 在“前操作”之后对相应的同步变量执行V操作 在“后操作”之前对相应的同步变量执行P操作 管程 前面的信号量机制实现的进程同步与互斥，会使大量的同步操作分散在各个进程中，为了减少这一现象，我们就要引入——管程。 一个管程包含一个数据结构和能为并发进程所执行（在该操作系统上）的一组操作，这组操作能同步进程和改变管程中的数据。 管程的组成 一组局部变量 对局部变量操作的一组过程 对局部变量进行初始化的语句。 管程的特点 局部数据变量只能被管程的过程访问，外部","date":"2023-03-17","objectID":"/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E4%B9%8B%E8%BF%9B%E7%A8%8B%E7%AE%A1%E7%90%86/:3:2","tags":[],"title":"操作系统之进程管理","uri":"/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E4%B9%8B%E8%BF%9B%E7%A8%8B%E7%AE%A1%E7%90%86/"},{"categories":["操作系统"],"content":"经典同步问题 生产者-消费者问题 单生产者-单消费者问题 问题描述 系统中有一组生产者进程和一组消费者进程，生产者进程每次生产一个产品放入缓冲区；消费者进程每次从临界缓冲区中取出一个产品并使用。生产者和消费者共享一个初始为空、大小为n的缓冲区。 只有缓冲区没满时，生产者才能把产品放入缓冲区，否则必须等待。 只有缓冲区不空时，消费者才能从缓冲区中取出产品，否则必须等待。 缓冲区是临界资源，各进程必须互斥的访问 问题分析 由于缓冲区是临界资源必须互斥使用，因此需要设置一个互斥信号量mutex，缓冲区有两种状态：进程正在访问和没有进程正在访问，因此可以给mutex赋初值为1。 缓冲区中的大小是生产者和消费者都可以进行操作的，当缓冲区满时，生产者要等待消费者取走产品，按一定次序访问这属于同步关系；当缓冲区空时，消费者要等待生产者生产产品，按一定次序访问这属于同步关系，因此需要设置两个同步信号量，full和empty; Semaphore mutex = 1; //互斥信号量实现对缓冲区的互斥访问 Semaphore empty = n; //同步信号量，表示空闲缓冲区的数量，初值为有界缓冲区大小n Semaphore full = 0; //同步信号量，表示产品数量，也是非空缓冲区数量，初值为0 伪代码实现 Producer(){ while (1) { 生产一个产品; P(empty); //消耗一个空闲缓冲区,后操作前进行p操作 P(mutex); 把产品放入缓冲区; V(mutex); V(full); //增加一个产品，前操作后进行v操作 } } Consumer(){ while (1) { P(full); //消耗一个产品 P(mutex); 从缓冲区取出一个产品; V(mutex); V(empty); //增加一个空闲缓冲区，前操作后进行v操作 使用产品; } } 不能交换P操作顺序，否则会出现死锁 多生产者-多消费者问题 问题描述 桌子上有一个盘子，每次只能向其中放入一个水果。爸爸只向盘子中放苹果，妈妈只向盘子中放橘子，儿子专等着吃盘子中的橘子，女儿专等着吃盘子中的苹果。只有盘子为空时，爸爸妈妈才能往盘子里放一个水果。仅当盘子中有自己需要的水果时，儿子或女儿可以从盘子中取出水果。 问题分析 盘子相当于一个初始为空，大小为1的缓冲区。爸爸妈妈分别可以看作生产者进程1、生产者进程2，儿子可以看作消费者进程1，女儿可以看作消费者进程2 互斥关系：(mutex = 1) 对缓冲区（盘子）的访问要互斥地进行 同步关系（一前一后）： 父亲将苹果放入盘子后，女儿才能取苹果 母亲将橘子放入盘子后，儿子才能取橘子 只有盘子为空时，父亲或母亲才能放入水果 伪代码实现 semaphore mutex = 1; //实现互斥访问盘子(缓冲区) semaphore apple = 0; //盘子中有几个苹果 semaphore orange = 0; //盘子中有几个橘子 semaphore plate = 1; //盘子中还可以放多少个水果 dad(){ while(1){ 准备一个苹果; P(plate); P(mutex); 把苹果放入盘子; V(mutex); V(apple); } } mom(){ while(1){ 准备一个橘子; P(plate); P(mutex); 把橘子放入盘子; V(mutex); V(orange); } } daughter(){ while(1){ P(apple); P(mutex); 从盘子中取出苹果; V(mutex); V(plate); 吃苹果 } } son(){ while(1){ P(orange); P(mutex); 从盘子中取出橘子; V(mutex); V(plate); 吃掉橘子 } } 读者-写者问题 问题描述 有两组并发进程: 读者和写者,共享一组数据区。 读者/写者问题是指，保证一个写者进程必须与其他进程互斥访问共享对象的同步问题 允许多个读者同时执行读操作 不允许读者、写者同时操作 不允许多个写者同时操作 问题分析 两类进程：写进程、读进程 互斥关系：写进程——写进程、写进程——读进程。读进程与读进程不存在互斥关系。 写进程和任何进程都要互斥，设置一个互斥信号量rw，在写者访问共享文件前后分别执行P、V操作，读者进程和写者进程也要互斥，因此读者访问共享文件前后也要对rw进行P、V操作，但是如果所有读者进程在访问共享文件之前都进行P(rw)操作，那么会导致各个读进程之间也无法同时访问文件。该如何解决这个问题？ P(rw)和V(rw)其实就是对共享文件的加锁和解锁，既然各个读进程可以同时访问文件，而读进程和写进程需要互斥访问文件，那么就让第一个读进程对文件进行加锁，最后一个读进程对文件解锁，如何知道是还有几个读进程呢？就需要设置一个整形变量count来记录当前有几个读进程在在访问文件。 解决上述问题后，我们再来看这一种情况: 当两个读进程并发来访问文件时，有可能第一个进程还没来的及进行count++，第二个进程就就已经过了判断count是否为0的操作了，这个时候第二个进程也被阻塞在P(rw)，再次出现了上述问题，该如何解决？其实仔细分析可知道会出现这种情况在于对count的判断和赋值没办法保证一致性，即不是原语操作，为了解决这个问题我们要引入一个互斥信号量mutex来保证对count的判断和赋值是一个互斥的操作。 semaphore rw = 1; //用于实现对文件的互斥访问，表示当前是否有进程在访问共享文件 int count = 0; // 记录当前有几个读进程在访问文件 semaphore mutex = 1; //用于保证对count变量的互斥访问 writer(){ while(1){ P(rw); //写之前\"加锁\" 写文件 V(rw); //写之后\"解锁\" } } reader(){ while(1){ P(mutex); //各进程互斥访问count if(count==0) P(rw); //第一个读进程负责加锁 count++; //读进程+1 V(mutex); 读文件 P(mutex); //各进程互斥访问count count--; //访问文件的读进程数-1 if(count == 0) V(rw); //最后一个读进程负责解锁 V(mutex); } } 在这个算法中，读进程是优先的，如果有一个读进程正在访问文件，这个时候来了一堆读进程和一个写进程，那么读进程一直在访问文件，写进程可能一直等待，发生”饿死“，如何解决这个问题？ semaphore rw = 1; //用于实现对文件的互斥访问，表示当前是否有进程在访问共享文件 int count = 0; // 记录当前有几个读进程在访问文件 semaphore mutex = 1; //用于保证对count变量的互斥访问 semaphore w = 1 // 实现写者优先 writer(){ while(1){ P(w); P(rw); //写之前\"加锁\" 写文件 V(rw); //写之后\"解锁\" V(w); } } reader(){ while(1){ p(w); P(mutex); //各进程互斥访问count if(count==0) P(rw); //第一个读进程负责加锁 count++; //读进程+1 V(mutex); V(w); 读文件 P(mutex); //各进程互斥访问count count--; // 访问文件的读进程数-1 if(count == 0) V(rw); //最后一个读进程负责解锁 V(mutex); } } 加入互斥信号量w就可以解决写者饿死的问题了，当一个读者在读文件时此时w已经被解锁了，这个时候一个写者进程尝试访问文件，先对w进行加锁，然后阻塞在rw，此时读者进程再来就会被阻塞在w，等待写进程执行. 哲学家进餐问题 问题描述 一张圆桌上坐着5名哲学家，每两个哲学家之间的桌上摆一根筷子，桌子的中间是一碗米饭。哲学家们倾注毕生的精力用于思考和进餐，哲学家在思考时，并不影响他人。只有当哲学家饥饿时，才试图拿起左、右两根筷子(一根一根地拿起)。如果筷子已在他人手上，则需等待。饥饿的哲学家只有同时拿起两根筷子才可以开始进餐，当进餐完毕后，放下筷子继续思考。 问题分析 五名哲学家相当于五个进程，每个进程只能访问“左右两边”的资源，且相邻的进程访问相同的资源，形成互斥关系。 关系分析。系统中有5个哲学家进程，5位哲学家与左右邻居对其中间筷子的访问是互斥关系。 整理思路。这个问题中只有互斥关系，但与之前遇到的问题不同的是，每个哲学家进程需要同时持有两个临界资源才能开始吃饭。如何避免临界资源分配不当造成的死锁现象，是哲学家问题的精髓。 信号量设置。定义互斥信号量数组chopstick[5]={1,1,1,1,1}用于实现对5个筷子的互斥访问。并对哲学家按0~4编号，哲学家i左边的筷子编号为i，右边的筷子编号为(i+1)%5。 伪代码实现 semaphore chopstick[5] = {1,1,1,1,1}; Philosopher i： while (1) { 思考; P(chopstick[i]); // 拿左边的筷子 P(chopstick[(i+1) % 5]); // 再拿右边的筷子 进食; V(chopstick[i]); // 吃完后，放下左筷子 V(chopstick[(i+1) % 5]);// 放下右筷子 } 这种解法会导致死锁，每个哲学家都拿一只筷子然后","date":"2023-03-17","objectID":"/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E4%B9%8B%E8%BF%9B%E7%A8%8B%E7%AE%A1%E7%90%86/:3:3","tags":[],"title":"操作系统之进程管理","uri":"/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E4%B9%8B%E8%BF%9B%E7%A8%8B%E7%AE%A1%E7%90%86/"},{"categories":["操作系统"],"content":"如何处理死锁问题？ ","date":"2023-03-17","objectID":"/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E4%B9%8B%E8%BF%9B%E7%A8%8B%E7%AE%A1%E7%90%86/:4:0","tags":[],"title":"操作系统之进程管理","uri":"/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E4%B9%8B%E8%BF%9B%E7%A8%8B%E7%AE%A1%E7%90%86/"},{"categories":["操作系统"],"content":"死锁概念 定义 所谓死锁是指多个进程因竞争资源而造成的一种僵局，若无外力作用，这些进程都将无法向前推进。 相似概念“饥饿”：在操作系统中，饥饿问题是指由于高优先级请求的不断涌入导致低优先级进程长时间被停滞，无法获得处理器或资源。通常情况下，当一个任务被无限期地推迟时，就会出现饥饿问题。 与死锁得区别在于： 死锁发生后操作系统若不干预，那么陷入死锁的所有进程永远被阻塞； 而饥饿发生，仅仅只是长时间没有被分配到处理机资源，但是还是有可能后面一段时间会被执行到，并没有永久阻塞。 死锁产生原因 系统资源的竞争 通常系统中拥有的不可剥夺资源，其数量不足以满足多个进程运行的需要，多个进程在运行过程中会因争夺资源而陷入僵局。只有对不可剥夺资源的竞争才可能产生死锁，对可剥夺资源的竞争是不会引起死锁的。 进程推进顺序非法 进程在运行过程中，请求和释放资源的顺序不当，同样会导致死锁。信号量使用不当也会造成死锁。进程间相互等待对方发来的消息，结果也会造成某些进程间无法继续向前推进。 死锁产生得必要条件 产生死锁必须同时满足以下四个条件，只要其中任一个条件不成立，死锁就不会发生。 注意！！！ 此处是必要条件，而非充要条件。四个条件都满足，但不一定发生死锁。 互斥条件：进程要求对所分配的资源进行排他性控制，即在一段时间内某资源仅为一个进程所占用。此时若有其他进程请求该资源，则请求进程只能等待。 不可剥夺条件：进程所获得的资源在未使用完毕之前，不能被其他进程强行夺走，即只能由获得该资源的进程自己来释放。 请求和保持条件：进程已经保持了至少一个资源，但又提出了新的资源请求，而该资源已被其他进程占有，此时请求进程被阻塞，但对自己已获得的资源保持不放。 循环等待条件：存在一种进程资源的循环等待链，连中每一个进程已获得的资源同时被链中下一个进程所请求。 ","date":"2023-03-17","objectID":"/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E4%B9%8B%E8%BF%9B%E7%A8%8B%E7%AE%A1%E7%90%86/:4:1","tags":[],"title":"操作系统之进程管理","uri":"/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E4%B9%8B%E8%BF%9B%E7%A8%8B%E7%AE%A1%E7%90%86/"},{"categories":["操作系统"],"content":"死锁处理策略 预防策略 所有进程运行起来之前采取的措施 破环互斥条件 方法 将只能互斥访问的资源改为同时共享访问，也就是将独占锁改为共享锁 注: 不是所有资源都能改成可共享的，某些资源无法通过这种方式达到解除死锁的目的 破坏不可剥夺条件 方法 请求新资源无法满足时，必须释放已有资源。由OS协助强制剥夺某进程持有的资源。 实现复杂代价高 此操作过多会导致原进程任务无法推进 破坏请求和保持条件 方法 进程开始运行时，一次性申请所有所需的资源，这样自身不会阻塞但是会： 造成资源浪费 其他进程会更加饥饿 阶段性请求和释放资源 相较于前面一次性申请所有资源更好，每使用完一个资源，就释放它，可以避免长时间暂用而导致其他进程的饥饿问题 破坏循环等待条件 方法 避免出现资源申请环路，即对资源事先分类编号，按号分配。 这种方式可以有效提高资源的利用率和系统吞吐量，但是增加了系统开销，增大了进程对资源的占用时间。 对资源的编号应相对稳定，限制了新设备增加 避免策略 死锁避免：在使用前进行判断，只允许不会产生死锁的进程申请资源； 死锁避免是利用额外的检验信息，在分配资源时判断是否会出现死锁，只在不会出现死锁的情况下才分配资源。 两种避免办法： 如果一个进程的请求会导致死锁，则不启动该进程 如果一个进程的增加资源请求会导致死锁，则拒绝该申请。 银行家算法 避免死锁的具体实现通常利用银行家算法 银行家算法的实质就是**要设法保证系统动态分配资源后不进入不安全状态，以避免可能产生的死锁。**即没当进程提出资源请求且系统的资源能够满足该请求时，系统将判断满足此次资源请求后系统状态是否安全，如果判断结果为安全，则给该进程分配资源，否则不分配资源，申请资源的进程将阻塞。 银行家算法的执行有个前提条件，即要求进程预先提出自己的最大资源请求，并假设系统拥有固定的资源总量。下面介绍银行家算法所用的主要的数据结构。 ","date":"2023-03-17","objectID":"/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E4%B9%8B%E8%BF%9B%E7%A8%8B%E7%AE%A1%E7%90%86/:4:2","tags":[],"title":"操作系统之进程管理","uri":"/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E4%B9%8B%E8%BF%9B%E7%A8%8B%E7%AE%A1%E7%90%86/"},{"categories":["操作系统"],"content":"死锁检测和解除 死锁检测 为了能对系统是否已发生了死锁进行检测，必须: 用某种数据结构来保存资源的请求和分配信息: 提供一种算法， 利用上述信息来检测系统是否已进入死锁状态。 如果系统中剩余的可用资源数足够满足进程的需求，那么这个进程暂时是不会阻塞的，可以顺利地执行下去。如果这个进程执行结束了把资源归还系统，就可能使某些正在等待资源的进程被激活，并顺利地执行下去。相应的，这些被激活的进程执行完了之后又会归还一些资源，这样可能又会激活另外一 些阻塞的进程。 如果按上述过程分析，最终能消除所有边， 就称这个图是可完全简化的。此时一定没有发生死锁(相当于能找到一个安全序列) 如果最终不能消除所有边，那么此时就是发生了死锁。最终还连着边的那些进程就是处于死锁状态的进程。 死锁检测算法 在资源分配图中，找出既不阻塞又不是孤点的进程Pi。即找出一条有向边与它相连，且该有向边对应资源的申请数量小于等于系统中已有空闲资源数量。如上图中，R1没有空闲资源，R2有一个空闲资源。若所有的连接该进程的边均满足上述条件，则这个进程能继续运行直至完成，然后释放它所占有的所有资源。消去它所有的请求边和分配边，使之称为孤立的结点。在下图中，P1是满足这一条件的进程结点，于是将P1的所有边消去。 进程Pi所释放的资源，可以唤醒某些因等待这些资源而阻塞的进程，原来的阻塞进程可能变为非阻塞进程。在下图中，P2 就满足这样的条件。根据中的方法进行一系列简化后，若能消去图中所有的边，则称该图是可完全简化的。 死锁定理:如果某时刻系统的资源分配图是不可完全简化的，那么此时系统死锁 死锁解除 一旦检测出死锁的发生，就应该立即解除死锁。 补充:并不是系统中所有的进程都是死锁状态，用死锁检测算法化简资源分配图后，还连着边的那些进程就是死锁进程 解除死锁的主要方法有: 资源剥夺法。挂起(新时放到外存上)某些死锁进程，并抢占它的资源，将这些资源分配给其他的死锁进程。但是应防止被挂起的进程长时间得不到资源而饥饿。 撤销进程法(或称终止进程法)。强制撤销部分、甚至全部死锁进程，并剥夺这些进程的资源.这种方式的优点是实现简单，但所付出的代价可能会很大。因为有些进程可能已经运行了很长时间，已经接近结束了，一旦被终止可谓功亏一篑，以后还得从头再来。 进程回退法。让一个或多个死锁进程回退到足以避免死锁的地步。这就要求系统要记录进程的历史信息，设置还原点。 ","date":"2023-03-17","objectID":"/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E4%B9%8B%E8%BF%9B%E7%A8%8B%E7%AE%A1%E7%90%86/:4:3","tags":[],"title":"操作系统之进程管理","uri":"/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E4%B9%8B%E8%BF%9B%E7%A8%8B%E7%AE%A1%E7%90%86/"},{"categories":["操作系统"],"content":"[toc] ","date":"2023-03-15","objectID":"/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E4%B9%8B%E6%A6%82%E8%BF%B0/:0:0","tags":[],"title":"操作系统之概述","uri":"/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E4%B9%8B%E6%A6%82%E8%BF%B0/"},{"categories":["操作系统"],"content":"操作系统概念 操作系统（operation system，简称OS），是管理计算机硬件与软件资源的计算机程序。 例如，在windows上win+r快捷键，然后在弹出输入框输入CMD，显示的黑框框也是一个操作系统。（在这个窗口里，可以控制计算机的各种资源） ","date":"2023-03-15","objectID":"/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E4%B9%8B%E6%A6%82%E8%BF%B0/:1:0","tags":[],"title":"操作系统之概述","uri":"/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E4%B9%8B%E6%A6%82%E8%BF%B0/"},{"categories":["操作系统"],"content":"计算机系统构成 由用户、应用程序、操作系统、硬件这四部分组成。 操作系统向用户和应用程序提供接口。 用户或应用程序可以通过这些接口访问计算机的硬件资源，如：用户开启或关闭声音、应用程序向操作系统申请磁盘空间或内存等行为。也就是说，操作系统通过封装具体的硬件细节形成接口，对于用户或应用程序而言是个黑盒，但是强大的封装性更易于被使用。 OS是一种系统软件，担任以下角色 与硬件交互 对资源共享进行调度管理 解决并发操作处理中存在的协调问题 数据结构复杂，外部接口多样化，便于用户反复使用 OS主要作用 管理与配置内存 决定系统资源供需的优先次序 控制输入设备与输出设备 操作网络与管理文件系统等基本事务 提供一个用户与系统交互的界面 =\u003e GUI ","date":"2023-03-15","objectID":"/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E4%B9%8B%E6%A6%82%E8%BF%B0/:1:1","tags":[],"title":"操作系统之概述","uri":"/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E4%B9%8B%E6%A6%82%E8%BF%B0/"},{"categories":["操作系统"],"content":"OS的目标和功能 目标 有效性：提高系统资源利用率、提高系统的吞吐量 方便性：方便用户使用 可扩充性：硬件不断发展，依然适配OS 开放性：兼容不同软件和硬件 功能 作为计算机系统资源的管理者 作为用户与计算机硬件系统之间的接口 接口主要体现在三个方面：操作系统提供的程序接口（也就是系统调用）、操作系统提供的命令接口、操作系统的图形用户接口（GUI） 实现了对计算机资源的抽象 抽象的含义就是：将复杂的硬件资源抽象成软件资源，从而提供对外接口的方式，也就是封装 举个例子 浏览器搜索：浏览器会将需要将发送的内容组装成报文，再通过系统调用需要使用浏览器所在设备的网络设备进行远程通信 ","date":"2023-03-15","objectID":"/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E4%B9%8B%E6%A6%82%E8%BF%B0/:1:2","tags":[],"title":"操作系统之概述","uri":"/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E4%B9%8B%E6%A6%82%E8%BF%B0/"},{"categories":["操作系统"],"content":"OS的特征 并发特征是共享、虚拟和异步的前提，其中共享和并发互为前提 并发是共享的前提 如果只有单道程序运行，则不存在共享的可能。 共享是并发的前提 如果多道程序无法共享部分资源（比如磁盘、网卡等），显然就没有并发这一说（因为相当于只有单道程序访问资源，自然无需调度和管理啦） 并发是虚拟的前提 虚拟的目的就是表达出多道程序并发时的运行态。如果没有并发，自然虚拟就没有意义了 并发是异步的前提 如果没有并发的多道程序，那么单处理机上的多道程序的分时占用处理机的情况就不会发生，自然就谈不上异步了。 并发 指的是同一时间间隔内执行和调度多个程序的能力 宏观上，处理机同时执行多道程序 微观上，处理机在多道程序间高速切换（分时交替执行）（毕竟操作系统的核是有限的，然后程序的数量显然要大于核的数量） 主要关注**单个处理机（单个核）**在同一时间段内处理任务数量的能力 并行与并发的区别： 并行指的是同一时刻发生，并发指的是同一时间段内发生 共享 指的是资源共享，系统中的资源供多个并发执行的应用程序共同使用（显然，共享的前提是并发） 同时访问方式：同一时段，允许多个程序同时访问共享资源 互斥共享方式：也叫独占式。允许多个应用程序在同一个共享资源上独立而互不干扰地工作 虚拟 指的是使用某种技术把一个物理实体变成多个逻辑上的对应物。 时分复用技术 虚拟处理机技术、虚拟设备技术 多道程序宏观上同时运行在多个核上，微观上是占用核的时间片之后，就会切换下一个应用程序来占用核的时间片，这样轮流工作就保障了多道程序同时运行。 空分复用技术 虚拟磁盘技术（磁盘分区）、虚拟存储器技术 异步 指的是：多道程序环境下，允许多个程序并发执行；单个处理机环境下，多个程序分时交替运行； 加之程序运行的不可预知性（运行的时机、程序暂停、程序的运行时间、程序的性能）就会导致这样的状况： 宏观上，所有的程序好像是一直在一起在运行，但是实际上由于多道程序的存在，那么单个处理机就一定会出现多道程序的分时占用处理机运行，也就是说微观上其实是”走走停停“，也就是说多道程序不是同步完成的，而是异步完成的。 显而易见，只有并发出现的多道程序，才会导致分时占用处理机的异步情况发生 ","date":"2023-03-15","objectID":"/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E4%B9%8B%E6%A6%82%E8%BF%B0/:1:3","tags":[],"title":"操作系统之概述","uri":"/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E4%B9%8B%E6%A6%82%E8%BF%B0/"},{"categories":["操作系统"],"content":"操作系统的发展与分类 ","date":"2023-03-15","objectID":"/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E4%B9%8B%E6%A6%82%E8%BF%B0/:2:0","tags":[],"title":"操作系统之概述","uri":"/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E4%B9%8B%E6%A6%82%E8%BF%B0/"},{"categories":["操作系统"],"content":"1、手工操作阶段 在计算机发展的初期，没有操作系统，人们只能依靠手工操作来使用计算机。 举个栗子，很多年前，人先手工把专门的 “打孔纸” 打好孔，交给机器计算。因为计算机算的快，人手工打孔慢，这就造成了资源的浪费。如下图所示： ","date":"2023-03-15","objectID":"/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E4%B9%8B%E6%A6%82%E8%BF%B0/:2:1","tags":[],"title":"操作系统之概述","uri":"/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E4%B9%8B%E6%A6%82%E8%BF%B0/"},{"categories":["操作系统"],"content":"2、批处理阶段 单道批处理系统 单道批处理系统：一种引入了脱机输入/输出技术 ，并由监督程序负责控制作业输入和输出，但在工作期间只能对单份作业进行处理的系统。 接着举个栗子，这次我们把很多人手工打好的 “打孔纸” 先通过某种手段(卫星机/外围机)包装好，假设我们把它们包装到一盘“磁带”里。接着我们在把一盘接着一盘包装好的 “磁带” 独立地送入到计算机进行计算。如下图所示： 主要优点：缓解了一定程度的人机速度矛盾，资源利用率有所提升。 主要缺点：内存中仅能有一道程序运行，只有该程序运行结束之后才能调入下一道程序。CPU有大量的时间是在空闲等待I/O完成。资源利用率依然很低。 多道批处理系统 多道批处理系统：也是一种引入了脱机输入/输出的技术 ，并由监督程序负责控制作业输入和输出，在工作期间能对多份作业同时进行处理的系统。 它就解决了 单道批处理系统 资源利用率依然很低的缺点 在接着举个栗子，在 “磁带” 都准备好后，我们在把一盘接着一盘包装好的 “磁带” 耦合地送入到计算机进行计算。如下图所示： 主要优点：多道程序并发执行，共享计算机资源。资源利用率大幅提升，CPU和其他资源更能保持“忙碌”状态，系统吞吐量增大。 主要缺点：用户响应时间长，没有人机交互功能（用户提交自己的作业之后就只能等待计算机处理完成，中间不能控制自己的作业执行。比如：无法调试程序/无法在程序运行过程中输入一些参数）。 ","date":"2023-03-15","objectID":"/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E4%B9%8B%E6%A6%82%E8%BF%B0/:2:2","tags":[],"title":"操作系统之概述","uri":"/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E4%B9%8B%E6%A6%82%E8%BF%B0/"},{"categories":["操作系统"],"content":"3、分时操作系统 分时操作系统：计算机以时间片为单位，轮流为各个用户/作业服务，各个用户可通过终端与计算机进行交互。 它就解决了 多道批处理系统 不能人机交互的缺点 比如，由4个人在玩电脑，但他们用的是同一个分时操作系统。那么该系统就会按照 “①②③④①②③…” 的顺序轮流分 50ms 给每个用户使用。 主要优点：用户请求可以被即时响应，解决了人机交互问题。允许多个用户同时使用一台计算机，并且用户对计算机的操作相互独立，感受不到别人的存在。 主要缺点：不能优先处理一些紧急任务。操作系统对各个用户/作业都是完全公平的，循环地为每个用户/作业服务一个时间片，不区分任务的紧急性。 ","date":"2023-03-15","objectID":"/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E4%B9%8B%E6%A6%82%E8%BF%B0/:2:3","tags":[],"title":"操作系统之概述","uri":"/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E4%B9%8B%E6%A6%82%E8%BF%B0/"},{"categories":["操作系统"],"content":"4、实时操作系统 实时操作系统：实时反映的一种操作系统，不像分时操作系统那样呆板。在实时操作系统的控制下，计算机系统接收到外部信号后及时进行处理，并且要在严格的时限内处理完事件。实时操作系统的主要特点是及时性和可靠性。 主要优点：能够优先响应一些紧急任务，某些紧急任务不需时间片排队。 实时操作系统有时还可以被分为：硬实时操作系统和软实时操作系统。 ","date":"2023-03-15","objectID":"/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E4%B9%8B%E6%A6%82%E8%BF%B0/:2:4","tags":[],"title":"操作系统之概述","uri":"/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E4%B9%8B%E6%A6%82%E8%BF%B0/"},{"categories":["操作系统"],"content":"5、其他几种操作系统 网络操作系统：是伴随着计算机网络的发展而诞生的，能把网络中各个计算机有机地结合起来，实现数据传送等功能，实现网络中各种资源的共享（如文件共享）和各台计算机之间的通信。（如：Windows NT 就是一种典型的网络操作系统，网站服务器就可以使用） 分布式操作系统：主要特点是分布性和并行性。系统中的各台计算机地位相同，任何工作都可以分布在这些计算机上，由它们并行、协同完成这个任务。 个人计算机操作系统：如 Windows XP、MacOS，方便个人使用。 这些操作系统的每一个抠出来单独讲的话，都是一门学问。我们在这里只了初步的了解。 ","date":"2023-03-15","objectID":"/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E4%B9%8B%E6%A6%82%E8%BF%B0/:2:5","tags":[],"title":"操作系统之概述","uri":"/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E4%B9%8B%E6%A6%82%E8%BF%B0/"},{"categories":["操作系统"],"content":"操作系统的运行环境 ","date":"2023-03-15","objectID":"/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E4%B9%8B%E6%A6%82%E8%BF%B0/:3:0","tags":[],"title":"操作系统之概述","uri":"/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E4%B9%8B%E6%A6%82%E8%BF%B0/"},{"categories":["操作系统"],"content":"操作系统的运行机制 操作系统其实也是一种程序，程序的执行过程也就是CPU执行一行行机器指令的过程。区别于一般程序员开发的应用程序，实现操作系统的程序就是内核程序。 1、内核程序\u0026应用程序 内核程序：实现操作系统的程序称为内核程序，许多内核程序结合在一起便组成了操作系统内核。 应用程序：普通程序员借助编程工具以及高级语言所完成的程序叫做应用程序。 2、特权指令\u0026非特权指令 **特权指令：**作为系统资源的管理者，操作系统可以使用一些直接关系重大的指令（内存清零等），这些指令被称为特权指令，而且应用程序没有办法直接使用特权指令，这保证了操作系统的安全。 **非特权指令：**应用程序可以使用的指令，如加减乘除指令等。 CPU在设计的时候便已经划分出了特权指令以及非特权指令，因此在执行这条命令前CPU就可以先判断指令的内容。 3、内核态\u0026用户态 CPU有两种状态：内核态（或称核心态，管态）以及用户态（目态）。 当CPU处于内核态时，说明此时正在运行的是内核程序（CPU上加载的是内核程序，是操作系统这个管理者角色在发挥作用），此时可以执行特权指令； 当CPU处于用户态时，说明此时正在运行的是应用程序，此时只能执行非特权指令。在CPU中有一个寄存器叫做程序状态寄存器（PSW），该寄存器的01状态来表示此时处于内核态还是用户态。 下面我们来讲解一下，用户态与内核态之间是如何相互进行变化的。 内核态–\u003e用户态：需要执行一条特权指令（指令内容是修改PSW寄存器状态），此时内核态转变为用户态，CPU主动让出使用权，CPU会去加载用户态。 用户态–\u003e内核态：由“中断”引发，当CPU检测到中断时，操作系统会强制将CPU变为内核态夺回CPU使用权，再执行与中断有关的一系列操作。 下面我们用一个小栗子来解释一下这个过程： 刚开机时，CPU 为“内核态”，操作系统内核程序先上CPU运行 开机完成后，用户可以启动某个应用程序 操作系统内核程序在合适的时候主动让出 CPU，让该应用程序上CPU运行 应用程序运行在“用户态” 此时，一位黑客在应用程序中植入了一条特权指令，企图破坏系统… CPU发现接下来要执行的这条指令是特权指令，但是自己又处于“用户态” 这个非法事件会引发一个中断信号 “中断”使操作系统再次夺回CPU的控制权 操作系统会对引发中断的事件进行处理，处理完了再把CPU使用权交给别的应用程序 中断是操作系统夺回CPU使用权的唯一方式（也是由用户态切换到内核态的唯一方式） 4、操作系统的内核 当我们刚开始使用操作系统的时候，会发现一些程序并不是必需的（记事本等），这些程序称为操作系统的非内核功能，下图是将操作系统进行了更进一步的划分。 有人将内核划分为大内核以及微内核，这两种划分在不同的时期各有优势。下面用企业来类比一下操作系统： 内核就是企业的管理层，负责一些重要的工作。只有管理层（内核程序）才能执行特权指令，普通员工（应用程序）只能执行非特权指令。用户态、核心态之间的切换相当于普通员工和管理层之间的工作交接。 大内核：企业初创时体量不大，管理层的人会负责大部分的事情。优点是效率高；缺点是组织结构混乱，难以维护。 微内核：随着企业体量越来越大，管理层只负责最核心的一些工作。优点是组织结构清晰，方便维护；缺点是效率低。 5、小结 需要注意的是：特权指令只能在核心态下执行； 内核程序只能在核心态下执行。 ","date":"2023-03-15","objectID":"/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E4%B9%8B%E6%A6%82%E8%BF%B0/:3:1","tags":[],"title":"操作系统之概述","uri":"/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E4%B9%8B%E6%A6%82%E8%BF%B0/"},{"categories":["操作系统"],"content":"操作系统的重要部件和机制 0、时钟 操作系统的时钟是一个很重要的核心部件。CPU时间片的计时、系统本身计时等 1、中断 操作系统会响应中断信号强制夺回CPU使用权，使用户态转换为内核态。**中断是操作系统夺回CPU使用权的唯一方式，**如果没有“中断”机制，那么一旦应用程序上CPU运行，CPU就会一直运行这个应用程序，其他程序就没有办法再使用CPU，这样的话就不存在并发机制了。一般来说，中断分为两种情况即内中断和外中断。 内中断 包括：陷入（由应用程序主动引发，如应用程序的系统调用）、故障（由错误条件引发）、终止（由致命错误引发） 内中断（也称异常）与当前执行的指令有关，中断信号来源于CPU内部。比如，当用户妄图执行特权指令时，CPU便会产生中断，由用户态变为内核态；再比如，当进行除法运算除数为0时同样会发生错误，产生中断。总的来说，就是当前指令是非法的指令，那么一定会引起中断。 当应用程序想要进行一次系统调用使用内核的服务时，会执行一条**陷入（trap）**指令，陷入指令会引发一条中断信号，操作系统就会夺回CPU的使用权，也就是说，执行“陷入指令”，意味着应用程序主动地将CPU控制权还给操作系统内核。 例如，一个读文件的应用程序开始运行时，并不是由应用应用程序来读取系统文件，而是该应用程序执行到读文件这一段程序时，自己会发生中断，之后操作系统产生陷入指令，再之后执行操作系统的特权指令进行读取 外中断 外中断与当前执行的指令无关，中断信号来源于CPU外部。每次在一条指令结束之后CPU都会检查是否有外中断信号。比如时钟信号，或者I/O设备都可以发出外中断信号。 中断分类 中断的原理：不同的中断信号，需要用不同的中断处理程序来处理。当CPU检测到中断信号后，会根据中断信号的类型去查询“中断向量表”，以此来找到相应的中断处理程序在内存中的存放位置。所以，显然中断处理程序一定是内核程序，工作在内核态。 中断处理过程 关中断 CPU不再响应高级中断的请求 保存断点 断点指的是PC（或叫IP，也就是执行程序的地址） 引出中断程序 每个中断都会有一个中断程序，拿到中断程序的地址，但并没有开始执行中断程序 保存现场和屏蔽字 将中断前CPU的所有状态，包括：寄存器的值等现场数据进行保存，方便后续中断执行完毕，CPU现场的恢复 开中断 可以响应其他中断请求 执行中断程序 再关中断 恢复现场和屏蔽字 开中断 在上面的中断处理过程中，只有执行中断程序的过程里CPU才能去响应其他中断信号，其余阶段都不能响应。 2、原语 由若干条指令组成的一个程序段，用以完成某个特定的功能，且执行过程中不会被中断，也就是说具备原子性 本质上就是多条指令的封装，通过中断机制里的开关中断来实现 3、系统数据结构 进程管理：作业控制块、进程控制块 存储器管理：存储器分配与回收 设备管理：缓冲区、设备控制块 4、系统调用 有操作系统实现，给应用程序调用 是一套接口的集合 应用程序访问内核服务的方式 会导致内核从用户态切换到内核态 如拷贝问题：我如果频繁的读取某项数据，会导致用户态和内核态频繁的切换，就比较消耗资源。可以通过零拷贝的方式来避免这种情况的发生 虽然出现了用户态和内核态，比较消耗资源，但是更加安全，避免用户态的肆意妄为导致系统崩溃掉 ","date":"2023-03-15","objectID":"/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E4%B9%8B%E6%A6%82%E8%BF%B0/:3:2","tags":[],"title":"操作系统之概述","uri":"/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E4%B9%8B%E6%A6%82%E8%BF%B0/"},{"categories":["操作系统"],"content":"操作系统的结构设计 ","date":"2023-03-15","objectID":"/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E4%B9%8B%E6%A6%82%E8%BF%B0/:4:0","tags":[],"title":"操作系统之概述","uri":"/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E4%B9%8B%E6%A6%82%E8%BF%B0/"},{"categories":["操作系统"],"content":"大内核OS结构-传统的操作系统结构 无结构OS 设计基于功能实现和获得高的效率 OS是为数众多的一组过程的集合 OS整体无结构（程序之间） 程序内部代码无结构（goto语句非常多） 模块化OS 模块的独立性 模块大小的划分 衡量模块独立性的指标 内聚性：模块内部各部分间联系的紧密程度 耦合度：模块间相互联系和相互影响的程度 模块化OS的优点 提高OS设计的正确性、可理解性和可维护性 增强OS的可适应性 加速OS的开发过程 模块化OS的缺点 设计时对模块的划分与接口的规定不精确 模块间存在复杂依赖关系 分层式OS 分层设计的基本原则：每一层都仅使用其底层所提供的功能和服务。 分层设计考虑的因素 程序嵌套 作业调度/进程控制/内存分配 运行频率 频率高-\u003e 低层（速度快）时钟管理 公共模块 用户接口 最高层 ","date":"2023-03-15","objectID":"/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E4%B9%8B%E6%A6%82%E8%BF%B0/:4:1","tags":[],"title":"操作系统之概述","uri":"/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E4%B9%8B%E6%A6%82%E8%BF%B0/"},{"categories":["操作系统"],"content":"微内核OS结构 微内核技术：精心设计的、能实现现代OS核心功能的小型内核，它与一般OS不同，更小更精练，运行于核心态，开机后 常驻内存。但是微内核并不是完成的操作系统，只是抽出了操作系统最核心部分的功能 微内核OS结构的特征 以微内核为OS核心 以客户/服务器为基础 采用面向对象的程序设计方法 ","date":"2023-03-15","objectID":"/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E4%B9%8B%E6%A6%82%E8%BF%B0/:4:2","tags":[],"title":"操作系统之概述","uri":"/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E4%B9%8B%E6%A6%82%E8%BF%B0/"},{"categories":[],"content":"建站感言 在网络的海洋，我筑起了灯塔，是思考的港湾，也是生活的塔。 学习的河流在此流淌， 知识的种子在此生长。 生活的点滴在此被记录， 每个微笑，每滴泪水，在文字中跳跃。 独立思考是我前行的灯， 在复杂中寻找自我声音。 这博客是我灵魂的寄托， 也是对生活的热爱和追求。 愿你在这里找到共鸣， 在你心中留下印记。 ","date":"2023-02-27","objectID":"/about/:1:0","tags":[],"title":"关于我","uri":"/about/"},{"categories":[],"content":"友链 kun-kun lrc lmh ","date":"2023-02-27","objectID":"/about/:2:0","tags":[],"title":"关于我","uri":"/about/"},{"categories":["汇编"],"content":"指令系统总结 我们对8086CPU的指令系统进行一下总结。读者若要详细了解8086指令系统中的各个指令的用，可以查看有关的指令手册。 8086CPU提供以下几大类指令。 数据传送指令 mov、push、pop、pushf、popf、xchg 等都是数据传送指令，这些指令实现寄存器和内存、寄器和寄存器之间的单个数据传送。 算术运算指令 add、sub、adc、sbb、inc、dec、cmp、imul、idiv、aaa等都是算术运算指令，这些指令实现存器和内存中的数据的算数运算。它们的执行结果影响标志寄存器的sf、zf、of、cf、pf、af位。 逻辑指令 and、or、not、xor、test、shl、shr、sal、sar、rol、ror、rcl、rcr等都是逻辑指令。除了not指外，它们的执行结果都影响标志寄存器的相关标志位。 转移指令 可以修改IP，或同时修改CS和IP的指令统称为转移指令。转移指令分为以下几类。 （1）无条件转移指令，比如，jmp； （2）条件转移指令，比如，jcxz、je、jb、ja、jnb、jna等； （3）循环指令，比如，loop； （4）过程，比如，call、ret、retf； （5）中断，比如，int、iret。 处理机控制指令 对标志寄存器或其他处理机状态进行设置，cld、std、cli、sti、nop、clc、cmc、stc、hlt、wait、esc、lock等都是处理机控制指令。 串处理指令 对内存中的批量数据进行处理，movsb、movsw、cmps、scas、lods、stos等。若要使用这些指令方便地进行批量数据的处理，则需要和rep、repe、repne 等前缀指令配合使用。 ","date":"2023-02-01","objectID":"/%E6%B1%87%E7%BC%96%E4%B9%8B%E6%80%BB%E7%BB%93/:0:0","tags":[],"title":"汇编之总结","uri":"/%E6%B1%87%E7%BC%96%E4%B9%8B%E6%80%BB%E7%BB%93/"},{"categories":["汇编"],"content":"外中断 ","date":"2023-01-29","objectID":"/%E6%B1%87%E7%BC%96%E4%B9%8B%E5%A4%96%E4%B8%AD%E6%96%AD/:0:0","tags":[],"title":"汇编之外中断","uri":"/%E6%B1%87%E7%BC%96%E4%B9%8B%E5%A4%96%E4%B8%AD%E6%96%AD/"},{"categories":["汇编"],"content":"1、外中断 CPU在计算机系统中，除了能够执行指令，进行运算以外，还应该能够对外部设备进行控制，接收它们的输入，向它们进行输出（I/O能力） PC系统的接口卡和主板上，装有各种接口芯片。这些外设接口芯片的内部有若干寄存器，CPU将这些寄存器当作端口来访问 外设的输入不直接送入内存和CPU，而是送入相关的接口芯片的端口中； CPU向外设的输出也不是直接送入外设，而是先送入端口中，再由相关的芯片送到外设。 CPU还可以向外设输出控制命令，而这些控制命令也是先送到相关芯片的端口中，然后再由相关的芯片根据命令对外设实施控制。 即：CPU通过端口和外部设备进行联系 当CPU外部有需要处理的事情发生的时候，比如说，外设的输入到达，相关芯片将向CPU发出相应的中断信息。CPU在执行完当前指令后，可以检测到发送过来的中断信息，引发中断过程，处理外设的输入。 PC系统中，外中断源有两类 1、可屏蔽中断 可屏蔽中断是CPU可以不响应的外中断。CPU是否响应可屏蔽中断，要看标志寄存器的IF位的设置。 当CPU检测到可屏蔽中断信息时，如果IF=1，则CPU在执行完当前指令后响应中断，引发中断过程；如果IF=0，则不响应可屏蔽中断。 可屏蔽中断信息来自于CPU外部，中断类型码是通过数据总线送入CPU的；而内中断的中断类型码是在CPU内部产生的。 中断过程中将IF置0的原因就是，在进入中断处理程序后，禁止其他的可屏蔽中断。 如果在中断处理程序中需要处理可屏蔽中断，可以用指令将IF置1。 8086CPU提供的设置IF的指令：sti，设置IF=1；cli，设置IF=0。 2、不可屏蔽中断 不可屏蔽中断是CPU必须响应的外中断。当CPU检测到不可屏蔽中断信息时，则在执行完当前指令后，立即响应，引发中断过程。 对于8086CPU，不可屏蔽中断的中断类型码固定为2，所以中断过程中，不需要取中断类型码。则不可屏蔽中断的中断过程为：①标志寄存器入栈，IF=0，TF=0；②CS、IP入栈；③（IP）=（8），（CS）=（0AH）。 几乎所有由外设引发的外中断，都是可屏蔽中断。当外设有需要处理的事件（比如说键盘输入）发生时，相关芯片向CPU发出可屏蔽中断信息。不可屏蔽中断是在系统中有必须处理的紧急情况发生时用来通知CPU的中断信息。 ","date":"2023-01-29","objectID":"/%E6%B1%87%E7%BC%96%E4%B9%8B%E5%A4%96%E4%B8%AD%E6%96%AD/:1:0","tags":[],"title":"汇编之外中断","uri":"/%E6%B1%87%E7%BC%96%E4%B9%8B%E5%A4%96%E4%B8%AD%E6%96%AD/"},{"categories":["汇编"],"content":"2、PC机键盘的处理过程 键盘中有一个芯片对键盘上的每一个键的开关状态进行扫描。按下一个键时，开关接通，该芯片就产生一个扫描码，扫描码说明了按下的键在键盘上的位置。扫描码被送入主板上的相关接口芯片的寄存器中，该寄存器的端口地址为60h。松开按下的键时，也产生一个扫描码，扫描码说明了松开的键在键盘上的位置。松开按键时产生的扫描码也被送入60h端口中。 一般将按下一个键时产生的扫描码称为通码，松开一个键产生的扫描码称为断码。 扫描码长度为一个字节，通码的第7位为0，断码的第7位为1 即：断码 = 通码 + 80h。比如，g键的通码为22h，断码为a2h 键盘的输入到达60h端口时，相关的芯片就会向CPU发出中断类型码为9的可屏蔽中断信息。CPU检测到该中断信息后，如果IF=1，则响应中断，引发中断过程，转去执行int 9中断例程。 BIOS提供了int 9中断例程，用来进行基本的键盘输入处理，主要的工作如下： （1）读出60h端口中的扫描码； （2）如果是字符键的扫描码，将该扫描码和它所对应的字符码（即ASCII码）送入内存中的BIOS键盘缓冲区； 如果是控制键（比如Ctrl）和切换键（比如CapsLock）的扫描码，则将其转变为状态字节写入内存中存储状态字节的单元； （3）对键盘系统进行相关的控制，比如说，向相关芯片发出应答信息。 BIOS键盘缓冲区可以存储15个键盘输入，一个键盘输入用一个字单元存放，高位字节存放扫描码，低位字节存放字符码。 0040:17单元存储键盘状态字节，该字节记录了控制键和切换键的状态。键盘状态字节各位记录的信息如下。 0 右shift状态 置1表示按下右shift键 1 左shift状态 置1表示按下左shift键 2 Ctrl状态 置1表示按下Ctrl键 3 Alt状态 置1表示按下Alt键 4 ScrollLock状态 置1表示Scroll指示灯亮 5 NumLock状态 置1表示小键盘输入的是数字 6 CapsLock状态 置1表示输入大写字母 7 Insert状态 置1表示处于删除态 编写int 9中断例程 ;编程：在屏幕中间依次显示“a”~“z”，并可以让人看清。在显示的过程中，按下'Esc'键后，改变显示的颜色。 ;完整功能代码： assume cs:code stack segment db 128 dup (0) stack ends data segment dw 0,0 data ends code segment start: mov ax,stack mov ss,ax mov sp,128 mov ax,data mov ds,ax mov ax,0 mov es,ax push es:[9*4] pop ds:[0] push es:[9*4+2] pop ds:[2] ;将原来的int 9中断例程的入口地址保存在ds:0、ds:2单元中 mov word ptr es:[9*4], offset int9 mov es:[9*4+2], cs ;在中断向量表中设置新的int 9中断例程的入口地址 ;显示字符串 mov ax, 0b800h mov es, ax mov ah, 'a' s: mov es:[160*12+40*2], ah call delay inc ah cmp ah, 'z' jna s mov ax,0 mov es,ax push ds:[0] pop es:[9*4] push ds;[2] pop es;[9*4+2] ;将中断向量表中int 9中断例程的入口恢复为原来的地址 mov ax,4c00h int 21h ;将循环延时的程序段写为一个子程序 delay: push ax push dx mov dx, 2000h ;用两个16位寄存器来存放32位的循环次数 mov ax, 0 s1: sub ax, 1 sbb dx, 0 cmp ax, 0 jne s1 cmp dx, 0 jne s1 pop dx pop ax ret ;------以下为新的int 9中断例程-------------------- int9: push ax push bx push es in al, 60h;从端口60h读出键盘的输入 pushf ;标志寄存器入栈 pushf pop bx and bh,11111100b push bx popf ;TF=0,IF=0 call dword ptr ds:[0] ;对int指令进行模拟，调用原来的int 9中断例程 cmp al,1 jne int9ret mov ax,0b800h mov es,ax inc byte ptr es:[160*12+40*2+1] ;属性增加1，改变颜色 int9ret: pop es pop bx pop ax iret code ends end start CPU对外设输入的通常处理方法 （1）外设的输入送入端口； （2）向CPU发出外中断（可屏蔽中断）信息； （3）CPU检测到可屏蔽中断信息，如果IF=1，CPU在执行完当前指令后响应中断，执行相应的中断例程； （4）可在中断例程中实现对外设输入的处理。 端口和中断机制，是CPU进行I/O的基础。 ","date":"2023-01-29","objectID":"/%E6%B1%87%E7%BC%96%E4%B9%8B%E5%A4%96%E4%B8%AD%E6%96%AD/:2:0","tags":[],"title":"汇编之外中断","uri":"/%E6%B1%87%E7%BC%96%E4%B9%8B%E5%A4%96%E4%B8%AD%E6%96%AD/"},{"categories":["汇编"],"content":"[toc] 端口 在PC机系统中，和CPU通过总线相连的芯片除各种存储器外，还有以下3种芯片。 各种接口卡（比如，网卡、显卡）上的接口芯片，它们控制接口卡进行工作； 主板上的接口芯片，CPU通过它们对部分外设进行访问； 其他芯片，用来存储相关的系统信息，或进行相关的输入输出处理。 在这些芯片中，都有一组可以由CPU读写的寄存器。这些寄存器，它们在物理上可能处于不同的芯片中， 但是它们在以下两点上相同。 都和CPU的总线相连，这种连接是通过它们所在的芯片进行的； CPU对它们（芯片上的寄存器）进行读或写的时候都通过控制线向它们所在的芯片发出端口读写命令。 从CPU的角度，将这些寄存器都当作端口，对它们进行统一编址，从而建立了一个统一的端口地址空间。 每一个端口在地址空间中都有一个地址。在访问端口的时候，CPU通过端口地址来定位端口。因为端口所在的芯片和CPU通过总线相连， CPU可以直接读写以下3个地方的数据。 CPU内部的寄存器 内存单元 端口 ","date":"2023-01-28","objectID":"/%E6%B1%87%E7%BC%96%E4%B9%8B%E7%AB%AF%E5%8F%A3/:0:0","tags":[],"title":"汇编之端口","uri":"/%E6%B1%87%E7%BC%96%E4%B9%8B%E7%AB%AF%E5%8F%A3/"},{"categories":["汇编"],"content":"1、端口的读写 端口地址和内存地址一样，通过地址总线来传送。在PC系统中，CPU最多可以定位64KB个不同的端口。则端口地址的范围为0-65535。 端口的读写指令只有两条：in和out，分别用于从端口读取数据和往端口写入数据。 在in和out指令中，只能使用ax或al来存放从端口中读入的数据或要发送到端口中的数据。 ;对0~255以内的端口进行读写时： in al, 20h ;从20h端口读入一个字节 out 20h, al ;往20h端口写入一个字节 ;对256~65535的端口进行读写时，端口号放在dx中： mov dx, 3f8h ;将端口号3f8h送入dx in al, dx ;从3f8h端口读入一个字节 out dx, al ;向3f8h端口写入一个字节 ","date":"2023-01-28","objectID":"/%E6%B1%87%E7%BC%96%E4%B9%8B%E7%AB%AF%E5%8F%A3/:1:0","tags":[],"title":"汇编之端口","uri":"/%E6%B1%87%E7%BC%96%E4%B9%8B%E7%AB%AF%E5%8F%A3/"},{"categories":["汇编"],"content":"2、CMOS RAM芯片 PC机中，有一个CMOS RAM芯片，一般简称为CMOS。此芯片的特征如下 包含一个实时钟和一个有128个存储单元的RAM存储器 该芯片靠电池供电。关机后内部的实时钟正常工作，RAM中的信息不丢失 128个字节的RAM中，内部实时钟占用0~0dh单元来保存时间信息，其余大部分单元用于保存系统配置信息，供系统启动时BIOS程序读取。BIOS也提供了相关的程序，使我们可以在开机的时候配置CMOS RAM中的系统信息。 该芯片内部有两个端口，端口地址为70h和71h。CPU通过这两个端口来读写CMOS RAM 70h为地址端口，存放要访问的CMOS RAM单元的地址；71h为数据端口，存放从选定的CMOS RAM单元中读取的数据，或要写入到其中的数据。 可见，CPU对CMOS RAM的读写分两步进行，比如，读CMOS RAM的2号单元： ①将2送入端口70h； ②从端口71h读出2号单元的内容。 CMOS RAM中存储的时间信息 在CMOS RAM中，存放着当前的时间：年、月、日、时、分、秒。长度都为1个字节， 存放单元为： 9 8 7 6 5 4 3 2 1 0 年 月 日 时 分 秒 BCD码是以4位二进制数表示十进制数码的编码方法 4 == 0100B 一个字节可表示两个BCD码。则CMOS RAM存储时间信息的单元中，存储了用两个BCD码表示的两位十进制数，高4位的BCD码表示十位，低4位的BCD码表示个位。比如，00010100b表示14。 ;编程，在屏幕中间显示当前的月份。 assume cs:code code segment start: mov al，8 ;从CMOS RAM的8号单元读出当前月份的BCD码。 out 70h，al in al, 71h ;从数据端口71h中取得指定单元中的数据： mov ah, al ;al中为从CMOS RAM的8号单元中读出的数据 mov cl, 4 shr ah, cl ;ah中为月份的十位数码值,左移四位空出四位 and al, 00001111b ;al中为月份的个位数码值 add ah, 30h ;BCD码值+30h=十进制数对应的ASCII add al, 30h mov bx, 0b800h mov es, bx mov byte ptr es:[160*12+40*2], ah ;显示月份的十位数码 mov byte ptr es:[160*12+40*2+2], al ;接着显示月份的个位数码 mov ax，4c00h int 21h code ends end start ","date":"2023-01-28","objectID":"/%E6%B1%87%E7%BC%96%E4%B9%8B%E7%AB%AF%E5%8F%A3/:2:0","tags":[],"title":"汇编之端口","uri":"/%E6%B1%87%E7%BC%96%E4%B9%8B%E7%AB%AF%E5%8F%A3/"},{"categories":["汇编"],"content":"3、shl和shr指令 shl和shr是逻辑移位指令 shl是逻辑左移指令，它的功能为： 将一个寄存器或内存单元中的数据向左移位； 将最后移出的一位写入CF中； 最低位用0补充。 shr是逻辑右移指令，同理 mov al, 01001000b shl al, 1 ;将a1中的数据左移一位执行后（al）=10010000b，CF=0。 mov al, 01010001b mov cl, 3 ;如果移动位数大于1时，必须将移动位数放在cl中 shl al, c1 mov al, 10000001b shr al, 1 ;将al中的数据右移一位执行后（al）=01000000b，CF=1。 将X逻辑左移一位，相当于执行X=X*2 将X逻辑右移一位，相当于执行X=X/2 ","date":"2023-01-28","objectID":"/%E6%B1%87%E7%BC%96%E4%B9%8B%E7%AB%AF%E5%8F%A3/:3:0","tags":[],"title":"汇编之端口","uri":"/%E6%B1%87%E7%BC%96%E4%B9%8B%E7%AB%AF%E5%8F%A3/"},{"categories":["汇编"],"content":"4、总结 什么是端口？ CPU通过总线连着很多芯片（网卡、显卡、某些外设）。CPU和这些芯片“沟通”的渠道就是端口。例如使用cmos ram芯片时，CPU通过地址总线将地址信息传输到这个芯片接收地址信号的端口上，这样就可以指定芯片内部的CPU指令的操作对象地址，之后，数据则会通过芯片的另一个端口进行读或写。 对端口的读写实际上就是遵照“先传地址，再操作数据”的原则，本质上就是对和CPU相联的芯片里面的寄存器进行读写。 ","date":"2023-01-28","objectID":"/%E6%B1%87%E7%BC%96%E4%B9%8B%E7%AB%AF%E5%8F%A3/:4:0","tags":[],"title":"汇编之端口","uri":"/%E6%B1%87%E7%BC%96%E4%B9%8B%E7%AB%AF%E5%8F%A3/"},{"categories":["redis"],"content":"[toc] ","date":"2023-01-26","objectID":"/redis%E9%AB%98%E7%BA%A7%E7%AF%87%E4%B9%8B%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5/:0:0","tags":[],"title":"Redis高级篇之最佳实践","uri":"/redis%E9%AB%98%E7%BA%A7%E7%AF%87%E4%B9%8B%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5/"},{"categories":["redis"],"content":"Redis高级篇之最佳实践 今日内容 Redis键值设计 批处理优化 服务端优化 集群最佳实践 ","date":"2023-01-26","objectID":"/redis%E9%AB%98%E7%BA%A7%E7%AF%87%E4%B9%8B%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5/:1:0","tags":[],"title":"Redis高级篇之最佳实践","uri":"/redis%E9%AB%98%E7%BA%A7%E7%AF%87%E4%B9%8B%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5/"},{"categories":["redis"],"content":"1、Redis键值设计 ","date":"2023-01-26","objectID":"/redis%E9%AB%98%E7%BA%A7%E7%AF%87%E4%B9%8B%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5/:2:0","tags":[],"title":"Redis高级篇之最佳实践","uri":"/redis%E9%AB%98%E7%BA%A7%E7%AF%87%E4%B9%8B%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5/"},{"categories":["redis"],"content":"1.1、优雅的key结构 Redis的Key虽然可以自定义，但最好遵循下面的几个最佳实践约定： 遵循基本格式：[业务名称]:[数据名]:[id] 长度不超过44字节 不包含特殊字符 例如：我们的登录业务，保存用户信息，其key可以设计成如下格式： 这样设计的好处： 可读性强 避免key冲突 方便管理 更节省内存： key是string类型，底层编码包含int、embstr和raw三种。embstr在小于44字节使用，采用连续内存空间，内存占用更小。当字节数大于44字节时，会转为raw模式存储，在raw模式下，内存空间不是连续的，而是采用一个指针指向了另外一段内存空间，在这段空间里存储SDS内容，这样空间不连续，访问的时候性能也就会收到影响，还有可能产生内存碎片 ","date":"2023-01-26","objectID":"/redis%E9%AB%98%E7%BA%A7%E7%AF%87%E4%B9%8B%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5/:2:1","tags":[],"title":"Redis高级篇之最佳实践","uri":"/redis%E9%AB%98%E7%BA%A7%E7%AF%87%E4%B9%8B%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5/"},{"categories":["redis"],"content":"1.2、拒绝BigKey BigKey通常以Key的大小和Key中成员的数量来综合判定，例如： Key本身的数据量过大：一个String类型的Key，它的值为 5 MB Key中的成员数过多：一个ZSET类型的Key，它的成员数量为10,000个 Key中成员的数据量过大：一个Hash类型的Key，它的成员数量虽然只有1,000个但这些成员的Value（值）总大小为100 MB 那么如何判断元素的大小呢？redis也给我们提供了命令 推荐值： 单个key的value小于10KB 对于集合类型的key，建议元素数量小于1000 1.2.1、BigKey的危害 网络阻塞 对BigKey执行读请求时，少量的QPS就可能导致带宽使用率被占满，导致Redis实例，乃至所在物理机变慢 数据倾斜 BigKey所在的Redis实例内存使用率远超其他实例，无法使数据分片的内存资源达到均衡 Redis阻塞 对元素较多的hash、list、zset等做运算会耗时较旧，使主线程被阻塞 CPU压力 对BigKey的数据序列化和反序列化会导致CPU的使用率飙升，影响Redis实例和本机其它应用 1.2.2、如何发现BigKey ①redis-cli –bigkeys 利用redis-cli提供的–bigkeys参数，可以遍历分析所有key，并返回Key的整体统计信息与每个数据的Top1的big key 命令：redis-cli -a 密码 --bigkeys ②scan扫描 自己编程，利用scan扫描Redis中的所有key，利用strlen、hlen等命令判断key的长度（此处不建议使用MEMORY USAGE） scan 命令调用完后每次会返回2个元素，第一个是下一次迭代的光标，第一次光标会设置为0，当最后一次scan 返回的光标等于0时，表示整个scan遍历结束了，第二个返回的是List，一个匹配的key的数组 import com.heima.jedis.util.JedisConnectionFactory; import org.junit.jupiter.api.AfterEach; import org.junit.jupiter.api.BeforeEach; import org.junit.jupiter.api.Test; import redis.clients.jedis.Jedis; import redis.clients.jedis.ScanResult; import java.util.HashMap; import java.util.List; import java.util.Map; public class JedisTest { private Jedis jedis; @BeforeEach void setUp() { // 1.建立连接 // jedis = new Jedis(\"192.168.150.101\", 6379); jedis = JedisConnectionFactory.getJedis(); // 2.设置密码 jedis.auth(\"123321\"); // 3.选择库 jedis.select(0); } final static int STR_MAX_LEN = 10 * 1024; final static int HASH_MAX_LEN = 500; @Test void testScan() { int maxLen = 0; long len = 0; String cursor = \"0\"; do { // 扫描并获取一部分key ScanResult\u003cString\u003e result = jedis.scan(cursor); // 记录cursor cursor = result.getCursor(); List\u003cString\u003e list = result.getResult(); if (list == null || list.isEmpty()) { break; } // 遍历 for (String key : list) { // 判断key的类型 String type = jedis.type(key); switch (type) { case \"string\": len = jedis.strlen(key); maxLen = STR_MAX_LEN; break; case \"hash\": len = jedis.hlen(key); maxLen = HASH_MAX_LEN; break; case \"list\": len = jedis.llen(key); maxLen = HASH_MAX_LEN; break; case \"set\": len = jedis.scard(key); maxLen = HASH_MAX_LEN; break; case \"zset\": len = jedis.zcard(key); maxLen = HASH_MAX_LEN; break; default: break; } if (len \u003e= maxLen) { System.out.printf(\"Found big key : %s, type: %s, length or size: %d %n\", key, type, len); } } } while (!cursor.equals(\"0\")); } @AfterEach void tearDown() { if (jedis != null) { jedis.close(); } } } ③第三方工具 利用第三方工具，如 Redis-Rdb-Tools 分析RDB快照文件，全面分析内存使用情况 https://github.com/sripathikrishnan/redis-rdb-tools ④网络监控 自定义工具，监控进出Redis的网络数据，超出预警值时主动告警 一般阿里云搭建的云服务器就有相关监控页面 1.2.3、如何删除BigKey BigKey内存占用较多，即便时删除这样的key也需要耗费很长时间，导致Redis主线程阻塞，引发一系列问题。 redis 3.0 及以下版本 如果是集合类型，则遍历BigKey的元素，先逐个删除子元素，最后删除BigKey Redis 4.0以后 Redis在4.0后提供了异步删除的命令：unlink ","date":"2023-01-26","objectID":"/redis%E9%AB%98%E7%BA%A7%E7%AF%87%E4%B9%8B%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5/:2:2","tags":[],"title":"Redis高级篇之最佳实践","uri":"/redis%E9%AB%98%E7%BA%A7%E7%AF%87%E4%B9%8B%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5/"},{"categories":["redis"],"content":"1.3、恰当的数据类型 例1 比如存储一个User对象，我们有三种存储方式： ①方式一：json字符串 user:1 {“name”: “Jack”, “age”: 21} 优点：实现简单粗暴 缺点：数据耦合，不够灵活 ②方式二：字段打散 user:1:name Jack user:1:age 21 优点：可以灵活访问对象任意字段 缺点：占用空间大、没办法做统一控制 ③方式三：hash（推荐） user:1 name jack age 21 优点：底层使用ziplist，空间占用小，可以灵活访问对象的任意字段 缺点：代码相对复杂 例2 假如有hash类型的key，其中有100万对field和value，field是自增id，这个key存在什么问题？如何优化？ key field value someKey id:0 value0 ..... ..... id:999999 value999999 存在的问题： hash的entry数量超过500时，会使用哈希表而不是ZipList，内存占用较多 可以通过hash-max-ziplist-entries配置entry上限。但是如果entry过多就会导致BigKey问题 方案一 拆分为string类型 key value id:0 value0 ..... ..... id:999999 value999999 存在的问题： string结构底层没有太多内存优化，内存占用较多 想要批量获取这些数据比较麻烦 方案二 拆分为小的hash，将 id / 100 作为key， 将id % 100 作为field，这样每100个元素为一个Hash key field value key:0 id:00 value0 ..... ..... id:99 value99 key:1 id:00 value100 ..... ..... id:99 value199 .... key:9999 id:00 value999900 ..... ..... id:99 value999999 package com.heima.test; import com.heima.jedis.util.JedisConnectionFactory; import org.junit.jupiter.api.AfterEach; import org.junit.jupiter.api.BeforeEach; import org.junit.jupiter.api.Test; import redis.clients.jedis.Jedis; import redis.clients.jedis.Pipeline; import redis.clients.jedis.ScanResult; import java.util.HashMap; import java.util.List; import java.util.Map; public class JedisTest { private Jedis jedis; @BeforeEach void setUp() { // 1.建立连接 // jedis = new Jedis(\"192.168.150.101\", 6379); jedis = JedisConnectionFactory.getJedis(); // 2.设置密码 jedis.auth(\"123321\"); // 3.选择库 jedis.select(0); } @Test void testSetBigKey() { Map\u003cString, String\u003e map = new HashMap\u003c\u003e(); for (int i = 1; i \u003c= 650; i++) { map.put(\"hello_\" + i, \"world!\"); } jedis.hmset(\"m2\", map); } @Test void testBigHash() { Map\u003cString, String\u003e map = new HashMap\u003c\u003e(); for (int i = 1; i \u003c= 100000; i++) { map.put(\"key_\" + i, \"value_\" + i); } jedis.hmset(\"test:big:hash\", map); } @Test void testBigString() { for (int i = 1; i \u003c= 100000; i++) { jedis.set(\"test:str:key_\" + i, \"value_\" + i); } } @Test void testSmallHash() { int hashSize = 100; Map\u003cString, String\u003e map = new HashMap\u003c\u003e(hashSize); for (int i = 1; i \u003c= 100000; i++) { int k = (i - 1) / hashSize; int v = i % hashSize; map.put(\"key_\" + v, \"value_\" + v); if (v == 0) { jedis.hmset(\"test:small:hash_\" + k, map); } } } @AfterEach void tearDown() { if (jedis != null) { jedis.close(); } } } ","date":"2023-01-26","objectID":"/redis%E9%AB%98%E7%BA%A7%E7%AF%87%E4%B9%8B%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5/:2:3","tags":[],"title":"Redis高级篇之最佳实践","uri":"/redis%E9%AB%98%E7%BA%A7%E7%AF%87%E4%B9%8B%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5/"},{"categories":["redis"],"content":"1.4、总结 Key的最佳实践 固定格式：[业务名]:[数据名]:[id] 足够简短：不超过44字节 不包含特殊字符 Value的最佳实践： 合理的拆分数据，拒绝BigKey 选择合适数据结构 Hash结构的entry数量不要超过1000 设置合理的超时时间 ","date":"2023-01-26","objectID":"/redis%E9%AB%98%E7%BA%A7%E7%AF%87%E4%B9%8B%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5/:2:4","tags":[],"title":"Redis高级篇之最佳实践","uri":"/redis%E9%AB%98%E7%BA%A7%E7%AF%87%E4%B9%8B%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5/"},{"categories":["redis"],"content":"2、批处理优化 ","date":"2023-01-26","objectID":"/redis%E9%AB%98%E7%BA%A7%E7%AF%87%E4%B9%8B%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5/:3:0","tags":[],"title":"Redis高级篇之最佳实践","uri":"/redis%E9%AB%98%E7%BA%A7%E7%AF%87%E4%B9%8B%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5/"},{"categories":["redis"],"content":"2.1、Pipeline 2.1.1、我们的客户端与redis服务器是这样交互的 单个命令的执行流程 N条命令的执行流程 redis处理指令是很快的，主要花费的时候在于网络传输。于是乎很容易想到将多条指令批量的传输给redis 2.1.2、MSet Redis提供了很多Mxxx这样的命令，可以实现批量插入数据，例如： mset hmset 利用mset批量插入10万条数据 @Test void testMxx() { String[] arr = new String[2000]; int j; long b = System.currentTimeMillis(); for (int i = 1; i \u003c= 100000; i++) { j = (i % 1000) \u003c\u003c 1; arr[j] = \"test:key_\" + i; arr[j + 1] = \"value_\" + i; if (j == 0) { jedis.mset(arr); } } long e = System.currentTimeMillis(); System.out.println(\"time: \" + (e - b)); } 2.1.3、Pipeline MSET虽然可以批处理，但是却只能操作部分数据类型，因此如果有对复杂数据类型的批处理需要，建议使用Pipeline @Test void testPipeline() { // 创建管道 Pipeline pipeline = jedis.pipelined(); long b = System.currentTimeMillis(); for (int i = 1; i \u003c= 100000; i++) { // 放入命令到管道 pipeline.set(\"test:key_\" + i, \"value_\" + i); if (i % 1000 == 0) { // 每放入1000条命令，批量执行 pipeline.sync(); } } long e = System.currentTimeMillis(); System.out.println(\"time: \" + (e - b)); } ","date":"2023-01-26","objectID":"/redis%E9%AB%98%E7%BA%A7%E7%AF%87%E4%B9%8B%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5/:3:1","tags":[],"title":"Redis高级篇之最佳实践","uri":"/redis%E9%AB%98%E7%BA%A7%E7%AF%87%E4%B9%8B%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5/"},{"categories":["redis"],"content":"2.2、集群下的批处理 Redis Cluster及hash slot 算法 如MSET或Pipeline这样的批处理需要在一次请求中携带多条命令，而此时如果Redis是一个集群，那批处理命令的多个key必须落在一个插槽中(否则redis会直接报错)，否则就会导致执行失败。大家可以想一想这样的要求其实很难实现，因为我们在批处理时，可能一次要插入很多条数据，这些数据很有可能不会都落在相同的节点上，这就会导致报错了 这个时候，我们可以找到4种解决方案 第一种方案：串行执行，所以这种方式没有什么意义，当然，执行起来就很简单了，缺点就是耗时过久。 第二种方案：串行slot，简单来说，就是执行前，客户端先计算一下对应的key的slot，一样slot的key就放到一个组里边，不同的，就放到不同的组里边，然后对每个组执行pipeline的批处理，他就能串行执行各个组的命令，这种做法比第一种方法耗时要少，但是缺点呢，相对来说复杂一点，所以这种方案还需要优化一下 第三种方案：并行slot，相较于第二种方案，在分组完成后串行执行，第三种方案，就变成了并行执行各个命令，所以他的耗时就非常短，但是实现呢，也更加复杂。 第四种：hash_tag，redis计算key的slot的时候，其实是根据key的有效部分来计算的，通过这种方式就能一次处理所有的key，这种方式耗时最短，实现也简单，但是如果通过操作key的有效部分，那么就会导致所有的key都落在一个节点上，产生数据倾斜的问题，所以我们推荐使用第三种方式。 我们可以在redis的key字符串前添加{}符号，这样可以将大括号里面的字符内容作为有效部分，让redis计算key，寻找slot的有效值，从而找到对应master节点。这样，我们可以做到：将批处理命令的某一部分放到同一个节点时，我们需要在大括号里分配相同的字符值，这样在计算查找slot的对应master节点才会相同。 2.2.1 串行化执行代码实践 public class JedisClusterTest { private JedisCluster jedisCluster; @BeforeEach void setUp() { // 配置连接池 JedisPoolConfig poolConfig = new JedisPoolConfig(); poolConfig.setMaxTotal(8); poolConfig.setMaxIdle(8); poolConfig.setMinIdle(0); poolConfig.setMaxWaitMillis(1000); HashSet\u003cHostAndPort\u003e nodes = new HashSet\u003c\u003e(); nodes.add(new HostAndPort(\"192.168.150.101\", 7001)); nodes.add(new HostAndPort(\"192.168.150.101\", 7002)); nodes.add(new HostAndPort(\"192.168.150.101\", 7003)); nodes.add(new HostAndPort(\"192.168.150.101\", 8001)); nodes.add(new HostAndPort(\"192.168.150.101\", 8002)); nodes.add(new HostAndPort(\"192.168.150.101\", 8003)); jedisCluster = new JedisCluster(nodes, poolConfig); } @Test void testMSet() { jedisCluster.mset(\"name\", \"Jack\", \"age\", \"21\", \"sex\", \"male\"); } @Test void testMSet2() { Map\u003cString, String\u003e map = new HashMap\u003c\u003e(3); map.put(\"name\", \"Jack\"); map.put(\"age\", \"21\"); map.put(\"sex\", \"Male\"); //对Map数据进行分组。根据相同的slot放在一个分组 //key就是slot，value就是一个组 Map\u003cInteger, List\u003cMap.Entry\u003cString, String\u003e\u003e\u003e result = map.entrySet() .stream() .collect(Collectors.groupingBy( entry -\u003e ClusterSlotHashUtil.calculateSlot(entry.getKey())) ); //串行的去执行mset的逻辑 for (List\u003cMap.Entry\u003cString, String\u003e\u003e list : result.values()) { String[] arr = new String[list.size() * 2]; int j = 0; for (int i = 0; i \u003c list.size(); i++) { j = i\u003c\u003c2; Map.Entry\u003cString, String\u003e e = list.get(0); arr[j] = e.getKey(); arr[j + 1] = e.getValue(); } jedisCluster.mset(arr); } } @AfterEach void tearDown() { if (jedisCluster != null) { jedisCluster.close(); } } } 2.2.2 Spring集群环境下批处理代码 @Test void testMSetInCluster() { Map\u003cString, String\u003e map = new HashMap\u003c\u003e(3); map.put(\"name\", \"Rose\"); map.put(\"age\", \"21\"); map.put(\"sex\", \"Female\"); stringRedisTemplate.opsForValue().multiSet(map); List\u003cString\u003e strings = stringRedisTemplate.opsForValue().multiGet(Arrays.asList(\"name\", \"age\", \"sex\")); strings.forEach(System.out::println); } 原理分析 在RedisAdvancedClusterAsyncCommandsImpl 类中 首先根据slotHash算出来一个partitioned的map，map中的key就是slot，而他的value就是对应的对应相同slot的key对应的数据 通过 RedisFuture mset = super.mset(op);进行异步的消息发送 @Override public RedisFuture\u003cString\u003e mset(Map\u003cK, V\u003e map) { Map\u003cInteger, List\u003cK\u003e\u003e partitioned = SlotHash.partition(codec, map.keySet()); if (partitioned.size() \u003c 2) { return super.mset(map); } Map\u003cInteger, RedisFuture\u003cString\u003e\u003e executions = new HashMap\u003c\u003e(); for (Map.Entry\u003cInteger, List\u003cK\u003e\u003e entry : partitioned.entrySet()) { Map\u003cK, V\u003e op = new HashMap\u003c\u003e(); entry.getValue().forEach(k -\u003e op.put(k, map.get(k))); RedisFuture\u003cString\u003e mset = super.mset(op); executions.put(entry.getKey(), mset); } return MultiNodeExecution.firstOfAsync(executions); } ","date":"2023-01-26","objectID":"/redis%E9%AB%98%E7%BA%A7%E7%AF%87%E4%B9%8B%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5/:3:2","tags":[],"title":"Redis高级篇之最佳实践","uri":"/redis%E9%AB%98%E7%BA%A7%E7%AF%87%E4%B9%8B%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5/"},{"categories":["redis"],"content":"3、服务器端优化-持久化配置 Redis的持久化虽然可以保证数据安全，但也会带来很多额外的开销，因此持久化请遵循下列建议： 用来做缓存的Redis实例尽量不要开启持久化功能 建议关闭RDB持久化功能，使用AOF持久化 利用脚本定期在slave节点做RDB，实现数据备份 设置合理的rewrite阈值，避免频繁的bgrewrite 配置no-appendfsync-on-rewrite = yes，禁止在rewrite期间做aof，避免因AOF引起的阻塞 部署有关建议： Redis实例的物理机要预留足够内存，应对fork和rewrite 单个Redis实例内存上限不要太大，例如4G或8G。可以加快fork的速度、减少主从同步、数据迁移压力 不要与CPU密集型应用部署在一起 不要与高硬盘负载应用一起部署。例如：数据库、消息队列 ","date":"2023-01-26","objectID":"/redis%E9%AB%98%E7%BA%A7%E7%AF%87%E4%B9%8B%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5/:4:0","tags":[],"title":"Redis高级篇之最佳实践","uri":"/redis%E9%AB%98%E7%BA%A7%E7%AF%87%E4%B9%8B%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5/"},{"categories":["redis"],"content":"4、服务器端优化-慢查询优化 ","date":"2023-01-26","objectID":"/redis%E9%AB%98%E7%BA%A7%E7%AF%87%E4%B9%8B%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5/:5:0","tags":[],"title":"Redis高级篇之最佳实践","uri":"/redis%E9%AB%98%E7%BA%A7%E7%AF%87%E4%B9%8B%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5/"},{"categories":["redis"],"content":"4.1 什么是慢查询 并不是很慢的查询才是慢查询，而是：在Redis执行时耗时超过某个阈值的命令，称为慢查询。 慢查询的危害：由于Redis是单线程的，所以当客户端发出指令后，他们都会进入到redis底层的queue来执行，如果此时有一些慢查询的数据，就会导致大量请求阻塞，从而引起报错，所以我们需要解决慢查询问题。 慢查询的阈值可以通过配置指定： slowlog-log-slower-than：慢查询阈值，单位是微秒。默认是10000，建议1000 慢查询会被放入慢查询日志中，日志的长度有上限，可以通过配置指定： slowlog-max-len：慢查询日志（本质是一个队列）的长度。默认是128，建议1000 修改这两个配置可以使用：config set命令： ","date":"2023-01-26","objectID":"/redis%E9%AB%98%E7%BA%A7%E7%AF%87%E4%B9%8B%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5/:5:1","tags":[],"title":"Redis高级篇之最佳实践","uri":"/redis%E9%AB%98%E7%BA%A7%E7%AF%87%E4%B9%8B%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5/"},{"categories":["redis"],"content":"4.2 如何查看慢查询 知道了以上内容之后，那么咱们如何去查看慢查询日志列表呢： slowlog len：查询慢查询日志长度 slowlog get [n]：读取n条慢查询日志 slowlog reset：清空慢查询列表 ","date":"2023-01-26","objectID":"/redis%E9%AB%98%E7%BA%A7%E7%AF%87%E4%B9%8B%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5/:5:2","tags":[],"title":"Redis高级篇之最佳实践","uri":"/redis%E9%AB%98%E7%BA%A7%E7%AF%87%E4%B9%8B%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5/"},{"categories":["redis"],"content":"5、服务器端优化-命令及安全配置 安全可以说是服务器端一个非常重要的话题，如果安全出现了问题，那么一旦这个漏洞被一些坏人知道了之后，并且进行攻击，那么这就会给咱们的系统带来很多的损失，所以我们这节课就来解决这个问题。 Redis会绑定在0.0.0.0:6379，这样将会将Redis服务暴露到公网上，而Redis如果没有做身份认证，会出现严重的安全漏洞. 漏洞重现方式：https://cloud.tencent.com/developer/article/1039000 为什么会出现不需要密码也能够登录呢，主要是Redis考虑到每次登录都比较麻烦，所以Redis就有一种ssh免秘钥登录的方式，生成一对公钥和私钥，私钥放在本地，公钥放在redis端，当我们登录时服务器，再登录时候，他会去解析公钥和私钥，如果没有问题，则不需要利用redis的登录也能访问，这种做法本身也很常见，但是这里有一个前提，前提就是公钥必须保存在服务器上，才行，但是Redis的漏洞在于在不登录的情况下，也能把秘钥送到Linux服务器，从而产生漏洞 漏洞出现的核心的原因有以下几点： Redis未设置密码 利用了Redis的config set命令动态修改Redis配置 使用了Root账号权限启动Redis 所以：如何解决呢？我们可以采用如下几种方案 为了避免这样的漏洞，这里给出一些建议： Redis一定要设置密码 禁止线上使用下面命令：keys、flushall、flushdb、config set等命令。可以利用rename-command禁用。 bind：限制网卡，禁止外网网卡访问 开启防火墙 不要使用Root账户启动Redis 尽量不是有默认的端口 ","date":"2023-01-26","objectID":"/redis%E9%AB%98%E7%BA%A7%E7%AF%87%E4%B9%8B%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5/:6:0","tags":[],"title":"Redis高级篇之最佳实践","uri":"/redis%E9%AB%98%E7%BA%A7%E7%AF%87%E4%B9%8B%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5/"},{"categories":["redis"],"content":"6、服务器端优化-Redis内存划分和内存配置 当Redis内存不足时，可能导致Key频繁被删除、响应时间变长、QPS不稳定等问题。当内存使用率达到90%以上时就需要我们警惕，并快速定位到内存占用的原因。 有关碎片问题分析 Redis底层分配并不是这个key有多大，他就会分配多大，而是有他自己的分配策略，比如8,16,20等等，假定当前key只需要10个字节，此时分配8肯定不够，那么他就会分配16个字节，多出来的6个字节就不能被使用，这就是我们常说的碎片问题 进程内存问题分析： 这片内存，通常我们都可以忽略不计 缓冲区内存问题分析： 一般包括客户端缓冲区、AOF缓冲区、复制缓冲区等。客户端缓冲区又包括输入缓冲区和输出缓冲区两种。这部分内存占用波动较大，所以这片内存也是我们需要重点分析的内存问题。 内存占用 说明 数据内存 是Redis最主要的部分，存储Redis的键值信息。主要问题是BigKey问题、内存碎片问题 进程内存 Redis主进程本身运⾏肯定需要占⽤内存，如代码、常量池等等；这部分内存⼤约⼏兆，在⼤多数⽣产环境中与Redis数据占⽤的内存相⽐可以忽略。 缓冲区内存 一般包括客户端缓冲区、AOF缓冲区、复制缓冲区等。客户端缓冲区又包括输入缓冲区和输出缓冲区两种。这部分内存占用波动较大，不当使用BigKey，可能导致内存溢出。 于是我们就需要通过一些命令，可以查看到Redis目前的内存分配状态： info memory：查看内存分配的情况 memory xxx：查看key的主要占用情况 接下来我们看到了这些配置，最关键的缓存区内存如何定位和解决呢？ 内存缓冲区常见的有三种： 复制缓冲区：主从复制的repl_backlog_buf，如果太小可能导致频繁的全量复制，影响性能。通过replbacklog-size来设置，默认1mb AOF缓冲区：AOF刷盘之前的缓存区域，AOF执行rewrite的缓冲区。无法设置容量上限 客户端缓冲区：分为输入缓冲区和输出缓冲区，输入缓冲区最大1G且不能设置。输出缓冲区可以设置 以上复制缓冲区和AOF缓冲区 不会有问题，最关键就是客户端缓冲区的问题 客户端缓冲区：指的就是我们发送命令时，客户端用来缓存命令的一个缓冲区，也就是我们向redis输入数据的输入端缓冲区和redis向客户端返回数据的响应缓存区，输入缓冲区最大1G且不能设置，所以这一块我们根本不用担心，如果超过了这个空间，redis会直接断开，因为本来此时此刻就代表着redis处理不过来了，我们需要担心的就是输出端缓冲区 我们在使用redis过程中，处理大量的big value，那么会导致我们的输出结果过多，如果输出缓存区过大，会导致redis直接断开，而默认配置的情况下， 其实他是没有大小的，这就比较坑了，内存可能一下子被占满，会直接导致咱们的redis断开，所以解决方案有两个 设置一个大小 增加我们带宽的大小，避免我们出现大量数据从而直接超过了redis的承受能力 ","date":"2023-01-26","objectID":"/redis%E9%AB%98%E7%BA%A7%E7%AF%87%E4%B9%8B%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5/:7:0","tags":[],"title":"Redis高级篇之最佳实践","uri":"/redis%E9%AB%98%E7%BA%A7%E7%AF%87%E4%B9%8B%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5/"},{"categories":["redis"],"content":"7、服务器端集群优化-集群还是主从 集群虽然具备高可用特性，能实现自动故障恢复，但是如果使用不当，也会存在一些问题： 集群完整性问题 集群带宽问题 数据倾斜问题 客户端性能问题 命令的集群兼容性问题 lua和事务问题 问题1、在Redis的默认配置中，如果发现任意一个插槽不可用，则整个集群都会停止对外服务。 大家可以设想一下，如果有几个slot不能使用，那么此时整个集群都不能用了，我们在开发中，其实最重要的是可用性，所以需要把如下配置修改成no，即有slot不能使用时，我们的redis集群还是可以对外提供服务 问题2、集群带宽问题 集群节点之间会不断的互相Ping来确定集群中其它节点的状态。每次Ping携带的信息至少包括： 插槽信息 集群状态信息 集群中节点越多，集群状态信息数据量也越大，10个节点的相关信息可能达到1kb，此时每次集群互通需要的带宽会非常高，这样会导致集群中大量的带宽都会被ping信息所占用，这是一个非常可怕的问题，所以我们需要去解决这样的问题 解决途径： 避免大集群，集群节点数不要太多，最好少于1000，如果业务庞大，则建立多个集群。 避免在单个物理机中运行太多Redis实例 配置合适的cluster-node-timeout值 问题3、命令的集群兼容性问题 有关这个问题咱们已经探讨过了，当我们使用批处理的命令时，redis要求我们的key必须落在相同的slot上，然后大量的key同时操作时，是无法完成的，所以客户端必须要对这样的数据进行处理，这些方案我们之前已经探讨过了，所以不再这个地方赘述了。 问题4、lua和事务的问题 lua和事务都是要保证原子性问题，如果你的key不在一个节点，那么是无法保证lua的执行和事务的特性的，所以在集群模式是没有办法执行lua和事务的 那我们到底是集群还是主从 单体Redis（主从Redis）已经能达到万级别的QPS，并且也具备很强的高可用特性。如果主从能满足业务需求的情况下，所以如果不是在万不得已的情况下，尽量不搭建Redis集群 Redis主从就是常见的主从模式，从节点自动同步主节点数据，实现数据的热备份。 Redis哨兵就是在Redis主从上添加了一个监控系统（Redis Sentinel系统），实现故障转移，Redis哨兵会监控Redis主从节点运行状态，当主节点故障下线后，Redis哨兵会选择一个从节点充当新的主节点，继续提供服务。 Redis集群在Redis主从上添加了监控机制和数据分片机制（Redis中是分槽位），实现故障转移和数据水平扩展，Redis集群中组合了多个Redis主从，并且每个Redis主节点都负责存储集群中的一部分数据，当某个主节点故障下线后，Redis集群会选择该节点的一个从节点充当新的主节点，继续提供服务。 生产环境应该很少使用单纯的Redis主从吧，如果数据量比较少，可以使用哨兵模式，但Redis集群的稳定性、可扩展性都优于哨兵模式，所以使用Redis集群的场景应该是最多的吧。 ","date":"2023-01-26","objectID":"/redis%E9%AB%98%E7%BA%A7%E7%AF%87%E4%B9%8B%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5/:8:0","tags":[],"title":"Redis高级篇之最佳实践","uri":"/redis%E9%AB%98%E7%BA%A7%E7%AF%87%E4%B9%8B%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5/"},{"categories":["汇编"],"content":"内中断 ","date":"2023-01-24","objectID":"/%E6%B1%87%E7%BC%96%E4%B9%8B%E5%86%85%E4%B8%AD%E6%96%AD/:0:0","tags":[],"title":"汇编之内中断","uri":"/%E6%B1%87%E7%BC%96%E4%B9%8B%E5%86%85%E4%B8%AD%E6%96%AD/"},{"categories":["汇编"],"content":"1、内中断的产生 任何一个通用的CPU，都具备一种能力，可以在执行完当前正在执行的指令之后，检测到从CPU外部发送过来的或内部产生的一种特殊信息，并且可以立即对所接收到的信息进行处理。这种特殊的信息，我们可以称其为：中断信息。中断的意思是指，CPU不再接着（刚执行完的指令）向下执行，而是转去处理这个特殊信息。 中断信息可以来自CPU的内部和外部（内中断，外中断） 内中断：当CPU的内部有需要处理的事情发生的时候，将产生中断信息，引发中断过程。这种中断信息来自CPU的内部 8086CPU的内中断（下面四种情况将产生中断信息） 除法错误，比如，执行div指令产生的除法溢出； 单步执行； 执行 into指令； 执行 int指令。 中断信息中包含中断类型码，中断类型码为一个字节型数据，可以表示256种中断信息的来源（中断源） 上述的4种中断源，在8086CPU中的中断类型码如下。 除法错误：0 单步执行：1 执行into指令：4 执行int指令，该指令的格式为int n，指令中的n为字节型立即数，是提供给CPU的中断类型码。 ","date":"2023-01-24","objectID":"/%E6%B1%87%E7%BC%96%E4%B9%8B%E5%86%85%E4%B8%AD%E6%96%AD/:1:0","tags":[],"title":"汇编之内中断","uri":"/%E6%B1%87%E7%BC%96%E4%B9%8B%E5%86%85%E4%B8%AD%E6%96%AD/"},{"categories":["汇编"],"content":"2、中断处理程序、中断向量表、中断过程 中断处理程序 用来处理中断信息的程序被称为中断处理程序。 根据CPU的设计，中断类型码的作用就是用来定位中断处理程序。比如CPU根据中断类型码4，就可以找到4号中断的处理程序 中断向量表 中断向量就是中断处理程序的入口地址。中断向量表就是中断处理程序入口地址的列表 CPU用8位的中断类型码通过中断向量表找到相应的中断处理程序的入口地址 中断过程 中断过程的主要任务就是用中断类型码在中断向量表中找到中断处理程序的入口地址，设置CS和IP 简要描述如下 取得中断类型码N； pushf TF=0，IF=0 （为什么这样参考单步中断） push CS , push IP （IP）=（N * 4），（CS）=（N * 4 + 2） 硬件在完成中断过程后，CS:IP将指向中断处理程序的入口，CPU开始执行中断处理程序。 ","date":"2023-01-24","objectID":"/%E6%B1%87%E7%BC%96%E4%B9%8B%E5%86%85%E4%B8%AD%E6%96%AD/:2:0","tags":[],"title":"汇编之内中断","uri":"/%E6%B1%87%E7%BC%96%E4%B9%8B%E5%86%85%E4%B8%AD%E6%96%AD/"},{"categories":["汇编"],"content":"3、iret指令 CPU随时都可能执行中断处理程序，中断处理程序必须一直存储在内存某段空间之中 而中断处理程序的入口地址，即中断向量，必须存储在对应的中断向量表表项中。 中断处理程序的常规编写步骤： 保存用到的寄存器； 处理中断； 恢复用到的寄存器； 用iret指令返回。 iret 指令描述为：pop IP pop CS popf iret 指令执行后，CPU回到执行中断处理程序前的执行点继续执行程序 ","date":"2023-01-24","objectID":"/%E6%B1%87%E7%BC%96%E4%B9%8B%E5%86%85%E4%B8%AD%E6%96%AD/:3:0","tags":[],"title":"汇编之内中断","uri":"/%E6%B1%87%E7%BC%96%E4%B9%8B%E5%86%85%E4%B8%AD%E6%96%AD/"},{"categories":["汇编"],"content":"4、除法错误中断的处理 mov ax, 1000h mov bh, 1 div bh ;除法溢出错误 1、当CPU执行div bh时，发生了除法溢出错误，产生0号中断信息，从而引发中断过程， 2、CPU执行0号中断处理程序 3、系统中的0号中断处理程序的功能：显示提示信息“Divide overflow”后，返回到操作系统中。 编程实验 编程：编写0号中断处理程序do0，当发生除法溢出时，在屏幕中间显示“overflow！”，返回DOS。 1、0000:0200至0000:02FF的256个字节的空间所对应的中断向量表项都是空的，可以将中断处理程序do0传送到内存0000:0200处。 2、中断处理程序do0放到0000:0200,再将其地址登记在中断向量表对应表项 0号表项的地址0:0。0:0字单元存放偏移地址，0:2字单元存放段地址 将do0的段地址0存放在0000:0002字单元中，将偏移地址200H存放在0000:0000字单元 assume cs:code code segment start: mov ax, cs mov ds, ax mov si, offset do0 ;设置ds:si指向源地址 mov ax, 0 mov es, ax mov di, 200h ;设置es:di指向目的地址0000:0200 mov cx, offset do0end - offset do0 ;设置cx为传输长度 编译时给出do0部分代码长度 cld ;设置传输方向为正 rep movsb ;将do0的代码送入0:200处 mov ax, 0 ;设置中断向量表 mov es, ax mov word ptr es:[0*4], 200h mov word ptr es:[0*4+2], 0 mov ax,4c00h int 21h ;do0程序的主要任务是显示字符串 do0: jmp short do0 start db \"overflow!\" do0start: mov ax, cs mov ds, ax mov si, 202h ;设置ds:si指向字符串 mov ax, 0b800h mov es, ax mov di, 12*160+36*2 ;设置es:di指向显存空间的中间位置 mov cx, 9 ;设置cx为字符串长度 s: mov al, [si] mov es:[di], al inc si add di, 1 mov al, 02h ;设置颜色 mov es:[di], al add di, 1 loop s mov ax, 4c00h int 21h do0end: nop code ends end start ","date":"2023-01-24","objectID":"/%E6%B1%87%E7%BC%96%E4%B9%8B%E5%86%85%E4%B8%AD%E6%96%AD/:4:0","tags":[],"title":"汇编之内中断","uri":"/%E6%B1%87%E7%BC%96%E4%B9%8B%E5%86%85%E4%B8%AD%E6%96%AD/"},{"categories":["汇编"],"content":"5、单步中断 CPU在执行完一条指令之后，如果检测到标志寄存器的TF位为1，则产生单步中断，引发中断过程。单步中断的中断类型码为1 Debug是如何利用CPU所提供的单步中断的功能进行调试？如使用t命令查看寄存器状态 Debug提供了单步中断的中断处理程序，功能为显示所有寄存器中的内容后等待输入命令 在使用t命令执行指令时，Debug将TF设置为1，在CPU执行完这条指令后就引发单步中断，执行单步中断的中断处理程序，所有寄存器中的内容被显示在屏幕上，并且等待输入命令。 在进入中断处理程序之前，设置TF=0。从而避免CPU在执行中断处理程序的时候发生单步中断 ","date":"2023-01-24","objectID":"/%E6%B1%87%E7%BC%96%E4%B9%8B%E5%86%85%E4%B8%AD%E6%96%AD/:5:0","tags":[],"title":"汇编之内中断","uri":"/%E6%B1%87%E7%BC%96%E4%B9%8B%E5%86%85%E4%B8%AD%E6%96%AD/"},{"categories":["汇编"],"content":"6、int指令 int指令的格式为：int n ，n为中断类型码，它的功能是引发中断过程。 CPU执行int n指令，相当于引发一个n号中断的中断过程 在程序中使用int指令调用任何一个中断的中断处理程序(中断例程) 编写供应用程序调用的中断例程 实验1 ;求2 * 3456^2 assume cs:code code segment start: mov ax, 3456 ;(ax)=3456 int 7ch ; 调用中断7ch的中断例程，计算ax中的数据的平方 add ax, ax adc dx, dx ;存放结果，将结果乘以2 mov ax,4c00h int 21h code ends end start ;编程：安装中断7ch的中断例程 ;功能：求一word型数据的平方。 ;参数：(ax) = 要计算的数据。 ;返回值：dx、ax中存放结果的高16位和低16位。 assume cs:code code segment start: mov ax,cs mov ds,ax mov si,offset sqr ;设置ds:si指向源地址 mov ax,0 mov es,ax mov di,200h ;设置es:di指向目的地址 mov cx,offset sqrend - offset sqr ;设置cx为传输长度 cld ;设置传输方向为正 rep movsb mov ax,0 mov es,ax mov word ptr es:[7ch*4], 200h mov word ptr es:[7ch*4+2], 0 mov ax,4c00h int 21h sqr: mul ax iret ;CPU执行int 7ch指令进入中断例程之前，标志寄存器、当前的CS和IP被压入栈 ;在执行完中断例程后，应该用iret 指令恢复int 7ch执行前的标志寄存器和CS、IP的 sqrend: nop code ends end start 实验2 ;功能：将一个全是字母，以0结尾的字符串，转化为大写。 ;参数：ds:si指向字符串的首地址。 ;应用举例：将data段中的字符串转化为大写。 assume cs:code data segment db 'conversation',0 data ends code segment start: mov ax, data mov ds, ax mov si, 0 int 7ch mov ax,4c00h int 21h code ends end start 12345678910111213141516171819 assume cs:code code segment start: mov ax,cs mov ds,ax mov si,offset capital mov ax,0 mov es,ax mov di,200h mov cx,offset capitalend - offset capital cld rep movsb mov ax,0 mov es,ax mov word ptr es:[7ch*4],200h mov word ptr es:[7ch*4+2],0 mov ax,4c00h int 21h capital: push cx push si change: mov cl,[si] mov ch,0 jcxz ok and byte ptr [si],11011111b inc si jmp short change ok: pop si pop cx iret capitalend:nop code ends end start ","date":"2023-01-24","objectID":"/%E6%B1%87%E7%BC%96%E4%B9%8B%E5%86%85%E4%B8%AD%E6%96%AD/:6:0","tags":[],"title":"汇编之内中断","uri":"/%E6%B1%87%E7%BC%96%E4%B9%8B%E5%86%85%E4%B8%AD%E6%96%AD/"},{"categories":["汇编"],"content":"7、BIOS和DOS所提供的中断例程 在系统板的ROM中存放着一套程序，称为BIOS（基本输入输出系统） BIOS中主要包含以下几部分内容 硬件系统的检测和初始化程序； 外部中断和内部中断的中断例程； 用于对硬件设备进行I/O操作的中断例程； 其他和硬件系统相关的中断例程。 程序员在编程的时候，可以用int 指令直接调用BIOS和DOS系统提供的中断例程，来完成某些工作。 和硬件设备相关的DOS中断例程中，一般都调用了BIOS的中断例程。 BIOS和DOS中断例程的安装过程 BIOS和DOS提供的中断例程是如何安装到内存中的呢？ 1、开机后，CPU一加电，初始化（CS）= 0FFFFH，（IP）= 0，自动从FFFF:0单元开始执行程序。FFFF:0处有一条转跳指令，CPU执行该指令后，转去执行BIOS中的硬件系统检测和初始化程序。 2、初始化程序将建立BIOS所支持的中断向量，即将BIOS提供的中断例程的入口地址登记在中断向量表中。 注意，对于BIOS所提供的中断例程，只需将入口地址登记在中断向量表中即可，因为它们是固化到ROM中的程序，一直在内存中存在。 3、硬件系统检测和初始化完成后，调用int 19h进行操作系统的引导。从此将计算机交由操作系统控制。 4、DOS启动后，除完成其他工作外，还将它所提供的中断例程装入内存，并建立相应的中断向量。 BIOS中断例程应用 一般来说，一个供程序员调用的中断例程中往往包括多个子程序，中断例程内部用传递进来的参数来决定执行哪一个子程序。 BIOS和DOS提供的中断例程，都用 ah 来传递内部子程序的编号。 编程：在屏幕的5行12列显示3个红底高亮闪烁绿色的“al。 assume cs:code code segment ;int 10h中断例程的\"设置光标位置\"功能 mov ah, 2;设置光标调用第10h号中断例程的2号子程序，功能为设置光标位置(可以提供光标所在的行号、列号和页号作为参数) ;设置光标到第0页，第5行，第12列 mov bh, 0；第0页 mov dh, 5；dh中放行号 mov dl, 12；dl中放列号 int 10h ;int10h中断例程的\"在光标位置显示字符\"功能。 mov ah，9 ;调用第10h号中断例程的9号子程序，功能为在光标位置显示字符 ;提供要显示的字符、颜色属性、页号、字符重复个数作为参数 mov al，'a' ;字符 mov b1，11001010b ;颜色属性 mov bh，0 ;第0页 mov cx，3 ;字符重复个数 int 10h code ends end bh中页号的含义：内存地址空间中，B8000H~BFFFFH共32kB的空间，为80*25彩色字符模式的显示缓冲区。 一屏的内容在显示缓冲区中共占4000个字节。显示缓冲区分为8页，每页4KB（约4000B），显示器可以显示任意一页的内容。一般情况下，显示第0页的内容。也就是说，通常情况下，B8000H~B8F9FH中的4000个字节的内容将出现在显示器上。 DOS中断例程应用 int 21h中断例程是DOS提供的中断例程，4ch号功能，即程序返回功能 mov ah, 4ch ;调用第21h号中断例程的4ch号子程序，功能为程序返回,可以提供返回值作为参数 mov al, 0 ;返回值 int 21h 编程：在屏幕的5行12列显示字符串“Welcome to masm！”。 assume cs:code data segment db 'Welcome to masm', '$' ;“$”本身并不显示，只起到边界的作用 data ends code segment start: mov ah, 2 ;10号中断设置光标位置功能 mov bh, 0 ;第0页 mov dh, 5；dh中放行号 mov dl, 12 ;dl中放列号 int 10h mov ax, data mov ds, ax mov dx, 0 ;ds:dx指向字符串的首地址data:0 （参数） mov ah, 9 ;调用第21h号中断例程的9号子程序，功能为在光标位置显示字符串，可以提供要显示字符串的地址作为参数 int 21h mov ax, 4c00h ;21号中断程序返回功能 int 21h code ends end start ","date":"2023-01-24","objectID":"/%E6%B1%87%E7%BC%96%E4%B9%8B%E5%86%85%E4%B8%AD%E6%96%AD/:7:0","tags":[],"title":"汇编之内中断","uri":"/%E6%B1%87%E7%BC%96%E4%B9%8B%E5%86%85%E4%B8%AD%E6%96%AD/"},{"categories":["redis"],"content":"[toc] 多级缓存 1.什么是多级缓存 传统的缓存策略一般是请求到达Tomcat后，先查询Redis，如果未命中则查询数据库，如图： 存在下面的问题： 请求要经过Tomcat处理，Tomcat的性能成为整个系统的瓶颈 Redis缓存失效时，会对数据库产生冲击 多级缓存就是充分利用请求处理的每个环节，分别添加缓存，减轻Tomcat压力，提升服务性能： 浏览器访问静态资源时，优先读取浏览器本地缓存 访问非静态资源（ajax查询数据）时，访问服务端 请求到达Nginx后，优先读取Nginx本地缓存 如果Nginx本地缓存未命中，则去直接查询Redis（不经过Tomcat） 如果Redis查询未命中，则查询Tomcat 请求进入Tomcat后，优先查询JVM进程缓存 如果JVM进程缓存未命中，则查询数据库 在多级缓存架构中，Nginx内部需要编写本地缓存查询、Redis查询、Tomcat查询的业务逻辑，因此这样的nginx服务不再是一个反向代理服务器，而是一个编写业务的Web服务器了。 因此这样的业务Nginx服务也需要搭建集群来提高并发，再有专门的nginx服务来做反向代理，如图： 另外，我们的Tomcat服务将来也会部署为集群模式： 可见，多级缓存的关键有两个： 一个是在nginx中编写业务，实现nginx本地缓存、Redis、Tomcat的查询 另一个就是在Tomcat中实现JVM进程缓存 其中Nginx编程则会用到OpenResty框架结合Lua这样的语言。 2.JVM进程缓存 为了演示多级缓存的案例，我们先准备一个商品查询的业务。 ","date":"2023-01-24","objectID":"/redis%E9%AB%98%E7%BA%A7%E7%AF%87%E4%B9%8B%E5%A4%9A%E7%BA%A7%E7%BC%93%E5%AD%98/:0:0","tags":["lua"],"title":"Redis高级篇之多级缓存","uri":"/redis%E9%AB%98%E7%BA%A7%E7%AF%87%E4%B9%8B%E5%A4%9A%E7%BA%A7%E7%BC%93%E5%AD%98/"},{"categories":["redis"],"content":"2.1.导入案例 ","date":"2023-01-24","objectID":"/redis%E9%AB%98%E7%BA%A7%E7%AF%87%E4%B9%8B%E5%A4%9A%E7%BA%A7%E7%BC%93%E5%AD%98/:1:0","tags":["lua"],"title":"Redis高级篇之多级缓存","uri":"/redis%E9%AB%98%E7%BA%A7%E7%AF%87%E4%B9%8B%E5%A4%9A%E7%BA%A7%E7%BC%93%E5%AD%98/"},{"categories":["redis"],"content":"2.2.初识Caffeine 缓存在日常开发中启动至关重要的作用，由于是存储在内存中，数据的读取速度是非常快的，能大量减少对数据库的访问，减少数据库的压力。我们把缓存分为两类： 分布式缓存，例如Redis： 优点：存储容量更大、可靠性更好、RDB\\AOF持久化、可以在集群间共享 缺点：访问缓存有网络开销 场景：缓存数据量较大、可靠性要求较高、需要在集群间共享 进程本地缓存，例如HashMap、GuavaCache： 优点：读取本地内存，没有网络开销，速度更快 缺点：存储容量有限、可靠性较低、无法共享 场景：性能要求较高，缓存数据量较小 我们今天会利用Caffeine框架来实现JVM进程缓存。 Caffeine是一个基于Java8开发的，提供了近乎最佳命中率的高性能的本地缓存库。目前Spring内部的缓存使用的就是Caffeine。GitHub地址：https://github.com/ben-manes/caffeine Caffeine的性能非常好，下图是官方给出的性能对比： 可以看到Caffeine的性能遥遥领先！ 缓存使用的基本API： @Test void testBasicOps() { // 构建cache对象 Cache\u003cString, String\u003e cache = Caffeine.newBuilder().build(); // 存数据 cache.put(\"gf\", \"迪丽热巴\"); // 取数据 String gf = cache.getIfPresent(\"gf\"); System.out.println(\"gf = \" + gf); // 取数据，包含两个参数： // 参数一：缓存的key // 参数二：Lambda表达式，表达式参数就是缓存的key，方法体是查询数据库的逻辑 // 优先根据key查询JVM缓存，如果未命中，则执行参数二的Lambda表达式 String defaultGF = cache.get(\"defaultGF\", key -\u003e { // 根据key去数据库查询数据 return \"柳岩\"; }); System.out.println(\"defaultGF = \" + defaultGF); } Caffeine既然是缓存的一种，肯定需要有缓存的清除策略，不然的话内存总会有耗尽的时候。 Caffeine提供了三种缓存驱逐策略： 基于容量：设置缓存的数量上限 // 创建缓存对象 Cache\u003cString, String\u003e cache = Caffeine.newBuilder() .maximumSize(1) // 设置缓存大小上限为 1 .build(); 基于时间：设置缓存的有效时间 // 创建缓存对象 Cache\u003cString, String\u003e cache = Caffeine.newBuilder() // 设置缓存有效期为 10 秒，从最后一次写入开始计时 .expireAfterWrite(Duration.ofSeconds(10)) .build(); 基于引用：设置缓存为软引用或弱引用，利用GC来回收缓存数据。性能较差，不建议使用。 注意：在默认情况下，当一个缓存元素过期的时候，Caffeine不会自动立即将其清理和驱逐。而是在一次读或写操作后，或者在空闲时间完成对失效数据的驱逐。 ","date":"2023-01-24","objectID":"/redis%E9%AB%98%E7%BA%A7%E7%AF%87%E4%B9%8B%E5%A4%9A%E7%BA%A7%E7%BC%93%E5%AD%98/:2:0","tags":["lua"],"title":"Redis高级篇之多级缓存","uri":"/redis%E9%AB%98%E7%BA%A7%E7%AF%87%E4%B9%8B%E5%A4%9A%E7%BA%A7%E7%BC%93%E5%AD%98/"},{"categories":["redis"],"content":"2.3.实现JVM进程缓存 ","date":"2023-01-24","objectID":"/redis%E9%AB%98%E7%BA%A7%E7%AF%87%E4%B9%8B%E5%A4%9A%E7%BA%A7%E7%BC%93%E5%AD%98/:3:0","tags":["lua"],"title":"Redis高级篇之多级缓存","uri":"/redis%E9%AB%98%E7%BA%A7%E7%AF%87%E4%B9%8B%E5%A4%9A%E7%BA%A7%E7%BC%93%E5%AD%98/"},{"categories":["redis"],"content":"2.3.1.需求 利用Caffeine实现下列需求： 给根据id查询商品的业务添加缓存，缓存未命中时查询数据库 给根据id查询商品库存的业务添加缓存，缓存未命中时查询数据库 缓存初始大小为100 缓存上限为10000 ","date":"2023-01-24","objectID":"/redis%E9%AB%98%E7%BA%A7%E7%AF%87%E4%B9%8B%E5%A4%9A%E7%BA%A7%E7%BC%93%E5%AD%98/:3:1","tags":["lua"],"title":"Redis高级篇之多级缓存","uri":"/redis%E9%AB%98%E7%BA%A7%E7%AF%87%E4%B9%8B%E5%A4%9A%E7%BA%A7%E7%BC%93%E5%AD%98/"},{"categories":["redis"],"content":"2.3.2.实现 首先，我们需要定义两个Caffeine的缓存对象，分别保存商品、库存的缓存数据。 在item-service的com.heima.item.config包下定义CaffeineConfig类： package com.heima.item.config; import com.github.benmanes.caffeine.cache.Cache; import com.github.benmanes.caffeine.cache.Caffeine; import com.heima.item.pojo.Item; import com.heima.item.pojo.ItemStock; import org.springframework.context.annotation.Bean; import org.springframework.context.annotation.Configuration; @Configuration public class CaffeineConfig { @Bean public Cache\u003cLong, Item\u003e itemCache(){ return Caffeine.newBuilder() .initialCapacity(100) .maximumSize(10_000) .build(); } @Bean public Cache\u003cLong, ItemStock\u003e stockCache(){ return Caffeine.newBuilder() .initialCapacity(100) .maximumSize(10_000) .build(); } } 然后，修改item-service中的com.heima.item.web包下的ItemController类，添加缓存逻辑： @RestController @RequestMapping(\"item\") public class ItemController { @Autowired private IItemService itemService; @Autowired private IItemStockService stockService; @Autowired private Cache\u003cLong, Item\u003e itemCache; @Autowired private Cache\u003cLong, ItemStock\u003e stockCache; // ...其它略 @GetMapping(\"/{id}\") public Item findById(@PathVariable(\"id\") Long id) { return itemCache.get(id, key -\u003e itemService.query() .ne(\"status\", 3).eq(\"id\", key) .one() ); } @GetMapping(\"/stock/{id}\") public ItemStock findStockById(@PathVariable(\"id\") Long id) { return stockCache.get(id, key -\u003e stockService.getById(key)); } } 3.Lua语法入门 Nginx编程需要用到Lua语言，因此我们必须先入门Lua的基本语法。 ","date":"2023-01-24","objectID":"/redis%E9%AB%98%E7%BA%A7%E7%AF%87%E4%B9%8B%E5%A4%9A%E7%BA%A7%E7%BC%93%E5%AD%98/:3:2","tags":["lua"],"title":"Redis高级篇之多级缓存","uri":"/redis%E9%AB%98%E7%BA%A7%E7%AF%87%E4%B9%8B%E5%A4%9A%E7%BA%A7%E7%BC%93%E5%AD%98/"},{"categories":["redis"],"content":"3.1.初识Lua Lua 是一种轻量小巧的脚本语言，用标准C语言编写并以源代码形式开放， 其设计目的是为了嵌入应用程序中，从而为应用程序提供灵活的扩展和定制功能。官网：https://www.lua.org/ Lua经常嵌入到C语言开发的程序中，例如游戏开发、游戏插件等。 Nginx本身也是C语言开发，因此也允许基于Lua做拓展。 ","date":"2023-01-24","objectID":"/redis%E9%AB%98%E7%BA%A7%E7%AF%87%E4%B9%8B%E5%A4%9A%E7%BA%A7%E7%BC%93%E5%AD%98/:4:0","tags":["lua"],"title":"Redis高级篇之多级缓存","uri":"/redis%E9%AB%98%E7%BA%A7%E7%AF%87%E4%B9%8B%E5%A4%9A%E7%BA%A7%E7%BC%93%E5%AD%98/"},{"categories":["redis"],"content":"3.1.HelloWorld CentOS7默认已经安装了Lua语言环境，所以可以直接运行Lua代码。 1）在Linux虚拟机的任意目录下，新建一个hello.lua文件 2）添加下面的内容 print(\"Hello World!\") 3）运行 ","date":"2023-01-24","objectID":"/redis%E9%AB%98%E7%BA%A7%E7%AF%87%E4%B9%8B%E5%A4%9A%E7%BA%A7%E7%BC%93%E5%AD%98/:5:0","tags":["lua"],"title":"Redis高级篇之多级缓存","uri":"/redis%E9%AB%98%E7%BA%A7%E7%AF%87%E4%B9%8B%E5%A4%9A%E7%BA%A7%E7%BC%93%E5%AD%98/"},{"categories":["redis"],"content":"3.2.变量和循环 学习任何语言必然离不开变量，而变量的声明必须先知道数据的类型。 ","date":"2023-01-24","objectID":"/redis%E9%AB%98%E7%BA%A7%E7%AF%87%E4%B9%8B%E5%A4%9A%E7%BA%A7%E7%BC%93%E5%AD%98/:6:0","tags":["lua"],"title":"Redis高级篇之多级缓存","uri":"/redis%E9%AB%98%E7%BA%A7%E7%AF%87%E4%B9%8B%E5%A4%9A%E7%BA%A7%E7%BC%93%E5%AD%98/"},{"categories":["redis"],"content":"3.2.1.Lua的数据类型 Lua中支持的常见数据类型包括： 另外，Lua提供了type()函数来判断一个变量的数据类型： ","date":"2023-01-24","objectID":"/redis%E9%AB%98%E7%BA%A7%E7%AF%87%E4%B9%8B%E5%A4%9A%E7%BA%A7%E7%BC%93%E5%AD%98/:6:1","tags":["lua"],"title":"Redis高级篇之多级缓存","uri":"/redis%E9%AB%98%E7%BA%A7%E7%AF%87%E4%B9%8B%E5%A4%9A%E7%BA%A7%E7%BC%93%E5%AD%98/"},{"categories":["redis"],"content":"3.2.2.声明变量 Lua声明变量的时候无需指定数据类型，而是用local来声明变量为局部变量： -- 声明字符串，可以用单引号或双引号， local str = 'hello' -- 字符串拼接可以使用 .. local str2 = 'hello' .. 'world' -- 声明数字 local num = 21 -- 声明布尔类型 local flag = true Lua中的table类型既可以作为数组，又可以作为Java中的map来使用。数组就是特殊的table，key是数组角标而已： -- 声明数组 ，key为角标的 table local arr = {'java', 'python', 'lua'} -- 声明table，类似java的map local map = {name='Jack', age=21} Lua中的数组角标是从1开始，访问的时候与Java中类似： -- 访问数组，lua数组的角标从1开始 print(arr[1]) Lua中的table可以用key来访问： -- 访问table print(map['name']) print(map.name) ","date":"2023-01-24","objectID":"/redis%E9%AB%98%E7%BA%A7%E7%AF%87%E4%B9%8B%E5%A4%9A%E7%BA%A7%E7%BC%93%E5%AD%98/:6:2","tags":["lua"],"title":"Redis高级篇之多级缓存","uri":"/redis%E9%AB%98%E7%BA%A7%E7%AF%87%E4%B9%8B%E5%A4%9A%E7%BA%A7%E7%BC%93%E5%AD%98/"},{"categories":["redis"],"content":"3.2.3.循环 对于table，我们可以利用for循环来遍历。不过数组和普通table遍历略有差异。 遍历数组： -- 声明数组 key为索引的 table local arr = {'java', 'python', 'lua'} -- 遍历数组 for index,value in ipairs(arr) do print(index, value) end 遍历普通table -- 声明map，也就是table local map = {name='Jack', age=21} -- 遍历table for key,value in pairs(map) do print(key, value) end ","date":"2023-01-24","objectID":"/redis%E9%AB%98%E7%BA%A7%E7%AF%87%E4%B9%8B%E5%A4%9A%E7%BA%A7%E7%BC%93%E5%AD%98/:6:3","tags":["lua"],"title":"Redis高级篇之多级缓存","uri":"/redis%E9%AB%98%E7%BA%A7%E7%AF%87%E4%B9%8B%E5%A4%9A%E7%BA%A7%E7%BC%93%E5%AD%98/"},{"categories":["redis"],"content":"3.3.条件控制、函数 Lua中的条件控制和函数声明与Java类似。 ","date":"2023-01-24","objectID":"/redis%E9%AB%98%E7%BA%A7%E7%AF%87%E4%B9%8B%E5%A4%9A%E7%BA%A7%E7%BC%93%E5%AD%98/:7:0","tags":["lua"],"title":"Redis高级篇之多级缓存","uri":"/redis%E9%AB%98%E7%BA%A7%E7%AF%87%E4%B9%8B%E5%A4%9A%E7%BA%A7%E7%BC%93%E5%AD%98/"},{"categories":["redis"],"content":"3.3.1.函数 定义函数的语法： function 函数名( argument1, argument2..., argumentn) -- 函数体 return 返回值 end 例如，定义一个函数，用来打印数组： function printArr(arr) for index, value in ipairs(arr) do print(value) end end ","date":"2023-01-24","objectID":"/redis%E9%AB%98%E7%BA%A7%E7%AF%87%E4%B9%8B%E5%A4%9A%E7%BA%A7%E7%BC%93%E5%AD%98/:7:1","tags":["lua"],"title":"Redis高级篇之多级缓存","uri":"/redis%E9%AB%98%E7%BA%A7%E7%AF%87%E4%B9%8B%E5%A4%9A%E7%BA%A7%E7%BC%93%E5%AD%98/"},{"categories":["redis"],"content":"3.3.2.条件控制 类似Java的条件控制，例如if、else语法： if(布尔表达式) then --[ 布尔表达式为 true 时执行该语句块 --] else --[ 布尔表达式为 false 时执行该语句块 --] end 与java不同，布尔表达式中的逻辑运算是基于英文单词： ","date":"2023-01-24","objectID":"/redis%E9%AB%98%E7%BA%A7%E7%AF%87%E4%B9%8B%E5%A4%9A%E7%BA%A7%E7%BC%93%E5%AD%98/:7:2","tags":["lua"],"title":"Redis高级篇之多级缓存","uri":"/redis%E9%AB%98%E7%BA%A7%E7%AF%87%E4%B9%8B%E5%A4%9A%E7%BA%A7%E7%BC%93%E5%AD%98/"},{"categories":["redis"],"content":"3.3.3.案例 需求：自定义一个函数，可以打印table，当参数为nil时，打印错误信息 function printArr(arr) if not arr then print('数组不能为空！') end for index, value in ipairs(arr) do print(value) end end 4.实现多级缓存 多级缓存的实现离不开Nginx编程，而Nginx编程又离不开OpenResty。 ","date":"2023-01-24","objectID":"/redis%E9%AB%98%E7%BA%A7%E7%AF%87%E4%B9%8B%E5%A4%9A%E7%BA%A7%E7%BC%93%E5%AD%98/:7:3","tags":["lua"],"title":"Redis高级篇之多级缓存","uri":"/redis%E9%AB%98%E7%BA%A7%E7%AF%87%E4%B9%8B%E5%A4%9A%E7%BA%A7%E7%BC%93%E5%AD%98/"},{"categories":["redis"],"content":"4.1.安装OpenResty OpenResty® 是一个基于 Nginx的高性能 Web 平台，用于方便地搭建能够处理超高并发、扩展性极高的动态 Web 应用、Web 服务和动态网关。具备下列特点： 具备Nginx的完整功能 基于Lua语言进行扩展，集成了大量精良的 Lua 库、第三方模块 允许使用Lua自定义业务逻辑、自定义库 官方网站： https://openresty.org/cn/ ","date":"2023-01-24","objectID":"/redis%E9%AB%98%E7%BA%A7%E7%AF%87%E4%B9%8B%E5%A4%9A%E7%BA%A7%E7%BC%93%E5%AD%98/:8:0","tags":["lua"],"title":"Redis高级篇之多级缓存","uri":"/redis%E9%AB%98%E7%BA%A7%E7%AF%87%E4%B9%8B%E5%A4%9A%E7%BA%A7%E7%BC%93%E5%AD%98/"},{"categories":["redis"],"content":"4.2.OpenResty快速入门 我们希望达到的多级缓存架构如图： 其中： windows上的nginx用来做反向代理服务，将前端的查询商品的ajax请求代理到OpenResty集群 OpenResty集群用来编写多级缓存业务 ","date":"2023-01-24","objectID":"/redis%E9%AB%98%E7%BA%A7%E7%AF%87%E4%B9%8B%E5%A4%9A%E7%BA%A7%E7%BC%93%E5%AD%98/:9:0","tags":["lua"],"title":"Redis高级篇之多级缓存","uri":"/redis%E9%AB%98%E7%BA%A7%E7%AF%87%E4%B9%8B%E5%A4%9A%E7%BA%A7%E7%BC%93%E5%AD%98/"},{"categories":["redis"],"content":"4.2.1.反向代理流程 现在，商品详情页使用的是假的商品数据。不过在浏览器中，可以看到页面有发起ajax请求查询真实商品数据。 这个请求如下： 请求地址是localhost，端口是80，就被windows上安装的Nginx服务给接收到了。然后代理给了OpenResty集群： 我们需要在OpenResty中编写业务，查询商品数据并返回到浏览器。 但是这次，我们先在OpenResty接收请求，返回假的商品数据。 ","date":"2023-01-24","objectID":"/redis%E9%AB%98%E7%BA%A7%E7%AF%87%E4%B9%8B%E5%A4%9A%E7%BA%A7%E7%BC%93%E5%AD%98/:9:1","tags":["lua"],"title":"Redis高级篇之多级缓存","uri":"/redis%E9%AB%98%E7%BA%A7%E7%AF%87%E4%B9%8B%E5%A4%9A%E7%BA%A7%E7%BC%93%E5%AD%98/"},{"categories":["redis"],"content":"4.2.2.OpenResty监听请求 OpenResty的很多功能都依赖于其目录下的Lua库，需要在nginx.conf中指定依赖库的目录，并导入依赖： 1）添加对OpenResty的Lua模块的加载 修改/usr/local/openresty/nginx/conf/nginx.conf文件，在其中的http下面，添加下面代码： # lua模块 lua_package_path \"/usr/local/openresty/lualib/?.lua;;\"; # c模块 lua_package_cpath \"/usr/local/openresty/lualib/?.so;;\"; 2）监听/api/item路径 修改/usr/local/openresty/nginx/conf/nginx.conf文件，在nginx.conf的server下面，添加对/api/item这个路径的监听： location /api/item { # 默认的响应类型 default_type application/json; # 响应结果由lua/item.lua文件来决定 content_by_lua_file lua/item.lua; } 这个监听，就类似于SpringMVC中的@GetMapping(\"/api/item\")做路径映射。 而content_by_lua_file lua/item.lua则相当于调用item.lua这个文件，执行其中的业务，把结果返回给用户。相当于java中调用service。 ","date":"2023-01-24","objectID":"/redis%E9%AB%98%E7%BA%A7%E7%AF%87%E4%B9%8B%E5%A4%9A%E7%BA%A7%E7%BC%93%E5%AD%98/:9:2","tags":["lua"],"title":"Redis高级篇之多级缓存","uri":"/redis%E9%AB%98%E7%BA%A7%E7%AF%87%E4%B9%8B%E5%A4%9A%E7%BA%A7%E7%BC%93%E5%AD%98/"},{"categories":["redis"],"content":"4.2.3.编写item.lua 1）在/usr/loca/openresty/nginx目录创建文件夹：lua 2）在/usr/loca/openresty/nginx/lua文件夹下，新建文件：item.lua 3）编写item.lua，返回假数据 item.lua中，利用ngx.say()函数返回数据到Response中 ngx.say('{\"id\":10001,\"name\":\"SALSA AIR\",\"title\":\"RIMOWA 21寸托运箱拉杆箱 SALSA AIR系列果绿色 820.70.36.4\",\"price\":17900,\"image\":\"https://m.360buyimg.com/mobilecms/s720x720_jfs/t6934/364/1195375010/84676/e9f2c55f/597ece38N0ddcbc77.jpg!q70.jpg.webp\",\"category\":\"拉杆箱\",\"brand\":\"RIMOWA\",\"spec\":\"\",\"status\":1,\"createTime\":\"2019-04-30T16:00:00.000+00:00\",\"updateTime\":\"2019-04-30T16:00:00.000+00:00\",\"stock\":2999,\"sold\":31290}') 4）重新加载配置 nginx -s reload 刷新商品页面：http://localhost/item.html?id=1001，即可看到效果： ","date":"2023-01-24","objectID":"/redis%E9%AB%98%E7%BA%A7%E7%AF%87%E4%B9%8B%E5%A4%9A%E7%BA%A7%E7%BC%93%E5%AD%98/:9:3","tags":["lua"],"title":"Redis高级篇之多级缓存","uri":"/redis%E9%AB%98%E7%BA%A7%E7%AF%87%E4%B9%8B%E5%A4%9A%E7%BA%A7%E7%BC%93%E5%AD%98/"},{"categories":["redis"],"content":"4.3.请求参数处理 上一节中，我们在OpenResty接收前端请求，但是返回的是假数据。 要返回真实数据，必须根据前端传递来的商品id，查询商品信息才可以。 那么如何获取前端传递的商品参数呢？ ","date":"2023-01-24","objectID":"/redis%E9%AB%98%E7%BA%A7%E7%AF%87%E4%B9%8B%E5%A4%9A%E7%BA%A7%E7%BC%93%E5%AD%98/:10:0","tags":["lua"],"title":"Redis高级篇之多级缓存","uri":"/redis%E9%AB%98%E7%BA%A7%E7%AF%87%E4%B9%8B%E5%A4%9A%E7%BA%A7%E7%BC%93%E5%AD%98/"},{"categories":["redis"],"content":"4.3.1.获取参数的API OpenResty中提供了一些API用来获取不同类型的前端请求参数： ","date":"2023-01-24","objectID":"/redis%E9%AB%98%E7%BA%A7%E7%AF%87%E4%B9%8B%E5%A4%9A%E7%BA%A7%E7%BC%93%E5%AD%98/:10:1","tags":["lua"],"title":"Redis高级篇之多级缓存","uri":"/redis%E9%AB%98%E7%BA%A7%E7%AF%87%E4%B9%8B%E5%A4%9A%E7%BA%A7%E7%BC%93%E5%AD%98/"},{"categories":["redis"],"content":"4.3.2.获取参数并返回 在前端发起的ajax请求如图： 可以看到商品id是以路径占位符方式传递的，因此可以利用正则表达式匹配的方式来获取ID 1）获取商品id 修改/usr/loca/openresty/nginx/nginx.conf文件中监听/api/item的代码，利用正则表达式获取ID： location ~ /api/item/(\\d+) { # 默认的响应类型 default_type application/json; # 响应结果由lua/item.lua文件来决定 content_by_lua_file lua/item.lua; } 2）拼接ID并返回 修改/usr/loca/openresty/nginx/lua/item.lua文件，获取id并拼接到结果中返回： -- 获取商品id local id = ngx.var[1] -- 拼接并返回 ngx.say('{\"id\":' .. id .. ',\"name\":\"SALSA AIR\",\"title\":\"RIMOWA 21寸托运箱拉杆箱 SALSA AIR系列果绿色 820.70.36.4\",\"price\":17900,\"image\":\"https://m.360buyimg.com/mobilecms/s720x720_jfs/t6934/364/1195375010/84676/e9f2c55f/597ece38N0ddcbc77.jpg!q70.jpg.webp\",\"category\":\"拉杆箱\",\"brand\":\"RIMOWA\",\"spec\":\"\",\"status\":1,\"createTime\":\"2019-04-30T16:00:00.000+00:00\",\"updateTime\":\"2019-04-30T16:00:00.000+00:00\",\"stock\":2999,\"sold\":31290}') 3）重新加载并测试 运行命令以重新加载OpenResty配置： nginx -s reload 刷新页面可以看到结果中已经带上了ID： ","date":"2023-01-24","objectID":"/redis%E9%AB%98%E7%BA%A7%E7%AF%87%E4%B9%8B%E5%A4%9A%E7%BA%A7%E7%BC%93%E5%AD%98/:10:2","tags":["lua"],"title":"Redis高级篇之多级缓存","uri":"/redis%E9%AB%98%E7%BA%A7%E7%AF%87%E4%B9%8B%E5%A4%9A%E7%BA%A7%E7%BC%93%E5%AD%98/"},{"categories":["redis"],"content":"4.4.查询Tomcat 拿到商品ID后，本应去缓存中查询商品信息，不过目前我们还未建立nginx、redis缓存。因此，这里我们先根据商品id去tomcat查询商品信息。我们实现如图部分： 需要注意的是，我们的OpenResty是在虚拟机，Tomcat是在Windows电脑上。两者IP一定不要搞错了。 ","date":"2023-01-24","objectID":"/redis%E9%AB%98%E7%BA%A7%E7%AF%87%E4%B9%8B%E5%A4%9A%E7%BA%A7%E7%BC%93%E5%AD%98/:11:0","tags":["lua"],"title":"Redis高级篇之多级缓存","uri":"/redis%E9%AB%98%E7%BA%A7%E7%AF%87%E4%B9%8B%E5%A4%9A%E7%BA%A7%E7%BC%93%E5%AD%98/"},{"categories":["redis"],"content":"4.4.1.发送http请求的API nginx提供了内部API用以发送http请求： local resp = ngx.location.capture(\"/path\",{ method = ngx.HTTP_GET, -- 请求方式 args = {a=1,b=2}, -- get方式传参数 }) 返回的响应内容包括： resp.status：响应状态码 resp.header：响应头，是一个table resp.body：响应体，就是响应数据 注意：这里的path是路径，并不包含IP和端口。这个请求会被nginx内部的server监听并处理。 但是我们希望这个请求发送到Tomcat服务器，所以还需要编写一个server来对这个路径做反向代理： location /path { # 这里是windows电脑的ip和Java服务端口，需要确保windows防火墙处于关闭状态 proxy_pass http://192.168.150.1:8081; } 原理如图： ","date":"2023-01-24","objectID":"/redis%E9%AB%98%E7%BA%A7%E7%AF%87%E4%B9%8B%E5%A4%9A%E7%BA%A7%E7%BC%93%E5%AD%98/:11:1","tags":["lua"],"title":"Redis高级篇之多级缓存","uri":"/redis%E9%AB%98%E7%BA%A7%E7%AF%87%E4%B9%8B%E5%A4%9A%E7%BA%A7%E7%BC%93%E5%AD%98/"},{"categories":["redis"],"content":"4.4.2.封装http工具 下面，我们封装一个发送Http请求的工具，基于ngx.location.capture来实现查询tomcat。 1）添加反向代理，到windows的Java服务 因为item-service中的接口都是/item开头，所以我们监听/item路径，代理到windows上的tomcat服务。 修改 /usr/local/openresty/nginx/conf/nginx.conf文件，添加一个location： location /item { proxy_pass http://192.168.150.1:8081; } 以后，只要我们调用ngx.location.capture(\"/item\")，就一定能发送请求到windows的tomcat服务。 2）封装工具类 之前我们说过，OpenResty启动时会加载以下两个目录中的工具文件： 所以，自定义的http工具也需要放到这个目录下。 在/usr/local/openresty/lualib目录下，新建一个common.lua文件： vi /usr/local/openresty/lualib/common.lua 内容如下: -- 封装函数，发送http请求，并解析响应 local function read_http(path, params) local resp = ngx.location.capture(path,{ method = ngx.HTTP_GET, args = params, }) if not resp then -- 记录错误信息，返回404 ngx.log(ngx.ERR, \"http请求查询失败, path: \", path , \", args: \", args) ngx.exit(404) end return resp.body end -- 将方法导出 local _M = { read_http = read_http } return _M 这个工具将read_http函数封装到_M这个table类型的变量中，并且返回，这类似于导出。 使用的时候，可以利用require('common')来导入该函数库，这里的common是函数库的文件名。 3）实现商品查询 最后，我们修改/usr/local/openresty/lua/item.lua文件，利用刚刚封装的函数库实现对tomcat的查询： -- 引入自定义common工具模块，返回值是common中返回的 _M local common = require(\"common\") -- 从common中获取read_http这个函数 local read_http = common.read_http -- 获取路径参数 local id = ngx.var[1] -- 根据id查询商品 local itemJSON = read_http(\"/item/\".. id, nil) -- 根据id查询商品库存 local itemStockJSON = read_http(\"/item/stock/\".. id, nil) 这里查询到的结果是json字符串，并且包含商品、库存两个json字符串，页面最终需要的是把两个json拼接为一个json： 这就需要我们先把JSON变为lua的table，完成数据整合后，再转为JSON。 ","date":"2023-01-24","objectID":"/redis%E9%AB%98%E7%BA%A7%E7%AF%87%E4%B9%8B%E5%A4%9A%E7%BA%A7%E7%BC%93%E5%AD%98/:11:2","tags":["lua"],"title":"Redis高级篇之多级缓存","uri":"/redis%E9%AB%98%E7%BA%A7%E7%AF%87%E4%B9%8B%E5%A4%9A%E7%BA%A7%E7%BC%93%E5%AD%98/"},{"categories":["redis"],"content":"4.4.3.CJSON工具类 OpenResty提供了一个cjson的模块用来处理JSON的序列化和反序列化。 官方地址： https://github.com/openresty/lua-cjson/ 1）引入cjson模块： local cjson = require \"cjson\" 2）序列化： local obj = { name = 'jack', age = 21 } -- 把 table 序列化为 json local json = cjson.encode(obj) 3）反序列化： local json = '{\"name\": \"jack\", \"age\": 21}' -- 反序列化 json为 table local obj = cjson.decode(json); print(obj.name) ","date":"2023-01-24","objectID":"/redis%E9%AB%98%E7%BA%A7%E7%AF%87%E4%B9%8B%E5%A4%9A%E7%BA%A7%E7%BC%93%E5%AD%98/:11:3","tags":["lua"],"title":"Redis高级篇之多级缓存","uri":"/redis%E9%AB%98%E7%BA%A7%E7%AF%87%E4%B9%8B%E5%A4%9A%E7%BA%A7%E7%BC%93%E5%AD%98/"},{"categories":["redis"],"content":"4.4.4.实现Tomcat查询 下面，我们修改之前的item.lua中的业务，添加json处理功能： -- 导入common函数库 local common = require('common') local read_http = common.read_http -- 导入cjson库 local cjson = require('cjson') -- 获取路径参数 local id = ngx.var[1] -- 根据id查询商品 local itemJSON = read_http(\"/item/\".. id, nil) -- 根据id查询商品库存 local itemStockJSON = read_http(\"/item/stock/\".. id, nil) -- JSON转化为lua的table local item = cjson.decode(itemJSON) local stock = cjson.decode(stockJSON) -- 组合数据 item.stock = stock.stock item.sold = stock.sold -- 把item序列化为json返回结果 ngx.say(cjson.encode(item)) ","date":"2023-01-24","objectID":"/redis%E9%AB%98%E7%BA%A7%E7%AF%87%E4%B9%8B%E5%A4%9A%E7%BA%A7%E7%BC%93%E5%AD%98/:11:4","tags":["lua"],"title":"Redis高级篇之多级缓存","uri":"/redis%E9%AB%98%E7%BA%A7%E7%AF%87%E4%B9%8B%E5%A4%9A%E7%BA%A7%E7%BC%93%E5%AD%98/"},{"categories":["redis"],"content":"4.4.5.基于ID负载均衡 刚才的代码中，我们的tomcat是单机部署。而实际开发中，tomcat一定是集群模式： 因此，OpenResty需要对tomcat集群做负载均衡。 而默认的负载均衡规则是轮询模式，当我们查询/item/10001时： 第一次会访问8081端口的tomcat服务，在该服务内部就形成了JVM进程缓存 第二次会访问8082端口的tomcat服务，该服务内部没有JVM缓存（因为JVM缓存无法共享），会查询数据库 … 你看，因为轮询的原因，第一次查询8081形成的JVM缓存并未生效，直到下一次再次访问到8081时才可以生效，缓存命中率太低了。 怎么办？ 如果能让同一个商品，每次查询时都访问同一个tomcat服务，那么JVM缓存就一定能生效了。 也就是说，我们需要根据商品id做负载均衡，而不是轮询。 1）原理 nginx提供了基于请求路径做负载均衡的算法： nginx根据请求路径做hash运算，把得到的数值对tomcat服务的数量取余，余数是几，就访问第几个服务，实现负载均衡。 例如： 我们的请求路径是 /item/10001 tomcat总数为2台（8081、8082） 对请求路径/item/1001做hash运算求余的结果为1 则访问第一个tomcat服务，也就是8081 只要id不变，每次hash运算结果也不会变，那就可以保证同一个商品，一直访问同一个tomcat服务，确保JVM缓存生效。 2）实现 修改/usr/local/openresty/nginx/conf/nginx.conf文件，实现基于ID做负载均衡。 首先，定义tomcat集群，并设置基于路径做负载均衡： upstream tomcat-cluster { hash $request_uri; server 192.168.150.1:8081; server 192.168.150.1:8082; } 然后，修改对tomcat服务的反向代理，目标指向tomcat集群： location /item { proxy_pass http://tomcat-cluster; } 重新加载OpenResty nginx -s reload 3）测试 启动两台tomcat服务： 同时启动： 清空日志后，再次访问页面，可以看到不同id的商品，访问到了不同的tomcat服务： ","date":"2023-01-24","objectID":"/redis%E9%AB%98%E7%BA%A7%E7%AF%87%E4%B9%8B%E5%A4%9A%E7%BA%A7%E7%BC%93%E5%AD%98/:11:5","tags":["lua"],"title":"Redis高级篇之多级缓存","uri":"/redis%E9%AB%98%E7%BA%A7%E7%AF%87%E4%B9%8B%E5%A4%9A%E7%BA%A7%E7%BC%93%E5%AD%98/"},{"categories":["redis"],"content":"4.5.Redis缓存预热 Redis缓存会面临冷启动问题： 冷启动：服务刚刚启动时，Redis中并没有缓存，如果所有商品数据都在第一次查询时添加缓存，可能会给数据库带来较大压力。 缓存预热：在实际开发中，我们可以利用大数据统计用户访问的热点数据，在项目启动时将这些热点数据提前查询并保存到Redis中。 我们数据量较少，并且没有数据统计相关功能，目前可以在启动时将所有数据都放入缓存中。 1）利用Docker安装Redis docker run --name redis -p 6379:6379 -d redis redis-server --appendonly yes 2）在item-service服务中引入Redis依赖 \u003cdependency\u003e \u003cgroupId\u003eorg.springframework.boot\u003c/groupId\u003e \u003cartifactId\u003espring-boot-starter-data-redis\u003c/artifactId\u003e \u003c/dependency\u003e 3）配置Redis地址 spring: redis: host: 192.168.150.101 4）编写初始化类 缓存预热需要在项目启动时完成，并且必须是拿到RedisTemplate之后。 这里我们利用InitializingBean接口来实现，因为InitializingBean可以在对象被Spring创建并且成员变量全部注入后执行。 package com.heima.item.config; import com.fasterxml.jackson.core.JsonProcessingException; import com.fasterxml.jackson.databind.ObjectMapper; import com.heima.item.pojo.Item; import com.heima.item.pojo.ItemStock; import com.heima.item.service.IItemService; import com.heima.item.service.IItemStockService; import org.springframework.beans.factory.InitializingBean; import org.springframework.beans.factory.annotation.Autowired; import org.springframework.data.redis.core.StringRedisTemplate; import org.springframework.stereotype.Component; import java.util.List; @Component public class RedisHandler implements InitializingBean { @Autowired private StringRedisTemplate redisTemplate; @Autowired private IItemService itemService; @Autowired private IItemStockService stockService; private static final ObjectMapper MAPPER = new ObjectMapper(); @Override public void afterPropertiesSet() throws Exception { // 初始化缓存 // 1.查询商品信息 List\u003cItem\u003e itemList = itemService.list(); // 2.放入缓存 for (Item item : itemList) { // 2.1.item序列化为JSON String json = MAPPER.writeValueAsString(item); // 2.2.存入redis redisTemplate.opsForValue().set(\"item:id:\" + item.getId(), json); } // 3.查询商品库存信息 List\u003cItemStock\u003e stockList = stockService.list(); // 4.放入缓存 for (ItemStock stock : stockList) { // 2.1.item序列化为JSON String json = MAPPER.writeValueAsString(stock); // 2.2.存入redis redisTemplate.opsForValue().set(\"item:stock:id:\" + stock.getId(), json); } } } ","date":"2023-01-24","objectID":"/redis%E9%AB%98%E7%BA%A7%E7%AF%87%E4%B9%8B%E5%A4%9A%E7%BA%A7%E7%BC%93%E5%AD%98/:12:0","tags":["lua"],"title":"Redis高级篇之多级缓存","uri":"/redis%E9%AB%98%E7%BA%A7%E7%AF%87%E4%B9%8B%E5%A4%9A%E7%BA%A7%E7%BC%93%E5%AD%98/"},{"categories":["redis"],"content":"4.6.查询Redis缓存 现在，Redis缓存已经准备就绪，我们可以再OpenResty中实现查询Redis的逻辑了。如下图红框所示： 当请求进入OpenResty之后： 优先查询Redis缓存 如果Redis缓存未命中，再查询Tomcat ","date":"2023-01-24","objectID":"/redis%E9%AB%98%E7%BA%A7%E7%AF%87%E4%B9%8B%E5%A4%9A%E7%BA%A7%E7%BC%93%E5%AD%98/:13:0","tags":["lua"],"title":"Redis高级篇之多级缓存","uri":"/redis%E9%AB%98%E7%BA%A7%E7%AF%87%E4%B9%8B%E5%A4%9A%E7%BA%A7%E7%BC%93%E5%AD%98/"},{"categories":["redis"],"content":"4.6.1.封装Redis工具 OpenResty提供了操作Redis的模块，我们只要引入该模块就能直接使用。但是为了方便，我们将Redis操作封装到之前的common.lua工具库中。 修改/usr/local/openresty/lualib/common.lua文件： 1）引入Redis模块，并初始化Redis对象 -- 导入redis local redis = require('resty.redis') -- 初始化redis local red = redis:new() red:set_timeouts(1000, 1000, 1000) 2）封装函数，用来释放Redis连接，其实是放入连接池 -- 关闭redis连接的工具方法，其实是放入连接池 local function close_redis(red) local pool_max_idle_time = 10000 -- 连接的空闲时间，单位是毫秒 local pool_size = 100 --连接池大小 local ok, err = red:set_keepalive(pool_max_idle_time, pool_size) if not ok then ngx.log(ngx.ERR, \"放入redis连接池失败: \", err) end end 3）封装函数，根据key查询Redis数据 -- 查询redis的方法 ip和port是redis地址，key是查询的key local function read_redis(ip, port, key) -- 获取一个连接 local ok, err = red:connect(ip, port) if not ok then ngx.log(ngx.ERR, \"连接redis失败 : \", err) return nil end -- 查询redis local resp, err = red:get(key) -- 查询失败处理 if not resp then ngx.log(ngx.ERR, \"查询Redis失败: \", err, \", key = \" , key) end --得到的数据为空处理 if resp == ngx.null then resp = nil ngx.log(ngx.ERR, \"查询Redis数据为空, key = \", key) end close_redis(red) return resp end 4）导出 -- 将方法导出 local _M = { read_http = read_http, read_redis = read_redis } return _M 完整的common.lua： -- 导入redis local redis = require('resty.redis') -- 初始化redis local red = redis:new() red:set_timeouts(1000, 1000, 1000) -- 关闭redis连接的工具方法，其实是放入连接池 local function close_redis(red) local pool_max_idle_time = 10000 -- 连接的空闲时间，单位是毫秒 local pool_size = 100 --连接池大小 local ok, err = red:set_keepalive(pool_max_idle_time, pool_size) if not ok then ngx.log(ngx.ERR, \"放入redis连接池失败: \", err) end end -- 查询redis的方法 ip和port是redis地址，key是查询的key local function read_redis(ip, port, key) -- 获取一个连接 local ok, err = red:connect(ip, port) if not ok then ngx.log(ngx.ERR, \"连接redis失败 : \", err) return nil end -- 查询redis local resp, err = red:get(key) -- 查询失败处理 if not resp then ngx.log(ngx.ERR, \"查询Redis失败: \", err, \", key = \" , key) end --得到的数据为空处理 if resp == ngx.null then resp = nil ngx.log(ngx.ERR, \"查询Redis数据为空, key = \", key) end close_redis(red) return resp end -- 封装函数，发送http请求，并解析响应 local function read_http(path, params) local resp = ngx.location.capture(path,{ method = ngx.HTTP_GET, args = params, }) if not resp then -- 记录错误信息，返回404 ngx.log(ngx.ERR, \"http查询失败, path: \", path , \", args: \", args) ngx.exit(404) end return resp.body end -- 将方法导出 local _M = { read_http = read_http, read_redis = read_redis } return _M ","date":"2023-01-24","objectID":"/redis%E9%AB%98%E7%BA%A7%E7%AF%87%E4%B9%8B%E5%A4%9A%E7%BA%A7%E7%BC%93%E5%AD%98/:13:1","tags":["lua"],"title":"Redis高级篇之多级缓存","uri":"/redis%E9%AB%98%E7%BA%A7%E7%AF%87%E4%B9%8B%E5%A4%9A%E7%BA%A7%E7%BC%93%E5%AD%98/"},{"categories":["redis"],"content":"4.6.2.实现Redis查询 接下来，我们就可以去修改item.lua文件，实现对Redis的查询了。 查询逻辑是： 根据id查询Redis 如果查询失败则继续查询Tomcat 将查询结果返回 1）修改/usr/local/openresty/lua/item.lua文件，添加一个查询函数： -- 导入common函数库 local common = require('common') local read_http = common.read_http local read_redis = common.read_redis -- 封装查询函数 function read_data(key, path, params) -- 查询本地缓存 local val = read_redis(\"127.0.0.1\", 6379, key) -- 判断查询结果 if not val then ngx.log(ngx.ERR, \"redis查询失败，尝试查询http， key: \", key) -- redis查询失败，去查询http val = read_http(path, params) end -- 返回数据 return val end 2）而后修改商品查询、库存查询的业务： 3）完整的item.lua代码： -- 导入common函数库 local common = require('common') local read_http = common.read_http local read_redis = common.read_redis -- 导入cjson库 local cjson = require('cjson') -- 封装查询函数 function read_data(key, path, params) -- 查询本地缓存 local val = read_redis(\"127.0.0.1\", 6379, key) -- 判断查询结果 if not val then ngx.log(ngx.ERR, \"redis查询失败，尝试查询http， key: \", key) -- redis查询失败，去查询http val = read_http(path, params) end -- 返回数据 return val end -- 获取路径参数 local id = ngx.var[1] -- 查询商品信息 local itemJSON = read_data(\"item:id:\" .. id, \"/item/\" .. id, nil) -- 查询库存信息 local stockJSON = read_data(\"item:stock:id:\" .. id, \"/item/stock/\" .. id, nil) -- JSON转化为lua的table local item = cjson.decode(itemJSON) local stock = cjson.decode(stockJSON) -- 组合数据 item.stock = stock.stock item.sold = stock.sold -- 把item序列化为json 返回结果 ngx.say(cjson.encode(item)) ","date":"2023-01-24","objectID":"/redis%E9%AB%98%E7%BA%A7%E7%AF%87%E4%B9%8B%E5%A4%9A%E7%BA%A7%E7%BC%93%E5%AD%98/:13:2","tags":["lua"],"title":"Redis高级篇之多级缓存","uri":"/redis%E9%AB%98%E7%BA%A7%E7%AF%87%E4%B9%8B%E5%A4%9A%E7%BA%A7%E7%BC%93%E5%AD%98/"},{"categories":["redis"],"content":"4.7.Nginx本地缓存 现在，整个多级缓存中只差最后一环，也就是nginx的本地缓存了。如图： ","date":"2023-01-24","objectID":"/redis%E9%AB%98%E7%BA%A7%E7%AF%87%E4%B9%8B%E5%A4%9A%E7%BA%A7%E7%BC%93%E5%AD%98/:14:0","tags":["lua"],"title":"Redis高级篇之多级缓存","uri":"/redis%E9%AB%98%E7%BA%A7%E7%AF%87%E4%B9%8B%E5%A4%9A%E7%BA%A7%E7%BC%93%E5%AD%98/"},{"categories":["redis"],"content":"4.7.1.本地缓存API OpenResty为Nginx提供了shard dict的功能，可以在nginx的多个worker之间共享数据，实现缓存功能。 1）开启共享字典，在nginx.conf的http下添加配置： # 共享字典，也就是本地缓存，名称叫做：item_cache，大小150m lua_shared_dict item_cache 150m; 2）操作共享字典： -- 获取本地缓存对象 local item_cache = ngx.shared.item_cache -- 存储, 指定key、value、过期时间，单位s，默认为0代表永不过期 item_cache:set('key', 'value', 1000) -- 读取 local val = item_cache:get('key') ","date":"2023-01-24","objectID":"/redis%E9%AB%98%E7%BA%A7%E7%AF%87%E4%B9%8B%E5%A4%9A%E7%BA%A7%E7%BC%93%E5%AD%98/:14:1","tags":["lua"],"title":"Redis高级篇之多级缓存","uri":"/redis%E9%AB%98%E7%BA%A7%E7%AF%87%E4%B9%8B%E5%A4%9A%E7%BA%A7%E7%BC%93%E5%AD%98/"},{"categories":["redis"],"content":"4.7.2.实现本地缓存查询 1）修改/usr/local/openresty/lua/item.lua文件，修改read_data查询函数，添加本地缓存逻辑： -- 导入共享词典，本地缓存 local item_cache = ngx.shared.item_cache -- 封装查询函数 function read_data(key, expire, path, params) -- 查询本地缓存 local val = item_cache:get(key) if not val then ngx.log(ngx.ERR, \"本地缓存查询失败，尝试查询Redis， key: \", key) -- 查询redis val = read_redis(\"127.0.0.1\", 6379, key) -- 判断查询结果 if not val then ngx.log(ngx.ERR, \"redis查询失败，尝试查询http， key: \", key) -- redis查询失败，去查询http val = read_http(path, params) end end -- 查询成功，把数据写入本地缓存 item_cache:set(key, val, expire) -- 返回数据 return val end 2）修改item.lua中查询商品和库存的业务，实现最新的read_data函数： 其实就是多了缓存时间参数，过期后nginx缓存会自动删除，下次访问即可更新缓存。 这里给商品基本信息设置超时时间为30分钟，库存为1分钟。 因为库存更新频率较高，如果缓存时间过长，可能与数据库差异较大。 3）完整的item.lua文件： -- 导入common函数库 local common = require('common') local read_http = common.read_http local read_redis = common.read_redis -- 导入cjson库 local cjson = require('cjson') -- 导入共享词典，本地缓存 local item_cache = ngx.shared.item_cache -- 封装查询函数 function read_data(key, expire, path, params) -- 查询本地缓存 local val = item_cache:get(key) if not val then ngx.log(ngx.ERR, \"本地缓存查询失败，尝试查询Redis， key: \", key) -- 查询redis val = read_redis(\"127.0.0.1\", 6379, key) -- 判断查询结果 if not val then ngx.log(ngx.ERR, \"redis查询失败，尝试查询http， key: \", key) -- redis查询失败，去查询http val = read_http(path, params) end end -- 查询成功，把数据写入本地缓存 item_cache:set(key, val, expire) -- 返回数据 return val end -- 获取路径参数 local id = ngx.var[1] -- 查询商品信息 local itemJSON = read_data(\"item:id:\" .. id, 1800, \"/item/\" .. id, nil) -- 查询库存信息 local stockJSON = read_data(\"item:stock:id:\" .. id, 60, \"/item/stock/\" .. id, nil) -- JSON转化为lua的table local item = cjson.decode(itemJSON) local stock = cjson.decode(stockJSON) -- 组合数据 item.stock = stock.stock item.sold = stock.sold -- 把item序列化为json 返回结果 ngx.say(cjson.encode(item)) 5.缓存同步 大多数情况下，浏览器查询到的都是缓存数据，如果缓存数据与数据库数据存在较大差异，可能会产生比较严重的后果。 所以我们必须保证数据库数据、缓存数据的一致性，这就是缓存与数据库的同步。 ","date":"2023-01-24","objectID":"/redis%E9%AB%98%E7%BA%A7%E7%AF%87%E4%B9%8B%E5%A4%9A%E7%BA%A7%E7%BC%93%E5%AD%98/:14:2","tags":["lua"],"title":"Redis高级篇之多级缓存","uri":"/redis%E9%AB%98%E7%BA%A7%E7%AF%87%E4%B9%8B%E5%A4%9A%E7%BA%A7%E7%BC%93%E5%AD%98/"},{"categories":["redis"],"content":"5.1.数据同步策略 缓存数据同步的常见方式有三种： 设置有效期：给缓存设置有效期，到期后自动删除。再次查询时更新 优势：简单、方便 缺点：时效性差，缓存过期之前可能不一致 场景：更新频率较低，时效性要求低的业务 同步双写：在修改数据库的同时，直接修改缓存 优势：时效性强，缓存与数据库强一致 缺点：有代码侵入，耦合度高 场景：对一致性、时效性要求较高的缓存数据 **异步通知：**修改数据库时发送事件通知，相关服务监听到通知后修改缓存数据 优势：低耦合，可以同时通知多个缓存服务 缺点：时效性一般，可能存在中间不一致状态 场景：时效性要求一般，有多个服务需要同步 而异步实现又可以基于MQ或者Canal来实现： 1）基于MQ的异步通知： 解读： 商品服务完成对数据的修改后，只需要发送一条消息到MQ中。 缓存服务监听MQ消息，然后完成对缓存的更新 依然有少量的代码侵入 2）基于Canal的通知 解读： 商品服务完成商品修改后，业务直接结束，没有任何代码侵入 Canal监听MySQL变化，当发现变化后，立即通知缓存服务 缓存服务接收到canal通知，更新缓存 代码零侵入 ","date":"2023-01-24","objectID":"/redis%E9%AB%98%E7%BA%A7%E7%AF%87%E4%B9%8B%E5%A4%9A%E7%BA%A7%E7%BC%93%E5%AD%98/:15:0","tags":["lua"],"title":"Redis高级篇之多级缓存","uri":"/redis%E9%AB%98%E7%BA%A7%E7%AF%87%E4%B9%8B%E5%A4%9A%E7%BA%A7%E7%BC%93%E5%AD%98/"},{"categories":["redis"],"content":"5.2.安装Canal ","date":"2023-01-24","objectID":"/redis%E9%AB%98%E7%BA%A7%E7%AF%87%E4%B9%8B%E5%A4%9A%E7%BA%A7%E7%BC%93%E5%AD%98/:16:0","tags":["lua"],"title":"Redis高级篇之多级缓存","uri":"/redis%E9%AB%98%E7%BA%A7%E7%AF%87%E4%B9%8B%E5%A4%9A%E7%BA%A7%E7%BC%93%E5%AD%98/"},{"categories":["redis"],"content":"5.2.1.认识Canal Canal [kə’næl]，译意为水道/管道/沟渠，canal是阿里巴巴旗下的一款开源项目，基于Java开发。基于数据库增量日志解析，提供增量数据订阅\u0026消费。GitHub的地址：https://github.com/alibaba/canal Canal是基于mysql的主从同步来实现的，MySQL主从同步的原理如下： MySQL master 将数据变更写入二进制日志( binary log），其中记录的数据叫做binary log events MySQL slave 将 master 的 binary log events拷贝到它的中继日志(relay log) MySQL slave 重放 relay log 中事件，将数据变更反映它自己的数据 而Canal就是把自己伪装成MySQL的一个slave节点，从而监听master的binary log变化。再把得到的变化信息通知给Canal的客户端，进而完成对其它数据库的同步。 ","date":"2023-01-24","objectID":"/redis%E9%AB%98%E7%BA%A7%E7%AF%87%E4%B9%8B%E5%A4%9A%E7%BA%A7%E7%BC%93%E5%AD%98/:16:1","tags":["lua"],"title":"Redis高级篇之多级缓存","uri":"/redis%E9%AB%98%E7%BA%A7%E7%AF%87%E4%B9%8B%E5%A4%9A%E7%BA%A7%E7%BC%93%E5%AD%98/"},{"categories":["redis"],"content":"5.2.2.安装Canal ","date":"2023-01-24","objectID":"/redis%E9%AB%98%E7%BA%A7%E7%AF%87%E4%B9%8B%E5%A4%9A%E7%BA%A7%E7%BC%93%E5%AD%98/:16:2","tags":["lua"],"title":"Redis高级篇之多级缓存","uri":"/redis%E9%AB%98%E7%BA%A7%E7%AF%87%E4%B9%8B%E5%A4%9A%E7%BA%A7%E7%BC%93%E5%AD%98/"},{"categories":["redis"],"content":"5.3.监听Canal Canal提供了各种语言的客户端，当Canal监听到binlog变化时，会通知Canal的客户端。 我们可以利用Canal提供的Java客户端，监听Canal通知消息。当收到变化的消息时，完成对缓存的更新。 不过这里我们会使用GitHub上的第三方开源的canal-starter客户端。地址：https://github.com/NormanGyllenhaal/canal-client 与SpringBoot完美整合，自动装配，比官方客户端要简单好用很多。 ","date":"2023-01-24","objectID":"/redis%E9%AB%98%E7%BA%A7%E7%AF%87%E4%B9%8B%E5%A4%9A%E7%BA%A7%E7%BC%93%E5%AD%98/:17:0","tags":["lua"],"title":"Redis高级篇之多级缓存","uri":"/redis%E9%AB%98%E7%BA%A7%E7%AF%87%E4%B9%8B%E5%A4%9A%E7%BA%A7%E7%BC%93%E5%AD%98/"},{"categories":["redis"],"content":"5.3.1.引入依赖： \u003cdependency\u003e \u003cgroupId\u003etop.javatool\u003c/groupId\u003e \u003cartifactId\u003ecanal-spring-boot-starter\u003c/artifactId\u003e \u003cversion\u003e1.2.1-RELEASE\u003c/version\u003e \u003c/dependency\u003e ","date":"2023-01-24","objectID":"/redis%E9%AB%98%E7%BA%A7%E7%AF%87%E4%B9%8B%E5%A4%9A%E7%BA%A7%E7%BC%93%E5%AD%98/:17:1","tags":["lua"],"title":"Redis高级篇之多级缓存","uri":"/redis%E9%AB%98%E7%BA%A7%E7%AF%87%E4%B9%8B%E5%A4%9A%E7%BA%A7%E7%BC%93%E5%AD%98/"},{"categories":["redis"],"content":"5.3.2.编写配置： canal: destination: heima # canal的集群名字，要与安装canal时设置的名称一致 server: 192.168.150.101:11111 # canal服务地址 ","date":"2023-01-24","objectID":"/redis%E9%AB%98%E7%BA%A7%E7%AF%87%E4%B9%8B%E5%A4%9A%E7%BA%A7%E7%BC%93%E5%AD%98/:17:2","tags":["lua"],"title":"Redis高级篇之多级缓存","uri":"/redis%E9%AB%98%E7%BA%A7%E7%AF%87%E4%B9%8B%E5%A4%9A%E7%BA%A7%E7%BC%93%E5%AD%98/"},{"categories":["redis"],"content":"5.3.3.修改Item实体类 通过@Id、@Column、等注解完成Item与数据库表字段的映射： package com.heima.item.pojo; import com.baomidou.mybatisplus.annotation.IdType; import com.baomidou.mybatisplus.annotation.TableField; import com.baomidou.mybatisplus.annotation.TableId; import com.baomidou.mybatisplus.annotation.TableName; import lombok.Data; import org.springframework.data.annotation.Id; import org.springframework.data.annotation.Transient; import javax.persistence.Column; import java.util.Date; @Data @TableName(\"tb_item\") public class Item { @TableId(type = IdType.AUTO) @Id private Long id;//商品id @Column(name = \"name\") private String name;//商品名称 private String title;//商品标题 private Long price;//价格（分） private String image;//商品图片 private String category;//分类名称 private String brand;//品牌名称 private String spec;//规格 private Integer status;//商品状态 1-正常，2-下架 private Date createTime;//创建时间 private Date updateTime;//更新时间 @TableField(exist = false) @Transient private Integer stock; @TableField(exist = false) @Transient private Integer sold; } ","date":"2023-01-24","objectID":"/redis%E9%AB%98%E7%BA%A7%E7%AF%87%E4%B9%8B%E5%A4%9A%E7%BA%A7%E7%BC%93%E5%AD%98/:17:3","tags":["lua"],"title":"Redis高级篇之多级缓存","uri":"/redis%E9%AB%98%E7%BA%A7%E7%AF%87%E4%B9%8B%E5%A4%9A%E7%BA%A7%E7%BC%93%E5%AD%98/"},{"categories":["redis"],"content":"5.3.4.编写监听器 通过实现EntryHandler\u003cT\u003e接口编写监听器，监听Canal消息。注意两点： 实现类通过@CanalTable(\"tb_item\")指定监听的表信息 EntryHandler的泛型是与表对应的实体类 package com.heima.item.canal; import com.github.benmanes.caffeine.cache.Cache; import com.heima.item.config.RedisHandler; import com.heima.item.pojo.Item; import org.springframework.beans.factory.annotation.Autowired; import org.springframework.stereotype.Component; import top.javatool.canal.client.annotation.CanalTable; import top.javatool.canal.client.handler.EntryHandler; @CanalTable(\"tb_item\") @Component public class ItemHandler implements EntryHandler\u003cItem\u003e { @Autowired private RedisHandler redisHandler; @Autowired private Cache\u003cLong, Item\u003e itemCache; @Override public void insert(Item item) { // 写数据到JVM进程缓存 itemCache.put(item.getId(), item); // 写数据到redis redisHandler.saveItem(item); } @Override public void update(Item before, Item after) { // 写数据到JVM进程缓存 itemCache.put(after.getId(), after); // 写数据到redis redisHandler.saveItem(after); } @Override public void delete(Item item) { // 删除数据到JVM进程缓存 itemCache.invalidate(item.getId()); // 删除数据到redis redisHandler.deleteItemById(item.getId()); } } 在这里对Redis的操作都封装到了RedisHandler这个对象中，是我们之前做缓存预热时编写的一个类，内容如下： package com.heima.item.config; import com.fasterxml.jackson.core.JsonProcessingException; import com.fasterxml.jackson.databind.ObjectMapper; import com.heima.item.pojo.Item; import com.heima.item.pojo.ItemStock; import com.heima.item.service.IItemService; import com.heima.item.service.IItemStockService; import org.springframework.beans.factory.InitializingBean; import org.springframework.beans.factory.annotation.Autowired; import org.springframework.data.redis.core.StringRedisTemplate; import org.springframework.stereotype.Component; import java.util.List; @Component public class RedisHandler implements InitializingBean { @Autowired private StringRedisTemplate redisTemplate; @Autowired private IItemService itemService; @Autowired private IItemStockService stockService; private static final ObjectMapper MAPPER = new ObjectMapper(); @Override public void afterPropertiesSet() throws Exception { // 初始化缓存 // 1.查询商品信息 List\u003cItem\u003e itemList = itemService.list(); // 2.放入缓存 for (Item item : itemList) { // 2.1.item序列化为JSON String json = MAPPER.writeValueAsString(item); // 2.2.存入redis redisTemplate.opsForValue().set(\"item:id:\" + item.getId(), json); } // 3.查询商品库存信息 List\u003cItemStock\u003e stockList = stockService.list(); // 4.放入缓存 for (ItemStock stock : stockList) { // 2.1.item序列化为JSON String json = MAPPER.writeValueAsString(stock); // 2.2.存入redis redisTemplate.opsForValue().set(\"item:stock:id:\" + stock.getId(), json); } } public void saveItem(Item item) { try { String json = MAPPER.writeValueAsString(item); redisTemplate.opsForValue().set(\"item:id:\" + item.getId(), json); } catch (JsonProcessingException e) { throw new RuntimeException(e); } } public void deleteItemById(Long id) { redisTemplate.delete(\"item:id:\" + id); } } ","date":"2023-01-24","objectID":"/redis%E9%AB%98%E7%BA%A7%E7%AF%87%E4%B9%8B%E5%A4%9A%E7%BA%A7%E7%BC%93%E5%AD%98/:17:4","tags":["lua"],"title":"Redis高级篇之多级缓存","uri":"/redis%E9%AB%98%E7%BA%A7%E7%AF%87%E4%B9%8B%E5%A4%9A%E7%BA%A7%E7%BC%93%E5%AD%98/"},{"categories":["汇编"],"content":"标志寄存器 ","date":"2023-01-17","objectID":"/%E6%B1%87%E7%BC%96%E4%B9%8B%E6%A0%87%E5%BF%97%E5%AF%84%E5%AD%98%E5%99%A8/:0:0","tags":[],"title":"汇编之标志寄存器","uri":"/%E6%B1%87%E7%BC%96%E4%B9%8B%E6%A0%87%E5%BF%97%E5%AF%84%E5%AD%98%E5%99%A8/"},{"categories":["汇编"],"content":"1、标志寄存器 CPU内部的寄存器中，有一种特殊的寄存器（对于不同的处理机，个数和结构都可能不同）具有以下3种作用。 （1）用来存储相关指令的某些执行结果； （2）用来为CPU执行相关指令提供行为依据； （3）用来控制CPU的相关工作方式。 这种特殊的寄存器在8086CPU中，被称为标志寄存器（flag）。 8086CPU的标志寄存器有16位，其中存储的信息通常被称为程序状态字（PSW-Program Status Word） flag寄存器是按位起作用的，它的每一位都有专门的含义，记录特定的信息。 在8086CPU的指令集中，有的指令的执行是影响标志寄存器的，比如，add、sub、mul、div、inc、or、and等，它们大都是运算指令（进行逻辑或算术运算）；有的指令的执行对标志寄存器没有影响，比如，mov、push、pop等，它们大都是传送指令 ","date":"2023-01-17","objectID":"/%E6%B1%87%E7%BC%96%E4%B9%8B%E6%A0%87%E5%BF%97%E5%AF%84%E5%AD%98%E5%99%A8/:1:0","tags":[],"title":"汇编之标志寄存器","uri":"/%E6%B1%87%E7%BC%96%E4%B9%8B%E6%A0%87%E5%BF%97%E5%AF%84%E5%AD%98%E5%99%A8/"},{"categories":["汇编"],"content":"1、零标志位 (ZF) 零标志位（Zero Flag）。它记录相关指令执行后，其结果是否为0。 如果结果为0，那么zf = 1(表示结果是0)；如果结果不为0，那么zf = 0。 mov ax, 1 sub ax, 1 ;执行后，结果为0，则zf = 1 mov ax, 2 sub ax, 1 ;执行后，结果不为0，则zf = 0 ","date":"2023-01-17","objectID":"/%E6%B1%87%E7%BC%96%E4%B9%8B%E6%A0%87%E5%BF%97%E5%AF%84%E5%AD%98%E5%99%A8/:1:1","tags":[],"title":"汇编之标志寄存器","uri":"/%E6%B1%87%E7%BC%96%E4%B9%8B%E6%A0%87%E5%BF%97%E5%AF%84%E5%AD%98%E5%99%A8/"},{"categories":["汇编"],"content":"2、奇偶标志位 (PF) 奇偶标志位（Parity Flag）。它记录相关指令执行后，其结果的所有bit位中1的个数是否为偶数。 如果1的个数为偶数，pf = 1，如果为奇数，那么pf = 0。 mov al, 1 add al, 10 ;执行后，结果为00001011B，其中有3（奇数）个1，则pf = 0； mov al, 1 or al, 2 ;执行后，结果为00000011B，其中有2（偶数）个1，则pf = 1； ","date":"2023-01-17","objectID":"/%E6%B1%87%E7%BC%96%E4%B9%8B%E6%A0%87%E5%BF%97%E5%AF%84%E5%AD%98%E5%99%A8/:1:2","tags":[],"title":"汇编之标志寄存器","uri":"/%E6%B1%87%E7%BC%96%E4%B9%8B%E6%A0%87%E5%BF%97%E5%AF%84%E5%AD%98%E5%99%A8/"},{"categories":["汇编"],"content":"3、符号标志位(SF) 符号标志位(Symbol Flag)。它记录相关指令执行后，其结果是否为负。 如果结果为负，sf = 1；如果非负，sf = 0。 计算机中通常用补码来表示有符号数据。计算机中的一个数据可以看作是有符号数，也可以看成是无符号数。 00000001B，可以看作为无符号数1，或有符号数+1； 10000001B，可以看作为无符号数129，也可以看作有符号数-127。 对于同一个二进制数据，计算机可以将它当作无符号数据来运算，也可以当作有符号数据来运算 CPU在执行add等指令的时候，就包含了两种含义:可以将add指令进行的运算当作无符号数的运算，也可以将add指令进行的运算当作有符号数的运算 SF标志，就是CPU对有符号数运算结果的一种记录，它记录数据的正负。在我们将数据当作有符号数来运算的时候，可以通过它来得知结果的正负。如果我们将数据当作无符号数来运算，SF的值则没有意义，虽然相关的指令影响了它的值 mov al, 10000001B add al, 1 ;执行后，结果为10000010B，sf = 1，表示：如果指令进行的是有符号数运算，那么结果为负； mov al, 10000001B add al, 01111111B ;执行后，结果为0，sf = 0，表示：如果指令进行的是有符号数运算，那么结果为非负 ","date":"2023-01-17","objectID":"/%E6%B1%87%E7%BC%96%E4%B9%8B%E6%A0%87%E5%BF%97%E5%AF%84%E5%AD%98%E5%99%A8/:1:3","tags":[],"title":"汇编之标志寄存器","uri":"/%E6%B1%87%E7%BC%96%E4%B9%8B%E6%A0%87%E5%BF%97%E5%AF%84%E5%AD%98%E5%99%A8/"},{"categories":["汇编"],"content":"3、进位标志位(CF) 进位标志位(Carry Flag)。一般情况下，在进行无符号数运算的时候，它记录了运算结果的最高有效位向更高位的进位值，或从更高位的借位值 97H - 98H 产生借位CF = 1 ==\u003e (al) = 197H - 98H = FFH ","date":"2023-01-17","objectID":"/%E6%B1%87%E7%BC%96%E4%B9%8B%E6%A0%87%E5%BF%97%E5%AF%84%E5%AD%98%E5%99%A8/:1:4","tags":[],"title":"汇编之标志寄存器","uri":"/%E6%B1%87%E7%BC%96%E4%B9%8B%E6%A0%87%E5%BF%97%E5%AF%84%E5%AD%98%E5%99%A8/"},{"categories":["汇编"],"content":"4、溢出标志位(OF) 溢出标志位(Overflow Flag)。一般情况下，OF记录了有符号数运算的结果是否发生了溢出。 如果发生溢出，OF = 1；如果没有，OF = 0。 CF和OF的区别：CF是对无符号数运算有意义的标志位，而OF是对有符号数运算有意义的标志位 CPU在执行add等指令的时候，就包含了两种含义：无符号数运算和有符号数运算。 对于无符号数运算，CPU用CF位来记录是否产生了进位； 对于有符号数运算，CPU用OF位来记录是否产生了溢出，当然，还要用SF位来记录结果的符号。 mov al, 98 add al, 99 ;执行后将产生溢出。因为进行的\"有符号数\"运算是：（al）=（al）+ 99 = 98 + 99=197 = C5H 为-59的补码 ;而结果197超出了机器所能表示的8位有符号数的范围：-128-127。 ;add 指令执行后：无符号运算没有进位CF=0，有符号运算溢出OF=1 ;当取出的数据C5H按无符号解析C5H = 197, 当按有符号解析通过SP得知数据为负,即C5H为-59补码存储， mov al，0F0H ;F0H，为有符号数-16的补码 -Not(F0 - 1) add al，088H ;88H，为有符号数-120的补码 -Not(88- 1) ;执行后，将产生溢出。因为add al, 088H进行的有符号数运算结果是：（al）= -136 ;而结果-136超出了机器所能表示的8位有符号数的范围：-128-127。 ;add 指令执行后：无符号运算有进位CF=1，有符号运算溢出OF=1 ","date":"2023-01-17","objectID":"/%E6%B1%87%E7%BC%96%E4%B9%8B%E6%A0%87%E5%BF%97%E5%AF%84%E5%AD%98%E5%99%A8/:1:5","tags":[],"title":"汇编之标志寄存器","uri":"/%E6%B1%87%E7%BC%96%E4%B9%8B%E6%A0%87%E5%BF%97%E5%AF%84%E5%AD%98%E5%99%A8/"},{"categories":["汇编"],"content":"2、adc指令和sbb指令 adc是带进位加法指令，它利用了CF位上记录的进位值。 指令格式：adc 操作对象1, 操作对象2 功能：操作对象1 = 操作对象1 + 操作对象2 + CF mov ax, 2 mov bx, 1 sub bx, ax ;无符号运算借位CF=1，有符号运算OF = 0 adc ax, 1 ;执行后，（ax）= 4。adc执行时，相当于计算：(ax)+1+CF = 2+1+1 = 4。 ;计算1EF000H+201000H，结果放在ax（高16位）和bx（低16位）中。 ;将计算分两步进行，先将低16位相加，然后将高16位和进位值相加。 mov ax, 001EH mov bx, 0F000H add bx, 1000H adc ax, 0020H sbb指令 sbb是带借位减法指令，它利用了CF位上记录的借位值。 指令格式：sbb 操作对象1, 操作对象2 功能：操作对象1 = 操作对象1 - 操作对象2 - CF ;计算 003E1000H - 00202000H，结果放在ax，bx中，程序如下： mov bx, 1000H mov ax, 003EH sub bx, 2000H sbb ax, 0020H ","date":"2023-01-17","objectID":"/%E6%B1%87%E7%BC%96%E4%B9%8B%E6%A0%87%E5%BF%97%E5%AF%84%E5%AD%98%E5%99%A8/:2:0","tags":[],"title":"汇编之标志寄存器","uri":"/%E6%B1%87%E7%BC%96%E4%B9%8B%E6%A0%87%E5%BF%97%E5%AF%84%E5%AD%98%E5%99%A8/"},{"categories":["汇编"],"content":"3、cmp指令 cmp是比较指令，cmp的功能相当于减法指令，只是不保存结果。cmp指令执行后，将对标志寄存器产生影响。 其他相关指令通过识别这些被影响的标志寄存器位来得知比较结果。 cmp指令格式：cmp 操作对象1，操作对象2 例如： 指令cmp ax, ax，做（ax）-（ax）的运算，结果为0，但并不在ax中保存，仅影响flag的相关各位。 指令执行后：zf=1，pf=1，sf=0，cf=0，of=0。 CPU在执行cmp指令的时候，也包含两种含义：进行无符号数运算和进行有符号数运算。 cmp ax, bx 无符号比较时 (ax) = (bx) zf = 1 (ax) ≠ (bx) zf = 0 (ax) \u003c (bx) cf = 1 (ax) ≥ (bx) cf = 0 (ax) \u003e (bx) cf = 0 且 zf = 0 (ax) ≤ (bx) cf = 1 且 zf = 1 上面的表格可以正推也可以逆推 如果用cmp来进行有符号数比较时 SF只能记录实际结果的正负，发生溢出的时候，实际结果的正负不能说明逻辑上真正结果的正负。 但是逻辑上的结果的正负，才是cmp指令所求的真正结果，所以我们在考察SF的同时考察OF，就可以得知逻辑上真正结果的正负，同时就知道比较的结果。 mov ah, 08AH ; -Not(8A-1) = -118 即当成有符号数时为-118 mov bh, 070H ; 有符号数时最高位为0为正数， 70H = 112 cmp ah, bh ;（ah）-（bh）实际得到的结果是1AH ; 在逻辑上，运算所应该得到的结果是：（-118）- 112 = -230 ; sf记录实际结果的正负，所以sf=0 cmp ah, bh （1）如果sf=1，而of=0 。 of=0说明没有溢出，逻辑上真正结果的正负=实际结果的正负； sf=1，实际结果为负，所以逻辑上真正的结果为负，所以（ah）\u003c（bh） （2）如果sf=1，而of=1： of=1，说明有溢出，逻辑上真正结果的正负≠实际结果的正负； sf=1，实际结果为负。 实际结果为负，而又有溢出，这说明是由于溢出导致了实际结果为负，，如果因为溢出导致了实际结果为负，那么逻辑上真正的结果必然为正。 这样，sf=1，of=1，说明了（ah）\u003e（bh）。 （3）如果sf=0，而of=1。of=1，说明有溢出，逻辑上真正结果的正负≠实际结果的正负；sf=0，实际结果非负。而of=1说明有溢出，则结果非0，所以，实际结果为正。 实际结果为正，而又有溢出，这说明是由于溢出导致了实际结果非负，如果因为溢出导致了实际结果为正，那么逻辑上真正的结果必然为负。这样，sf=0，of=1，说明了（ah）\u003c（bh）。 （4）如果sf=0，而of=0 of=0，说明没有溢出，逻辑上真正结果的正负=实际结果的正负；sf=0，实际结果非负，所以逻辑上真正的结果非负，所以（ah）≥（bh）。 ","date":"2023-01-17","objectID":"/%E6%B1%87%E7%BC%96%E4%B9%8B%E6%A0%87%E5%BF%97%E5%AF%84%E5%AD%98%E5%99%A8/:3:0","tags":[],"title":"汇编之标志寄存器","uri":"/%E6%B1%87%E7%BC%96%E4%B9%8B%E6%A0%87%E5%BF%97%E5%AF%84%E5%AD%98%E5%99%A8/"},{"categories":["汇编"],"content":"4、检测比较结果的条件转移指令 可以根据某种条件，决定是否修改IP的指令 jcxz它可以检测cx中的数值，如果（cx）=0，就修改IP，否则什么也不做。 所有条件转移指令的转移位移都是[-128，127]。 多数条件转移指令都检测标志寄存器的相关标志位，根据检测的结果来决定是否修改IP 这些条件转移指令通常都和cmp相配合使用,它们所检测的标志位，都是cmp指令进行无符号数比较的时记录比较结果的标志位 根据无符号数的比较结果进行转移的条件转移指令（它们检测zf、cf的值） 指令 含义 检测的相关标志位 je 等于则转移 zf = 1 jne 不等于则转移 zf = 0 jb 低于则转移 cf = 1 jnb 不低于则转移 cf = 0 ja 高于则转移 cf = 0 且 zf = 0 jna 不高于则转移 cf = 1 且 zf = 1 j：jump，e：equal，b：below，a：above，n：not ;编程，统计data段中数值为8的字节的个数，用ax保存统计结果。 mov ax, data mov ds, ax mov bx, 0 ;ds:bx指向第一个字节 mov ax, 0 ;初始化累加器mov cx，8 s: cmp byte ptr [bx], 8 ;和8进行比较 jne next ;如果不相等转到next，继续循环 inc ax ;如果相等就将计数值加1 next: inc bx loop s ;程序执行后：（ax）=3 ","date":"2023-01-17","objectID":"/%E6%B1%87%E7%BC%96%E4%B9%8B%E6%A0%87%E5%BF%97%E5%AF%84%E5%AD%98%E5%99%A8/:4:0","tags":[],"title":"汇编之标志寄存器","uri":"/%E6%B1%87%E7%BC%96%E4%B9%8B%E6%A0%87%E5%BF%97%E5%AF%84%E5%AD%98%E5%99%A8/"},{"categories":["汇编"],"content":"5、DF标志和串传送指令 方向标志位。在串处理指令中，控制每次操作后si、di的增减。 df = 0每次操作后si、di递增； df = 1每次操作后si、di递减。 格式：movsb 功能：将ds:si指向的内存单元中的字节送入es:di中，然后根据标志寄存器df位的值，将si和di递增或递减 格式：movsw 功能：将ds:si指向的内存字单元中的字送入es:di中，然后根据标志寄存器df位的值，将si和di递增2或递减2。 格式：rep movsb movsb和movsw进行的是串传送操作中的一个步骤，一般来说，movsb和movsw都和rep配合使用， 功能：rep的作用是根据cx的值，重复执行后面的串传送指令 8086CPU提供下面两条指令对df位进行设置。 cld指令：将标志寄存器的df位置0 std指令：将标志寄存器的df位置1 ;将data段中的第一个字符串复制到它后面的空间中。 data segment db 'Welcome to masm!' db 16 dup (0) data ends mov ax, data mov ds, ax mov si, 0 ;ds:si 指向data:0 mov es, ax mov di, 16 ;es:di指向data:0010 mov cx, 16 ;（cx）=16，rep循环16次 cld ;设置df=0，正向传送 rep movsb ","date":"2023-01-17","objectID":"/%E6%B1%87%E7%BC%96%E4%B9%8B%E6%A0%87%E5%BF%97%E5%AF%84%E5%AD%98%E5%99%A8/:5:0","tags":[],"title":"汇编之标志寄存器","uri":"/%E6%B1%87%E7%BC%96%E4%B9%8B%E6%A0%87%E5%BF%97%E5%AF%84%E5%AD%98%E5%99%A8/"},{"categories":["汇编"],"content":"6、pushf和popf pushf的功能是将标志寄存器的值压栈，而popf是从栈中弹出数据，送入标志寄存器中 pushf和popf，为直接访问标志寄存器提供了一种方法。 ","date":"2023-01-17","objectID":"/%E6%B1%87%E7%BC%96%E4%B9%8B%E6%A0%87%E5%BF%97%E5%AF%84%E5%AD%98%E5%99%A8/:6:0","tags":[],"title":"汇编之标志寄存器","uri":"/%E6%B1%87%E7%BC%96%E4%B9%8B%E6%A0%87%E5%BF%97%E5%AF%84%E5%AD%98%E5%99%A8/"},{"categories":["汇编"],"content":"7、总结 当使用某些运算指令运算时，如果数据发生溢出或需要借位，都会在标志寄存器的某个位有所体现。那么可以根据这个标志寄存器实现大数之间的运算 串转移指令，算是封装的汇编api ","date":"2023-01-17","objectID":"/%E6%B1%87%E7%BC%96%E4%B9%8B%E6%A0%87%E5%BF%97%E5%AF%84%E5%AD%98%E5%99%A8/:7:0","tags":[],"title":"汇编之标志寄存器","uri":"/%E6%B1%87%E7%BC%96%E4%B9%8B%E6%A0%87%E5%BF%97%E5%AF%84%E5%AD%98%E5%99%A8/"},{"categories":["redis","分布式系统"],"content":"[toc] 基于Redis集群解决单机Redis存在的问题 单机的Redis存在四大问题： 分布式缓存 1.Redis持久化 Redis有两种持久化方案： RDB持久化 AOF持久化 ","date":"2023-01-15","objectID":"/redis%E9%AB%98%E7%BA%A7%E7%AF%87%E4%B9%8B%E5%88%86%E5%B8%83%E5%BC%8F%E7%BC%93%E5%AD%98/:0:0","tags":[],"title":"Redis高级篇之分布式缓存","uri":"/redis%E9%AB%98%E7%BA%A7%E7%AF%87%E4%B9%8B%E5%88%86%E5%B8%83%E5%BC%8F%E7%BC%93%E5%AD%98/"},{"categories":["redis","分布式系统"],"content":"1.1.RDB持久化 RDB全称Redis Database Backup file（Redis数据备份文件），也被叫做Redis数据快照。简单来说就是把内存中的所有数据都记录到磁盘中。当Redis实例故障重启后，从磁盘读取快照文件，恢复数据。快照文件称为RDB文件，默认是保存在当前运行目录。 ","date":"2023-01-15","objectID":"/redis%E9%AB%98%E7%BA%A7%E7%AF%87%E4%B9%8B%E5%88%86%E5%B8%83%E5%BC%8F%E7%BC%93%E5%AD%98/:1:0","tags":[],"title":"Redis高级篇之分布式缓存","uri":"/redis%E9%AB%98%E7%BA%A7%E7%AF%87%E4%B9%8B%E5%88%86%E5%B8%83%E5%BC%8F%E7%BC%93%E5%AD%98/"},{"categories":["redis","分布式系统"],"content":"1.1.1.执行时机 RDB持久化在四种情况下会执行： 执行save命令 执行bgsave命令 Redis停机时 触发RDB条件时 1）save命令 执行下面的命令，可以立即执行一次RDB： save命令会导致主进程执行RDB，这个过程中其它所有命令都会被阻塞。只有在数据迁移时可能用到。 2）bgsave命令 下面的命令可以异步执行RDB： 这个命令执行后会开启独立进程完成RDB，主进程可以持续处理用户请求，不受影响。 3）停机时 Redis停机时会执行一次save命令，实现RDB持久化。 4）触发RDB条件 Redis内部有触发RDB的机制，可以在redis.conf文件中找到，格式如下： # 900秒内，如果至少有1个key被修改，则执行bgsave ， 如果是save \"\" 则表示禁用RDB save 900 1 # 依次类推 save 300 10 save 60 10000 RDB的其它配置也可以在redis.conf文件中设置： # 是否压缩 ,建议不开启，压缩也会消耗cpu，磁盘的话不值钱 rdbcompression yes # RDB文件名称 dbfilename dump.rdb # 文件保存的路径目录 dir ./ SAVE 保存是阻塞主进程，客户端无法连接redis，等SAVE完成后，主进程才开始工作，客户端可以连接 BGSAVE 是fork一个save的子进程，在执行save过程中，不影响主进程，客户端可以正常链接redis，等子进程fork执行save完成后，通知主进程，子进程关闭。 ","date":"2023-01-15","objectID":"/redis%E9%AB%98%E7%BA%A7%E7%AF%87%E4%B9%8B%E5%88%86%E5%B8%83%E5%BC%8F%E7%BC%93%E5%AD%98/:1:1","tags":[],"title":"Redis高级篇之分布式缓存","uri":"/redis%E9%AB%98%E7%BA%A7%E7%AF%87%E4%B9%8B%E5%88%86%E5%B8%83%E5%BC%8F%E7%BC%93%E5%AD%98/"},{"categories":["redis","分布式系统"],"content":"1.1.2.RDB原理 RDB持久化是指在指定的时间间隔内将redis内存中的数据集快照写入磁盘，实现原理是redis服务在指定的时间间隔内先fork一个子进程，由子进程将数据集写入临时文件，写入成功后，再替换之前的文件，用二进制压缩存储，生成dump.rdb文件存放在磁盘中。 fork采用的是copy-on-write技术： 当主进程执行读操作时，访问共享内存； 当主进程执行写操作时，则会拷贝一份数据，执行写操作。 ","date":"2023-01-15","objectID":"/redis%E9%AB%98%E7%BA%A7%E7%AF%87%E4%B9%8B%E5%88%86%E5%B8%83%E5%BC%8F%E7%BC%93%E5%AD%98/:1:2","tags":[],"title":"Redis高级篇之分布式缓存","uri":"/redis%E9%AB%98%E7%BA%A7%E7%AF%87%E4%B9%8B%E5%88%86%E5%B8%83%E5%BC%8F%E7%BC%93%E5%AD%98/"},{"categories":["redis","分布式系统"],"content":"1.1.3.小结 RDB方式bgsave的基本流程？ fork主进程得到一个子进程，共享内存空间 子进程读取内存数据并写入新的RDB文件 用新RDB文件替换旧的RDB文件 RDB会在什么时候执行？save 60 1000代表什么含义？ 默认是服务停止时 代表60秒内至少执行1000次修改则触发RDB RDB优点？ 一旦采用该方式，那么你的整个Redis数据库将只包含一个文件，这对于文件备份而言是非常完美的。比如，你可能打算每个小时归档一次最近24小时的数据，同时还要每天归档一次最近30天的数据。通过这样的备份策略，一旦系统出现灾难性故障，我们可以非常容易的进行恢复。 对于灾难恢复而言，RDB是非常不错的选择。因为我们可以非常轻松的将一个单独的文件压缩后再转移到其它存储介质上。 性能最大化。对于Redis的服务进程而言，在开始持久化(bgsave)时，它唯一需要做的只是fork出子进程，之后再由子进程完成这些持久化的工作，这样就可以极大的避免服务进程执行IO操作了。(save会阻塞主进程) 相比于AOF机制，如果数据集很大，RDB的启动效率会更高。 RDB的缺点？ RDB执行间隔时间长，两次RDB之间写入数据有丢失的风险 当内存里数据集较大时，fork子进程、压缩、写出RDB文件都比较耗时，那么fork子进程就比较消耗性能，容易拖垮服务 ","date":"2023-01-15","objectID":"/redis%E9%AB%98%E7%BA%A7%E7%AF%87%E4%B9%8B%E5%88%86%E5%B8%83%E5%BC%8F%E7%BC%93%E5%AD%98/:1:3","tags":[],"title":"Redis高级篇之分布式缓存","uri":"/redis%E9%AB%98%E7%BA%A7%E7%AF%87%E4%B9%8B%E5%88%86%E5%B8%83%E5%BC%8F%E7%BC%93%E5%AD%98/"},{"categories":["redis","分布式系统"],"content":"1.2.AOF持久化 ","date":"2023-01-15","objectID":"/redis%E9%AB%98%E7%BA%A7%E7%AF%87%E4%B9%8B%E5%88%86%E5%B8%83%E5%BC%8F%E7%BC%93%E5%AD%98/:2:0","tags":[],"title":"Redis高级篇之分布式缓存","uri":"/redis%E9%AB%98%E7%BA%A7%E7%AF%87%E4%B9%8B%E5%88%86%E5%B8%83%E5%BC%8F%E7%BC%93%E5%AD%98/"},{"categories":["redis","分布式系统"],"content":"1.2.1.AOF原理 AOF全称为Append Only File（追加文件）。Redis处理的每一个写命令都会记录在AOF文件，可以看做是命令日志文件。 ","date":"2023-01-15","objectID":"/redis%E9%AB%98%E7%BA%A7%E7%AF%87%E4%B9%8B%E5%88%86%E5%B8%83%E5%BC%8F%E7%BC%93%E5%AD%98/:2:1","tags":[],"title":"Redis高级篇之分布式缓存","uri":"/redis%E9%AB%98%E7%BA%A7%E7%AF%87%E4%B9%8B%E5%88%86%E5%B8%83%E5%BC%8F%E7%BC%93%E5%AD%98/"},{"categories":["redis","分布式系统"],"content":"1.2.2.AOF配置 AOF默认是关闭的，需要修改redis.conf配置文件来开启AOF： # 是否开启AOF功能，默认是no appendonly yes # AOF文件的名称 appendfilename \"appendonly.aof\" AOF的命令记录的频率也可以通过redis.conf文件来配： # 表示每执行一次写命令，立即记录到AOF文件 appendfsync always # 写命令执行完先放入AOF缓冲区，然后表示每隔1秒将缓冲区数据写到AOF文件，是默认方案 appendfsync everysec # 写命令执行完先放入AOF缓冲区，由操作系统决定何时将缓冲区内容写回磁盘 appendfsync no 三种策略对比： ","date":"2023-01-15","objectID":"/redis%E9%AB%98%E7%BA%A7%E7%AF%87%E4%B9%8B%E5%88%86%E5%B8%83%E5%BC%8F%E7%BC%93%E5%AD%98/:2:2","tags":[],"title":"Redis高级篇之分布式缓存","uri":"/redis%E9%AB%98%E7%BA%A7%E7%AF%87%E4%B9%8B%E5%88%86%E5%B8%83%E5%BC%8F%E7%BC%93%E5%AD%98/"},{"categories":["redis","分布式系统"],"content":"1.2.3.AOF文件重写 因为是记录命令，AOF文件会比RDB文件大的多。而且AOF会记录对同一个key的多次写操作，但只有最后一次写操作才有意义。通过执行bgrewriteaof命令，可以让AOF文件执行重写功能，用最少的命令达到相同效果。 如图，AOF原本有三个命令，但是set num 123 和 set num 666都是对num的操作，第二次会覆盖第一次的值，因此第一个命令记录下来没有意义。 所以重写命令后，AOF文件内容就是：mset name jack num 666 Redis也会在触发阈值时自动去重写AOF文件。阈值也可以在redis.conf中配置： # AOF文件比上次文件增长超过多少百分比，则触发重写 auto-aof-rewrite-percentage 100 # AOF文件体积最小多大以上才触发重写 auto-aof-rewrite-min-size 64mb ","date":"2023-01-15","objectID":"/redis%E9%AB%98%E7%BA%A7%E7%AF%87%E4%B9%8B%E5%88%86%E5%B8%83%E5%BC%8F%E7%BC%93%E5%AD%98/:2:3","tags":[],"title":"Redis高级篇之分布式缓存","uri":"/redis%E9%AB%98%E7%BA%A7%E7%AF%87%E4%B9%8B%E5%88%86%E5%B8%83%E5%BC%8F%E7%BC%93%E5%AD%98/"},{"categories":["redis","分布式系统"],"content":"1.3.RDB与AOF对比 RDB和AOF各有自己的优缺点，如果对数据安全性要求较高，在实际开发中往往会结合两者来使用。 2.Redis主从 ","date":"2023-01-15","objectID":"/redis%E9%AB%98%E7%BA%A7%E7%AF%87%E4%B9%8B%E5%88%86%E5%B8%83%E5%BC%8F%E7%BC%93%E5%AD%98/:3:0","tags":[],"title":"Redis高级篇之分布式缓存","uri":"/redis%E9%AB%98%E7%BA%A7%E7%AF%87%E4%B9%8B%E5%88%86%E5%B8%83%E5%BC%8F%E7%BC%93%E5%AD%98/"},{"categories":["redis","分布式系统"],"content":"2.1.搭建主从架构 单节点Redis的并发能力是有上限的，要进一步提高Redis的并发能力，就需要搭建主从集群，实现读写分离。 像MySQL一样，redis是支持主从同步的，而且也支持一主多从以及多级从结构。 主从结构，一是为了纯粹的冗余备份，二是为了提升读性能，比如很消耗性能的SORT就可以由从服务器来承担。 redis的主从同步是异步进行的，这意味着主从同步不会影响主逻辑，也不会降低redis的处理性能。 主从架构中，可以考虑关闭主服务器的数据持久化功能，只让从服务器进行持久化，这样可以提高主服务器的处理性能。 ","date":"2023-01-15","objectID":"/redis%E9%AB%98%E7%BA%A7%E7%AF%87%E4%B9%8B%E5%88%86%E5%B8%83%E5%BC%8F%E7%BC%93%E5%AD%98/:4:0","tags":[],"title":"Redis高级篇之分布式缓存","uri":"/redis%E9%AB%98%E7%BA%A7%E7%AF%87%E4%B9%8B%E5%88%86%E5%B8%83%E5%BC%8F%E7%BC%93%E5%AD%98/"},{"categories":["redis","分布式系统"],"content":"2.2.主从数据同步原理 ","date":"2023-01-15","objectID":"/redis%E9%AB%98%E7%BA%A7%E7%AF%87%E4%B9%8B%E5%88%86%E5%B8%83%E5%BC%8F%E7%BC%93%E5%AD%98/:5:0","tags":[],"title":"Redis高级篇之分布式缓存","uri":"/redis%E9%AB%98%E7%BA%A7%E7%AF%87%E4%B9%8B%E5%88%86%E5%B8%83%E5%BC%8F%E7%BC%93%E5%AD%98/"},{"categories":["redis","分布式系统"],"content":"2.2.1.全量同步 主从第一次建立连接时，会执行全量同步，将master节点的所有数据都拷贝给slave节点，流程： 这里有一个问题，master如何得知salve是第一次来连接呢？？ 有几个概念，可以作为判断依据： Replication Id：简称replid，是数据集的标记，id一致则说明是同一数据集。每一个master都有唯一的replid，slave则会继承master节点的replid offset：偏移量，随着记录在repl_baklog中的数据增多而逐渐增大。slave完成同步时也会记录当前同步的offset。如果slave的offset小于master的offset，说明slave数据落后于master，需要更新。 因此slave做数据同步，必须向master声明自己的replication id 和offset，master才可以判断到底需要同步哪些数据。 因为slave原本也是一个master，有自己的replid和offset，当第一次变成slave，与master建立连接时，发送的replid和offset是自己的replid和offset。 master判断发现slave发送来的replid与自己的不一致，说明这是一个全新的slave，就知道要做全量同步了。 master会将自己的replid和offset都发送给这个slave，slave保存这些信息。以后slave的replid就与master一致了。 因此，master判断一个节点是否是第一次同步的依据，就是看replid是否一致。 如图： 完整流程描述： slave节点请求增量同步 master节点判断replid，发现不一致，拒绝增量同步，开启全量同步 master将完整内存数据生成RDB，发送RDB到slave slave清空本地数据，加载master的RDB master将RDB期间的命令记录在repl_baklog，并持续将log中的命令发送给slave slave执行接收到的命令，保持与master之间的同步 ","date":"2023-01-15","objectID":"/redis%E9%AB%98%E7%BA%A7%E7%AF%87%E4%B9%8B%E5%88%86%E5%B8%83%E5%BC%8F%E7%BC%93%E5%AD%98/:5:1","tags":[],"title":"Redis高级篇之分布式缓存","uri":"/redis%E9%AB%98%E7%BA%A7%E7%AF%87%E4%B9%8B%E5%88%86%E5%B8%83%E5%BC%8F%E7%BC%93%E5%AD%98/"},{"categories":["redis","分布式系统"],"content":"2.2.2.增量同步 全量同步需要先做RDB，然后将RDB文件通过网络传输个slave，成本太高了。因此除了第一次做全量同步，其它大多数时候slave与master都是做增量同步。 什么是增量同步？就是只更新slave与master存在差异的部分数据。如图： 那么master怎么知道slave与自己的数据差异在哪里呢? ","date":"2023-01-15","objectID":"/redis%E9%AB%98%E7%BA%A7%E7%AF%87%E4%B9%8B%E5%88%86%E5%B8%83%E5%BC%8F%E7%BC%93%E5%AD%98/:5:2","tags":[],"title":"Redis高级篇之分布式缓存","uri":"/redis%E9%AB%98%E7%BA%A7%E7%AF%87%E4%B9%8B%E5%88%86%E5%B8%83%E5%BC%8F%E7%BC%93%E5%AD%98/"},{"categories":["redis","分布式系统"],"content":"2.2.3.repl_backlog原理 master怎么知道slave与自己的数据差异在哪里呢? 这就要说到全量同步时的repl_baklog文件了。 这个文件是一个固定大小的数组，只不过数组是环形，也就是说角标到达数组末尾后，会再次从0开始读写，这样数组头部的数据就会被覆盖。 repl_baklog中会记录Redis处理过的命令日志及offset，包括master当前的offset，和slave已经拷贝到的offset： slave与master的offset之间的差异，就是salve需要增量拷贝的数据了。 随着不断有数据写入，master的offset逐渐变大，slave也不断的拷贝，追赶master的offset： 直到数组被填满： 此时，如果有新的数据写入，就会覆盖数组中的旧数据。不过，旧的数据只要是绿色的，说明是已经被同步到slave的数据，即便被覆盖了也没什么影响。因为未同步的仅仅是红色部分。 但是，如果slave出现网络阻塞，导致master的offset远远超过了slave的offset： 如果master继续写入新数据，其offset就会覆盖旧的数据，直到将slave现在的offset也覆盖： 棕色框中的红色部分，就是尚未同步，但是却已经被覆盖的数据。此时如果slave恢复，需要同步，却发现自己的offset都没有了，无法完成增量同步了。只能做全量同步。 ","date":"2023-01-15","objectID":"/redis%E9%AB%98%E7%BA%A7%E7%AF%87%E4%B9%8B%E5%88%86%E5%B8%83%E5%BC%8F%E7%BC%93%E5%AD%98/:5:3","tags":[],"title":"Redis高级篇之分布式缓存","uri":"/redis%E9%AB%98%E7%BA%A7%E7%AF%87%E4%B9%8B%E5%88%86%E5%B8%83%E5%BC%8F%E7%BC%93%E5%AD%98/"},{"categories":["redis","分布式系统"],"content":"2.3.主从同步优化 主从同步可以保证主从数据的一致性，非常重要 可以从以下几个方面来优化Redis主从就集群： 在master中配置repl-diskless-sync yes启用无磁盘复制，避免全量同步时的磁盘IO。 Redis单节点上的内存占用不要太大，减少RDB导致的过多磁盘IO 适当提高repl_baklog的大小，发现slave宕机时尽快实现故障恢复，尽可能避免全量同步 限制一个master上的slave节点数量，如果实在是太多slave，则可以采用主-从-从链式结构，减少master压力 主从从架构图： ","date":"2023-01-15","objectID":"/redis%E9%AB%98%E7%BA%A7%E7%AF%87%E4%B9%8B%E5%88%86%E5%B8%83%E5%BC%8F%E7%BC%93%E5%AD%98/:6:0","tags":[],"title":"Redis高级篇之分布式缓存","uri":"/redis%E9%AB%98%E7%BA%A7%E7%AF%87%E4%B9%8B%E5%88%86%E5%B8%83%E5%BC%8F%E7%BC%93%E5%AD%98/"},{"categories":["redis","分布式系统"],"content":"2.4.小结 简述全量同步和增量同步区别？ 全量同步：master将完整内存数据生成RDB，发送RDB到slave。后续命令则记录在repl_baklog，逐个发送给slave 增量同步：slave提交自己的offset到master，master获取repl_baklog中从offset之后的命令给slave 什么时候执行全量同步？ slave节点第一次连接master节点时 slave节点断开时间太久，repl_baklog中的offset已经被覆盖时 什么时候执行增量同步？ slave节点断开又恢复，并且在repl_baklog中能找到offset时 3.Redis哨兵 Redis提供了哨兵（Sentinel）机制来实现主从集群的自动故障恢复。 ","date":"2023-01-15","objectID":"/redis%E9%AB%98%E7%BA%A7%E7%AF%87%E4%B9%8B%E5%88%86%E5%B8%83%E5%BC%8F%E7%BC%93%E5%AD%98/:7:0","tags":[],"title":"Redis高级篇之分布式缓存","uri":"/redis%E9%AB%98%E7%BA%A7%E7%AF%87%E4%B9%8B%E5%88%86%E5%B8%83%E5%BC%8F%E7%BC%93%E5%AD%98/"},{"categories":["redis","分布式系统"],"content":"3.1.哨兵原理 ","date":"2023-01-15","objectID":"/redis%E9%AB%98%E7%BA%A7%E7%AF%87%E4%B9%8B%E5%88%86%E5%B8%83%E5%BC%8F%E7%BC%93%E5%AD%98/:8:0","tags":[],"title":"Redis高级篇之分布式缓存","uri":"/redis%E9%AB%98%E7%BA%A7%E7%AF%87%E4%B9%8B%E5%88%86%E5%B8%83%E5%BC%8F%E7%BC%93%E5%AD%98/"},{"categories":["redis","分布式系统"],"content":"3.1.1.集群结构和作用 哨兵的结构如图： 哨兵的作用如下： 监控：Sentinel 会不断检查您的master和slave是否按预期工作 自动故障恢复：如果master故障，Sentinel会将一个slave提升为master。当故障实例恢复后也以新的master为主 通知：Sentinel充当Redis客户端的服务发现来源，当集群发生故障转移时，会将最新信息推送给Redis的客户端 ","date":"2023-01-15","objectID":"/redis%E9%AB%98%E7%BA%A7%E7%AF%87%E4%B9%8B%E5%88%86%E5%B8%83%E5%BC%8F%E7%BC%93%E5%AD%98/:8:1","tags":[],"title":"Redis高级篇之分布式缓存","uri":"/redis%E9%AB%98%E7%BA%A7%E7%AF%87%E4%B9%8B%E5%88%86%E5%B8%83%E5%BC%8F%E7%BC%93%E5%AD%98/"},{"categories":["redis","分布式系统"],"content":"3.1.2.集群监控原理 Sentinel基于心跳机制监测服务状态，每隔1秒向集群的每个实例发送ping命令： 主观下线：如果某sentinel节点发现某实例未在规定时间响应，则认为该实例主观下线。 客观下线：若超过指定数量（quorum）的sentinel都认为该实例主观下线，则该实例客观下线。quorum值最好超过Sentinel实例数量的一半。 ","date":"2023-01-15","objectID":"/redis%E9%AB%98%E7%BA%A7%E7%AF%87%E4%B9%8B%E5%88%86%E5%B8%83%E5%BC%8F%E7%BC%93%E5%AD%98/:8:2","tags":[],"title":"Redis高级篇之分布式缓存","uri":"/redis%E9%AB%98%E7%BA%A7%E7%AF%87%E4%B9%8B%E5%88%86%E5%B8%83%E5%BC%8F%E7%BC%93%E5%AD%98/"},{"categories":["redis","分布式系统"],"content":"3.1.3.集群故障恢复原理 一旦发现master故障，sentinel需要在salve中选择一个作为新的master，选择依据是这样的： 首先会判断slave节点与master节点断开时间长短，如果超过指定值（down-after-milliseconds * 10）则会排除该slave节点 然后判断slave节点的slave-priority值，越小优先级越高，如果是0则永不参与选举 如果slave-prority一样，则判断slave节点的offset值，越大说明数据越新，优先级越高 最后是判断slave节点的运行id大小，越小优先级越高。 当选出一个新的master后，该如何实现切换呢？ 流程如下： sentinel给备选的slave1节点发送slaveof no one命令，让该节点成为master sentinel给所有其它slave发送slaveof 192.168.150.101 7002命令，让这些slave成为新master的从节点，开始从新的master上同步数据。 最后，sentinel将故障节点标记为slave，当故障节点恢复后会自动成为新的master的slave节点 ","date":"2023-01-15","objectID":"/redis%E9%AB%98%E7%BA%A7%E7%AF%87%E4%B9%8B%E5%88%86%E5%B8%83%E5%BC%8F%E7%BC%93%E5%AD%98/:8:3","tags":[],"title":"Redis高级篇之分布式缓存","uri":"/redis%E9%AB%98%E7%BA%A7%E7%AF%87%E4%B9%8B%E5%88%86%E5%B8%83%E5%BC%8F%E7%BC%93%E5%AD%98/"},{"categories":["redis","分布式系统"],"content":"3.1.4.小结 Sentinel的三个作用是什么？ 监控 故障转移 通知 Sentinel如何判断一个redis实例是否健康？ 每隔1秒发送一次ping命令，如果超过一定时间没有相向则认为是主观下线 如果大多数sentinel都认为实例主观下线，则判定服务下线 故障转移步骤有哪些？ 首先选定一个slave作为新的master，执行slaveof no on命令 然后让所有节点都执行slaveof 新master主机 端口号 修改故障节点配置，添加slaveof 新master 端口号 ","date":"2023-01-15","objectID":"/redis%E9%AB%98%E7%BA%A7%E7%AF%87%E4%B9%8B%E5%88%86%E5%B8%83%E5%BC%8F%E7%BC%93%E5%AD%98/:8:4","tags":[],"title":"Redis高级篇之分布式缓存","uri":"/redis%E9%AB%98%E7%BA%A7%E7%AF%87%E4%B9%8B%E5%88%86%E5%B8%83%E5%BC%8F%E7%BC%93%E5%AD%98/"},{"categories":["redis","分布式系统"],"content":"3.2.搭建哨兵集群 ","date":"2023-01-15","objectID":"/redis%E9%AB%98%E7%BA%A7%E7%AF%87%E4%B9%8B%E5%88%86%E5%B8%83%E5%BC%8F%E7%BC%93%E5%AD%98/:9:0","tags":[],"title":"Redis高级篇之分布式缓存","uri":"/redis%E9%AB%98%E7%BA%A7%E7%AF%87%E4%B9%8B%E5%88%86%E5%B8%83%E5%BC%8F%E7%BC%93%E5%AD%98/"},{"categories":["redis","分布式系统"],"content":"3.3.RedisTemplate 在Sentinel集群监管下的Redis主从集群，其节点会因为自动故障转移而发生变化，Redis的客户端必须感知这种变化，及时更新连接信息。Spring的RedisTemplate底层利用lettuce实现了节点的感知和自动切换。 下面，我们通过一个测试来实现RedisTemplate集成哨兵机制。 ","date":"2023-01-15","objectID":"/redis%E9%AB%98%E7%BA%A7%E7%AF%87%E4%B9%8B%E5%88%86%E5%B8%83%E5%BC%8F%E7%BC%93%E5%AD%98/:10:0","tags":[],"title":"Redis高级篇之分布式缓存","uri":"/redis%E9%AB%98%E7%BA%A7%E7%AF%87%E4%B9%8B%E5%88%86%E5%B8%83%E5%BC%8F%E7%BC%93%E5%AD%98/"},{"categories":["redis","分布式系统"],"content":"3.3.1.导入Demo工程 首先，我们引入课前资料提供的Demo工程： ","date":"2023-01-15","objectID":"/redis%E9%AB%98%E7%BA%A7%E7%AF%87%E4%B9%8B%E5%88%86%E5%B8%83%E5%BC%8F%E7%BC%93%E5%AD%98/:10:1","tags":[],"title":"Redis高级篇之分布式缓存","uri":"/redis%E9%AB%98%E7%BA%A7%E7%AF%87%E4%B9%8B%E5%88%86%E5%B8%83%E5%BC%8F%E7%BC%93%E5%AD%98/"},{"categories":["redis","分布式系统"],"content":"3.3.2.引入依赖 在项目的pom文件中引入依赖： \u003cdependency\u003e \u003cgroupId\u003eorg.springframework.boot\u003c/groupId\u003e \u003cartifactId\u003espring-boot-starter-data-redis\u003c/artifactId\u003e \u003c/dependency\u003e ","date":"2023-01-15","objectID":"/redis%E9%AB%98%E7%BA%A7%E7%AF%87%E4%B9%8B%E5%88%86%E5%B8%83%E5%BC%8F%E7%BC%93%E5%AD%98/:10:2","tags":[],"title":"Redis高级篇之分布式缓存","uri":"/redis%E9%AB%98%E7%BA%A7%E7%AF%87%E4%B9%8B%E5%88%86%E5%B8%83%E5%BC%8F%E7%BC%93%E5%AD%98/"},{"categories":["redis","分布式系统"],"content":"3.3.3.配置Redis地址 然后在配置文件application.yml中指定redis的sentinel相关信息： spring: redis: sentinel: master: mymaster nodes: - 192.168.150.101:27001 - 192.168.150.101:27002 - 192.168.150.101:27003 ","date":"2023-01-15","objectID":"/redis%E9%AB%98%E7%BA%A7%E7%AF%87%E4%B9%8B%E5%88%86%E5%B8%83%E5%BC%8F%E7%BC%93%E5%AD%98/:10:3","tags":[],"title":"Redis高级篇之分布式缓存","uri":"/redis%E9%AB%98%E7%BA%A7%E7%AF%87%E4%B9%8B%E5%88%86%E5%B8%83%E5%BC%8F%E7%BC%93%E5%AD%98/"},{"categories":["redis","分布式系统"],"content":"3.3.4.配置读写分离 在项目的启动类中，添加一个新的bean： @Bean public LettuceClientConfigurationBuilderCustomizer clientConfigurationBuilderCustomizer(){ return clientConfigurationBuilder -\u003e clientConfigurationBuilder.readFrom(ReadFrom.REPLICA_PREFERRED); } 这个bean中配置的就是读写策略，包括四种： MASTER：从主节点读取 MASTER_PREFERRED：优先从master节点读取，master不可用才读取replica REPLICA：从slave（replica）节点读取 REPLICA _PREFERRED：优先从slave（replica）节点读取，所有的slave都不可用才读取master 4.Redis分片集群 ","date":"2023-01-15","objectID":"/redis%E9%AB%98%E7%BA%A7%E7%AF%87%E4%B9%8B%E5%88%86%E5%B8%83%E5%BC%8F%E7%BC%93%E5%AD%98/:10:4","tags":[],"title":"Redis高级篇之分布式缓存","uri":"/redis%E9%AB%98%E7%BA%A7%E7%AF%87%E4%B9%8B%E5%88%86%E5%B8%83%E5%BC%8F%E7%BC%93%E5%AD%98/"},{"categories":["redis","分布式系统"],"content":"4.1.搭建分片集群 主从和哨兵可以解决高可用、高并发读的问题。但是依然有两个问题没有解决： 海量数据存储问题：我们知道，主从集群模式下，redis的单个master就将所有数据饱揽下来。那么这样始终有一天，面临内存不足的问题，某些关键数据可能会被LRU所淘汰。 高并发写的问题：虽然主从架构可以提高并发读的性能，但是对于高并发写性能提升意义不大。因为在主从集群下，master只要一个，也就是redis的写只能写在一个master节点上（虽然我们利用主从集群抽离master上的读命令执行，但依然提升有限）。问题主要出现在master只有一个，而且redis执行命令都是单线程的，只能一条一条的执行。那么解决方案显而易见：我们可以多选几个master出来执行写命令。而且这几个master的数据是不一致的，这样可以减轻主从集群模式下，单节点master数据存储的压力。 使用分片集群可以解决上述问题（分片感觉就是多个集群有机结合），如图: 分片集群特征： 集群中有多个master，每个master保存不同数据 每个master都可以有多个slave节点 master之间通过ping监测彼此健康状态 客户端请求可以访问集群任意节点，最终都会被转发到正确节点 这里面最大的问题是：请求如何知道自己的数据在哪个master？那么就涉及到查找和寻址的问题，典型的解决方案就是哈希的思想，我们将 ","date":"2023-01-15","objectID":"/redis%E9%AB%98%E7%BA%A7%E7%AF%87%E4%B9%8B%E5%88%86%E5%B8%83%E5%BC%8F%E7%BC%93%E5%AD%98/:11:0","tags":[],"title":"Redis高级篇之分布式缓存","uri":"/redis%E9%AB%98%E7%BA%A7%E7%AF%87%E4%B9%8B%E5%88%86%E5%B8%83%E5%BC%8F%E7%BC%93%E5%AD%98/"},{"categories":["redis","分布式系统"],"content":"4.2.散列插槽 ","date":"2023-01-15","objectID":"/redis%E9%AB%98%E7%BA%A7%E7%AF%87%E4%B9%8B%E5%88%86%E5%B8%83%E5%BC%8F%E7%BC%93%E5%AD%98/:12:0","tags":[],"title":"Redis高级篇之分布式缓存","uri":"/redis%E9%AB%98%E7%BA%A7%E7%AF%87%E4%B9%8B%E5%88%86%E5%B8%83%E5%BC%8F%E7%BC%93%E5%AD%98/"},{"categories":["redis","分布式系统"],"content":"4.2.1.插槽原理 Redis会把每一个master节点映射到0~16383共16384个插槽（hash slot）上，查看集群信息时就能看到： 数据key不是与节点绑定，而是与插槽绑定。redis会根据key的有效部分计算插槽值，分两种情况： key中包含\"{}\"，且“{}”中至少包含1个字符，“{}”中的部分是有效部分 key中不包含“{}”，整个key都是有效部分 例如：key是num，那么就根据num计算，如果是{itcast}num，则根据itcast计算。计算方式是利用CRC16算法得到一个hash值，然后对16384取余，得到的结果就是slot值。 如图，在7001这个节点执行set a 1时，对a做hash运算，对16384取余，得到的结果是15495，因此要存储到7003节点。 到了7003后，执行get num时，对num做hash运算，对16384取余，得到的结果是2765，因此需要切换到7001节点 散列过程里存在一个巨严重的问题： 当我们需要增删redis分片集群的master节点时，那么hash的散列表映射长度发生变化，那么就会导致后续请求的master节点命中失败。 那么怎么解决呢？这时候就需要引入高级一点的哈希————一致性哈希 redis的一致性哈希有三大特性： key哈希结果尽可能分配到不同Redis实例 当实例增加或移除，需要保护已映射的内容不会重新被分配到新实例上 对key的哈希应尽量避免重复 分片实现： 前面谈到主从切换的哨兵模式已经提到，哨兵模式可以实现高可用以及读写分离，但是缺点在于所有Redis实例存储的数据全部一致，所以Redis支持cluster模式，可以简单将cluster理解为Redis集群管理的一个插件，通过它可以实现Redis的分布式存储。 数据分片方式一般有三种：客户端分片、代理分片和服务器分片。 1）客户端分片 定义：客户端自己计算key需要映射到哪一个Redis实例。 优点：客户端分片最明显的好处在于降低了集群的复杂度，而服务器之间没有任何关联性，数据分片由客户端来负责实现。 缺点：客户端实现分片则客户端需要知道当前集群下不同Redis实例的信息，当新增Redis实例时需要支持动态分片，多数Redis需要重启才能实现该功能。 2）代理分片 定义：客户端将请求发送到代理，代理通过计算得到需要映射的集群实例信息，然后将客户端的请求转发到对应的集群实例上，然后返回响应给客户端。 优点：降低了客户端的复杂度，客户端不用关心后端Redis实例的状态信息。 缺点：多了一个中间分发环节，所以对性能有些取的损失。 3）服务器分片 定义：客户端可以和集群中任意Redis实例通信，当客户端访问某个实例时，服务器进行计算key应该映射到哪个具体的Redis实例中存储，如果映射的实例不是当前实例，则该实例主动引导客户端去对应实例对key进行操作。这其实是一个重定向的过程**。**这个过程不是从当前Redis实例转发到对应的Redis实例，而是客户端收到服务器通知具体映射的Redis实例重定向到映射的实例中。当前还不能完全适用于生产环境。 优点：支持高可用，任意实例都有主从，主挂了从会自动接管。 缺点：需要客户端语言实现服务器集群协议，但是目前大多数语言都有其客户端实现版本。 4）预分片 从上面可以清楚地看出，分片机制增加或移除实例是非常麻烦的一件事，所以我们可以考虑一开始就开启32个节点实例，当我们可以新增Redis服务器时，我们可以将一半的节点移动到新的Redis服务器。这样我们只需要在新服务器启动一个空节点，然后移动数据，配置新节点为源节点的从节点，然后更新被移动节点的ip信息，然后向新服务器发送slaveof命令关闭主从配置，最后关闭旧服务器不需要使用的实例并且重新启动客户端。这样我们就可以在几乎不需要停机时间时完成数据的移动。 分片机制的缺点 分片是由多台Redis实例共同运转，所以如果其中一个Redis实例宕机，则整个分片都将无法使用，所以分片机制无法实现高可用。 如果有不同的key映射到不同的Redis实例，这时候不能对这两个key做交集或者使用事务。=\u003e 单机上的redis事务不支持，只有使用分布式事务 使用分片机制因为涉及多实例，数据处理比较复杂。 分片中对于实例的添加或删除会很复杂，不过可以使用预分片技术进行改善。 ","date":"2023-01-15","objectID":"/redis%E9%AB%98%E7%BA%A7%E7%AF%87%E4%B9%8B%E5%88%86%E5%B8%83%E5%BC%8F%E7%BC%93%E5%AD%98/:12:1","tags":[],"title":"Redis高级篇之分布式缓存","uri":"/redis%E9%AB%98%E7%BA%A7%E7%AF%87%E4%B9%8B%E5%88%86%E5%B8%83%E5%BC%8F%E7%BC%93%E5%AD%98/"},{"categories":["redis","分布式系统"],"content":"4.2.1.小结 Redis如何判断某个key应该在哪个实例？ 将16384个插槽分配到不同的实例 根据key的有效部分计算哈希值，对16384取余 余数作为插槽，寻找插槽所在实例即可 如何将同一类数据固定的保存在同一个Redis实例？ 这一类数据使用相同的有效部分，例如key都以{typeId}为前缀 redis分片机制详解 ","date":"2023-01-15","objectID":"/redis%E9%AB%98%E7%BA%A7%E7%AF%87%E4%B9%8B%E5%88%86%E5%B8%83%E5%BC%8F%E7%BC%93%E5%AD%98/:12:2","tags":[],"title":"Redis高级篇之分布式缓存","uri":"/redis%E9%AB%98%E7%BA%A7%E7%AF%87%E4%B9%8B%E5%88%86%E5%B8%83%E5%BC%8F%E7%BC%93%E5%AD%98/"},{"categories":["redis","分布式系统"],"content":"4.3.集群伸缩 redis-cli –cluster提供了很多操作集群的命令，可以通过下面方式查看： 比如，添加节点的命令： ","date":"2023-01-15","objectID":"/redis%E9%AB%98%E7%BA%A7%E7%AF%87%E4%B9%8B%E5%88%86%E5%B8%83%E5%BC%8F%E7%BC%93%E5%AD%98/:13:0","tags":[],"title":"Redis高级篇之分布式缓存","uri":"/redis%E9%AB%98%E7%BA%A7%E7%AF%87%E4%B9%8B%E5%88%86%E5%B8%83%E5%BC%8F%E7%BC%93%E5%AD%98/"},{"categories":["redis","分布式系统"],"content":"4.3.1.需求分析 需求：向集群中添加一个新的master节点，并向其中存储 num = 10 启动一个新的redis实例，端口为7004 添加7004到之前的集群，并作为一个master节点 给7004节点分配插槽，使得num这个key可以存储到7004实例 这里需要两个新的功能： 添加一个节点到集群中 将部分插槽分配到新插槽 ","date":"2023-01-15","objectID":"/redis%E9%AB%98%E7%BA%A7%E7%AF%87%E4%B9%8B%E5%88%86%E5%B8%83%E5%BC%8F%E7%BC%93%E5%AD%98/:13:1","tags":[],"title":"Redis高级篇之分布式缓存","uri":"/redis%E9%AB%98%E7%BA%A7%E7%AF%87%E4%B9%8B%E5%88%86%E5%B8%83%E5%BC%8F%E7%BC%93%E5%AD%98/"},{"categories":["redis","分布式系统"],"content":"4.3.2.创建新的redis实例 创建一个文件夹： mkdir 7004 拷贝配置文件： cp redis.conf /7004 修改配置文件： sed /s/6379/7004/g 7004/redis.conf 启动 redis-server 7004/redis.conf ","date":"2023-01-15","objectID":"/redis%E9%AB%98%E7%BA%A7%E7%AF%87%E4%B9%8B%E5%88%86%E5%B8%83%E5%BC%8F%E7%BC%93%E5%AD%98/:13:2","tags":[],"title":"Redis高级篇之分布式缓存","uri":"/redis%E9%AB%98%E7%BA%A7%E7%AF%87%E4%B9%8B%E5%88%86%E5%B8%83%E5%BC%8F%E7%BC%93%E5%AD%98/"},{"categories":["redis","分布式系统"],"content":"4.3.3.添加新节点到redis 添加节点的语法如下： 执行命令： redis-cli --cluster add-node 192.168.150.101:7004 192.168.150.101:7001 通过命令查看集群状态： redis-cli -p 7001 cluster nodes 如图，7004加入了集群，并且默认是一个master节点： 但是，可以看到7004节点的插槽数量为0，因此没有任何数据可以存储到7004上 ","date":"2023-01-15","objectID":"/redis%E9%AB%98%E7%BA%A7%E7%AF%87%E4%B9%8B%E5%88%86%E5%B8%83%E5%BC%8F%E7%BC%93%E5%AD%98/:13:3","tags":[],"title":"Redis高级篇之分布式缓存","uri":"/redis%E9%AB%98%E7%BA%A7%E7%AF%87%E4%B9%8B%E5%88%86%E5%B8%83%E5%BC%8F%E7%BC%93%E5%AD%98/"},{"categories":["redis","分布式系统"],"content":"4.3.4.转移插槽 我们要将num存储到7004节点，因此需要先看看num的插槽是多少： 如上图所示，num的插槽为2765. 我们可以将0~3000的插槽从7001转移到7004，命令格式如下： 具体命令如下： 建立连接： 得到下面的反馈： 询问要移动多少个插槽，我们计划是3000个： 新的问题来了： 那个node来接收这些插槽？？ 显然是7004，那么7004节点的id是多少呢？ 复制这个id，然后拷贝到刚才的控制台后： 这里询问，你的插槽是从哪里移动过来的？ all：代表全部，也就是三个节点各转移一部分 具体的id：目标节点的id done：没有了 这里我们要从7001获取，因此填写7001的id： 填完后，点击done，这样插槽转移就准备好了： 确认要转移吗？输入yes： 然后，通过命令查看结果： 可以看到： 目的达成。 ","date":"2023-01-15","objectID":"/redis%E9%AB%98%E7%BA%A7%E7%AF%87%E4%B9%8B%E5%88%86%E5%B8%83%E5%BC%8F%E7%BC%93%E5%AD%98/:13:4","tags":[],"title":"Redis高级篇之分布式缓存","uri":"/redis%E9%AB%98%E7%BA%A7%E7%AF%87%E4%B9%8B%E5%88%86%E5%B8%83%E5%BC%8F%E7%BC%93%E5%AD%98/"},{"categories":["redis","分布式系统"],"content":"4.4.故障转移 集群初识状态是这样的： 其中7001、7002、7003都是master，我们计划让7002宕机。 ","date":"2023-01-15","objectID":"/redis%E9%AB%98%E7%BA%A7%E7%AF%87%E4%B9%8B%E5%88%86%E5%B8%83%E5%BC%8F%E7%BC%93%E5%AD%98/:14:0","tags":[],"title":"Redis高级篇之分布式缓存","uri":"/redis%E9%AB%98%E7%BA%A7%E7%AF%87%E4%B9%8B%E5%88%86%E5%B8%83%E5%BC%8F%E7%BC%93%E5%AD%98/"},{"categories":["redis","分布式系统"],"content":"4.4.1.自动故障转移 当集群中有一个master宕机会发生什么呢？ 直接停止一个redis实例，例如7002： redis-cli -p 7002 shutdown 1）首先是该实例与其它实例失去连接 2）然后是疑似宕机： 3）最后是确定下线，自动提升一个slave为新的master： 4）当7002再次启动，就会变为一个slave节点了： ","date":"2023-01-15","objectID":"/redis%E9%AB%98%E7%BA%A7%E7%AF%87%E4%B9%8B%E5%88%86%E5%B8%83%E5%BC%8F%E7%BC%93%E5%AD%98/:14:1","tags":[],"title":"Redis高级篇之分布式缓存","uri":"/redis%E9%AB%98%E7%BA%A7%E7%AF%87%E4%B9%8B%E5%88%86%E5%B8%83%E5%BC%8F%E7%BC%93%E5%AD%98/"},{"categories":["redis","分布式系统"],"content":"4.4.2.手动故障转移 利用cluster failover命令可以手动让集群中的某个master宕机，切换到执行cluster failover命令的这个slave节点，实现无感知的数据迁移。其流程如下： 这种failover命令可以指定三种模式： 缺省：默认的流程，如图1~6歩 force：省略了对offset的一致性校验 takeover：直接执行第5歩，忽略数据一致性、忽略master状态和其它master的意见 案例需求：在7002这个slave节点执行手动故障转移，重新夺回master地位 步骤如下： 1）利用redis-cli连接7002这个节点 2）执行cluster failover命令 如图： 效果： ","date":"2023-01-15","objectID":"/redis%E9%AB%98%E7%BA%A7%E7%AF%87%E4%B9%8B%E5%88%86%E5%B8%83%E5%BC%8F%E7%BC%93%E5%AD%98/:14:2","tags":[],"title":"Redis高级篇之分布式缓存","uri":"/redis%E9%AB%98%E7%BA%A7%E7%AF%87%E4%B9%8B%E5%88%86%E5%B8%83%E5%BC%8F%E7%BC%93%E5%AD%98/"},{"categories":["redis","分布式系统"],"content":"4.5.RedisTemplate访问分片集群 RedisTemplate底层同样基于lettuce实现了分片集群的支持，而使用的步骤与哨兵模式基本一致： 1）引入redis的starter依赖 2）配置分片集群地址 3）配置读写分离 与哨兵模式相比，其中只有分片集群的配置方式略有差异，如下： spring: redis: cluster: nodes: - 192.168.150.101:7001 - 192.168.150.101:7002 - 192.168.150.101:7003 - 192.168.150.101:8001 - 192.168.150.101:8002 - 192.168.150.101:8003 ","date":"2023-01-15","objectID":"/redis%E9%AB%98%E7%BA%A7%E7%AF%87%E4%B9%8B%E5%88%86%E5%B8%83%E5%BC%8F%E7%BC%93%E5%AD%98/:15:0","tags":[],"title":"Redis高级篇之分布式缓存","uri":"/redis%E9%AB%98%E7%BA%A7%E7%AF%87%E4%B9%8B%E5%88%86%E5%B8%83%E5%BC%8F%E7%BC%93%E5%AD%98/"},{"categories":["汇编"],"content":"call和ret指令 call和ret指令都是转移指令，它们都修改IP，或同时修改CS和IP。 ","date":"2023-01-14","objectID":"/%E6%B1%87%E7%BC%96%E4%B9%8Bcall%E5%92%8Cret%E6%8C%87%E4%BB%A4/:0:0","tags":[],"title":"汇编之call和ret指令","uri":"/%E6%B1%87%E7%BC%96%E4%B9%8Bcall%E5%92%8Cret%E6%8C%87%E4%BB%A4/"},{"categories":["汇编"],"content":"1、ret 和 retf ret指令用栈中的数据，修改IP的内容，从而实现近转移； retf指令用栈中的数据，修改CS和IP的内容，从而实现远转移。 CPU执行ret指令时，相当于进行： pop IP： （1）(IP) = ( (ss) * 16 + (sp) ) （2）(sp) = (sp) + 2 CPU执行retf指令时，相当于进行：pop IP, pop CS： （1）(IP) = ( (ss) * 16 + (sp) ) （2）(sp) = (sp) + 2 （3）(CS) = ( (ss) * 16 + (sp) ) （4）(sp) = (sp) + 2 assume cs:code stack seqment db 16 dup (0) stack ends code segment mov ax, 4c00h int 21h start: mov ax, stack mov ss, ax mov sp, 16 mov ax, 0 push ax ;ax入栈 mov bx, 0 ret ;ret指令执行后，(IP)=0，CS:IP指向代码段的第一条指令。可以push cs push ax retf code ends end start ","date":"2023-01-14","objectID":"/%E6%B1%87%E7%BC%96%E4%B9%8Bcall%E5%92%8Cret%E6%8C%87%E4%BB%A4/:1:0","tags":[],"title":"汇编之call和ret指令","uri":"/%E6%B1%87%E7%BC%96%E4%B9%8Bcall%E5%92%8Cret%E6%8C%87%E4%BB%A4/"},{"categories":["汇编"],"content":"2、call 指令 call指令经常跟ret指令配合使用，因此CPU执行call指令，进行两步操作： （1）将当前的 IP 或 CS和IP 压入栈中； （2）转移（jmp）。 call指令不能实现短转移，除此之外，call指令实现转移的方法和 jmp 指令的原理相同。 call 标号（近转移） CPU执行此种格式的call指令时，相当于进行 push IP jmp near ptr 标号 call far ptr 标号（段间转移） CPU执行此种格式的call指令时，相当于进行：push CS，push IP jmp far ptr 标号 call 16位寄存器 CPU执行此种格式的call指令时，相当于进行： push IP jmp 16位寄存器 call word ptr 内存单元地址 CPU执行此种格式的call指令时，相当于进行：push IP jmp word ptr 内存单元地址 mov sp, 10h mov ax, 0123h mov ds:[0], ax call word ptr ds:[0] ;执行后，(IP)=0123H，(sp)=0EH call dword ptr 内存单元地址 CPU执行此种格式的call指令时，相当于进行：push CS push IP jmp dword ptr 内存单元地址 mov sp, 10h mov ax, 0123h mov ds:[0], ax mov word ptr ds:[2], 0 call dword ptr ds:[0] ;执行后，(CS)=0，(IP)=0123H，(sp)=0CH ","date":"2023-01-14","objectID":"/%E6%B1%87%E7%BC%96%E4%B9%8Bcall%E5%92%8Cret%E6%8C%87%E4%BB%A4/:2:0","tags":[],"title":"汇编之call和ret指令","uri":"/%E6%B1%87%E7%BC%96%E4%B9%8Bcall%E5%92%8Cret%E6%8C%87%E4%BB%A4/"},{"categories":["汇编"],"content":"3、call 和 ret 的配合使用 分析下面程序 assume cs:code code segment start: mov ax,1 mov cx,3 call s ;（1）CPU指令缓冲器存放call指令，IP指向下一条指令（mov bx, ax），执行call指令，IP入栈，jmp mov bx,ax ;（4）IP重新指向这里 bx = 8 mov ax,4c00h int 21h s: add ax,ax loop s;（2）循环3次ax = 8 ret;（3）return : pop IP code ends end start call 与 ret 指令共同支持了汇编语言编程中的模块化设计，用来编写子程序 ","date":"2023-01-14","objectID":"/%E6%B1%87%E7%BC%96%E4%B9%8Bcall%E5%92%8Cret%E6%8C%87%E4%BB%A4/:3:0","tags":[],"title":"汇编之call和ret指令","uri":"/%E6%B1%87%E7%BC%96%E4%B9%8Bcall%E5%92%8Cret%E6%8C%87%E4%BB%A4/"},{"categories":["汇编"],"content":"4、寄存器冲突 在设计子程序时，由于寄存器资源有限，难免会出现子程序和调用程序使用了同一个寄存器，而且子程序还修改了这个寄存器，那么这样就存在巨大的问题。如何解决呢？ 其实，这个就类似于高级语言的函数调用。既然都要使用某个寄存器，那么就可以这样：当进入子程序时，我们将调用程序寄存器里的数据放到栈里面存放，这样子程序就可以放心使用啦。子程序结束之后，我们再将数据恢复到寄存器里即可。 ","date":"2023-01-14","objectID":"/%E6%B1%87%E7%BC%96%E4%B9%8Bcall%E5%92%8Cret%E6%8C%87%E4%BB%A4/:4:0","tags":[],"title":"汇编之call和ret指令","uri":"/%E6%B1%87%E7%BC%96%E4%B9%8Bcall%E5%92%8Cret%E6%8C%87%E4%BB%A4/"},{"categories":["汇编"],"content":"转移指令的原理 可以修改IP，或同时修改CS和IP的指令统称为转移指令。概括地讲，转移指令就是可以控制CPU执行内存中某处代码的指令。 8086CPU的转移行为有以下几类。 只修改IP时，称为段内转移，比如：jmp ax。 同时修改CS和IP时，称为段间转移，比如：jmp 1000:0。 由于转移指令对IP的修改范围不同，段内转移又分为：短转移和近转移。 短转移IP的修改范围为-128 ~ 127。 近转移IP的修改范围为-32768 ~ 32767。 8086CPU的转移指令分为以下几类。 无条件转移指令（如：jmp） 条件转移指令 循环指令（如：loop） 过程 中断 ","date":"2023-01-13","objectID":"/%E6%B1%87%E7%BC%96%E4%B9%8B%E8%BD%AC%E7%A7%BB%E6%8C%87%E4%BB%A4%E5%8E%9F%E7%90%86/:0:0","tags":[],"title":"汇编之转移指令原理","uri":"/%E6%B1%87%E7%BC%96%E4%B9%8B%E8%BD%AC%E7%A7%BB%E6%8C%87%E4%BB%A4%E5%8E%9F%E7%90%86/"},{"categories":["汇编"],"content":"1、操作符offset 操作符offset在汇编语言中是由编译器处理的符号，它的功能是取得标号的偏移地址。 ;将s处的一条指令复制到s0处 assume cs:codesg codesg segment s: mov ax, bx ;（mov ax,bx 的机器码占两个字节） mov si, offset s ;获得标号s的偏移地址 mov di, offset s0 ;获得标号s0的偏移地址 mov ax, cs:[si] mov cs:[di], ax s0: nop ;（nop的机器码占一个字节） nop codesg ends ends ","date":"2023-01-13","objectID":"/%E6%B1%87%E7%BC%96%E4%B9%8B%E8%BD%AC%E7%A7%BB%E6%8C%87%E4%BB%A4%E5%8E%9F%E7%90%86/:1:0","tags":[],"title":"汇编之转移指令原理","uri":"/%E6%B1%87%E7%BC%96%E4%B9%8B%E8%BD%AC%E7%A7%BB%E6%8C%87%E4%BB%A4%E5%8E%9F%E7%90%86/"},{"categories":["汇编"],"content":"2、jmp指令 jmp为无条件转移，转到标号处执行指令可以只修改IP，也可以同时修改CS和IP； jmp指令要给出两种信息： 转移的目的地址 转移的距离（段间转移、段内短转移，段内近转移） jmp short 标号 jmp near ptr 标号 jcxz 标号 loop 标号 等几种汇编指令，它们对 IP的修改 是根据转移目的地址和转移起始地址之间的位移来进行的。在它们对应的机器码中不包含转移的目的地址，而包含的是到目的地址的位移距离。 ","date":"2023-01-13","objectID":"/%E6%B1%87%E7%BC%96%E4%B9%8B%E8%BD%AC%E7%A7%BB%E6%8C%87%E4%BB%A4%E5%8E%9F%E7%90%86/:2:0","tags":[],"title":"汇编之转移指令原理","uri":"/%E6%B1%87%E7%BC%96%E4%B9%8B%E8%BD%AC%E7%A7%BB%E6%8C%87%E4%BB%A4%E5%8E%9F%E7%90%86/"},{"categories":["汇编"],"content":"1、依据位移进行转移的jmp指令 jmp short 标号（段内短转移） 指令“jmp short 标号”的功能为(IP)=(IP)+8位位移，转到标号处执行指令 （1）8位位移 = “标号”处的地址 - jmp指令后的第一个字节的地址； （2）short指明此处的位移为8位位移； （3）8位位移的范围为-128~127，用补码表示 （4）8位位移由编译程序在编译时算出。 assume cs:codesg codesg segment start:mov ax,0 jmp short s ;s不是被翻译成目的地址 add ax, 1 s:inc ax ;程序执行后， ax中的值为 1 codesg ends end start CPU不需要这个目的地址就可以实现对IP的修改。这里是依据位移进行转移 jmp short s指令的读取和执行过程： (CS)=0BBDH，(IP)=0006，上一条指令执行结束后CS:IP指向EB 03（jmp short s的机器码）； 读取指令码EB 03进入指令缓冲器； (IP) = (IP) + 所读取指令的长度 = (IP) + 2 = 0008，CS:IP指向add ax,1； CPU指行指令缓冲器中的指令EB 03； 指令EB 03执行后，(IP)=000BH，CS:IP指向inc ax jmp near ptr 标号 （段内近转移） 指令“jmp near ptr 标号”的功能为：(IP) = (IP) + 16位位移。 ","date":"2023-01-13","objectID":"/%E6%B1%87%E7%BC%96%E4%B9%8B%E8%BD%AC%E7%A7%BB%E6%8C%87%E4%BB%A4%E5%8E%9F%E7%90%86/:2:1","tags":[],"title":"汇编之转移指令原理","uri":"/%E6%B1%87%E7%BC%96%E4%B9%8B%E8%BD%AC%E7%A7%BB%E6%8C%87%E4%BB%A4%E5%8E%9F%E7%90%86/"},{"categories":["汇编"],"content":"2、转移的目的地址在指令中的jmp指令 jmp far ptr 标号（段间转移或远转移） 指令 “jmp far ptr 标号” 功能如下： (CS) = 标号所在段的段地址； (IP) = 标号所在段中的偏移地址。 far ptr指明了指令用标号的段地址和偏移地址修改CS和IP。 assume cs:codesg codesg segment start: mov ax, 0 mov bx, 0 jmp far ptr s ;s被翻译成转移的目的地址0B01 BD0B db 256 dup (0) ;转移的段地址：0BBDH，偏移地址：010BH s: add ax,1 inc ax codesg ends end start 12345678910 ","date":"2023-01-13","objectID":"/%E6%B1%87%E7%BC%96%E4%B9%8B%E8%BD%AC%E7%A7%BB%E6%8C%87%E4%BB%A4%E5%8E%9F%E7%90%86/:2:2","tags":[],"title":"汇编之转移指令原理","uri":"/%E6%B1%87%E7%BC%96%E4%B9%8B%E8%BD%AC%E7%A7%BB%E6%8C%87%E4%BB%A4%E5%8E%9F%E7%90%86/"},{"categories":["汇编"],"content":"3、转移地址在寄存器或内存中的jmp指令 jmp 16位寄存器 功能：IP =（16位寄存器） 转移地址在内存中的jmp指令有两种格式： jmp word ptr 内存单元地址（段内转移） 功能：从内存单元地址处开始存放着一个字，是转移的目的偏移地址。 mov ax, 0123H mov ds:[0], ax jmp word ptr ds:[0] ;执行后，(IP)=0123H jmp dword ptr 内存单元地址（段间转移） 功能：从内存单元地址处开始存放着两个字，高地址处的字是转移的目的段地址，低地址处是转移的目的偏移地址。 (CS)=(内存单元地址+2) (IP)=(内存单元地址) mov ax, 0123H mov ds:[0], ax;偏移地址 mov word ptr ds:[2], 0;段地址 jmp dword ptr ds:[0] ;执行后， ;(CS)=0 ;(IP)=0123H ;CS:IP 指向 0000:0123。 ","date":"2023-01-13","objectID":"/%E6%B1%87%E7%BC%96%E4%B9%8B%E8%BD%AC%E7%A7%BB%E6%8C%87%E4%BB%A4%E5%8E%9F%E7%90%86/:2:3","tags":[],"title":"汇编之转移指令原理","uri":"/%E6%B1%87%E7%BC%96%E4%B9%8B%E8%BD%AC%E7%A7%BB%E6%8C%87%E4%BB%A4%E5%8E%9F%E7%90%86/"},{"categories":["汇编"],"content":"4、jcxz指令和loop指令 jcxz指令 jcxz指令为有条件转移指令，所有的有条件转移指令都是短转移， 在对应的机器码中包含转移的位移，而不是目的地址。对IP的修改范围都为-128~127。 指令格式：jcxz 标号（如果(cx)=0，则转移到标号处执行。） 当(cx) = 0时，(IP) = (IP) + 8位位移 8位位移 = “标号”处的地址 - jcxz指令后的第一个字节的地址； 8位位移的范围为-128~127，用补码表示； 8位位移由编译程序在编译时算出。 当(cx)!=0时，什么也不做（程序向下执行） loop指令 loop指令为循环指令，所有的循环指令都是短转移，在对应的机器码中包含转移的位移，而不是目的地址。 对IP的修改范围都为-128~127。 指令格式：loop 标号 ((cx) = (cx) - 1，如果(cx) ≠ 0，转移到标号处执行)。 (cx) = (cx) - 1；如果 (cx) != 0，(IP) = (IP) + 8位位移。 8位位移 = 标号处的地址 - loop指令后的第一个字节的地址； 8位位移的范围为-128~127，用补码表示； 8位位移由编译程序在编译时算出。 如果（cx）= 0，什么也不做（程序向下执行）。 ","date":"2023-01-13","objectID":"/%E6%B1%87%E7%BC%96%E4%B9%8B%E8%BD%AC%E7%A7%BB%E6%8C%87%E4%BB%A4%E5%8E%9F%E7%90%86/:2:4","tags":[],"title":"汇编之转移指令原理","uri":"/%E6%B1%87%E7%BC%96%E4%B9%8B%E8%BD%AC%E7%A7%BB%E6%8C%87%E4%BB%A4%E5%8E%9F%E7%90%86/"},{"categories":["汇编"],"content":"3、总结 转移指令时，都是以下一个指令的偏移量为依据，如果是下一个指令的目标地址，那么这段程序就不能随意放到某个地址了。使用偏移量就可以很好解决这个问题：无论这段代码放到哪里都可以通过位移来找寻指令 归根结底，指令其实就是cs:ip指向地址的内容，那么我们要转移指令，就是修改cs:ip的指向地址 实现循环：每次循环体结束都会返回到循环代码的头部，那么如何实现呢？答案就是：转移指令，每当循环执行完毕时，我们就将指令转移到循环体头部； 实现if-else：类似循环，不过使用的是条件转移指令 ","date":"2023-01-13","objectID":"/%E6%B1%87%E7%BC%96%E4%B9%8B%E8%BD%AC%E7%A7%BB%E6%8C%87%E4%BB%A4%E5%8E%9F%E7%90%86/:3:0","tags":[],"title":"汇编之转移指令原理","uri":"/%E6%B1%87%E7%BC%96%E4%B9%8B%E8%BD%AC%E7%A7%BB%E6%8C%87%E4%BB%A4%E5%8E%9F%E7%90%86/"},{"categories":["汇编"],"content":"div指令、dd、dup、mul指令 div是除法指令 除数：有8位和16位两种，在一个寄存器或内存单元中。 被除数：默认放在AX或DX和AX中， 如果除数为8位，被除数则为16位，默认在AX中存放； 如果除数为16位，被除数则为32位，在DX和AX中存放，DX存放高16位，AX存放低16位。 结果： 如果除数为8位，则AL存储除法操作的商，AH存储除法操作的余数； 如果除数为16位，则AX存储除法操作的商，DX存储除法操作的余数。 ;利用除法指令计算100001/100。 ;100001D = 186A1H mov dx, 1 mov ax, 86A1H ;(dx)*10000H+(ax)=100001 mov bx, 100 div bx ;利用除法指令计算1001/100 mov ax, 1001 mov bl, 100 div b1 伪指令dd db和dw定义字节型数据和字型数据。 dd是用来定义dword（double word，双字）型数据的伪指令 操作符dup dup在汇编语言中同db、dw、dd等一样，也是由编译器识别处理的符号。 它和db、dw、dd等数据定义伪指令配合使用，用来进行数据的重复 db 3 dup (0) ;定义了3个字节，它们的值都是0，相当于db 0，0，0。 db 3 dup (0, 1, 2) ;定义了9个字节，它们是0、1、2、0、1、2、0、1、2，相当于db 0，1，2，0，1，2，0，1，2。 db 3 dup ('abc', 'ABC') ;定义了18个字节，它们是abcABCabcABCabcABCC，相当于db 'abc', 'ABC' ,'abc' , 'ABC, 'abc', 'ABC'。 mul 指令 mul是乘法指令，使用 mul 做乘法的时候：相乘的两个数：要么都是8位，要么都是16位。 8 位： AL中和 8位寄存器或内存字节单元中； 16 位： AX中和 16 位寄存器或内存字单元中。 结果 8位：AX中； 16位：DX（高位）和 AX（低位）中。 格式：mul 寄存器 或 mul 内存单元 ;计算100*10 ;100和10小于255，可以做8位乘法 mov al,100 mov bl,10 mul bl ;结果： (ax)=1000（03E8H） 12345678 ;计算100*10000 ;100小于255，可10000大于255，所以必须做16位乘法，程序如下： mov ax,100 mov bx,10000 mul bx ;结果： (ax)=4240H，(dx)=000FH （F4240H=1000000） ","date":"2023-01-10","objectID":"/%E6%B1%87%E7%BC%96%E4%B9%8B%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E9%97%AE%E9%A2%98/:1:0","tags":[],"title":"汇编之数据处理问题","uri":"/%E6%B1%87%E7%BC%96%E4%B9%8B%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E9%97%AE%E9%A2%98/"},{"categories":["linux"],"content":"linux常用命令 ","date":"2023-01-10","objectID":"/linux%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/:0:0","tags":[],"title":"Linux常用命令","uri":"/linux%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/"},{"categories":["linux"],"content":"零、linux初窥 ","date":"2023-01-10","objectID":"/linux%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/:1:0","tags":[],"title":"Linux常用命令","uri":"/linux%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/"},{"categories":["linux"],"content":"一、命令行操作体验 在 linux 中，最最重要的就是命令，这就包含了 2 个过程，输入和输出 输入：输入当然就是打开终端，然后按键盘输入，然后按回车，输入格式一般就是这类的 #创建一个名为 file 的文件，touch是一个命令 touch file #进入一个目录，cd是一个命令 cd /etc/ #查看当前所在目录 pwd 输出：输出会返回你想要的结果，比如你要看什么文件，就会返回文件的内容。如果只是执行，执行失败会告诉你哪里错了，如果执行成功那么会没有输出，因为 linux 的哲学就是：没有结果就是最好的结果 ","date":"2023-01-10","objectID":"/linux%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/:2:0","tags":[],"title":"Linux常用命令","uri":"/linux%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/"},{"categories":["linux"],"content":"1. 开始 如图，双击桌面上的 Xfce 终端 图标打开终端后系统会自动运行 Shell 程序，然后我们就可以输入命令让系统来执行了： 1) 重要快捷键 真正学习命令行之前，你先要掌握几个十分有用、必需掌握的小技巧： [Tab] 使用Tab键来进行命令补全，Tab键一般是在字母Q旁边，这个技巧给你带来的最大的好处就是当你忘记某个命令的全称时可以只输入它的开头的一部分，然后按下Tab键就可以得到提示或者帮助完成： 当然不止补全命令，补全目录、补全命令参数都是没问题的： [Ctrl+c] 想想你有没有遇到过这种情况，当你在 Linux 命令行中无意输入了一个不知道的命令，或者错误地使用了一个命令，导致在终端里出现了你无法预料的情况，比如，屏幕上只有光标在闪烁却无法继续输入命令，或者不停地输出一大堆你不想要的结果。你想要立即停止并恢复到你可控的状态，那该怎么办呢？这时候你就可以使用Ctrl+c键来强行终止当前程序（你可以放心它并不会使终端退出）。 尝试输入以下命令： tail 然后你会发现你接下来的输入都没有任何反应了，只是将你输入的东西显示出来，现在你可以使用Ctrl+c，来中断这个你目前可能还不知道是什么的程序（在后续课程中我们会具体解释这个tail命令是什么）。 又或者输入： find / 显然这不是你想的结果，可以使用Ctrl+c结束。 虽然这个按着很方便，但不要随便按，因为有时候，当你看到终端没有任何反应或提示，也不能接受你的输入时，可能只是运行的程序需要你耐心等一下，就不要急着按Ctrl+c了。 其他一些常用快捷键 按键 作用 Ctrl+d 键盘输入结束或退出终端 Ctrl+s 暂停当前程序，暂停后按下任意键恢复运行 Ctrl+z 将当前程序放到后台运行，恢复到前台为命令fg Ctrl+a 将光标移至输入行头，相当于Home键 Ctrl+e 将光标移至输入行末，相当于End键 Ctrl+k 删除从光标所在位置到行末 Alt+Backspace 向前删除一个单词 Shift+PgUp 将终端显示向上滚动 Shift+PgDn 将终端显示向下滚动 2) 利用历史输入命令 很简单，你可以使用键盘上的方向上键↑，恢复你之前输入过的命令，你一试便知。 3) 通配符 通配符是一种特殊语句，主要有星号（*）和问号（?），用来对字符串进行模糊匹配（比如文件名、参数名）。当查找文件夹时，可以使用它来代替一个或多个真正字符；当不知道真正字符或者懒得输入完整名字时，常常使用通配符代替一个或多个真正字符。 终端里面输入的通配符是由 Shell 处理的，不是由所涉及的命令语句处理的，它只会出现在命令的“参数值”里（它不能出现在命令名称里， 命令不记得，那就用Tab补全）。当 Shell 在“参数值”中遇到了通配符时，Shell 会将其当作路径或文件名在磁盘上搜寻可能的匹配：若符合要求的匹配存在，则进行代换（路径扩展）；否则就将该通配符作为一个普通字符传递给“命令”，然后再由命令进行处理。总之，通配符实际上就是一种 Shell 实现的路径扩展功能。在通配符被处理后， Shell 会先完成该命令的重组，然后继续处理重组后的命令，直至执行该命令。 首先回到用户家目录： cd /home/shiyanlou 然后使用 touch 命令创建 2 个文件，后缀都为 txt： touch asd.txt fgh.txt 可以给文件随意命名，假如过了很长时间，你已经忘了这两个文件的文件名，现在你想在一大堆文件中找到这两个文件，就可以使用通配符： ls *.txt 在创建文件的时候，如果需要一次性创建多个文件，比如：“love_1_linux.txt，love_2_linux.txt，… love_10_linux.txt”。在 Linux 中十分方便： touch love_{1..10}_shiyanlou.txt Shell 常用通配符： 字符 含义 * 匹配 0 或多个字符 ? 匹配任意一个字符 [list] 匹配 list 中的任意单一字符 [^list] 匹配 除 list 中的任意单一字符以外的字符 [c1-c2] 匹配 c1-c2 中的任意单一字符 如：[0-9][a-z] {string1,string2,...} 匹配 string1 或 string2 (或更多)其一字符串 {c1..c2} 匹配 c1-c2 中全部字符 如{1..10} 4) 在命令行中获取帮助 在 Linux 环境中，如果你遇到困难，可以使用man命令，它是Manual pages的缩写。 Manual pages 是 UNIX 或类 UNIX 操作系统中在线软件文档的一种普遍的形式， 内容包括计算机程序（包括库和系统调用）、正式的标准和惯例，甚至是抽象的概念。用户可以通过执行man命令调用手册页。 你可以使用如下方式来获得某个命令的说明和使用方式的详细介绍： man \u003ccommand_name\u003e 比如你想查看 man 命令本身的使用方式，你可以输入： man man 通常情况下，man 手册里面的内容都是英文的，这就要求你有一定的英文基础。man 手册的内容很多，涉及了 Linux 使用过程中的方方面面。为了便于查找，man 手册被进行了分册（分区段）处理，在 Research UNIX、BSD、OS X 和 Linux 中，手册通常被分为 8 个区段，安排如下： 区段 说明 1 一般命令 2 系统调用 3 库函数，涵盖了 C 标准函数库 4 特殊文件（通常是/dev 中的设备）和驱动程序 5 文件格式和约定 6 游戏和屏保 7 杂项 8 系统管理命令和守护进程 要查看相应区段的内容，就在 man 后面加上相应区段的数字即可，如： man 1 ls 会显示第一区段中的ls命令 man 页面。 所有的手册页遵循一个常见的布局，为了通过简单的 ASCII 文本展示而被优化，而这种情况下可能没有任何形式的高亮或字体控制。一般包括以下部分内容： NAME（名称） 该命令或函数的名称，接着是一行简介。 SYNOPSIS（概要） 对于命令，正式的描述它如何运行，以及需要什么样的命令行参数。对于函数，介绍函数所需的参数，以及哪个头文件包含该函数的定义。 DESCRIPTION（说明） 命令或函数功能的文本描述。 EXAMPLES（示例） 常用的一些示例。 SEE ALSO（参见） 相关命令或函数的列表。 也可能存在其它部分内容，但这些部分没有得到跨手册页的标准化。常见的例子包括：OPTIONS（选项），EXIT STATUS（退出状态），ENVIRONMENT（环境），BUGS（程序漏洞），FILES（文件），AUTHOR（作者），REPORTING BUGS（已知漏洞），HISTORY（历史）和 COPYRIGHT（版权）。 通常 man 手册中的内容很多，你可能不太容易找到你想要的结果，不过幸运的是你可以在 man 中使用搜索/\u003c你要搜索的关键字\u003e，查找完毕后你可以使用n键切换到下一个关键字所在处，shift+n为上一个关键字所在处。使用Space（空格键）翻页，Enter（回车键）向下滚动一行，或者使用k，j（vim 编辑器的移动键）进行向前向后滚动一行。按下h键为显示使用帮助（因为 man 使用 less 作为阅读器，实为less工具的帮助），按下q退出。 想要获得更详细的帮助，你还可以使用info命令，不过通常使用man就足够了。如果你知道某个命令的作用，只是想快速查看一些它的某个具体参数的作用，那么你可以使用--help参数，大部分命令都会带有这个参数，如： ls --help ","date":"2023-01-10","objectID":"/linux%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/:2:1","tags":[],"title":"Linux常用命令","uri":"/linux%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/"},{"categories":["linux"],"content":"二、用户及文件权限管理 Linux 是一个可以实现多用户登录的操作系统，比如“李雷”和“韩梅梅”都可以同时登录同一台主机，他们共享一些主机的资源，但他们也分别有自己的用户空间，用于存放各自的文件。但实际上他们的文件都是放在同一个物理磁盘上的甚至同一个逻辑分区或者目录里，但是由于 Linux 的 用户管理 和 权限机制，不同用户不可以轻易地查看、修改彼此的文件。 ","date":"2023-01-10","objectID":"/linux%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/:3:0","tags":[],"title":"Linux常用命令","uri":"/linux%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/"},{"categories":["linux"],"content":"1. 查看用户 请打开终端，输入命令： who am i # 或者 who mom likes 输出的第一列表示打开当前伪终端的用户的用户名（要查看当前登录用户的用户名，去掉空格直接使用 whoami 即可），第二列的 pts/0 中 pts 表示伪终端，所谓伪是相对于 /dev/tty 设备而言的，还记得上一节讲终端时的那七个使用 [Ctrl]+[Alt]+[F1]～[F7] 进行切换的 /dev/tty 设备么，这是“真终端”，伪终端就是当你在图形用户界面使用 /dev/tty7 时每打开一个终端就会产生一个伪终端，pts/0 后面那个数字就表示打开的伪终端序号，你可以尝试再打开一个终端，然后在里面输入 who am i，看第二列是不是就变成 pts/1 了，第三列则表示当前伪终端的启动时间。 还有一点需要注意的是，在某些环境中 who am i 和 who mom likes 命令不会输出任何内容，这是因为当前使用的 SHELL 不是登录时的 SHELL，没有用户与 who 的 stdin 相关联，因此不会输出任何内容。例如我在本地的 Ubuntu 系统上输入这个命令就不会有提示。 此时我们只需要打开一个登录 SHELL 的终端例如 Tmux，或者通过 ssh 登录到本机，再在新的终端里执行命令即可。 tmux who 命令其它常用参数 参数 说明 -a 打印能打印的全部 -d 打印死掉的进程 -m 同am i，mom likes -q 打印当前登录用户数及用户名 -u 打印当前登录用户登录信息 -r 打印运行等级 ","date":"2023-01-10","objectID":"/linux%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/:3:1","tags":[],"title":"Linux常用命令","uri":"/linux%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/"},{"categories":["linux"],"content":"2. 创建及切换用户 在 Linux 系统里， root 账户拥有整个系统至高无上的权限，比如新建和添加用户。 root 权限，系统权限的一种，与 SYSTEM 权限可以理解成一个概念，但高于 Administrator 权限，root 是 Linux 和 UNIX 系统中的超级管理员用户帐户，该帐户拥有整个系统至高无上的权力，所有对象他都可以操作，所以很多黑客在入侵系统的时候，都要把权限提升到 root 权限，这个操作等同于在 Windows 下就是将新建的非法帐户添加到 Administrators 用户组。更比如安卓操作系统中（基于 Linux 内核）获得 root 权限之后就意味着已经获得了手机的最高权限，这时候你可以对手机中的任何文件（包括系统文件）执行所有增、删、改、查的操作。 大部分 Linux 系统在安装时都会建议用户新建一个用户而不是直接使用 root 用户进行登录，当然也有直接使用 root 登录的例如 Kali（基于 Debian 的 Linux 发行版，集成大量工具软件，主要用于数字取证的操作系统）。一般我们登录系统时都是以普通账户的身份登录的，要创建用户需要 root 权限，这里就要用到 sudo 这个命令了。不过使用这个命令有两个大前提，一是你要知道当前登录用户的密码，二是当前用户必须在 sudo 用户组。shiyanlou 用户也属于 sudo 用户组（稍后会介绍如何查看和添加用户组）。 su，su- 与 sudo 需要注意 Linux 环境下输入密码是不会显示的。 su \u003cuser\u003e 可以切换到用户 user，执行时需要输入目标用户的密码，sudo \u003ccmd\u003e 可以以特权级别运行 cmd 命令，需要当前用户属于 sudo 组，且需要输入当前用户的密码。su - \u003cuser\u003e 命令也是切换用户，但是同时用户的环境变量和工作目录也会跟着改变成目标用户所对应的。 现在我们新建一个叫 lilei 的用户： sudo adduser lilei 实验楼的环境目前设置为 shiyanlou 用户执行 sudo 不需要输入密码，通常此处需要按照提示输入 shiyanlou 密码（Linux 下密码输入是不显示任何内容的，shiyanlou 用户密码可以在右侧环境信息里查看，请勿自行设置密码）。然后是给 lilei 用户设置密码，后面的选项的一些内容你可以选择直接回车使用默认值。 这个命令不但可以添加用户到系统，同时也会默认为新用户在 /home 目录下创建一个工作目录： ls /home 现在你已经创建好一个用户，并且你可以使用你创建的用户登录了，使用如下命令切换登录用户： su -l lilei 输入刚刚设置的 lilei 的密码，然后输入如下命令并查看输出： who am i whoami pwd 你发现了区别了吗？这就是上一小节我们讲到的 who am i 和 whoami 命令的区别。 退出当前用户跟退出终端一样，可以使用 exit 命令或者使用快捷键 Ctrl+D。 ","date":"2023-01-10","objectID":"/linux%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/:3:2","tags":[],"title":"Linux常用命令","uri":"/linux%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/"},{"categories":["linux"],"content":"3. 用户组 在 Linux 里面每个用户都有一个归属（用户组），用户组简单地理解就是一组用户的集合，它们共享一些资源和权限，同时拥有私有资源，就跟家的形式差不多，你的兄弟姐妹（不同的用户）属于同一个家（用户组），你们可以共同拥有这个家（共享资源），爸妈对待你们都一样（共享权限），你偶尔写写日记，其他人未经允许不能查看（私有资源和权限）。当然一个用户是可以属于多个用户组的，正如你既属于家庭，又属于学校或公司。 在 Linux 里面如何知道自己属于哪些用户组呢？ 方法一：使用 groups 命令 groups shiyanlou 其中冒号之前表示用户，后面表示该用户所属的用户组。这里可以看到 shiyanlou 用户属于 shiyanlou 用户组，每次新建用户如果不指定用户组的话，默认会自动创建一个与用户名相同的用户组（差不多就相当于家长的意思）。 默认情况下在 sudo 用户组里的可以使用 sudo 命令获得 root 权限。shiyanlou 用户也可以使用 sudo 命令，为什么这里没有显示在 sudo 用户组里呢？可以查看下 /etc/sudoers.d/shiyanlou 文件，我们在 /etc/sudoers.d 目录下创建了这个文件，从而给 shiyanlou 用户赋予了 sudo 权限： 方法二：查看 /etc/group 文件 cat /etc/group | sort 这里 cat 命令用于读取指定文件的内容并打印到终端输出，后面会详细讲它的使用。 | sort 表示将读取的文本进行一个字典排序再输出，然后你将看到如下一堆输出，你可以在最下面看到 shiyanlou 的用户组信息： 没找到？没关系，你可以使用 grep 命令过滤掉一些你不想看到的结果： cat /etc/group | grep -E \"shiyanlou\" /etc/group 文件格式说明 /etc/group 的内容包括用户组（Group）、用户组口令、GID（组 ID） 及该用户组所包含的用户（User），每个用户组一条记录。格式如下： group_name:password:GID:user_list 你看到上面的 password 字段为一个 x，并不是说密码就是它，只是表示密码不可见而已。 这里需要注意，如果用户的 GID 等于用户组的 GID，那么最后一个字段 user_list 就是空的，这里的 GID 是指用户默认所在组的 GID，可以使用 id 命令查看。比如 shiyanlou 用户，在 /etc/group 中的 shiyanlou 用户组后面是不会显示的。lilei 用户，在 /etc/group 中的 lilei 用户组后面是不会显示的。 将其它用户加入 sudo 用户组usermod 命令 默认情况下新创建的用户是不具有 root 权限的，也不在 sudo 用户组，可以让其加入 sudo 用户组从而获取 root 权限： # 注意 Linux 上输入密码是不会显示的 su -l lilei sudo ls 会提示 lilei 不在 sudoers 文件中，意思就是 lilei 不在 sudo 用户组中，至于 sudoers 文件（/etc/sudoers）你现在最好不要动它，操作不慎会导致比较麻烦的后果。 使用 usermod 命令可以为用户添加用户组，同样使用该命令你必需有 root 权限，你可以直接使用 root 用户为其它用户添加用户组，或者用其它已经在 sudo 用户组的用户使用 sudo 命令获取权限来执行该命令。 这里我用 shiyanlou 用户执行 sudo 命令将 lilei 添加到 sudo 用户组，让它也可以使用 sudo 命令获得 root 权限，首先我们切换回 shiyanlou 用户。 su - shiyanlou 此处需要输入 shiyanlou 用户密码，shiyanlou 的密码可以在右侧工具栏的环境信息里看到。 当然也可以通过 sudo passwd shiyanlou 进行设置，或者你直接关闭当前终端打开一个新的终端。 groups lilei sudo usermod -G sudo lilei groups lilei 然后你再切换回 lilei 用户，现在就可以使用 sudo 获取 root 权限了。 ","date":"2023-01-10","objectID":"/linux%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/:3:3","tags":[],"title":"Linux常用命令","uri":"/linux%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/"},{"categories":["linux"],"content":"4. 删除用户和用户组 1) deluser命令 删除用户是很简单的事： sudo deluser lilei --remove-home 使用 --remove-home 参数在删除用户时候会一并将该用户的工作目录一并删除。如果不使用那么系统会自动在 /home 目录为该用户保留工作目录。 2）groupdel 命令 删除用户组可以使用 groupdel 命令，倘若该群组中仍包括某些用户，则必须先删除这些用户后，才能删除群组。 ","date":"2023-01-10","objectID":"/linux%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/:3:4","tags":[],"title":"Linux常用命令","uri":"/linux%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/"},{"categories":["linux"],"content":"三、linux文件权限 ","date":"2023-01-10","objectID":"/linux%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/:4:0","tags":[],"title":"Linux常用命令","uri":"/linux%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/"},{"categories":["linux"],"content":"1. 查看文件权限 1) ll或ls -l命令 ll或ls -l查看当前或指定目录下的非隐藏文件的所有属性，包括文件权限、修改时间、文件拥有及创建用户、大小、文件名等信息 另外，ls -a的-a参数可以显示隐藏文件 2）权限字母表示 ","date":"2023-01-10","objectID":"/linux%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/:4:1","tags":[],"title":"Linux常用命令","uri":"/linux%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/"},{"categories":["linux"],"content":"2. 变更文件所有者 chown 将指定文件的拥有者改为指定的用户或组，用户可以是用户名或者用户 ID；组可以是组名或者组 ID；文件是以空格分开的要改变权限的文件列表，支持通配符。 -c 显示更改的部分的信息 -R 处理指定目录及子目录 chown命令 （1）改变拥有者和群组 并显示改变信息 chown -c mail:mail log2012.log （2）改变文件群组 chown -c :mail t.log （3）改变文件夹及子文件目录属主及属组为 mail chown -cR mail: test/ ","date":"2023-01-10","objectID":"/linux%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/:4:2","tags":[],"title":"Linux常用命令","uri":"/linux%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/"},{"categories":["linux"],"content":"3. 修改文件权限 chmod命令 一种是包含字母和操作符表达式的文字设定法；另一种是包含数字的数字设定法。 每一文件或目录的访问权限都有三组，每组用三位表示，分别为文件属主的读、写和执行权限；与属主同组的用户的读、写和执行权限；系统中其他用户的读、写和执行权限。可使用 ls -l test.txt 查找。 以文件 log2012.log 为例： -rw-r--r-- 1 root root 296K 11-13 06:03 log2012.log 第一列共有 10 个位置，第一个字符指定了文件类型。在通常意义上，一个目录也是一个文件。如果第一个字符是横线，表示是一个非目录的文件。如果是 d，表示是一个目录。从第二个字符开始到第十个 9 个字符，3 个字符一组，分别表示了 3 组用户对文件或者目录的权限。权限字符用横线代表空许可，r 代表只读，w 代表写，x 代表可执行。 常用参数： -c 当发生改变时，报告处理信息 -R 处理指定目录以及其子目录下所有文件 权限范围： u ：目录或者文件的当前的用户 g ：目录或者文件的当前的群组 o ：除了目录或者文件的当前用户或群组之外的用户或者群组 a ：所有的用户及群组 权限代号： r ：读权限，用数字4表示 w ：写权限，用数字2表示 x ：执行权限，用数字1表示 - ：删除权限，用数字0表示 s ：特殊权限 （1）增加文件 t.log 所有用户可执行权限 chmod a+x t.log （2）减少文件 t.log 所有用户读写权限 chmod a-rw t.log （3）撤销原来所有的权限，然后使拥有者具有可读权限,并输出处理信息 chmod u=r t.log -c （4）给 file 的属主分配读、写、执行(7)的权限，给file的所在组分配读、执行(5)的权限，给其他用户分配执行(1)的权限 chmod 751 t.log -c（或者：chmod u=rwx,g=rx,o=x t.log -c) （5）将 test 目录及其子目录所有文件添加可读权限 chmod u+r,g+r,o+r -R text/ -c ","date":"2023-01-10","objectID":"/linux%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/:4:3","tags":[],"title":"Linux常用命令","uri":"/linux%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/"},{"categories":["linux"],"content":"4. 更多 adduser 和 useradd 答：useradd 只创建用户，不会创建用户密码和工作目录，创建完了需要使用 passwd \u003cusername\u003e 去设置新用户的密码。adduser 在创建用户的同时，会创建工作目录和密码（提示你设置），做这一系列的操作。其实 useradd、userdel 这类操作更像是一种命令，执行完了就返回。而 adduser 更像是一种程序，需要你输入、确定等一系列操作。 ","date":"2023-01-10","objectID":"/linux%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/:4:4","tags":[],"title":"Linux常用命令","uri":"/linux%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/"},{"categories":["linux"],"content":"四、Linux 目录结构及文件基本操作 ","date":"2023-01-10","objectID":"/linux%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/:5:0","tags":[],"title":"Linux常用命令","uri":"/linux%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/"},{"categories":["linux"],"content":"1. linux目录结构（FHS准则） ","date":"2023-01-10","objectID":"/linux%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/:5:1","tags":[],"title":"Linux常用命令","uri":"/linux%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/"},{"categories":["linux"],"content":"2. pwd命令 打印当前绝对路径 ","date":"2023-01-10","objectID":"/linux%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/:5:2","tags":[],"title":"Linux常用命令","uri":"/linux%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/"},{"categories":["linux"],"content":"3. cd命令 用于切换目录，不能切换至某个具体文件下。在 Linux 里面使用 . 表示当前目录，.. 表示上一级目录（注意，我们上一节介绍过的，以 . 开头的文件都是隐藏文件，所以这两个目录必然也是隐藏的，你可以使用 ls -a 命令查看隐藏文件），- 表示上一次所在目录，～ 通常表示当前用户的 home 目录。在 Linux 里面使用 . 表示当前目录，.. 表示上一级目录（注意，我们上一节介绍过的，以 . 开头的文件都是隐藏文件，所以这两个目录必然也是隐藏的，你可以使用 ls -a 命令查看隐藏文件），- 表示上一次所在目录，～ 通常表示当前用户的 home 目录。 1）绝对路径 关于绝对路径，简单地说就是以根\" / “目录为起点的完整路径，以你所要到的目录为终点，表现形式如： /usr/local/bin，表示根目录下的 usr 目录中的 local 目录中的 bin 目录。 2）相对路径 相对路径，也就是相对于你当前的目录的路径，相对路径是以当前目录 . 为起点，以你所要到的目录为终点，表现形式如： usr/local/bin （这里假设你当前目录为根目录）。你可能注意到，我们表示相对路径实际并没有加上表示当前目录的那个 . ，而是直接以目录名开头，因为这个 usr 目录为 / 目录下的子目录，是可以省略这个 . 的（以后会讲到一个类似不能省略的情况）；如果是当前目录的上一级目录，则需要使用 .. ，比如你当前目录为 /home/shiyanlou 目录下，根目录就应该表示为 ../../ ，表示上一级目录（ home 目录）的上一级目录（ / 目录）。 ","date":"2023-01-10","objectID":"/linux%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/:5:3","tags":[],"title":"Linux常用命令","uri":"/linux%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/"},{"categories":["linux"],"content":"4. 新建 1）新建空白文件touch命令 touch file{1..10}.txt#创建十个文件 #用法：touch 文件名 #没有的文件会创建一个新文件，若当前目录存在同名文件，则 touch 命令，则会更改该文件夹的时间戳而不是新建文件。 2）新建目录与多级目录mkdir 可用选项： -m: 对新建目录设置存取权限，也可以用 chmod 命令设置; -p: 可以是一个路径名称。此时若路径中的某些目录尚不存在,加上此选项后，系统将自动建立好那些尚不在的目录，即一次可以建立多个目录。 （1）当前工作目录下创建名为 t的文件夹 mkdir t （2）在 tmp 目录下创建路径为 test/t1/t 的目录，若不存在，则创建： mkdir -p /tmp/test/t1/t ","date":"2023-01-10","objectID":"/linux%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/:5:4","tags":[],"title":"Linux常用命令","uri":"/linux%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/"},{"categories":["linux"],"content":"5. 复制 1）复制文件cp命令 注意：命令行复制，如果目标文件已经存在会提示是否覆盖，而在 shell 脚本中，如果不加 -i 参数，则不会提示，而是直接覆盖！ -i 提示 -r 复制目录及目录内所有项目 -a 复制的文件与原文件时间一样 （1）复制 a.txt 到 test 目录下，保持原文件时间，如果原文件存在提示是否覆盖。 cp -ai a.txt test下 （2）为 a.txt 建议一个链接（快捷方式） cp -s a.txt link_a.txt 2）复制目录cp命令 添加参数-r即可 ","date":"2023-01-10","objectID":"/linux%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/:5:5","tags":[],"title":"Linux常用命令","uri":"/linux%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/"},{"categories":["linux"],"content":"6. 删除 1）删除文件rm命令 使用 rm（remove files or directories）命令删除一个文件： rm test 有时候你会遇到想要删除一些为只读权限的文件，直接使用 rm 删除会显示一个提示，如下： 你如果想忽略这提示，直接删除文件，可以使用 -f 参数强制删除： rm -f test 2）删除目录rm命令 跟复制目录一样，要删除一个目录，也需要加上 -r 或 -R 参数： rm -r family 遇到权限不足删除不了的目录也可以和删除文件一样加上 -f 参数： rm -rf family ","date":"2023-01-10","objectID":"/linux%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/:5:6","tags":[],"title":"Linux常用命令","uri":"/linux%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/"},{"categories":["linux"],"content":"7. 移动文件与文件重命名 移动文件或修改文件名，根据第二参数类型（如目录，则移动文件；如为文件则重命令该文件）。 当第二个参数为目录时，第一个参数可以是多个以空格分隔的文件或目录，然后移动第一个参数指定的多个文件到第二个参数指定的目录中。 （1）将文件 test.log 重命名为 test1.txt mv test.log test1.txt （2）将文件 log1.txt,log2.txt,log3.txt 移动到根的 test3 目录中 mv llog1.txt log2.txt log3.txt /test3 （3）将文件 file1 改名为 file2，如果 file2 已经存在，则询问是否覆盖 mv -i log1.txt log2.txt （4）移动当前文件夹下的所有文件到上一级目录 mv * ../ ","date":"2023-01-10","objectID":"/linux%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/:5:7","tags":[],"title":"Linux常用命令","uri":"/linux%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/"},{"categories":["linux"],"content":"8. 查看 1) cat，tac 和 nl 命令查看文件内容 前两个命令都是用来打印文件内容到标准输出（终端），其中 cat 为正序显示，tac 为倒序显示。 标准输入输出：当我们执行一个 shell 命令行时通常会自动打开三个标准文件，即标准输入文件（stdin），默认对应终端的键盘、标准输出文件（stdout）和标准错误输出文件（stderr），后两个文件都对应被重定向到终端的屏幕，以便我们能直接看到输出内容。进程将从标准输入文件中得到输入数据，将正常输出数据输出到标准输出文件，而将错误信息送到标准错误文件中。 比如我们要查看之前从 /etc 目录下拷贝来的 passwd 文件： cd /home/shiyanlou cp /etc/passwd passwd cat passwd 可以加上 -n 参数显示行号： cat -n passwd nl 命令，添加行号并打印，这是个比 cat -n 更专业的行号打印命令。 这里简单列举它的常用的几个参数： -b : 指定添加行号的方式，主要有两种： -b a:表示无论是否为空行，同样列出行号(\"cat -n\"就是这种方式) -b t:只列出非空行的编号并列出（默认为这种方式） -n : 设置行号的样式，主要有三种： -n ln:在行号字段最左端显示 -n rn:在行号字段最右边显示，且不加 0 -n rz:在行号字段最右边显示，且加 0 -w : 行号字段占用的位数(默认为 6 位) 你会发现使用这几个命令，默认的终端窗口大小，一屏显示不完文本的内容，得用鼠标拖动滚动条或者滑动滚轮才能继续往下翻页，要是可以直接使用键盘操作翻页就好了，那么你就可以使用下面要介绍的命令。 2) more 和 less 命令分页查看文件内容 如果说上面的 cat 是用来快速查看一个文件的内容的，那么这个 more 和 less 就是天生用来\"阅读\"一个文件的内容的，比如说 man 手册内部就是使用的 less 来显示内容。其中 more 命令比较简单，只能向一个方向滚动，而 less 为基于 more 和 vi （一个强大的编辑器，我们有单独的课程来让你学习）开发，功能更强大。less 的使用基本和 more 一致，具体使用请查看 man 手册，这里只介绍 more 命令的使用。 使用 more 命令打开 passwd 文件： more passwd 打开后默认只显示一屏内容，终端底部显示当前阅读的进度。可以使用 Enter 键向下滚动一行，使用 Space 键向下滚动一屏，按下 h 显示帮助，q 退出。 3) head 和 tail 命令查看文件内容 这两个命令，那些性子比较急的人应该会喜欢，因为它们一个是只查看文件的头几行（默认为 10 行，不足 10 行则显示全部）和尾几行。还是拿 passwd 文件举例，比如当我们想要查看最近新增加的用户，那么我们可以查看这个 /etc/passwd 文件，不过我们前面也看到了，这个文件里面一大堆乱糟糟的东西，看起来实在费神啊。因为系统新增加一个用户，会将用户的信息添加到 passwd 文件的最后，那么这时候我们就可以使用 tail 命令了： tail /etc/passwd 甚至更直接的只看一行， 加上 -n 参数，后面紧跟行数： tail -n 1 /etc/passwd 关于 tail 命令，不得不提的还有它一个很牛的参数 -f，这个参数可以实现不停地读取某个文件的内容并显示。这可以让我们动态查看日志，达到实时监视的目的。 用于显示指定文件末尾内容，不指定文件时，作为输入信息进行处理。常用查看日志文件。 常用参数： -f 循环读取（常用于查看递增的日志文件） -n\u003c行数\u003e 显示行数（从后向前） （1）循环读取逐渐增加的文件内容 ping 127.0.0.1 \u003e ping.log \u0026 后台运行：可使用 jobs -l 查看，也可使用 fg 将其移到前台运行。 tail -f ping.log （查看日志） 4) file命令查看文件类型 file /bin/ls 说明这是一个可执行文件，运行在 64 位平台，并使用了动态链接文件（共享库）。 与 Windows 不同的是，如果你新建了一个 shiyanlou.txt 文件，Windows 会自动把它识别为文本文件，而 file 命令会识别为一个空文件。这个前面我提到过，在 Linux 中文件的类型不是根据文件后缀来判断的。当你在文件里输入内容后才会显示文件类型。 ","date":"2023-01-10","objectID":"/linux%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/:5:8","tags":[],"title":"Linux常用命令","uri":"/linux%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/"},{"categories":["linux"],"content":"9. 编辑文件 在 Linux 下面编辑文件通常我们会直接使用专门的命令行编辑器比如（emacs，vim，nano），由于涉及 Linux 上的编辑器的内容比较多，且非常重要。 ","date":"2023-01-10","objectID":"/linux%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/:5:9","tags":[],"title":"Linux常用命令","uri":"/linux%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/"},{"categories":["linux"],"content":"五、环境变量与文件查找 ","date":"2023-01-10","objectID":"/linux%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/:6:0","tags":[],"title":"Linux常用命令","uri":"/linux%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/"},{"categories":["linux"],"content":"1. 环境变量 /etc/bashrc（有的 Linux 没有这个文件） 和 /etc/profile ，它们分别存放的是 shell 变量和环境变量。还有要注意区别的是每个用户目录下的一个隐藏文件： # .profile 可以用 ls -a 查看 cd /home/shiyanlou ls -a 这个 .profile 只对当前用户永久生效。因为它保存在当前用户的 Home 目录下，当切换用户时，工作目录可能一并被切换到对应的目录中，这个文件就无法生效。而写在 /etc/profile 里面的是对所有用户永久生效，所以如果想要添加一个全局、永久生效的环境变量（对所有用户都起作用的环境变量），只需要打开 /etc/profile，添加环境变量即可。 1）临时生效 使用 export 命令行声明即可，变量在关闭当前 shell 时失效，生效范围是当前的shell，其他shell窗口不生效。 2）永久生效 需要修改配置文件，变量永久生效，生效范围是全局生效。前面我们在 Shell 中修改了一个配置脚本文件之后（比如 zsh 的配置文件 home 目录下的 .zshrc），每次都要退出终端重新打开甚至重启主机之后其才能生效，很是麻烦，我们可以使用 source 命令来让其立即生效，如： cd /home/shiyanlou source .zshrc ","date":"2023-01-10","objectID":"/linux%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/:6:1","tags":[],"title":"Linux常用命令","uri":"/linux%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/"},{"categories":["linux"],"content":"2. linux上的可执行文件 windows上的可执行文件以.exe结尾，linux上则是以文件权限的方式展现是否可以执行。使用touch创建的空白文件夹默认权限是-rw-rw-r--不论是用户还是用户组还是其他用户及用户组都不具备可执行权，先编辑指定内容，再编译响应程序获得，再使用chmod修改编译文件的文件权限为可执行，然后直接调用可执行文件的文件名就可以运行该程序。此过程相当于windows上的编码、编译、生成可执行文件、点击运行即可 你可能很早之前就有疑问，我们在 Shell 中输入一个命令，Shell 是怎么知道去哪找到这个命令然后执行的呢？这是通过环境变量 PATH 来进行搜索的，熟悉 Windows 的用户可能知道 Windows 中的也是有这么一个 PATH 环境变量。这个 PATH 里面就保存了 Shell 中执行的命令的搜索路径。 查看 PATH 环境变量的内容： echo $PATH 默认情况下你会看到如下输出： /usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games 如果你还记得 Linux 目录结构那一节的内容，你就应该知道上面这些目录下放的是哪一类文件了。通常这一类目录下放的都是可执行文件，当我们在 Shell 中执行一个命令时，系统就会按照 PATH 中设定的路径按照顺序依次到目录中去查找，如果存在同名的命令，则执行先找到的那个。 创建一个 Shell 脚本文件，你可以使用 gedit，vim，sublime 等工具编辑。如果你是直接复制的话，建议使用 gedit 或者 sublime，否则可能导致代码缩进混乱。 cd /home/shiyanlou touch hello_shell.sh gedit hello_shell.sh 在脚本中添加如下内容，保存并退出。 注意不要省掉第一行，这不是注释，有用户反映有语法错误，就是因为没有了第一行。 #!/bin/bash for ((i=0; i\u003c10; i++));do echo \"hello shell\" done exit 0 为文件添加可执行权限，否则执行会报错没有权限： chmod 755 hello_shell.sh 执行脚本： cd /home/shiyanlou ./hello_shell.sh 创建一个 C 语言 hello world 程序： cd /home/shiyanlou gedit hello_world.c 输入如下内容，同样不能省略第一行。 #include \u003cstdio.h\u003e int main(void) { printf(\"hello world!\\n\"); return 0; } 保存后使用 gcc 生成可执行文件： gcc -o hello_world hello_world.c gcc 生成二进制文件默认具有可执行权限，不需要修改。 在 /home/shiyanlou 家目录创建一个 mybin 目录，并将上述 hello_shell.sh 和 hello_world 文件移动到其中： cd /home/shiyanlou mkdir mybin mv hello_shell.sh hello_world mybin/ 现在你可以在 mybin 目录中分别运行你刚刚创建的两个程序： cd mybin ./hello_shell.sh ./hello_world 回到上一级目录，也就是 shiyanlou 家目录，当再想运行那两个程序时，会发现提示命令找不到，除非加上命令的完整路径，但那样很不方便，如何做到像使用系统命令一样执行自己创建的脚本文件或者程序呢？那就要将命令所在路径添加到 PATH 环境变量了。 ","date":"2023-01-10","objectID":"/linux%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/:6:2","tags":[],"title":"Linux常用命令","uri":"/linux%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/"},{"categories":["linux"],"content":"3. 搜索文件 1）find命令足矣 用于在文件树中查找文件，并作出相应的处理。 命令格式： find pathname -options [-print -exec -ok ...] 命令参数： pathname: find命令所查找的目录路径。例如用.来表示当前目录，用/来表示系统根目录。 -print：find命令将匹配的文件输出到标准输出。 -exec：find命令对匹配的文件执行该参数所给出的shell命令。相应命令的形式为'command' { } \\;，注意{ }和\\；之间的空格。 -ok： 和-exec的作用相同，只不过以一种更为安全的模式来执行该参数所给出的shell命令，在执行每一个命令之前，都会给出提示，让用户来确定是否执行。 命令选项： -name 按照文件名查找文件 -perm 按文件权限查找文件 -user 按文件属主查找文件 -group 按照文件所属的组来查找文件。 -type 查找某一类型的文件，诸如： b - 块设备文件 d - 目录 c - 字符设备文件 l - 符号链接文件 p - 管道文件 f - 普通文件 （1）查找 48 小时内修改过的文件 find -atime -2 （2）在当前目录查找 以 .log 结尾的文件。. 代表当前目录 find ./ -name '*.log' （3）查找 /opt 目录下 权限为 777 的文件 find /opt -perm 777 （4）查找大于 1K 的文件 find -size +1000c (5)查找等于 1000 字符的文件 find -size 1000c -exec 参数后面跟的是 command 命令，它的终止是以 ; 为结束标志的，所以这句命令后面的分号是不可缺少的，考虑到各个系统中分号会有不同的意义，所以前面加反斜杠。{} 花括号代表前面find查找出来的文件名。 2）忽略which、whereis以及locate ","date":"2023-01-10","objectID":"/linux%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/:6:3","tags":[],"title":"Linux常用命令","uri":"/linux%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/"},{"categories":["linux"],"content":"六、文件打包与解压缩 ","date":"2023-01-10","objectID":"/linux%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/:7:0","tags":[],"title":"Linux常用命令","uri":"/linux%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/"},{"categories":["linux"],"content":"1. 概念储备 在 Windows 上最常见的不外乎这两种 *.zip，*.7z 后缀的压缩文件。而在 Linux 上面常见的格式除了以上两种外，还有 .rar，*.gz，*.xz，*.bz2，*.tar，*.tar.gz，*.tar.xz，*.tar.bz2，简单介绍如下： 文件后缀名 说明 *.zip zip 程序打包压缩的文件 *.rar rar 程序压缩的文件 *.7z 7zip 程序压缩的文件 *.tar tar 程序打包，未压缩的文件 *.gz gzip 程序（GNU zip）压缩的文件 *.xz xz 程序压缩的文件 *.bz2 bzip2 程序压缩的文件 *.tar.gz tar 打包，gzip 程序压缩的文件 *.tar.xz tar 打包，xz 程序压缩的文件 *tar.bz2 tar 打包，bzip2 程序压缩的文件 *.tar.7z tar 打包，7z 程序压缩的文件 这么多个命令，不过我们一般只需要掌握几个命令即可，包括 zip，tar。下面会依次介绍这几个命令及对应的解压命令。 ","date":"2023-01-10","objectID":"/linux%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/:7:1","tags":[],"title":"Linux常用命令","uri":"/linux%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/"},{"categories":["linux"],"content":"2. zip 压缩打包程序 使用 zip 打包文件夹，注意输入完整的参数和路径： cd /home/shiyanlou zip -r -q -o shiyanlou.zip /home/shiyanlou/Desktop du -h shiyanlou.zip file shiyanlou.zip 上面命令将目录 /home/shiyanlou/Desktop 打包成一个文件，并查看了打包后文件的大小和类型。第一行命令中，-r 参数表示递归打包包含子目录的全部内容，-q 参数表示为安静模式，即不向屏幕输出信息，-o，表示输出文件，需在其后紧跟打包输出文件名。后面使用 du 命令查看打包后文件的大小（后面会具体说明该命令）。 设置压缩级别为 9 和 1（9 最大，1 最小），重新打包： zip -r -9 -q -o shiyanlou_9.zip /home/shiyanlou/Desktop -x ~/*.zip zip -r -1 -q -o shiyanlou_1.zip /home/shiyanlou/Desktop -x ~/*.zip 这里添加了一个参数用于设置压缩级别 -[1-9]，1 表示最快压缩但体积大，9 表示体积最小但耗时最久。最后那个 -x 是为了排除我们上一次创建的 zip 文件，否则又会被打包进这一次的压缩文件中，注意：这里只能使用绝对路径，否则不起作用。 我们再用 du 命令分别查看默认压缩级别、最低、最高压缩级别及未压缩的文件的大小： du -h -d 0 *.zip ~ | sort 通过 man 手册可知： -h， –human-readable（顾名思义，你可以试试不加的情况） -d， –max-depth（所查看文件的深度） 这样一目了然，理论上来说默认压缩级别应该是最高的，但是由于文件不大，这里的差异不明显（几乎看不出差别），不过你在环境中操作之后看到的压缩文件大小可能跟图上的有些不同，因为系统在使用过程中，会随时生成一些缓存文件在当前用户的家目录中，这对于我们学习命令使用来说，是无关紧要的，可以忽略这些不同。 创建加密 zip 包 使用 -e 参数可以创建加密压缩包： zip -r -e -o shiyanlou_encryption.zip /home/shiyanlou/Desktop 注意： 关于 zip 命令，因为 Windows 系统与 Linux/Unix 在文本文件格式上的一些兼容问题，比如换行符（为不可见字符），在 Windows 为 CR+LF（Carriage-Return+Line-Feed：回车加换行），而在 Linux/Unix 上为 LF（换行），所以如果在不加处理的情况下，在 Linux 上编辑的文本，在 Windows 系统上打开可能看起来是没有换行的。如果你想让你在 Linux 创建的 zip 压缩文件在 Windows 上解压后没有任何问题，那么你还需要对命令做一些修改： zip -r -l -o shiyanlou.zip /home/shiyanlou/Desktop 需要加上 -l 参数将 LF 转换为 CR+LF 来达到以上目的。 ","date":"2023-01-10","objectID":"/linux%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/:7:2","tags":[],"title":"Linux常用命令","uri":"/linux%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/"},{"categories":["linux"],"content":"3. 使用 unzip 命令解压缩 zip 文件 将 shiyanlou.zip 解压到当前目录： unzip shiyanlou.zip 使用安静模式，将文件解压到指定目录： unzip -q shiyanlou.zip -d ziptest 上述指定目录不存在，将会自动创建。如果你不想解压只想查看压缩包的内容你可以使用 -l 参数： unzip -l shiyanlou.zip 注意： 使用 unzip 解压文件时我们同样应该注意兼容问题，不过这里我们关心的不再是上面的问题，而是中文编码的问题，通常 Windows 系统上面创建的压缩文件，如果有有包含中文的文档或以中文作为文件名的文件时默认会采用 GBK 或其它编码，而 Linux 上面默认使用的是 UTF-8 编码，如果不加任何处理，直接解压的话可能会出现中文乱码的问题（有时候它会自动帮你处理），为了解决这个问题，我们可以在解压时指定编码类型。 使用 -O（英文字母，大写 o）参数指定编码类型： unzip -O GBK 中文压缩文件.zip ","date":"2023-01-10","objectID":"/linux%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/:7:3","tags":[],"title":"Linux常用命令","uri":"/linux%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/"},{"categories":["linux"],"content":"4. tar 打包、其压缩与解压工具 tar可以实现*.tar.gz、*.tar.xz和*tar.bz2的压缩（xz、gzip 及 bzip2）,zip还是用zip压缩工具 用来压缩和解压文件。tar 本身不具有压缩功能，只具有打包功能，有关压缩及解压是调用其它的功能来完成。 弄清两个概念：打包和压缩。打包是指将一大堆文件或目录变成一个总的文件；压缩则是将一个大的文件通过一些压缩算法变成一个小文件 注意： -f参数后面需要紧跟文件名，否则无效 使用对应压缩方式打包时，应当加上对应方式压缩解包或者查看 常用参数： -c 建立新的压缩文件 -f 指定压缩文件名,该参数后面需要紧跟文件名，否则不会生效 -r 添加文件到已经压缩文件包中 -u 添加改了和现有的文件到压缩包中 -x 从压缩包中抽取文件，及解压 -t 显示压缩文件中的内容 -z 支持gzip压缩 -j 支持bzip2压缩 -Z 支持compress解压文件 -v 显示操作过程 压缩文件格式 参数 *.tar.gz -z *.tar.xz -J *tar.bz2 -j 有关 gzip 及 bzip2 压缩: gzip 实例：压缩 gzip fileName .tar.gz 和.tgz 解压：gunzip filename.gz 或 gzip -d filename.gz 对应：tar zcvf filename.tar.gz tar zxvf filename.tar.gz bz2实例：压缩 bzip2 -z filename .tar.bz2 解压：bunzip filename.bz2或bzip -d filename.bz2 对应：tar jcvf filename.tar.gz 解压：tar jxvf filename.tar.bz2 （1）将文件全部打包成 tar 包（未压缩） tar -cvf log.tar 1.log,2.log 或tar -cvf log.* （2）将 /etc 下的所有文件及目录打包到指定目录，并使用 gz 压缩 tar -zcvf /tmp/etc.tar.gz /etc （3）查看刚打包的文件内容（一定加z，因为是使用 gzip 压缩的） tar -ztvf /tmp/etc.tar.gz （4）要压缩打包 /home, /etc ，但不要 /home/dmtsai tar --exclude /home/dmtsai -zcvf myfile.tar.gz /home/* /etc ","date":"2023-01-10","objectID":"/linux%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/:7:4","tags":[],"title":"Linux常用命令","uri":"/linux%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/"},{"categories":["linux"],"content":"七、文件系统操作与磁盘管理 ","date":"2023-01-10","objectID":"/linux%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/:8:0","tags":[],"title":"Linux常用命令","uri":"/linux%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/"},{"categories":["linux"],"content":"1. df命令 显示磁盘空间使用情况。获取硬盘被占用了多少空间，目前还剩下多少空间等信息，如果没有文件名被指定，则所有当前被挂载的文件系统的可用空间将被显示。 默认情况下，磁盘空间将以 1KB 为单位进行显示，除非环境变量 POSIXLY_CORRECT 被指定，那样将以512字节为单位进行显示： -a 全部文件系统列表 -h 以方便阅读的方式显示信息 -i 显示inode信息 -k 区块为1024字节 -l 只显示本地磁盘 -T 列出文件系统类型 （1）显示磁盘使用情况 df -l （2）以易读方式列出所有文件系统及其类型 df -haT ","date":"2023-01-10","objectID":"/linux%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/:8:1","tags":[],"title":"Linux常用命令","uri":"/linux%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/"},{"categories":["linux"],"content":"2. du 命令 du 命令也是查看使用空间的，但是与 df 命令不同的是 Linux du 命令是对文件和目录磁盘使用的空间的查看： 命令格式： du [选项] [文件] 常用参数： -a 显示目录中所有文件大小 -k 以KB为单位显示文件大小 -m 以MB为单位显示文件大小 -g 以GB为单位显示文件大小 -h 以易读方式显示文件大小 -s 仅显示总计 -c或--total 除了显示个别目录或文件的大小外，同时也显示所有目录或文件的总和 （1）以易读方式显示文件夹内及子文件夹大小 du -h scf/ （2）以易读方式显示文件夹内所有文件大小 du -ah scf/ （3）显示几个文件或目录各自占用磁盘空间的大小，还统计它们的总和 du -hc test/ scf/ （4）输出当前目录下各个子目录所使用的空间 du -hc --max-depth=1 scf/ ","date":"2023-01-10","objectID":"/linux%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/:8:2","tags":[],"title":"Linux常用命令","uri":"/linux%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/"},{"categories":["linux"],"content":"八、linux下的帮助命令 ","date":"2023-01-10","objectID":"/linux%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/:9:0","tags":[],"title":"Linux常用命令","uri":"/linux%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/"},{"categories":["linux"],"content":"1. 内建命令与外部命令 一些查看帮助的工具在内建命令与外建命令上是有区别对待的。 内建命令实际上是 shell 程序的一部分，其中包含的是一些比较简单的 Linux 系统命令，这些命令是写在 bash 源码的 builtins 里面的，由 shell 程序识别并在 shell 程序内部完成运行，通常在 Linux 系统加载运行时 shell 就被加载并驻留在系统内存中。而且解析内部命令 shell 不需要创建子进程，因此其执行速度比外部命令快。比如：history、cd、exit 等等。 外部命令是 Linux 系统中的实用程序部分，因为实用程序的功能通常都比较强大，所以其包含的程序量也会很大，在系统加载时并不随系统一起被加载到内存中，而是在需要时才将其调入内存。虽然其不包含在 shell 中，但是其命令执行过程是由 shell 程序控制的。外部命令是在 Bash 之外额外安装的，通常放在/bin，/usr/bin，/sbin，/usr/sbin 等等。比如：ls、vi 等。 简单来说就是：一个是天生自带的天赋技能，一个是后天得来的附加技能。我们可以使用　type 命令来区分命令是内建的还是外部的。例如这两个得出的结果是不同的 type exit type vim 得到的是两种结果，若是对 ls 你还能得到第三种结果 # 得到这样的结果说明是内建命令，正如上文所说内建命令都是在 bash 源码中的 builtins 的.def中 xxx is a shell builtin # 得到这样的结果说明是外部命令，正如上文所说，外部命令在/usr/bin or /usr/sbin等等中 xxx is /usr/bin/xxx # 若是得到alias的结果，说明该指令为命令别名所设定的名称； xxx is an alias for xx --xxx ","date":"2023-01-10","objectID":"/linux%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/:9:1","tags":[],"title":"Linux常用命令","uri":"/linux%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/"},{"categories":["linux"],"content":"2. 帮助命令的使用 1）help命令 只用于查看内建命令的使用方法，无法运用于外部命令的查看，外部命令使用参数往往使用--help查看帮助 2）man命令 得到的内容比用help更多更详细，而且man没有内建与外部命令的区分，因为man工具是显示系统手册页中的内容 man手册章节含义： 章节数 说明 1 Standard commands （标准命令） 2 System calls （系统调用） 3 Library functions （库函数） 4 Special devices （设备说明） 5 File formats （文件格式） 6 Games and toys （游戏和娱乐） 7 Miscellaneous （杂项） 8 Administrative Commands （管理员命令） 9 其他（Linux 特定的）， 用来存放内核例行程序的文档。 3）info命令 info 来自自由软件基金会的 GNU 项目，是 GNU 的超文本帮助系统，能够更完整的显示出 GNU 信息。所以得到的信息当然更多 如果没有安装info，可以执行一下程序安装 # 安装 info sudo apt-get update sudo apt-get install info # 查看 ls 命令的 info info ls ","date":"2023-01-10","objectID":"/linux%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/:9:2","tags":[],"title":"Linux常用命令","uri":"/linux%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/"},{"categories":["linux"],"content":"九、Linux 任务计划 crontab ","date":"2023-01-10","objectID":"/linux%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/:10:0","tags":[],"title":"Linux常用命令","uri":"/linux%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/"},{"categories":["linux"],"content":"1. crontab的简介 crontab 命令常见于 Unix 和类 Unix 的操作系统之中（Linux 就属于类 Unix 操作系统），用于设置周期性被执行的指令。 crontab 命令从输入设备读取指令，并将其存放于 crontab 文件中，以供之后读取和执行。通常，crontab 储存的指令被守护进程激活，crond 为其守护进程，crond 常常在后台运行，每一分钟会检查一次是否有预定的作业需要执行。 通过 crontab 命令，我们可以在固定的间隔时间执行指定的系统指令或 shell 脚本。时间间隔的单位可以是分钟、小时、日、月、周的任意组合。 这里我们看一看 crontab 的格式： # Example of job definition: # .---------------- minute (0 - 59) # | .------------- hour (0 - 23) # | | .---------- day of month (1 - 31) # | | | .------- month (1 - 12) OR jan,feb,mar,apr ... # | | | | .---- day of week (0 - 6) (Sunday=0 or 7) OR sun,mon,tue,wed,thu,fri,sat # | | | | | # * * * * * user-name command to be executed ","date":"2023-01-10","objectID":"/linux%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/:10:1","tags":[],"title":"Linux常用命令","uri":"/linux%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/"},{"categories":["linux"],"content":"2. crontab的准备 日志监控 crontab 在本实验环境中需要做一些特殊的准备，首先我们会启动 rsyslog，以便我们可以通过日志中的信息来了解我们的任务是否真正的被执行了（在本实验环境中需要手动启动，而在自己本地中 Ubuntu 会默认自行启动不需要手动启动）。 sudo apt-get install -y rsyslog sudo service rsyslog start 启动crontab 在本实验环境中 crontab 也是不被默认启动的，同时不能在后台由 upstart 来管理，所以需要我们来启动它: sudo cron －f \u0026 ","date":"2023-01-10","objectID":"/linux%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/:10:2","tags":[],"title":"Linux常用命令","uri":"/linux%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/"},{"categories":["linux"],"content":"3. crontab的使用 1）crontab -e命令 我们通过下面一个命令来添加一个计划任务： crontab -e 第一次启动会出现这样一个画面，这是让我们选择编辑的工具，选择第二个基本的 vim 就可以了。 编写进程指定时间以及命令 crontab文件的含义： 用户所建立的crontab文件中，每一行都代表一项任务，每行的每个字段代表一项设置，它的格式共分为六个字段，前五段是时间设定段，第六段是要执行的命令段，格式如下： minute hour day month week command 其中： minute： 表示分钟，可以是从0到59之间的任何整数。 hour：表示小时，可以是从0到23之间的任何整数。 day：表示日期，可以是从1到31之间的任何整数。 month：表示月份，可以是从1到12之间的任何整数。 week：表示星期几，可以是从0到7之间的任何整数，这里的0或7代表星期日。 command：要执行的命令，可以是系统命令，也可以是自己编写的脚本文件。 在以上各个字段中，还可以使用以下特殊字符： 星号（*）：代表所有可能的值，例如month字段如果是星号，则表示在满足其它字段的制约条件后每月都执行该命令操作。 逗号（,）：可以用逗号隔开的值指定一个列表范围，例如，“1,2,5,7,8,9” 中杠（-）：可以用整数之间的中杠表示一个整数范围，例如“2-6”表示“2,3,4,5,6” 正斜线（/）：可以用正斜线指定时间的间隔频率，例如“0-23/2”表示每两小时执行一次。同时正斜线可以和星号一起使用，例如*/10，如果用在minute字段，表示每十分钟执行一次。 在了解命令格式之后，我们通过这样的一个例子来完成一个任务的添加，在文档的最后一排加上这样一排命令，该任务是每分钟我们会在/home/shiyanlou 目录下创建一个以当前的年月日时分秒为名字的空白文件 */1 * * * * touch /home/shiyanlou/$(date +\\%Y\\%m\\%d\\%H\\%M\\%S) 注意： “ % ” 在 crontab 文件中，有结束命令行、换行、重定向的作用，前面加 ” \\ ” 符号转义，否则，“ % ” 符号将执行其结束命令行或者换行的作用，并且其后的内容会被做为标准输入发送给前面的命令。 添加成功后我们会得到最后一排 installing new crontab 的一个提示： 2）crontab -l命令 当然我们也可以通过这样的一个指令来查看我们添加了哪些任务： crontab -l 通过图中的显示，我们也可以看出，我们正确的保存并且添加成功了该任务的： 3）检查cron的守护进程是否启动ps aux | grep cron 虽然我们添加了任务，但是如果 cron 的守护进程并没有启动，它根本都不会监测到有任务，当然也就不会帮我们执行，我们可以通过以下 2 种方式来确定我们的 cron 是否成功的在后台启动，默默的帮我们做事，若是没有就得执行上文准备中的第二步了。 ps aux | grep cron # or pgrep cron 通过下图可以看到任务在创建之后，执行了几次，生成了一些文件，且每分钟生成一个： 4）动态查看日志tail -f 我们通过这样一个命令可以查看到执行任务命令之后在日志中的信息反馈： sudo tail -f /var/log/syslog 从图中我们可以看到分别在 13 点 28、29、30 分的 01 秒为我们在 shiyanlou 用户的家目录下创建了文件。 5）crontab -r命令 当我们并不需要这个任务的时候我们可以使用这么一个命令去删除任务： crontab -r 通过图中我们可以看出我们删除之后再查看任务列表，系统已经显示该用户并没有任务哦。 ","date":"2023-01-10","objectID":"/linux%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/:10:3","tags":[],"title":"Linux常用命令","uri":"/linux%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/"},{"categories":["linux"],"content":"4. crontab的深入 每个用户使用 crontab -e 添加计划任务，都会在 /var/spool/cron/crontabs 中添加一个该用户自己的任务文档，这样目的是为了隔离。 如果是系统级别的定时任务，需要 root 权限执行的任务应该怎么处理？ 只需要使用 sudo 编辑 /etc/crontab 文件就可以。 cron 服务监测时间最小单位是分钟，所以 cron 会每分钟去读取一次 /etc/crontab 与 /var/spool/cron/crontabs 里面的內容。 在 /etc 目录下，cron 相关的目录有下面几个： 每个目录的作用： /etc/cron.daily，目录下的脚本会每天执行一次，在每天的 6 点 25 分时运行； /etc/cron.hourly，目录下的脚本会每个小时执行一次，在每小时的 17 分钟时运行； /etc/cron.monthly，目录下的脚本会每月执行一次，在每月 1 号的 6 点 52 分时运行； /etc/cron.weekly，目录下的脚本会每周执行一次，在每周第七天的 6 点 47 分时运行； 系统默认执行时间可以根据需求进行修改。 ","date":"2023-01-10","objectID":"/linux%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/:10:4","tags":[],"title":"Linux常用命令","uri":"/linux%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/"},{"categories":["linux"],"content":"十、命令执行顺序控制与管道 ","date":"2023-01-10","objectID":"/linux%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/:11:0","tags":[],"title":"Linux常用命令","uri":"/linux%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/"},{"categories":["linux"],"content":"1. 顺序执行多条命令:; 多条命令使用分号隔开，达到顺序执行的目的。 sudo apt-get update;sudo apt-get install some-tool;some-tool # 让它自己运行 ","date":"2023-01-10","objectID":"/linux%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/:11:1","tags":[],"title":"Linux常用命令","uri":"/linux%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/"},{"categories":["linux"],"content":"2. 有选择执行多条命令\u0026\u0026,||命令 \u0026\u0026表示如果前面的命令执行结果（不是表示终端输出的内容，而是表示命令执行状态的结果）返回 0 则执行后面的，否则不执行，你可以从 $? 环境变量获取上一次命令的返回结果： 学习过 C 语言的用户应该知道在 C 语言里面 \u0026\u0026 表示逻辑与，而且还有一个 || 表示逻辑或，同样 Shell 也有一个 ||，它们的区别就在于，shell 中的这两个符号除了也可用于表示逻辑与和或之外，就是可以实现这里的命令执行顺序的简单控制。|| 在这里就是与 \u0026\u0026 相反的控制效果，当上一条命令执行结果为 ≠0(\\$?≠0) 时则执行它后面的命令： which cowsay\u003e/dev/null || echo \"cowsay has not been install, please run 'sudo apt-get install cowsay' to install\" 除了上述基本的使用之外，我们还可以结合着 \u0026\u0026 和 || 来实现一些操作，比如： which cowsay\u003e/dev/null \u0026\u0026 echo \"exist\" || echo \"not exist\" #注意顺序：\u0026\u0026在前，表示前面执行结果不为0表示执行后面的操作，如果为0，不执行\u0026\u0026后面的而执行||后面的命令；||在前，表示如果命令返回0时，表示直接执行||后的语句 我画个流程图来解释一下上面的流程： ","date":"2023-01-10","objectID":"/linux%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/:11:2","tags":[],"title":"Linux常用命令","uri":"/linux%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/"},{"categories":["linux"],"content":"3. 管道 管道是一种通信机制，通常用于进程间的通信（也可通过 socket 进行网络通信），它表现出来的形式就是将前面每一个进程的输出（stdout）直接作为下一个进程的输入（stdin）。 管道又分为匿名管道和具名管道（这里将不会讨论在源程序中使用系统调用创建并使用管道的情况，它与命令行的管道在内核中实际都是采用相同的机制）。我们在使用一些过滤程序时经常会用到的就是匿名管道，在命令行中由 | 分隔符表示，| 在前面的内容中我们已经多次使用到了。具名管道简单的说就是有名字的管道，通常只会在源程序中用到具名管道。 1）管道的含义适用| 先试用一下管道，比如查看 /etc 目录下有哪些文件和目录，使用 ls 命令来查看： ls -al /etc 有太多内容，屏幕不能完全显示，这时候可以使用滚动条或快捷键滚动窗口来查看。不过这时候可以使用管道： ls -al /etc | less 通过管道将前一个命令(ls)的输出作为下一个命令(less)的输入，然后就可以一行一行地看。 2）cut 命令，打印每一行的某一字段 打印 /etc/passwd 文件中以 : 为分隔符的第 1 个字段和第 6 个字段分别表示用户名和其家目录： cut /etc/passwd -d ':' -f 1,6 打印 /etc/passwd 文件中每一行的前 N 个字符： # 前五个（包含第五个） cut /etc/passwd -c -5 # 前五个之后的（包含第五个） cut /etc/passwd -c 5- # 第五个 cut /etc/passwd -c 5 # 2 到 5 之间的（包含第五个） cut /etc/passwd -c 2-5 3）grep 命令，在文本中或 stdin 中查找匹配字符串 grep 命令是很强大的，也是相当常用的一个命令，它结合正则表达式可以实现很复杂却很高效的匹配和查找，不过在学习正则表达式之前，这里介绍它简单的使用，而关于正则表达式后面将会有单独一小节介绍到时会再继续学习 grep 命令和其他一些命令。 grep 命令的一般形式为： grep [命令选项]... 用于匹配的表达式 [文件]... 还是先体验一下，我们搜索/home/shiyanlou目录下所有包含\"shiyanlou\"的文本文件，并显示出现在文本中的行号： grep -rnI \"shiyanlou\" ~ -r 参数表示递归搜索子目录中的文件，-n 表示打印匹配项行号，-I 表示忽略二进制文件。这个操作实际没有多大意义，但可以感受到 grep 命令的强大与实用。 当然也可以在匹配字段中使用正则表达式，下面简单的演示： # 查看环境变量中以 \"yanlou\" 结尾的字符串 export | grep \".*yanlou$\" 其中$就表示一行的末尾。 4）wc 命令，简单小巧的计数工具 wc 命令用于统计并输出一个文件中行、单词和字节的数目，比如输出 /etc/passwd 文件的统计信息： wc /etc/passwd 分别只输出行数、单词数、字节数、字符数和输入文本中最长一行的字节数： # 行数 wc -l /etc/passwd # 单词数 wc -w /etc/passwd # 字节数 wc -c /etc/passwd # 字符数 wc -m /etc/passwd # 最长行字节数 wc -L /etc/passwd 注意：对于西文字符来说，一个字符就是一个字节，但对于中文字符一个汉字是大于 或等于2 个字节的，具体数目是由字符编码决定的。 再来结合管道来操作一下，下面统计 /etc 下面所有目录数： ls -dl /etc/*/ | wc -l 5）sort 排序命令 这个命令前面我们也是用过多次，功能很简单就是将输入按照一定方式排序，然后再输出，它支持的排序有按字典排序，数字排序，按月份排序，随机排序，反转排序，指定特定字段进行排序等等。 默认为字典排序： cat /etc/passwd | sort 反转排序： cat /etc/passwd | sort -r 按特定字段排序： cat /etc/passwd | sort -t':' -k 3 上面的-t参数用于指定字段的分隔符，这里是以”:“作为分隔符；-k 字段号用于指定对哪一个字段进行排序。这里/etc/passwd文件的第三个字段为数字，默认情况下是以字典序排序的，如果要按照数字排序就要加上-n参数： cat /etc/passwd | sort -t':' -k 3 -n 注意观察第二个冒号后的数字： 6）uniq 去重命令 uniq 命令可以用于过滤或者输出重复行。 过滤重复行 我们可以使用 history 命令查看最近执行过的命令（实际为读取 ${SHELL}_history 文件，如我们环境中的 .zsh_history 文件），不过你可能只想查看使用了哪个命令而不需要知道具体干了什么，那么你可能就会要想去掉命令后面的参数然后去掉重复的命令： history | cut -c 8- | cut -d ' ' -f 1 | uniq 然后经过层层过滤，你会发现确是只输出了执行的命令那一列，不过去重效果好像不明显，仔细看你会发现它确实去重了，只是不那么明显，之所以不明显是因为 uniq 命令只能去连续重复的行，不是全文去重，所以要达到预期效果，我们先排序： history | cut -c 8- | cut -d ' ' -f 1 | sort | uniq # 或者 history | cut -c 8- | cut -d ' ' -f 1 | sort -u 这就是 Linux/UNIX 哲学吸引人的地方，大繁至简，一个命令只干一件事却能干到最好。 输出重复行 # 输出重复过的行（重复的只输出一个）及重复次数 history | cut -c 8- | cut -d ' ' -f 1 | sort | uniq -dc # 输出所有重复的行 history | cut -c 8- | cut -d ' ' -f 1 | sort | uniq -D 文本处理命令还有很多，下一节将继续介绍一些常用的文本处理的命令。 ","date":"2023-01-10","objectID":"/linux%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/:11:3","tags":[],"title":"Linux常用命令","uri":"/linux%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/"},{"categories":["linux"],"content":"十一、简单的文本处理 ","date":"2023-01-10","objectID":"/linux%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/:12:0","tags":[],"title":"Linux常用命令","uri":"/linux%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/"},{"categories":["linux"],"content":"1. tr命令 tr 命令可以用来删除一段文本信息中的某些文字。或者将其进行转换。 使用方式 tr [option]...SET1 [SET2] 常用的选项有 选项 说明 -d 删除和 set1 匹配的字符，注意不是全词匹配也不是按字符顺序匹配 -s 去除 set1 指定的在输入文本中连续并重复的字符 操作举例 # 删除 \"hello shiyanlou\" 中所有的'o'，'l'，'h' $ echo 'hello shiyanlou' | tr -d 'olh' # 将\"hello\" 中的ll，去重为一个l $ echo 'hello' | tr -s 'l' # 将输入文本，全部转换为大写或小写输出（正则表达式） $ echo 'input some text here' | tr '[:lower:]' '[:upper:]' # 上面的'[:lower:]' '[:upper:]'你也可以简单的写作'[a-z]' '[A-Z]'，当然反过来将大写变小写也是可以的 更多 tr 的使用，你可以使用--help或者man tr获得。 ","date":"2023-01-10","objectID":"/linux%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/:12:1","tags":[],"title":"Linux常用命令","uri":"/linux%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/"},{"categories":["linux"],"content":"2. col命令 col 命令可以将Tab换成对等数量的空格键，或反转这个操作。 使用方式 col [option] 常用的选项有 选项 说明 -x 将Tab转换为空格 -h 将空格转换为Tab（默认选项） 操作举例 # 查看 /etc/protocols 中的不可见字符，可以看到很多 ^I ，这其实就是 Tab 转义成可见字符的符号 cat -A /etc/protocols # 使用 col -x 将 /etc/protocols 中的 Tab 转换为空格，然后再使用 cat 查看，你发现 ^I 不见了 cat /etc/protocols | col -x | cat -A ","date":"2023-01-10","objectID":"/linux%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/:12:2","tags":[],"title":"Linux常用命令","uri":"/linux%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/"},{"categories":["linux"],"content":"3. join命令 这个命令就是用于将两个文件中包含相同内容的那一行合并在一起。 使用方式 join [option]... file1 file2 常用的选项有 选项 说明 -t 指定分隔符，默认为空格 -i 忽略大小写的差异 -1 指明第一个文件要用哪个字段来对比，默认对比第一个字段 -2 指明第二个文件要用哪个字段来对比，默认对比第一个字段 操作举例 cd /home/shiyanlou # 创建两个文件 echo '1 hello' \u003e file1 echo '1 shiyanlou' \u003e file2 join file1 file2 # 将 /etc/passwd 与 /etc/shadow 两个文件合并，指定以':'作为分隔符 sudo join -t':' /etc/passwd /etc/shadow # 将 /etc/passwd 与 /etc/group 两个文件合并，指定以':'作为分隔符，分别比对第4和第3个字段 sudo join -t':' -1 4 /etc/passwd -2 3 /etc/group ","date":"2023-01-10","objectID":"/linux%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/:12:3","tags":[],"title":"Linux常用命令","uri":"/linux%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/"},{"categories":["linux"],"content":"4. paste命令 paste这个命令与join 命令类似，它是在不对比数据的情况下，简单地将多个文件合并一起，以Tab隔开。 使用方式 paste [option] file... 常用的选项有 选项 说明 -d 指定合并的分隔符，默认为 Tab -s 不合并到一行，每个文件为一行 操作举例 echo hello \u003e file1 echo shiyanlou \u003e file2 echo www.shiyanlou.com \u003e file3 paste -d ':' file1 file2 file3 paste -s file1 file2 file3 ","date":"2023-01-10","objectID":"/linux%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/:12:4","tags":[],"title":"Linux常用命令","uri":"/linux%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/"},{"categories":["linux"],"content":"十二、数据流重定向 重定向操作： echo 'hello shiyanlou' \u003e redirect echo 'www.shiyanlou.com' \u003e\u003e redirect cat redirect 当然前面没有用到的 \u003c 和 \u003c\u003c 操作也是没有问题的，如你理解的一样，它们的区别在于重定向的方向不一致而已，\u003e 表示是从左到右，\u003c 右到左。 Linux 默认提供了三个特殊设备，用于终端的显示和输出，分别为 stdin（标准输入，对应于你在终端的输入），stdout（标准输出，对应于终端的输出），stderr（标准错误输出，对应于终端的输出）。 文件描述符 设备文件 说明 0 /dev/stdin 标准输入 1 /dev/stdout 标准输出 2 /dev/stderr 标准错误 文件描述符：文件描述符在形式上是一个非负整数。实际上，它是一个索引值，指向内核为每一个进程所维护的该进程打开文件的记录表。当程序打开一个现有文件或者创建一个新文件时，内核向进程返回一个文件描述符。在程序设计中，一些涉及底层的程序编写往往会围绕着文件描述符展开。但是文件描述符这一概念往往只适用于 UNIX、Linux 这样的操作系统。 ","date":"2023-01-10","objectID":"/linux%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/:13:0","tags":[],"title":"Linux常用命令","uri":"/linux%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/"},{"categories":["linux"],"content":"1. 重定向与管道区别 我们可以这样使用这些文件描述符。例如默认使用终端的标准输入作为命令的输入和标准输出作为命令的输出： cat # 按 Ctrl+C 退出 将 cat 的连续输出（heredoc 方式）重定向到一个文件： mkdir Documents cat \u003e Documents/test.c \u003c\u003cEOF #include \u003cstdio.h\u003e int main() { printf(\"hello world\\n\"); return 0; } EOF 将一个文件作为命令的输入，标准输出作为命令的输出： cat Documents/test.c 将 echo 命令通过管道传过来的数据作为 cat 命令的输入，将标准输出作为命令的输出： echo 'hi' | cat 将 echo 命令的输出从默认的标准输出重定向到一个普通文件： echo 'hello shiyanlou' \u003e redirect cat redirect 初学者这里要注意不要将管道和重定向混淆，管道默认是连接前一个命令的输出到下一个命令的输入，而重定向通常是需要一个文件来建立两个命令的连接，你可以仔细体会一下上述第三个操作和最后两个操作的异同点。 ","date":"2023-01-10","objectID":"/linux%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/:13:1","tags":[],"title":"Linux常用命令","uri":"/linux%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/"},{"categories":["linux"],"content":"2. 标准错误重定向 重定向标准输出到文件，这是一个很实用的操作，另一个很实用的操作是将标准错误重定向，标准输出和标准错误都被指向伪终端的屏幕显示，所以我们经常看到的一个命令的输出通常是同时包含了标准输出和标准错误的结果的。比如下面的操作： # 使用cat 命令同时读取两个文件，其中一个存在，另一个不存在 cat Documents/test.c hello.c # 你可以看到除了正确输出了前一个文件的内容，还在末尾出现了一条错误信息 # 下面我们将输出重定向到一个文件 cat Documents/test.c hello.c \u003e somefile 遗憾的是，这里依然出现了那条错误信息，这正是因为如我上面说的那样，标准输出和标准错误虽然都指向终端屏幕，实际它们并不一样。那有的时候我们就是要隐藏某些错误或者警告，那又该怎么做呢。这就需要用到我们前面讲的文件描述符了： # 将标准错误重定向到标准输出，再将标准输出重定向到文件，注意要将重定向到文件写到前面 cat Documents/test.c hello.c \u003esomefile 2\u003e\u00261 # 或者只用bash提供的特殊的重定向符号\"\u0026\"将标准错误和标准输出同时重定向到文件 cat Documents/test.c hello.c \u0026\u003esomefilehell 注意你应该在输出重定向文件描述符前加上\u0026，否则 shell 会当做重定向到一个文件名为 1 的文件中 ","date":"2023-01-10","objectID":"/linux%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/:13:2","tags":[],"title":"Linux常用命令","uri":"/linux%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/"},{"categories":["linux"],"content":"3. 使用 tee 命令同时重定向到多个文件 你可能还有这样的需求，除了需要将输出重定向到文件，也需要将信息打印在终端。那么你可以使用 tee 命令来实现： echo 'hello shiyanlou' | tee hello ","date":"2023-01-10","objectID":"/linux%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/:13:3","tags":[],"title":"Linux常用命令","uri":"/linux%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/"},{"categories":["linux"],"content":"4. 永久重定向 当然不需要，我们可以使用 exec 命令实现永久重定向。exec 命令的作用是使用指定的命令替换当前的 Shell，即使用一个进程替换当前进程，或者指定新的重定向： # 先开启一个子 Shell zsh # 使用exec替换当前进程的重定向，将标准输出重定向到一个文件 exec 1\u003esomefile # 后面你执行的命令的输出都将被重定向到文件中，直到你退出当前子shell，或取消exec的重定向（后面将告诉你怎么做） ls exit cat somefile ","date":"2023-01-10","objectID":"/linux%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/:13:4","tags":[],"title":"Linux常用命令","uri":"/linux%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/"},{"categories":["linux"],"content":"十三、软件包管理（centos、乌班图） 包的组成： 二进制文件、库文件、配置文件、帮助文件 程序包管理器： debian： deb文件, dpkg包管理器 redhat： rpm文件, rpm包管理器 rpm：Redhat Package Manager RPM Package Manager 包之间：可能存在依赖关系，甚至循环依赖 解决依赖包管理工具： yum：rpm包管理器的前端工具 apt-get：deb包管理器前端工具 zypper: suse上的rpm前端管理工具 dnf: Fedora 18+ rpm包管理器前端管理工具 查看二进制程序所依赖的库文件： ldd /PATH/TO/BINARY_FILE 管理及查看本机装载的库文件： ldconfig :加载配置文件中指定的库文件 /sbin/ldconfig -p :显示本机已经缓存的所有可用库文件名及文件路径映射关系 配置文件: /etc/ld.so.conf /etc/ld.so.conf.d/*.conf 缓存文件：/etc/ld.so.cache 程序包管理器： 功能：将编译好的应用程序的各组成文件打包一个或几个程序包文件，从而方便快捷地实现程序包的安装、卸载、查询、升级和校验等管理操作 数据库(公共)：/var/lib/rpm 程序包名称及版本 依赖关系 功能说明 包安装后生成的各文件路径及校验码信息 获取程序包的途径： (1) 系统发版的光盘或官方的服务器 CentOS镜像： https://www.centos.org/download/ http://mirrors.aliyun.com http://mirrors.sohu.com http://mirrors.163.com (2) 项目官方站点 (3) 第三方组织： Fedora-EPEL： Extra Packages for Enterprise Linux Rpmforge:RHEL推荐，包很全 搜索引擎： http://pkgs.org http://rpmfind.net http://rpm.pbone.net https://sourceforge.net/ (4) 自己制作 注意：第三方包建议要检查其合法性来源合法性,程序包的完整性 CentOS系统上使用rpm命令管理程序包： 安装、卸载、升级、查询、校验、数据库维护 安装： rpm {-i|--install} [install-options] PACKAGE_FILE… -v: verbose -vv: -h: 以#显示程序包管理执行进度 rpm -ivh PACKAGE_FILE ... [install-options] --test: 测试安装，但不真正执行安装，即dry run模式 --nodeps：忽略依赖关系 --replacepkgs | replacefiles --nosignature: 不检查来源合法性 --nodigest：不检查包完整性 --noscripts：不执行程序包脚本 %pre: 安装前脚本 --nopre %post: 安装后脚本 --nopost %preun: 卸载前脚本 --nopreun %postun: 卸载后脚本 --nopostun 升级： rpm {-U|--upgrade} [install-options] PACKAGE_FILE... rpm {-F|--freshen} [install-options] PACKAGE_FILE... upgrade：安装有旧版程序包，则“升级” 如果不存在旧版程序包，则“安装” freshen：安装有旧版程序包，则“升级” 如果不存在旧版程序包，则不执行 rpm -Uvh PACKAGE_FILE ... rpm -Fvh PACKAGE_FILE ... --oldpackage：降级 --force: 强制安装 注意： (1) 不要对内核做升级操作；Linux支持多内核版本并存，因此，对直接安装新版本内核 (2) 如果原程序包的配置文件安装后曾被修改，升级时，新版本的提供的同一个配置文件并不会直接覆盖老版本的配置文件，而把新版本的文件重命名(FILENAME.rpmnew)后保留 包查询： rpm {-q|--query} [select-options] [query-options] [select-options] -a: 所有包 -f: 查看指定的文件由哪个程序包安装生成 -p rpmfile：针对尚未安装的程序包文件做查询操作 --whatprovides CAPABILITY：查询指定的CAPABILITY由哪个包所提供 --whatrequires CAPABILITY：查询指定的CAPABILITY被哪个包所依赖 常用查询用法： -qi PACKAGE, -qf FILE, -qc PACKAGE, -ql PACKAGE, -qd PACKAGE -qpi PACKAGE_FILE, -qpl PACKAGE_FILE, ... -qa rpm2cpio 包文件|cpio –itv 预览包内文件 rpm2cpio 包文件|cpio –id “*.conf” 释放包内文件 包校验： rpm {-V|--verify} [select-options] [verify-options] S file Size differs M Mode differs (includes permissions and file type) 5 digest (formerly MD5 sum) differs D Device major/minor number mismatch L readLink(2) path mismatch U User ownership differs G Group ownership differs T mTime differs P capabilities differ 包来源合法性验正及完整性验证 完整性验证：SHA256 来源合法性验证：RSA 公钥加密 对称加密：加密、解密使用同一密钥 非对称加密：密钥是成对儿的 public key: 公钥，公开所有人 secret key: 私钥, 不能公开 导入所需要公钥 rpm -K|checksig rpmfile 检查包的完整性和签名 rpm --import /etc/pki/rpm-gpg/RPM-GPG-KEY-CentOS-7 CentOS 7发行版光盘提供：RPM-GPG-KEY-CentOS-7 rpm -qa “gpg-pubkey*” rpm数据库： 数据库重建： /var/lib/rpm rpm {--initdb|--rebuilddb} initdb: 初始化 如果事先不存在数据库，则新建之 否则，不执行任何操作 rebuilddb：重建已安装的包头的数据库索引目录 CentOS系统上使用yum命令： YUM: Yellowdog Update Modifier，rpm的前端程序，可解决软件包相关依赖性，可在多个库之间定位软件包，up2date的替代工具 yum repository: yum repo，存储了众多rpm包，以及包的相关的元数据文件（放置于特定目录repodata下） 文件服务器： http:// https:// ftp:// file:// yum配置文件： yum客户端配置文件： /etc/yum.conf：为所有仓库提供公共配置 /etc/yum.repos.d/*.repo：为仓库的指向提供配置 仓库指向的定义： [repositoryID] name=Some name for this repository baseurl=url://path/to/repository/ enabled={1|0} gpgcheck={1|0} gpgkey=URL enablegroups={1|0} failovermethod={roundrobin|priority} roundrobin：意为随机挑选，默认值 priority:按顺序访问 cost= 默认为1000 yum的repo配置文件中可用的变量： $releasever: 当前OS的发行版的主版本号 $arch: 平台，i386,i486,i586,x86_64等 $basearch：基础平台；i386, x86_64 $YUM0-$YUM9:自定义变量 yum源： 阿里云repo文件 http://mirrors.aliyun.com/repo/ CentOS系统的yum源 阿里云：https://mirrors.aliyun.com/centos/$releasever/os/x86_64/ 清华大学：https://mirrors.tuna.tsinghua.edu.cn/centos/$releaseve EPEL的yum源 阿里云：https://mirrors.aliyun.com/epel/$releasever/x86_64 阿里巴巴开源软件https://opsx.alibaba.com/ yum命令： yum的命令行选项： --nogpgcheck：禁止进行gpg check -y: 自动回答为“yes” -q：静默模式 --disablerepo=repoidglob：临时禁用此处指定的repo --enablerepo=repoidglob：临时启用此处指定的repo --noplugins：禁用所有插件 yum","date":"2023-01-10","objectID":"/linux%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/:14:0","tags":[],"title":"Linux常用命令","uri":"/linux%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/"},{"categories":["linux"],"content":"yum 命令 yum（ Yellow dog Updater, Modified）是一个在Fedora和RedHat以及SUSE中的Shell前端软件包管理器。 基於RPM包管理，能够从指定的服务器自动下载RPM包并且安装，可以自动处理依赖性关系，并且一次安装所有依赖的软体包，无须繁琐地一次次下载、安装。 yum提供了查找、安装、删除某一个、一组甚至全部软件包的命令，而且命令简洁而又好记。 列出所有可更新的软件清单命令：yum check-update 更新所有软件命令：yum update 仅安装指定的软件命令：yum install \u003cpackage_name\u003e 仅更新指定的软件命令：yum update \u003cpackage_name\u003e 列出所有可安裝的软件清单命令：yum list 删除软件包命令：yum remove \u003cpackage_name\u003e 查找软件包 命令：yum search 清除缓存命令: yum clean packages: 清除缓存目录下的软件包 yum clean headers: 清除缓存目录下的 headers yum clean oldheaders: 清除缓存目录下旧的 headers yum clean, yum clean all (= yum clean packages; yum clean oldheaders) :清除缓存目录下的软件包及旧的headers 安装 pam-devel yum install pam-devel ","date":"2023-01-10","objectID":"/linux%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/:14:1","tags":[],"title":"Linux常用命令","uri":"/linux%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/"},{"categories":["linux"],"content":"十四、进程概念 ","date":"2023-01-10","objectID":"/linux%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/:15:0","tags":[],"title":"Linux常用命令","uri":"/linux%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/"},{"categories":["linux"],"content":"1. 概念的理解 首先程序与进程是什么？程序与进程又有什么区别？ 1）程序 程序（procedure）：不太精确地说，程序就是执行一系列有逻辑、有顺序结构的指令，帮我们达成某个结果。就如我们去餐馆，给服务员说我要牛肉盖浇饭，她执行了做牛肉盖浇饭这么一个程序，最后我们得到了这么一盘牛肉盖浇饭。它需要去执行，不然它就像一本武功秘籍，放在那里等人翻看。 2）进程 进程（process）：进程是程序在一个数据集合上的一次执行过程，在早期的 UNIX、Linux 2.4 及更早的版本中，它是系统进行资源分配和调度的独立基本单位。同上一个例子，就如我们去了餐馆，给服务员说我要牛肉盖浇饭，她执行了做牛肉盖浇饭这么一个程序，而里面做饭的是一个进程，做牛肉汤汁的是一个进程，把牛肉汤汁与饭混合在一起的是一个进程，把饭端上桌的是一个进程。它就像是我们在看武功秘籍这么一个过程，然后一个篇章一个篇章地去练。 简单来说，程序是为了完成某种任务而设计的软件，比如 vim 是程序。什么是进程呢？进程就是运行中的程序。 程序只是一些列指令的集合，是一个静止的实体，而进程不同，进程有以下的特性： 动态性：进程的实质是一次程序执行的过程，有创建、撤销等状态的变化。而程序是一个静态的实体。 并发性：进程可以做到在一个时间段内，有多个程序在运行中。程序只是静态的实体，所以不存在并发性。 独立性：进程可以独立分配资源，独立接受调度，独立地运行。 异步性：进程以不可预知的速度向前推进。 结构性：进程拥有代码段、数据段、PCB（进程控制块，进程存在的唯一标志）。也正是因为有结构性，进程才可以做到独立地运行。 **并发：**在一个时间段内，宏观来看有多个程序都在活动，有条不紊的执行（每一瞬间只有一个在执行，只是在一段时间有多个程序都执行过） **并行：**在每一个瞬间，都有多个程序都在同时执行，这个必须有多个 CPU 才行 引入进程是因为传统意义上的程序已经不足以描述 OS 中各种活动之间的动态性、并发性、独立性还有相互制约性。程序就像一个公司，只是一些证书，文件的堆积（静态实体）。而当公司运作起来就有各个部门的区分，财务部，技术部，销售部等等，就像各个进程，各个部门之间可以独立运作，也可以有交互（独立性、并发性）。 而随着程序的发展越做越大，又会继续细分，从而引入了线程的概念，当代多数操作系统、Linux 2.6 及更新的版本中，进程本身不是基本运行单位，而是线程的容器。就像上述所说的，每个部门又会细分为各个工作小组（线程），而工作小组需要的资源需要向上级（进程）申请。 线程（thread）是操作系统能够进行运算调度的最小单位。它被包含在进程之中，是进程中的实际运作单位。一条线程指的是进程中一个单一顺序的控制流，一个进程中可以并发多个线程，每条线程并行执行不同的任务。因为线程中几乎不包含系统资源，所以执行更快、更有效率。 简而言之，一个程序至少有一个进程，一个进程至少有一个线程。线程的划分尺度小于进程，使得多线程程序的并发性高。另外，进程在执行过程中拥有独立的内存单元，而多个线程共享内存，从而极大地提高了程序的运行效率。就如下图所示： ","date":"2023-01-10","objectID":"/linux%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/:15:1","tags":[],"title":"Linux常用命令","uri":"/linux%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/"},{"categories":["linux"],"content":"2. 进程的分类 大概明白进程是个什么样的存在后，我们需要进一步了解的就是进程分类。可以从两个角度来分： 以进程的功能与服务的对象来分； 以应用程序的服务类型来分； 第一个角度来看，我们可以分为用户进程与系统进程： 用户进程：通过执行用户程序、应用程序或称之为内核之外的系统程序而产生的进程，此类进程可以在用户的控制下运行或关闭。 系统进程：通过执行系统内核程序而产生的进程，比如可以执行内存资源分配和进程切换等相对底层的工作；而且该进程的运行不受用户的干预，即使是 root 用户也不能干预系统进程的运行。 第二角度来看，我们可以将进程分为交互进程、批处理进程、守护进程： 交互进程：由一个 shell 终端启动的进程，在执行过程中，需要与用户进行交互操作，可以运行于前台，也可以运行在后台。 批处理进程：该进程是一个进程集合，负责按顺序启动其他的进程。 守护进程：守护进程是一直运行的一种进程，在 Linux 系统启动时启动，在系统关闭时终止。它们独立于控制终端并且周期性的执行某种任务或等待处理某些发生的事件。例如 httpd 进程，一直处于运行状态，等待用户的访问。还有经常用的 cron（在 centOS 系列为 crond）进程，这个进程为 crontab 的守护进程，可以周期性的执行用户设定的某些任务。 ","date":"2023-01-10","objectID":"/linux%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/:15:2","tags":[],"title":"Linux常用命令","uri":"/linux%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/"},{"categories":["linux"],"content":"3. 进程组与 Sessions 每一个进程都会是一个进程组的成员，而且这个进程组是唯一存在的，他们是依靠 PGID（process group ID）来区别的，而每当一个进程被创建的时候，它便会成为其父进程所在组中的一员。 一般情况，进程组的 PGID 等同于进程组的第一个成员的 PID，并且这样的进程称为该进程组的领导者，也就是领导进程，进程一般通过使用 getpgrp() 系统调用来寻找其所在组的 PGID，领导进程可以先终结，此时进程组依然存在，并持有相同的 PGID，直到进程组中最后一个进程终结。 与进程组类似，每当一个进程被创建的时候，它便会成为其父进程所在 Session 中的一员，每一个进程组都会在一个 Session 中，并且这个 Session 是唯一存在的， Session 主要是针对一个 tty 建立，Session 中的每个进程都称为一个工作(job)。每个会话可以连接一个终端(control terminal)。当控制终端有输入输出时，都传递给该会话的前台进程组。Session 意义在于将多个 jobs 囊括在一个终端，并取其中的一个 job 作为前台，来直接接收该终端的输入输出以及终端信号。 其他 jobs 在后台运行。 前台（foreground）就是在终端中运行，能与你有交互的 后台（background）就是在终端中运行，但是你并不能与其任何的交互，也不会显示其执行的过程 ","date":"2023-01-10","objectID":"/linux%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/:15:3","tags":[],"title":"Linux常用命令","uri":"/linux%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/"},{"categories":["linux"],"content":"4. 工作管理 bash(Bourne-Again shell)支持工作控制（job control），而 sh（Bourne shell）并不支持。 并且每个终端或者说 bash 只能管理当前终端中的 job，不能管理其他终端中的 job。比如我当前存在两个 bash 分别为 bash1、bash2，bash1 只能管理其自己里面的 job 并不能管理 bash2 里面的 job 我们都知道当一个进程在前台运作时我们可以用 ctrl + c 来终止它，但是若是在后台的话就不行了。 1）\u0026命令 我们可以通过 \u0026 这个符号，让我们的命令在后台中运行： ls \u0026 图中所显示的 [1] 236分别是该 job 的 job number 与该进程的 PID，而最后一行的 Done 表示该命令已经在后台执行完毕。 2）ctrl + z 命令 我们还可以通过 ctrl + z 使我们的当前工作停止并丢到后台中去 被停止并放置在后台的工作我们可以使用这个命令来查看： jobs 其中第一列显示的为被放置后台 job 的编号，而第二列的 ＋ 表示最近(刚刚、最后)被放置后台的 job，同时也表示预设的工作，也就是若是有什么针对后台 job 的操作，首先对预设的 job，- 表示倒数第二（也就是在预设之前的一个）被放置后台的工作，倒数第三个（再之前的）以后都不会有这样的符号修饰，第三列表示它们的状态，而最后一列表示该进程执行的命令。 我们可以通过这样的一个命令将后台的工作拿到前台来： # 后面不加参数提取预设工作，加参数提取指定工作的编号 # ubuntu 在 zsh 中需要 %，在 bash 中不需要 % fg [%jobnumber] 之前我们通过 ctrl + z 使得工作停止放置在后台，若是我们想让其在后台运作我们就使用这样一个命令： #与fg类似，加参则指定，不加参则取预设 bg [%jobnumber] 3）kill命令 发送指定的信号到相应进程。不指定型号将发送SIGTERM（15）终止指定进程。如果仍无法终止该程序可用”-KILL\" 参数，其发送的信号为SIGKILL(9) ，将强制结束进程，使用ps命令或者jobs 命令可以查看进程号。root用户将影响用户的进程，非root用户只能影响自己的进程。 常用参数： -l 信号，若果不加信号的编号参数，则使用“-l”参数会列出全部的信号名称 -a 当处理当前进程时，不限制命令名和进程号的对应关系 -p 指定kill 命令只打印相关进程的进程号，而不发送任何信号 -s 指定发送信号 -u 指定用户 （1）先使用ps查找进程pro1，然后用kill杀掉 kill -9 $(ps -ef | grep pro1) 4）ps 命令 ps 也是我们最常用的查看进程的工具之一，我们通过这样的一个命令来了解一下，它能给我们带来哪些信息： ps -aux ps axjf 我们来总体了解下会出现哪些信息给我们，这些信息又代表着什么（更多的 keywords 大家可以通过 man ps 了解）。 内容 解释 F 进程的标志（process flags），当 flags 值为 1 则表示此子程序只是 fork 但没有执行 exec，为 4 表示此程序使用超级管理员 root 权限 USER 进程的拥有用户 PID 进程的 ID PPID 其父进程的 PID SID session 的 ID TPGID 前台进程组的 ID %CPU 进程占用的 CPU 百分比 %MEM 占用内存的百分比 NI 进程的 NICE 值 VSZ 进程使用虚拟内存大小 RSS 驻留内存中页的大小 TTY 终端 ID S or STAT 进程状态 WCHAN 正在等待的进程资源 START 启动进程的时间 TIME 进程消耗 CPU 的时间 COMMAND 命令的名称和参数 TPGID栏写着-1 的都是没有控制终端的进程，也就是守护进程 STAT表示进程的状态，而进程的状态有很多，如下表所示 状态 解释 R Running.运行中 S Interruptible Sleep.等待调用 D Uninterruptible Sleep.不可中断睡眠 T Stoped.暂停或者跟踪状态 X Dead.即将被撤销 Z Zombie.僵尸进程 W Paging.内存交换 N 优先级低的进程 \u003c 优先级高的进程 s 进程的领导者 L 锁定状态 l 多线程状态 + 前台进程 其中的 D 是不能被中断睡眠的状态，处在这种状态的进程不接受外来的任何 signal，所以无法使用 kill 命令杀掉处于 D 状态的进程，无论是 kill，kill -9 还是 kill -15，一般处于这种状态可能是进程 I/O 的时候出问题了。 ps 工具有许多的参数，下面给大家解释部分常用的参数。 使用 -l 参数可以显示自己这次登录的 bash 相关的进程信息罗列出来： ps -l 相对来说我们更加常用下面这个命令，他将会罗列出所有的进程信息： ps -aux 若是查找其中的某个进程的话，我们还可以配合着 grep 和正则表达式一起使用： ps -aux | grep zsh 此外我们还可以查看时，将连同部分的进程呈树状显示出来： ps axjf 当然如果你觉得使用这样的此时没有把你想要的信息放在一起，我们也可以是用这样的命令，来自定义我们所需要的参数显示： ps -afxo user,ppid,pid,pgid,command 这是一个简单而又实用的工具，想要更灵活的使用，想要知道更多的参数我们可以使用 man 来获取更多相关的信息。 ps(process status)，用来查看当前运行的进程状态，一次性查看，如果需要动态连续结果使用 top linux上进程有5种状态: 运行(正在运行或在运行队列中等待) 中断(休眠中, 受阻, 在等待某个条件的形成或接受到信号) 不可中断(收到信号不唤醒和不可运行, 进程必须等待直到有中断发生) 僵死(进程已终止, 但进程描述符存在, 直到父进程调用wait4()系统调用后释放) 停止(进程收到SIGSTOP, SIGSTP, SIGTIN, SIGTOU信号后停止运行运行) ps 工具标识进程的5种状态码: D 不可中断 uninterruptible sleep (usually IO) R 运行 runnable (on run queue) S 中断 sleeping T 停止 traced or stopped Z 僵死 a defunct (”zombie”) process 命令参数： -A 显示所有进程 a 显示所有进程 -a 显示同一终端下所有进程 c 显示进程真实名称 e 显示环境变量 f 显示进程间的关系 r 显示当前终端运行的进程 -aux 显示所有包含其它使用的进程 （1）显示当前所有进程环境变量及进程间关系 ps -ef （2）显示当前所有进程 ps -A （3）与grep联用查找某进程 ps -aux | grep apache （4）找出与 cron 与 syslog 这两个服务有关的 PID 号码 ps aux | grep '(cron|syslog)' ","date":"2023-01-10","objectID":"/linux%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/:15:4","tags":[],"title":"Linux常用命令","uri":"/linux%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/"},{"categories":["linux"],"content":"十五、网络通讯命令 ","date":"2023-01-10","objectID":"/linux%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/:16:0","tags":[],"title":"Linux常用命令","uri":"/linux%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/"},{"categories":["linux"],"content":"1. ifconfig 命令 ifconfig 用于查看和配置 Linux 系统的网络接口。 查看所有网络接口及其状态：ifconfig -a 。 使用 up 和 down 命令启动或停止某个接口：ifconfig eth0 up 和 ifconfig eth0 down 。 ","date":"2023-01-10","objectID":"/linux%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/:16:1","tags":[],"title":"Linux常用命令","uri":"/linux%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/"},{"categories":["linux"],"content":"2. iptables 命令 iptables ，是一个配置 Linux 内核防火墙的命令行工具。功能非常强大，对于我们开发来说，主要掌握如何开放端口即可。例如： 把来源 IP 为 192.168.1.101 访问本机 80 端口的包直接拒绝：iptables -I INPUT -s 192.168.1.101 -p tcp –dport 80 -j REJECT 。 开启 80 端口，因为web对外都是这个端口 iptables -A INPUT -p tcp --dport 80 -j ACCEP 另外，要注意使用 iptables save 命令，进行保存。否则，服务器重启后，配置的规则将丢失。 ","date":"2023-01-10","objectID":"/linux%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/:16:2","tags":[],"title":"Linux常用命令","uri":"/linux%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/"},{"categories":["linux"],"content":"3. netstat 命令 Linux netstat命令用于显示网络状态。 利用netstat指令可让你得知整个Linux系统的网络情况。 语法 netstat [-acCeFghilMnNoprstuvVwx][-A\u003c网络类型\u003e][--ip] 参数说明： -a或–all 显示所有连线中的Socket。 -A\u003c网络类型\u003e或–\u003c网络类型\u003e 列出该网络类型连线中的相关地址。 -c或–continuous 持续列出网络状态。 -C或–cache 显示路由器配置的快取信息。 -e或–extend 显示网络其他相关信息。 -F或–fib 显示FIB。 -g或–groups 显示多重广播功能群组组员名单。 -h或–help 在线帮助。 -i或–interfaces 显示网络界面信息表单。 -l或–listening 显示监控中的服务器的Socket。 -M或–masquerade 显示伪装的网络连线。 -n或–numeric 直接使用IP地址，而不通过域名服务器。 -N或–netlink或–symbolic 显示网络硬件外围设备的符号连接名称。 -o或–timers 显示计时器。 -p或–programs 显示正在使用Socket的程序识别码和程序名称。 -r或–route 显示Routing Table。 -s或–statistice 显示网络工作信息统计表。 -t或–tcp 显示TCP传输协议的连线状况。 -u或–udp 显示UDP传输协议的连线状况。 -v或–verbose 显示指令执行过程。 -V或–version 显示版本信息。 -w或–raw 显示RAW传输协议的连线状况。 -x或–unix 此参数的效果和指定\"-A unix\"参数相同。 –ip或–inet 此参数的效果和指定\"-A inet\"参数相同。 如何查看系统都开启了哪些端口？ [root@centos6 ~ 13:20 #55]# netstat -lnp Active Internet connections (only servers) Proto Recv-Q Send-Q Local Address Foreign Address State PID/Program name tcp 0 0 0.0.0.0:22 0.0.0.0:* LISTEN 1035/sshd tcp 0 0 :::22 :::* LISTEN 1035/sshd udp 0 0 0.0.0.0:68 0.0.0.0:* 931/dhclient Active UNIX domain sockets (only servers) Proto RefCnt Flags Type State I-Node PID/Program name Path unix 2 [ ACC ] STREAM LISTENING 6825 1/init @/com/ubuntu/upstart unix 2 [ ACC ] STREAM LISTENING 8429 1003/dbus-daemon /var/run/dbus/system_bus_socket 如何查看网络连接状况？ [root@centos6 ~ 13:22 #58]# netstat -an Active Internet connections (servers and established) Proto Recv-Q Send-Q Local Address Foreign Address State tcp 0 0 0.0.0.0:22 0.0.0.0:* LISTEN tcp 0 0 192.168.147.130:22 192.168.147.1:23893 ESTABLISHED tcp 0 0 :::22 :::* LISTEN udp 0 0 0.0.0.0:68 0.0.0.0:* 如何统计系统当前进程连接数？ 输入命令 netstat -an | grep ESTABLISHED | wc -l 。 输出结果 177 。一共有 177 连接数。 用 netstat 命令配合其他命令，按照源 IP 统计所有到 80 端口的 ESTABLISHED 状态链接的个数？ 严格来说，这个题目考验的是对 awk 的使用。 首先，使用 netstat -an|grep ESTABLISHED 命令。结果如下： tcp 0 0 120.27.146.122:80 113.65.18.33:62721 ESTABLISHED tcp 0 0 120.27.146.122:80 27.43.83.115:47148 ESTABLISHED tcp 0 0 120.27.146.122:58838 106.39.162.96:443 ESTABLISHED tcp 0 0 120.27.146.122:52304 203.208.40.121:443 ESTABLISHED tcp 0 0 120.27.146.122:33194 203.208.40.122:443 ESTABLISHED tcp 0 0 120.27.146.122:53758 101.37.183.144:443 ESTABLISHED tcp 0 0 120.27.146.122:27017 23.105.193.30:50556 ESTABLISHED ","date":"2023-01-10","objectID":"/linux%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/:16:3","tags":[],"title":"Linux常用命令","uri":"/linux%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/"},{"categories":["linux"],"content":"4. ping 命令 Linux ping命令用于检测主机。 执行ping指令会使用ICMP传输协议，发出要求回应的信息，若远端主机的网络功能没有问题，就会回应该信息，因而得知该主机运作正常。 指定接收包的次数 ping -c 2 www.baidu.com ","date":"2023-01-10","objectID":"/linux%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/:16:4","tags":[],"title":"Linux常用命令","uri":"/linux%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/"},{"categories":["linux"],"content":"5. telnet 命令 Linux telnet命令用于远端登入。 执行telnet指令开启终端机阶段作业，并登入远端主机。 语法 telnet [-8acdEfFKLrx][-b\u003c主机别名\u003e][-e\u003c脱离字符\u003e][-k\u003c域名\u003e][-l\u003c用户名称\u003e][-n\u003c记录文件\u003e][-S\u003c服务类型\u003e][-X\u003c认证形态\u003e][主机名称或IP地址\u003c通信端口\u003e] 参数说明： -8 允许使用8位字符资料，包括输入与输出。 -a 尝试自动登入远端系统。 -b\u003c主机别名\u003e 使用别名指定远端主机名称。 -c 不读取用户专属目录里的.telnetrc文件。 -d 启动排错模式。 -e\u003c脱离字符\u003e 设置脱离字符。 -E 滤除脱离字符。 -f 此参数的效果和指定\"-F\"参数相同。 -F 使用Kerberos V5认证时，加上此参数可把本地主机的认证数据上传到远端主机。 -k\u003c域名\u003e 使用Kerberos认证时，加上此参数让远端主机采用指定的领域名，而非该主机的域名。 -K 不自动登入远端主机。 -l\u003c用户名称\u003e 指定要登入远端主机的用户名称。 -L 允许输出8位字符资料。 -n\u003c记录文件\u003e 指定文件记录相关信息。 -r 使用类似rlogin指令的用户界面。 -S\u003c服务类型\u003e 设置telnet连线所需的IP TOS信息。 -x 假设主机有支持数据加密的功能，就使用它。 -X\u003c认证形态\u003e 关闭指定的认证形态。 登录远程主机 # 登录IP为 192.168.0.5 的远程主机 telnet 192.168.0.5 ","date":"2023-01-10","objectID":"/linux%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/:16:5","tags":[],"title":"Linux常用命令","uri":"/linux%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/"},{"categories":["汇编"],"content":"[toc] 更灵活的定位内存地址的方法 ","date":"2023-01-09","objectID":"/%E6%B1%87%E7%BC%96%E4%B9%8B%E5%85%B6%E4%BB%96%E5%AE%9A%E4%BD%8D%E5%9C%B0%E5%9D%80%E6%96%B9%E6%B3%95/:0:0","tags":[],"title":"汇编之其他定位地址方法","uri":"/%E6%B1%87%E7%BC%96%E4%B9%8B%E5%85%B6%E4%BB%96%E5%AE%9A%E4%BD%8D%E5%9C%B0%E5%9D%80%E6%96%B9%E6%B3%95/"},{"categories":["汇编"],"content":"1、and 和 or and指令：逻辑与指令，按位进行与运算。 mov al, 01100011B and al, 00111011B 执行后：al=00100011B即都为1才为1 or指令：逻辑或指令，按位进行或运算。 mov al, 01100011B or al, 00111011B 执行后：al=01111011B 即只要有一个为1就为1 关于ASCII码 世界上有很多编码方案，有一种方案叫做ASCII编码，是在计算机系统中通常被采用的。简单地说，所谓编码方案，就是一套规则，它约定了用什么样的信息来表示现实对象。比如说，在ASCII编码方案中，用61H表示“a”，62H表示“b”。一种规则需要人们遵守才有意义。 在文本编辑过程中，我们按一下键盘的a键，就会在屏幕上看到“a”。我们按下键盘的a键，这个按键的信息被送入计算机，计算机用ASCII码的规则对其进行编码，将其转化为61H存储在内存的指定空间中；文本编辑软件从内存中取出61H，将其送到显卡上的显存中；工作在文本模式下的显卡，用ASCII码的规则解释显存中的内容， 61H被当作字符“a”，显卡驱动显示器，将字符“a”的图像画在屏幕上。我们可以看到，显卡在处理文本信息的时候，是按照ASCII码的规则进行的。这也就是说，如果我们要想在显示器上看到“a”，就要给显卡提供“a”的ASCIⅡ码，61H。如何提供？当然是写入显存中。 以字符形式给出的数据 assume cs:code,ds:data data segment db 'unIx' ;相当于“db 75H，6EH，49H，58H” db 'foRK' data ends code segment start: mov al, 'a' ;相当于“mov al, 61H”，“a”的ASCI码为61H； mov b1, 'b' mov ax, 4c00h int 21h code ends end start 大小写转换的问题 小写字母的ASCII码值比大写字母的ASCII码值大20H 大写字母ASCII码的第5位为0，小写字母的第5位为1(其他一致) assume cs:codesg,ds:datasg datasg segment db 'BaSiC' db 'iNfOrMaTion' datasg end codesg segment start: mov ax, datasg mov ds, ax ;设置ds 指向 datasg段 mov bx, 0 ;设置（bx）=0，ds:bx指向’BaSic’的第一个字母 mov cx, 5 ;设置循环次数5，因为’Basic'有5个字母 s: mov al, [bx] ;将ASCII码从ds:bx所指向的单元中取出 and al, 11011111B;将al中的ASCII码的第5位置为0，变为大写字母 mov [bx], al ;将转变后的ASCII码写回原单元 inc bx ;（bx）加1，ds:bx指向下一个字母 loop s mov bx, 5 ;设置（bx）=5，ds:bx指向，iNfOrMaTion'的第一个字母 mov cx, 11 ;设置循环次数11，因为‘iNfOrMaTion'有11个字母 s0: mov al, [bx] or al, 00100000B;将a1中的ASCII码的第5位置为1，变为小写字母 mov [bx], al inc bx loop s0 mov ax, 4c00h int 21h codesg ends ","date":"2023-01-09","objectID":"/%E6%B1%87%E7%BC%96%E4%B9%8B%E5%85%B6%E4%BB%96%E5%AE%9A%E4%BD%8D%E5%9C%B0%E5%9D%80%E6%96%B9%E6%B3%95/:1:0","tags":[],"title":"汇编之其他定位地址方法","uri":"/%E6%B1%87%E7%BC%96%E4%B9%8B%E5%85%B6%E4%BB%96%E5%AE%9A%E4%BD%8D%E5%9C%B0%E5%9D%80%E6%96%B9%E6%B3%95/"},{"categories":["汇编"],"content":"2、[bx+idata] [bx+idata]表示一个内存单元, 例如：mov ax, [bx+200] 该指令也可以写成如下格式： mov ax, [200+bx] mov ax, 200[bx] mov ax, [bx].200 用[bx+idata]的方式进行数组的处理 assume cs:codesg,ds:datasg datasg segment db 'BaSiC';转为大写 db 'MinIx';转为小写 datasg ends codesg segment start: mov ax, datasg mov ds, ax mov bx, 0 ;初始ds:bx mov cx, 5 s: mov al, 0[bx] and al, 11011111b ;转为大写字母 mov 0[bx], al ;写回 mov al, 5[bx] ;[5 + bx] or al, 00100000b ;转为小写字母 mov 5[bx], al inc bx loop s mov ax, 4c00h int 21h codesg ends end start C语言描述 int main() { char a[] = \"BaSic\"; char b[] = \"MinIX\"; int i = 0; do { a[i] = a[i] \u0026 0xDF; b[i] = b[i] | 0x20; i++; } while(i \u003c 5); return 0; } ","date":"2023-01-09","objectID":"/%E6%B1%87%E7%BC%96%E4%B9%8B%E5%85%B6%E4%BB%96%E5%AE%9A%E4%BD%8D%E5%9C%B0%E5%9D%80%E6%96%B9%E6%B3%95/:2:0","tags":[],"title":"汇编之其他定位地址方法","uri":"/%E6%B1%87%E7%BC%96%E4%B9%8B%E5%85%B6%E4%BB%96%E5%AE%9A%E4%BD%8D%E5%9C%B0%E5%9D%80%E6%96%B9%E6%B3%95/"},{"categories":["汇编"],"content":"3、SI 、DI 与 寻址方式的灵活应用 1、si 、di si和di是8086CPU中和bx功能相近的寄存器，si和di不能够分成两个8位寄存器来使用。 assume cs: codesg, ds: datasg datasg segment db 'welcome to masm!';用si和di实现将字符串‘welcome to masm！\"复制到它后面的数据区中。 db '................' datasg ends codesg segment start: mov ax, datasg mov ds, ax mov si, 0 mov cx, 8 s: mov ax, 0[si] ;[0 + si] mov 16[si], ax ;[16 + si] 使用[bx +idata]方式代替di，使程序更简洁 add si, 2 loop s mov ax, 4c00h int 21h codesg ends end start 2、[bx + si] 和 [bx + di] [bx+si]和[bx+di]的含义相似 [bx+si]表示一个内存单元，它的偏移地址为（bx）+（si） 指令mov ax, [bx + si]的含义：将一个内存单元字数据的内容送入ax，段地址在ds中 该指令也可以写成如下格式：mov ax, [bx][si] 3、[bx+si+idata]和[bx+di+idata] [bx+si+idata]表示一个内存单元，它的偏移地址为（bx）+（si）+idata 指令mov ax，[bx+si+idata]的含义：将一个内存单元字数据的内容送入ax，段地址在ds中 4、不同的寻址方式的灵活应用 [idata]用一个常量来表示地址，可用于直接定位一个内存单元； [bx]用一个变量来表示内存地址，可用于间接定位一个内存单元； [bx+idata]用一个变量和常量表示地址，可在一个起始地址的基础上用变量间接定位一个内存单元； [bx+si]用两个变量表示地址； [bx+si+idata]用两个变量和一个常量表示地址。 ;将datasg段中每个单词改为大写字母 assume cs:codesg,ds:datasg,ss:stacksg datasg segment db 'ibm ' ;16 db 'dec ' db 'dos ' db 'vax ' ;看成二维数组 datasg ends stacksg segment ;定义一个段，用来做栈段，容量为16个字节 dw 0, 0, 0, 0, 0, 0, 0, 0 stacksg ends codesg segment start: mov ax, stacksg mov ss, ax mov sp, 16 mov ax, datasg mov ds, ax mov bx, 0 ;初始ds:bx ;cx为默认循环计数器，二重循环只有一个计数器，所以外层循环先保存cx值，再恢复，我们采用栈保存 mov cx, 4 s0: push cx ;将外层循环的cx值入栈 mov si, 0 mov cx, 3 ;cx设置为内层循环的次数 s: mov al, [bx+si] and al, 11011111b ;每个字符转为大写字母 mov [bx+si], al inc si loop s add bx, 16 ;下一行 pop cx ;恢复cx值 loop s0 ;外层循环的loop指令将cx中的计数值减1 mov ax，4c00H int 21H codesg ends end start ","date":"2023-01-09","objectID":"/%E6%B1%87%E7%BC%96%E4%B9%8B%E5%85%B6%E4%BB%96%E5%AE%9A%E4%BD%8D%E5%9C%B0%E5%9D%80%E6%96%B9%E6%B3%95/:3:0","tags":[],"title":"汇编之其他定位地址方法","uri":"/%E6%B1%87%E7%BC%96%E4%B9%8B%E5%85%B6%E4%BB%96%E5%AE%9A%E4%BD%8D%E5%9C%B0%E5%9D%80%E6%96%B9%E6%B3%95/"},{"categories":["汇编"],"content":"4、总结 本章介绍了一些地址定位的方式 对于数据暂存最好不要使用寄存器，因为寄存器资源有限。应该使用栈作为数据暂存的选择 ","date":"2023-01-09","objectID":"/%E6%B1%87%E7%BC%96%E4%B9%8B%E5%85%B6%E4%BB%96%E5%AE%9A%E4%BD%8D%E5%9C%B0%E5%9D%80%E6%96%B9%E6%B3%95/:4:0","tags":[],"title":"汇编之其他定位地址方法","uri":"/%E6%B1%87%E7%BC%96%E4%B9%8B%E5%85%B6%E4%BB%96%E5%AE%9A%E4%BD%8D%E5%9C%B0%E5%9D%80%E6%96%B9%E6%B3%95/"},{"categories":["redis"],"content":"[toc] Redis应用 ","date":"2023-01-09","objectID":"/redis%E5%BA%94%E7%94%A8/:0:0","tags":[],"title":"Redis应用","uri":"/redis%E5%BA%94%E7%94%A8/"},{"categories":["redis"],"content":"开篇导读 短信登录 这一块我们会使用redis共享session来实现 商户查询缓存 通过本章节，我们会理解缓存击穿，缓存穿透，缓存雪崩等问题，让小伙伴的对于这些概念的理解不仅仅是停留在概念上，更是能在代码中看到对应的内容 优惠卷秒杀 通过本章节，我们可以学会Redis的计数器功能， 结合Lua完成高性能的redis操作，同时学会Redis分布式锁的原理，包括Redis的三种消息队列 附近的商户 我们利用Redis的GEOHash来完成对于地理坐标的操作 UV统计 主要是使用Redis来完成统计功能 用户签到 使用Redis的BitMap数据统计功能 好友关注 基于Set集合的关注、取消关注，共同关注等等功能，这一块知识咱们之前就讲过，这次我们在项目中来使用一下 打人探店 基于List来完成点赞列表的操作，同时基于SortedSet来完成点赞的排行榜功能 以上这些内容咱们统统都会给小伙伴们讲解清楚，让大家充分理解如何使用Redis ","date":"2023-01-09","objectID":"/redis%E5%BA%94%E7%94%A8/:1:0","tags":[],"title":"Redis应用","uri":"/redis%E5%BA%94%E7%94%A8/"},{"categories":["redis"],"content":"1、短信登录 ","date":"2023-01-09","objectID":"/redis%E5%BA%94%E7%94%A8/:2:0","tags":[],"title":"Redis应用","uri":"/redis%E5%BA%94%E7%94%A8/"},{"categories":["redis"],"content":"1.1、导入黑马点评项目 1.1.1 、导入SQL 1.1.2、有关当前模型 手机或者app端发起请求，请求我们的nginx服务器，nginx基于七层模型走的事HTTP协议，可以实现基于Lua直接绕开tomcat访问redis，也可以作为静态资源服务器，轻松扛下上万并发， 负载均衡到下游tomcat服务器，打散流量，我们都知道一台4核8G的tomcat，在优化和处理简单业务的加持下，大不了就处理1000左右的并发， 经过nginx的负载均衡分流后，利用集群支撑起整个项目，同时nginx在部署了前端项目后，更是可以做到动静分离，进一步降低tomcat服务的压力，这些功能都得靠nginx起作用，所以nginx是整个项目中重要的一环。 在tomcat支撑起并发流量后，我们如果让tomcat直接去访问Mysql，根据经验Mysql企业级服务器只要上点并发，一般是16或32 核心cpu，32 或64G内存，像企业级mysql加上固态硬盘能够支撑的并发，大概就是4000起~7000左右，上万并发， 瞬间就会让Mysql服务器的cpu，硬盘全部打满，容易崩溃，所以我们在高并发场景下，会选择使用mysql集群，同时为了进一步降低Mysql的压力，同时增加访问的性能，我们也会加入Redis，同时使用Redis集群使得Redis对外提供更好的服务。 1.1.3、导入后端项目 在资料中提供了一个项目源码： 1.1.4、导入前端工程 1.1.5 运行前端项目 ","date":"2023-01-09","objectID":"/redis%E5%BA%94%E7%94%A8/:2:1","tags":[],"title":"Redis应用","uri":"/redis%E5%BA%94%E7%94%A8/"},{"categories":["redis"],"content":"1.2 、基于Session实现登录流程 发送验证码： 用户在提交手机号后，会校验手机号是否合法，如果不合法，则要求用户重新输入手机号 如果手机号合法，后台此时生成对应的验证码，同时将验证码进行保存，然后再通过短信的方式将验证码发送给用户 短信验证码登录、注册： 用户将验证码和手机号进行输入，后台从session中拿到当前验证码，然后和用户输入的验证码进行校验，如果不一致，则无法通过校验，如果一致，则后台根据手机号查询用户，如果用户不存在，则为用户创建账号信息，保存到数据库，无论是否存在，都会将用户信息保存到session中，方便后续获得当前登录信息 校验登录状态: 用户在请求时候，会从cookie中携带者JsessionId到后台，后台通过JsessionId从session中拿到用户信息，如果没有session信息，则进行拦截，如果有session信息，则将用户信息保存到threadLocal中，并且放行 ","date":"2023-01-09","objectID":"/redis%E5%BA%94%E7%94%A8/:2:2","tags":[],"title":"Redis应用","uri":"/redis%E5%BA%94%E7%94%A8/"},{"categories":["redis"],"content":"1.3 、实现发送短信验证码功能 页面流程 具体代码如下 贴心小提示： 具体逻辑上文已经分析，我们仅仅只需要按照提示的逻辑写出代码即可。 发送验证码 @Override public Result sendCode(String phone, HttpSession session) { // 1.校验手机号 if (RegexUtils.isPhoneInvalid(phone)) { // 2.如果不符合，返回错误信息 return Result.fail(\"手机号格式错误！\"); } // 3.符合，生成验证码 String code = RandomUtil.randomNumbers(6); // 4.保存验证码到 session session.setAttribute(\"code\",code); // 5.发送验证码 log.debug(\"发送短信验证码成功，验证码：{}\", code); // 返回ok return Result.ok(); } 登录 @Override public Result login(LoginFormDTO loginForm, HttpSession session) { // 1.校验手机号 String phone = loginForm.getPhone(); if (RegexUtils.isPhoneInvalid(phone)) { // 2.如果不符合，返回错误信息 return Result.fail(\"手机号格式错误！\"); } // 3.校验验证码 Object cacheCode = session.getAttribute(\"code\"); String code = loginForm.getCode(); if(cacheCode == null || !cacheCode.toString().equals(code)){ //3.不一致，报错 return Result.fail(\"验证码错误\"); } //一致，根据手机号查询用户 User user = query().eq(\"phone\", phone).one(); //5.判断用户是否存在 if(user == null){ //不存在，则创建 user = createUserWithPhone(phone); } //7.保存用户信息到session中 session.setAttribute(\"user\",user); return Result.ok(); } ","date":"2023-01-09","objectID":"/redis%E5%BA%94%E7%94%A8/:2:3","tags":[],"title":"Redis应用","uri":"/redis%E5%BA%94%E7%94%A8/"},{"categories":["redis"],"content":"1.4、实现登录拦截功能 java的拦截器类似于gin里的中间件，主要是做一些统一的操作，如：日志记录、权限检查、性能监控、某种状态维护等 温馨小贴士：tomcat的运行原理 当用户发起请求时，会访问我们像tomcat注册的端口，任何程序想要运行，都需要有一个线程对当前端口号进行监听，tomcat也不例外，当监听线程知道用户想要和tomcat连接连接时，那会由监听线程创建socket连接，socket都是成对出现的，用户通过socket像互相传递数据，当tomcat端的socket接受到数据后，此时监听线程会从tomcat的线程池中取出一个线程执行用户请求，在我们的服务部署到tomcat后，线程会找到用户想要访问的工程，然后用这个线程转发到工程中的controller，service，dao中，并且访问对应的DB，在用户执行完请求后，再统一返回，再找到tomcat端的socket，再将数据写回到用户端的socket，完成请求和响应 通过以上讲解，我们可以得知 每个用户其实对应都是去找tomcat线程池中的一个线程来完成工作的， 使用完成后再进行回收，既然每个请求都是独立的，所以在每个用户去访问我们的工程时，我们可以使用threadlocal来做到线程隔离，每个线程操作自己的一份数据 温馨小贴士：关于threadlocal 如果小伙伴们看过threadLocal的源码，你会发现在threadLocal中，无论是他的put方法和他的get方法， 都是先从获得当前用户的线程，然后从线程中取出线程的成员变量map，只要线程不一样，map就不一样，所以可以通过这种方式来做到线程隔离 拦截器代码 public class LoginInterceptor implements HandlerInterceptor { @Override public boolean preHandle(HttpServletRequest request, HttpServletResponse response, Object handler) throws Exception { //1.获取session HttpSession session = request.getSession(); //2.获取session中的用户 Object user = session.getAttribute(\"user\"); //3.判断用户是否存在 if(user == null){ //4.不存在，拦截，返回401状态码 response.setStatus(401); return false; } //5.存在，保存用户信息到Threadlocal UserHolder.saveUser((User)user); //6.放行 return true; } } 让拦截器生效 @Configuration public class MvcConfig implements WebMvcConfigurer { @Resource private StringRedisTemplate stringRedisTemplate; @Override public void addInterceptors(InterceptorRegistry registry) { // 登录拦截器 registry.addInterceptor(new LoginInterceptor()) .excludePathPatterns( \"/shop/**\", \"/voucher/**\", \"/shop-type/**\", \"/upload/**\", \"/blog/hot\", \"/user/code\", \"/user/login\" ).order(1); // token刷新的拦截器 registry.addInterceptor(new RefreshTokenInterceptor(stringRedisTemplate)).addPathPatterns(\"/**\").order(0); } } ","date":"2023-01-09","objectID":"/redis%E5%BA%94%E7%94%A8/:2:4","tags":[],"title":"Redis应用","uri":"/redis%E5%BA%94%E7%94%A8/"},{"categories":["redis"],"content":"1.5、隐藏用户敏感信息 我们通过浏览器观察到此时用户的全部信息都在，这样极为不靠谱，所以我们应当在返回用户信息之前，将用户的敏感信息进行隐藏，采用的核心思路就是书写一个UserDto对象，这个UserDto对象就没有敏感信息了，我们在返回前，将有用户敏感信息的User对象转化成没有敏感信息的UserDto对象，那么就能够避免这个尴尬的问题了 在登录方法处修改 //7.保存用户信息到session中 session.setAttribute(\"user\", BeanUtils.copyProperties(user,UserDTO.class)); 在拦截器处： //5.存在，保存用户信息到Threadlocal UserHolder.saveUser((UserDTO) user); 在UserHolder处：将user对象换成UserDTO public class UserHolder { private static final ThreadLocal\u003cUserDTO\u003e tl = new ThreadLocal\u003c\u003e(); public static void saveUser(UserDTO user){ tl.set(user); } public static UserDTO getUser(){ return tl.get(); } public static void removeUser(){ tl.remove(); } } ","date":"2023-01-09","objectID":"/redis%E5%BA%94%E7%94%A8/:2:5","tags":[],"title":"Redis应用","uri":"/redis%E5%BA%94%E7%94%A8/"},{"categories":["redis"],"content":"1.6、session共享问题 核心思路分析： 每个tomcat中都有一份属于自己的session,假设用户第一次访问第一台tomcat，并且把自己的信息存放到第一台服务器的session中，但是第二次这个用户访问到了第二台tomcat，那么在第二台服务器上，肯定没有第一台服务器存放的session，所以此时 整个登录拦截功能就会出现问题，我们能如何解决这个问题呢？早期的方案是session拷贝，就是说虽然每个tomcat上都有不同的session，但是每当任意一台服务器的session修改时，都会同步给其他的Tomcat服务器的session，这样的话，就可以实现session的共享了 但是这种方案具有两个大问题 1、每台服务器中都有完整的一份session数据，服务器压力过大。 2、session拷贝数据时，可能会出现延迟 所以咱们后来采用的方案都是基于redis来完成，我们把session换成redis，redis数据本身就是共享的，就可以避免session共享的问题了 ","date":"2023-01-09","objectID":"/redis%E5%BA%94%E7%94%A8/:2:6","tags":[],"title":"Redis应用","uri":"/redis%E5%BA%94%E7%94%A8/"},{"categories":["redis"],"content":"1.7 Redis代替session的业务流程 1.7.1、设计key的结构 首先我们要思考一下利用redis来存储数据，那么到底使用哪种结构呢？由于存入的数据比较简单，我们可以考虑使用String，或者是使用哈希，如下图，如果使用String，同学们注意他的value，用多占用一点空间，如果使用哈希，则他的value中只会存储他数据本身，如果不是特别在意内存，其实使用String就可以啦。 1.7.2、设计key的具体细节 所以我们可以使用String结构，就是一个简单的key，value键值对的方式，但是关于key的处理，session他是每个用户都有自己的session，但是redis的key是共享的，咱们就不能使用code了 在设计这个key的时候，我们之前讲过需要满足两点 1、key要具有唯一性 2、key要方便携带 如果我们采用phone：手机号这个的数据来存储当然是可以的，但是如果把这样的敏感数据存储到redis中并且从页面中带过来毕竟不太合适，所以我们在后台生成一个随机串token，然后让前端带来这个token就能完成我们的整体逻辑了 1.7.3、整体访问流程 当注册完成后，用户去登录会去校验用户提交的手机号和验证码，是否一致，如果一致，则根据手机号查询用户信息，不存在则新建，最后将用户数据保存到redis，并且生成token作为redis的key，当我们校验用户是否登录时，会去携带着token进行访问，从redis中取出token对应的value，判断是否存在这个数据，如果没有则拦截，如果存在则将其保存到threadLocal中，并且放行。 ","date":"2023-01-09","objectID":"/redis%E5%BA%94%E7%94%A8/:2:7","tags":[],"title":"Redis应用","uri":"/redis%E5%BA%94%E7%94%A8/"},{"categories":["redis"],"content":"1.8 基于Redis实现短信登录 这里具体逻辑就不分析了，之前咱们已经重点分析过这个逻辑啦。 UserServiceImpl代码 @Override public Result login(LoginFormDTO loginForm, HttpSession session) { // 1.校验手机号 String phone = loginForm.getPhone(); if (RegexUtils.isPhoneInvalid(phone)) { // 2.如果不符合，返回错误信息 return Result.fail(\"手机号格式错误！\"); } // 3.从redis获取验证码并校验 String cacheCode = stringRedisTemplate.opsForValue().get(LOGIN_CODE_KEY + phone); String code = loginForm.getCode(); if (cacheCode == null || !cacheCode.equals(code)) { // 不一致，报错 return Result.fail(\"验证码错误\"); } // 4.一致，根据手机号查询用户 select * from tb_user where phone = ? User user = query().eq(\"phone\", phone).one(); // 5.判断用户是否存在 if (user == null) { // 6.不存在，创建新用户并保存 user = createUserWithPhone(phone); } // 7.保存用户信息到 redis中 // 7.1.随机生成token，作为登录令牌 String token = UUID.randomUUID().toString(true); // 7.2.将User对象转为HashMap存储 UserDTO userDTO = BeanUtil.copyProperties(user, UserDTO.class); Map\u003cString, Object\u003e userMap = BeanUtil.beanToMap(userDTO, new HashMap\u003c\u003e(), CopyOptions.create() .setIgnoreNullValue(true) .setFieldValueEditor((fieldName, fieldValue) -\u003e fieldValue.toString())); // 7.3.存储 String tokenKey = LOGIN_USER_KEY + token; stringRedisTemplate.opsForHash().putAll(tokenKey, userMap); // 7.4.设置token有效期 stringRedisTemplate.expire(tokenKey, LOGIN_USER_TTL, TimeUnit.MINUTES); // 8.返回token return Result.ok(token); } ","date":"2023-01-09","objectID":"/redis%E5%BA%94%E7%94%A8/:2:8","tags":[],"title":"Redis应用","uri":"/redis%E5%BA%94%E7%94%A8/"},{"categories":["redis"],"content":"1.9 解决状态登录刷新问题 1.9.1 初始方案思路总结： 在这个方案中，他确实可以使用对应路径的拦截，同时刷新登录token令牌的存活时间，但是现在这个拦截器他只是拦截需要被拦截的路径，假设当前用户访问了一些不需要拦截的路径，那么这个拦截器就不会生效，所以此时令牌刷新的动作实际上就不会执行，所以这个方案他是存在问题的 1.9.2 优化方案 既然之前的拦截器无法对不需要拦截的路径生效，那么我们可以添加一个拦截器，在第一个拦截器中拦截所有的路径，把第二个拦截器做的事情放入到第一个拦截器中，同时刷新令牌，因为第一个拦截器有了threadLocal的数据，所以此时第二个拦截器只需要判断拦截器中的user对象是否存在即可，完成整体刷新功能。 1.9.3 代码 RefreshTokenInterceptor public class RefreshTokenInterceptor implements HandlerInterceptor { private StringRedisTemplate stringRedisTemplate; public RefreshTokenInterceptor(StringRedisTemplate stringRedisTemplate) { this.stringRedisTemplate = stringRedisTemplate; } @Override public boolean preHandle(HttpServletRequest request, HttpServletResponse response, Object handler) throws Exception { // 1.获取请求头中的token String token = request.getHeader(\"authorization\"); if (StrUtil.isBlank(token)) { return true; } // 2.基于TOKEN获取redis中的用户 String key = LOGIN_USER_KEY + token; Map\u003cObject, Object\u003e userMap = stringRedisTemplate.opsForHash().entries(key); // 3.判断用户是否存在 if (userMap.isEmpty()) { return true; } // 5.将查询到的hash数据转为UserDTO UserDTO userDTO = BeanUtil.fillBeanWithMap(userMap, new UserDTO(), false); // 6.存在，保存用户信息到 ThreadLocal UserHolder.saveUser(userDTO); // 7.刷新token有效期 stringRedisTemplate.expire(key, LOGIN_USER_TTL, TimeUnit.MINUTES); // 8.放行 return true; } @Override public void afterCompletion(HttpServletRequest request, HttpServletResponse response, Object handler, Exception ex) throws Exception { // 移除用户 UserHolder.removeUser(); } } LoginInterceptor public class LoginInterceptor implements HandlerInterceptor { @Override public boolean preHandle(HttpServletRequest request, HttpServletResponse response, Object handler) throws Exception { // 1.判断是否需要拦截（ThreadLocal中是否有用户） if (UserHolder.getUser() == null) { // 没有，需要拦截，设置状态码 response.setStatus(401); // 拦截 return false; } // 有用户，则放行 return true; } } ","date":"2023-01-09","objectID":"/redis%E5%BA%94%E7%94%A8/:2:9","tags":[],"title":"Redis应用","uri":"/redis%E5%BA%94%E7%94%A8/"},{"categories":["redis"],"content":"2、商户查询缓存 ","date":"2023-01-09","objectID":"/redis%E5%BA%94%E7%94%A8/:3:0","tags":[],"title":"Redis应用","uri":"/redis%E5%BA%94%E7%94%A8/"},{"categories":["redis"],"content":"2.1 什么是缓存? 前言:什么是缓存? 就像自行车,越野车的避震器 举个例子:越野车,山地自行车,都拥有\"避震器\",防止车体加速后因惯性,在酷似\"U\"字母的地形上飞跃,硬着陆导致的损害,像个弹簧一样; 同样,实际开发中,系统也需要\"避震器\",防止过高的数据访问猛冲系统,导致其操作线程无法及时处理信息而瘫痪; 这在实际开发中对企业讲,对产品口碑,用户评价都是致命的;所以企业非常重视缓存技术; 缓存(Cache),就是数据交换的缓冲区,俗称的缓存就是缓冲区内的数据,一般从数据库中获取,存储于本地代码(例如: //例1: static final ConcurrentHashMap\u003cK,V\u003e map = new ConcurrentHashMap\u003c\u003e(); 本地用于高并发 //例2: static final Cache\u003cK,V\u003e USER_CACHE = CacheBuilder.newBuilder().build(); 用于redis等缓存 //例3: static final Map\u003cK,V\u003e map = new HashMap(); 本地缓存 由于其被Static修饰,所以随着类的加载而被加载到内存之中,作为本地缓存,由于其又被final修饰,所以其引用(例3:map)和对象(例3:new HashMap())之间的关系是固定的,不能改变,因此不用担心赋值(=)导致缓存失效; 2.1.1 为什么要使用缓存 一句话:因为速度快,好用 缓存数据存储于代码中,而代码运行在内存中,内存的读写性能远高于磁盘,缓存可以大大降低用户访问并发量带来的服务器读写压力 实际开发过程中,企业的数据量,少则几十万,多则几千万,这么大数据量,如果没有缓存来作为\"避震器\",系统是几乎撑不住的,所以企业会大量运用到缓存技术; 但是缓存也会增加代码复杂度和运营的成本: 2.1.2 如何使用缓存 实际开发中,会构筑多级缓存来使系统运行速度进一步提升,例如:本地缓存与redis中的缓存并发使用 浏览器缓存：主要是存在于浏览器端的缓存 **应用层缓存：**可以分为tomcat本地缓存，比如之前提到的map，或者是使用redis作为缓存 **数据库缓存：**在数据库中有一片空间是 buffer pool，增改查数据都会先加载到mysql的缓存中 **CPU缓存：**当代计算机最大的问题是 cpu性能提升了，但内存读写速度没有跟上，所以为了适应当下的情况，增加了cpu的L1，L2，L3级的缓存 **磁盘缓存：**现代操作系统做了优化，当程序将数据写入磁盘时，第一步其实是先写入系统 page cache（fsync），之后再由系统线程决定何时真正刷盘（write） ","date":"2023-01-09","objectID":"/redis%E5%BA%94%E7%94%A8/:3:1","tags":[],"title":"Redis应用","uri":"/redis%E5%BA%94%E7%94%A8/"},{"categories":["redis"],"content":"2.2 添加商户缓存 在我们查询商户信息时，我们是直接操作从数据库中去进行查询的，大致逻辑是这样，直接查询数据库那肯定慢咯，所以我们需要增加缓存 @GetMapping(\"/{id}\") public Result queryShopById(@PathVariable(\"id\") Long id) { //这里是直接查询数据库 return shopService.queryById(id); } 2.2.1 、缓存模型和思路 标准的操作方式就是查询数据库之前先查询缓存，如果缓存数据存在，则直接从缓存中返回，如果缓存数据不存在，再查询数据库，然后将数据存入redis。 2.1.2、代码如下 代码思路：如果缓存有，则直接返回，如果缓存不存在，则查询数据库，然后存入redis。 ","date":"2023-01-09","objectID":"/redis%E5%BA%94%E7%94%A8/:3:2","tags":[],"title":"Redis应用","uri":"/redis%E5%BA%94%E7%94%A8/"},{"categories":["redis"],"content":"2.3 缓存淘汰策略 缓存淘汰是redis为了节约内存而设计出来的一个东西，主要是因为内存数据宝贵，当我们向redis插入太多数据，此时就可能会导致缓存中的数据过多，所以redis会对部分数据进行淘汰。 **内存淘汰：**redis自动进行，当redis内存达到咱们设定的max-memery的时候，会自动触发淘汰机制，淘汰掉一些不重要的数据(可以自己设置策略方式) **超时剔除：**当我们给redis设置了过期时间ttl之后，redis会将超时的数据进行删除，方便咱们继续使用缓存 **主动淘汰：**我们可以手动调用方法把缓存删掉，通常用于解决缓存和数据库不一致问题 低一致性：对于查询结果没有那么严苛，没有要求必须查询最新的数据。这种业务在使用redis作为缓存时，就没必要再查关系型数据库了，直接返回数据可以更好地降低系统的负载。 高一致性：对于查询结果严苛，要求必须查询最新的数据。所以，这种业务在使用redis作为缓存时，需要注意主动更新策略。 2.3.1 数据库缓存不一致解决方案： 由于我们的缓存的数据源来自于数据库,而数据库的数据是会发生变化的,因此,如果当数据库中数据发生变化,而缓存却没有同步,此时就会有一致性问题存在,其后果是: 用户使用缓存中的过时数据,就会产生类似多线程数据安全问题,从而影响业务,产品口碑等;怎么解决呢？有如下几种方案 Cache Aside Pattern 人工编码方式：缓存调用者在更新完数据库后再去更新缓存，也称之为双写方案 Read/Write Through Pattern : 由系统本身完成，数据库与缓存的问题交由系统本身去处理 Write Behind Caching Pattern ：调用者只操作缓存，其他线程去异步处理数据库，实现最终一致 2.3.2 数据库和缓存不一致采用什么方案 综合考虑使用方案一，但是方案一调用者如何处理呢？这里有几个问题 操作缓存和数据库时有三个问题需要考虑： 删除缓存还是更新缓存？ ~~更新缓存：每次更新数据库都更新缓存，无效写操作较多–总是期待会有很多查询缓存，类似于悲观锁机制。~~不推荐使用这种方案，悲观锁的机制，性能耗损较多。 删除缓存：更新数据库时让缓存失效，查询时再更新缓存 如果采用第一个方案，那么假设我们每次操作数据库后，都操作缓存，但是中间如果没有人查询，那么这个更新动作实际上只有最后一次生效，中间的更新动作意义并不大，我们可以把缓存删除，等待再次查询时，将缓存中的数据加载出来。第二种方案还能腾出缓存空间，属于是主动淘汰策略。 如何保证缓存与数据库的操作的同时成功或失败？–事务 单体系统：将缓存与数据库操作放在一个事务。这个事务的话，可以在代码层面实现逻辑。 分布式系统：利用TCC等分布式事务方案 先操作缓存还是先操作数据库？ 先删除缓存，再操作数据库 先操作数据库，再删除缓存 应该具体操作缓存还是操作数据库，我们应当是先操作数据库，再删除缓存，原因在于，如果你选择第一种方案，在两个线程并发来访问时，假设线程1先来，他先把缓存删了，此时线程2过来，他查询缓存数据并不存在，此时他写入缓存，当他写入缓存后，线程1再执行更新动作时，实际上写入的就是旧的数据，新的数据被旧数据覆盖了。 ","date":"2023-01-09","objectID":"/redis%E5%BA%94%E7%94%A8/:3:3","tags":[],"title":"Redis应用","uri":"/redis%E5%BA%94%E7%94%A8/"},{"categories":["redis"],"content":"2.4 实现商铺和缓存与数据库双写一致 核心思路如下： 修改ShopController中的业务逻辑，满足下面的需求： 根据id查询店铺时，如果缓存未命中，则查询数据库，将数据库结果写入缓存，并设置超时时间 =\u003e 双写策略 根据id修改店铺时，先修改数据库，再删除缓存 =\u003e 不一致性的最优解决方案 修改重点代码1：修改ShopServiceImpl的queryById方法 设置redis缓存时添加过期时间 修改重点代码2 代码分析：通过之前的淘汰，我们确定了采用删除策略，来解决双写问题，当我们修改了数据之后，然后把缓存中的数据进行删除，查询时发现缓存中没有数据，则会从mysql中加载最新的数据，从而避免数据库和缓存不一致的问题 ","date":"2023-01-09","objectID":"/redis%E5%BA%94%E7%94%A8/:3:4","tags":[],"title":"Redis应用","uri":"/redis%E5%BA%94%E7%94%A8/"},{"categories":["redis"],"content":"2.5 缓存穿透问题的解决思路 首先，我们来说说缓存穿透。什么是缓存穿透呢？缓存穿透问题在一定程度上与缓存命中率有关。如果我们的缓存设计的不合理，缓存的命中率非常低，那么，数据访问的绝大部分压力都会集中在后端数据库层面;当然也与请求有关，可能是某些恶意请求绕开了系统的安全机制，将请求打进了系统，但是这个请求的数据是恶意的随机量，那么这些请求就会在缓存和数据库这两层都失效，也就造成了缓存穿透问题 缓存穿透：如果在请求数据时，在缓存层和数据库层都没有找到符合条件的数据，也就是说，在缓存层和数据库层都没有命中数据，那么，这种情况就叫作缓存穿透。 造成缓存穿透的主要原因就是：查询某个 Key 对应的数据，Redis 缓存中没有相应的数据，则直接到数据库中查询。数据库中也不存在要查询的数据，则数据库会返回空，而 Redis 也不会缓存这个空结果。这就造成每次通过这样的 Key 去查询数据都会直接到数据库中查询，Redis 不会缓存空结果。这就造成了缓存穿透的问题。 常见的解决方案有两种： 缓存空对象 优点：实现简单，维护方便 缺点： 额外的内存消耗 可能造成短期的不一致 布隆过滤 优点：内存占用较少，没有多余key 缺点： 实现复杂 存在误判可能 **缓存空对象思路分析：**当我们客户端访问不存在的数据时，先请求redis，但是此时redis中没有数据，此时会访问到数据库，但是数据库中也没有数据，这个数据穿透了缓存，直击数据库，我们都知道数据库能够承载的并发不如redis这么高，如果大量的请求同时过来访问这种不存在的数据，这些请求就都会访问到数据库，简单的解决方案就是哪怕这个数据在数据库中也不存在，我们也把这个数据存入到redis中去，这样，下次用户过来访问这个不存在的数据，那么在redis中也能找到这个数据就不会进入到数据库了 **布隆过滤：**布隆过滤器其实采用的是哈希思想来解决这个问题，通过一个庞大的二进制数组，走哈希思想去判断当前这个要查询的这个数据是否存在，如果布隆过滤器判断存在，则放行，这个请求会去访问redis，哪怕此时redis中的数据过期了，但是数据库中一定存在这个数据，在数据库中查询出来这个数据后，再将其放入到redis中。 假设布隆过滤器判断这个数据不存在，则直接返回 这种方式优点在于节约内存空间，存在误判，误判原因在于：布隆过滤器走的是哈希思想，只要哈希思想，就可能存在哈希冲突 ","date":"2023-01-09","objectID":"/redis%E5%BA%94%E7%94%A8/:3:5","tags":[],"title":"Redis应用","uri":"/redis%E5%BA%94%E7%94%A8/"},{"categories":["redis"],"content":"2.6 编码解决商品查询的缓存穿透问题： 核心思路如下： 在原来的逻辑中，我们如果发现这个数据在mysql中不存在，直接就返回404了，这样是会存在缓存穿透问题的 现在的逻辑中：如果这个数据不存在，我们不会返回404 ，还是会把这个数据写入到Redis中，并且将value设置为空，欧当再次发起查询时，我们如果发现命中之后，判断这个value是否是null，如果是null，则是之前写入的数据，证明是缓存穿透数据，如果不是，则直接返回数据。 小总结： 缓存穿透产生的原因是什么？ 用户请求的数据在缓存中和数据库中都不存在，不断发起这样的请求，给数据库带来巨大压力 缓存穿透的解决方案有哪些？ 缓存null值，防止后面的相同的穿透请求再次打进来 布隆过滤，识别数据是否存在 增强id的复杂度，避免被猜测id规律，防止大量恶意请求的穿透 做好数据的基础格式校验，减少错误或恶意请求 加强用户权限校验，减少错误或恶意请求 做好热点参数的限流，降低并发量 ","date":"2023-01-09","objectID":"/redis%E5%BA%94%E7%94%A8/:3:6","tags":[],"title":"Redis应用","uri":"/redis%E5%BA%94%E7%94%A8/"},{"categories":["redis"],"content":"2.7 缓存雪崩问题及解决思路 缓存雪崩是指在同一时段大量的缓存key同时失效（同一时段，大量请求缓存未命中）或者Redis服务宕机，导致大量请求到达数据库，带来巨大压力。 解决方案： 给不同的Key的TTL添加随机值 利用Redis集群提高服务的可用性 给缓存业务添加降级限流策略 给业务添加多级缓存 ","date":"2023-01-09","objectID":"/redis%E5%BA%94%E7%94%A8/:3:7","tags":[],"title":"Redis应用","uri":"/redis%E5%BA%94%E7%94%A8/"},{"categories":["redis"],"content":"2.8 缓存击穿问题及解决思路 缓存击穿问题也叫热点Key问题，就是一个被高并发访问并且**缓存重建业务较复杂（耗费时间较长）的key突然失效了，无数的请求访问会在瞬间（也就是缓存重建阶段内）**给数据库带来巨大的冲击。 常见的解决方案有两种： 互斥锁 逻辑过期 逻辑分析：假设线程1在查询缓存之后，本来应该去查询数据库，然后把这个数据重新加载到缓存的，此时只要线程1走完这个逻辑，其他线程就都能从缓存中加载这些数据了，但是假设在线程1没有走完的时候，后续的线程2，线程3，线程4同时过来访问当前这个方法， 那么这些线程都不能从缓存中查询到数据，那么他们就会同一时刻来访问查询缓存，都没查到，接着同一时间去访问数据库，同时的去执行数据库代码，对数据库访问压力过大 解决方案一、使用锁来解决： 因为锁能实现互斥性。假设线程过来，只能一个人一个人的来访问数据库，从而避免对于数据库访问压力过大，但这也会影响查询的性能，因为此时会让查询的性能从并行变成了串行，我们可以采用tryLock方法 + double check来解决这样的问题。 假设现在线程1过来访问，他查询缓存没有命中，但是此时他获得到了锁的资源，那么线程1就会一个人去执行逻辑，假设现在线程2过来，线程2在执行过程中，并没有获得到锁，那么线程2就可以进行到休眠，直到线程1把锁释放后，线程2获得到锁，然后再来执行逻辑，此时就能够从缓存中拿到数据了。 解决方案二、逻辑过期方案 方案分析：我们之所以会出现这个缓存击穿问题，主要原因是在于我们对key设置了过期时间，假设我们不设置过期时间，其实就不会有缓存击穿的问题，但是不设置过期时间，这样数据不就一直占用我们内存了吗，我们可以采用逻辑过期方案。 我们把过期时间设置在 redis的value中，注意：这个过期时间并不会直接作用于redis，而是我们后续通过逻辑去处理。假设线程1去查询缓存，然后从value中判断出来当前的数据已经过期了，此时线程1去获得互斥锁，那么其他线程会进行阻塞，获得了锁的线程他会开启一个 线程去进行 以前的重构数据的逻辑，直到新开的线程完成这个逻辑后，才释放锁， 而线程1直接进行返回，假设现在线程3过来访问，由于线程线程2持有着锁，所以线程3无法获得锁，线程3也直接返回数据，只有等到新开的线程2把重建数据构建完后，其他线程才能走返回正确的数据。 这种方案巧妙在于，异步的构建缓存，缺点在于在构建完缓存之前，返回的都是脏数据。 进行对比 **互斥锁方案：**由于保证了互斥性，所以数据一致，且实现简单，因为仅仅只需要加一把锁而已，也没其他的事情需要操心，所以没有额外的内存消耗，缺点在于有锁就有死锁问题的发生，且只能串行执行性能肯定受到影响 逻辑过期方案： 线程读取过程中不需要等待，性能好，有一个额外的线程持有锁去进行重构数据，但是在重构数据完成前，其他的线程只能返回之前的数据，且实现起来麻烦 ","date":"2023-01-09","objectID":"/redis%E5%BA%94%E7%94%A8/:3:8","tags":[],"title":"Redis应用","uri":"/redis%E5%BA%94%E7%94%A8/"},{"categories":["redis"],"content":"2.9 利用互斥锁解决缓存击穿问题 核心思路：相较于原来从缓存中查询不到数据后直接查询数据库而言，现在的方案是：进行查询之后，如果从缓存没有查询到数据，则进行互斥锁的获取，获取互斥锁后，判断是否获得到了锁，如果没有获得到，则休眠，过一会再进行尝试，直到获取到锁为止，才能进行查询 如果获取到了锁的线程，再去进行查询，查询后将数据写入redis，再释放锁，返回数据，利用互斥锁就能保证只有一个线程去执行操作数据库的逻辑，防止缓存击穿 操作锁的代码： 核心思路就是利用redis的setnx方法来表示获取锁，该方法含义是redis中如果没有这个key，则插入成功，返回1，在stringRedisTemplate中返回true， 如果有这个key则插入失败，则返回0，在stringRedisTemplate返回false，我们可以通过true，或者是false，来表示是否有线程成功插入key，成功插入的key的线程我们认为他就是获得到锁的线程。 private boolean tryLock(String key) { Boolean flag = stringRedisTemplate.opsForValue().setIfAbsent(key, \"1\", 10, TimeUnit.SECONDS); return BooleanUtil.isTrue(flag); } private void unlock(String key) { stringRedisTemplate.delete(key); } 操作代码： public Shop queryWithMutex(Long id) { String key = CACHE_SHOP_KEY + id; // 1、从redis中查询商铺缓存 String shopJson = stringRedisTemplate.opsForValue().get(key); // 2、判断是否存在 if (StrUtil.isNotBlank(shopJson)) { // 存在,直接返回 return JSONUtil.toBean(shopJson, Shop.class); } // 判断命中的值是否是空值 if (shopJson != null) { //返回一个错误信息 return null; } // 4.实现缓存重构 // 4.1 获取互斥锁 String lockKey = \"lock:shop:\" + id; Shop shop = null; try { boolean isLock = tryLock(lockKey); // 4.2 判断否获取成功 if(!isLock){ // 4.3 失败，则休眠重试 Thread.sleep(50); return queryWithMutex(id); } // 4.4 成功，根据id查询数据库 shop = getById(id); // 5.不存在，也就是发生了缓存穿透，返回错误 if(shop == null){ // 将空值写入redis stringRedisTemplate.opsForValue().set(key,\"\",CACHE_NULL_TTL,TimeUnit.MINUTES); // 返回错误信息 return null; } // 6.写入redis stringRedisTemplate.opsForValue().set(key,JSONUtil.toJsonStr(shop),CACHE_NULL_TTL,TimeUnit.MINUTES); }catch (Exception e){ throw new RuntimeException(e); } finally { // 7.释放互斥锁 unlock(lockKey); } return shop; } ","date":"2023-01-09","objectID":"/redis%E5%BA%94%E7%94%A8/:3:9","tags":[],"title":"Redis应用","uri":"/redis%E5%BA%94%E7%94%A8/"},{"categories":["redis"],"content":"3.0 利用逻辑过期解决缓存击穿问题 需求：修改根据id查询商铺的业务，基于逻辑过期方式来解决缓存击穿问题 思路分析：当用户开始查询redis时，判断是否命中，如果没有命中则直接返回空数据，不查询数据库，而一旦命中后，将value取出，判断value中的过期时间是否满足，如果没有过期，则直接返回redis中的数据，如果过期，则在开启独立线程后直接返回之前的数据，独立线程去重构数据，重构完成后释放互斥锁。 如果封装数据：因为现在redis中存储的数据的value需要带上过期时间，此时要么你去修改原来的实体类，要么你 步骤一、 新建一个实体类，我们采用第二个方案，这个方案，对原来代码没有侵入性。 @Data public class RedisData { private LocalDateTime expireTime; private Object data; } 步骤二、 在ShopServiceImpl 新增此方法，利用单元测试进行缓存预热 在测试类中 步骤三：正式代码 ShopServiceImpl private static final ExecutorService CACHE_REBUILD_EXECUTOR = Executors.newFixedThreadPool(10); public Shop queryWithLogicalExpire( Long id ) { String key = CACHE_SHOP_KEY + id; // 1.从redis查询商铺缓存 String json = stringRedisTemplate.opsForValue().get(key); // 2.判断是否存在 if (StrUtil.isBlank(json)) { // 3.存在，直接返回 return null; } // 4.命中，需要先把json反序列化为对象 RedisData redisData = JSONUtil.toBean(json, RedisData.class); Shop shop = JSONUtil.toBean((JSONObject) redisData.getData(), Shop.class); LocalDateTime expireTime = redisData.getExpireTime(); // 5.判断是否过期 if(expireTime.isAfter(LocalDateTime.now())) { // 5.1.未过期，直接返回店铺信息 return shop; } // 5.2.已过期，需要缓存重建 // 6.缓存重建 // 6.1.获取互斥锁 String lockKey = LOCK_SHOP_KEY + id; boolean isLock = tryLock(lockKey); // 6.2.判断是否获取锁成功 if (isLock){ // 获取成功就新开线程去做缓存重建 CACHE_REBUILD_EXECUTOR.submit( ()-\u003e{ try{ //重建缓存 this.saveShop2Redis(id,20L); }catch (Exception e){ throw new RuntimeException(e); }finally { unlock(lockKey); } }); } // 6.4.返回过期的商铺信息 return shop; } ","date":"2023-01-09","objectID":"/redis%E5%BA%94%E7%94%A8/:3:10","tags":[],"title":"Redis应用","uri":"/redis%E5%BA%94%E7%94%A8/"},{"categories":["redis"],"content":"3.1 封装Redis工具类 基于StringRedisTemplate封装一个缓存工具类，满足下列需求： 方法1：将任意Java对象序列化为json并存储在string类型的key中，并且可以设置TTL过期时间 方法2：将任意Java对象序列化为json并存储在string类型的key中，并且可以设置逻辑过期时间，用于处理缓存击穿问题 方法3：根据指定的key查询缓存，并反序列化为指定类型，利用缓存空值的方式解决缓存穿透问题 方法4：根据指定的key查询缓存，并反序列化为指定类型，需要利用逻辑过期解决缓存击穿问题 将逻辑进行封装 @Slf4j @Component public class CacheClient { private final StringRedisTemplate stringRedisTemplate; private static final ExecutorService CACHE_REBUILD_EXECUTOR = Executors.newFixedThreadPool(10); public CacheClient(StringRedisTemplate stringRedisTemplate) { this.stringRedisTemplate = stringRedisTemplate; } public void set(String key, Object value, Long time, TimeUnit unit) { stringRedisTemplate.opsForValue().set(key, JSONUtil.toJsonStr(value), time, unit); } public void setWithLogicalExpire(String key, Object value, Long time, TimeUnit unit) { // 设置逻辑过期 RedisData redisData = new RedisData(); redisData.setData(value); redisData.setExpireTime(LocalDateTime.now().plusSeconds(unit.toSeconds(time))); // 写入Redis stringRedisTemplate.opsForValue().set(key, JSONUtil.toJsonStr(redisData)); } public \u003cR,ID\u003e R queryWithPassThrough( String keyPrefix, ID id, Class\u003cR\u003e type, Function\u003cID, R\u003e dbFallback, Long time, TimeUnit unit){ String key = keyPrefix + id; // 1.从redis查询商铺缓存 String json = stringRedisTemplate.opsForValue().get(key); // 2.判断是否存在 if (StrUtil.isNotBlank(json)) { // 3.存在，直接返回 return JSONUtil.toBean(json, type); } // 判断命中的是否是空值 if (json != null) { // 返回一个错误信息 return null; } // 4.不存在，根据id查询数据库 R r = dbFallback.apply(id); // 5.不存在，返回错误 if (r == null) { // 将空值写入redis stringRedisTemplate.opsForValue().set(key, \"\", CACHE_NULL_TTL, TimeUnit.MINUTES); // 返回错误信息 return null; } // 6.存在，写入redis this.set(key, r, time, unit); return r; } public \u003cR, ID\u003e R queryWithLogicalExpire( String keyPrefix, ID id, Class\u003cR\u003e type, Function\u003cID, R\u003e dbFallback, Long time, TimeUnit unit) { String key = keyPrefix + id; // 1.从redis查询商铺缓存 String json = stringRedisTemplate.opsForValue().get(key); // 2.判断是否存在 if (StrUtil.isBlank(json)) { // 3.存在，直接返回 return null; } // 4.命中，需要先把json反序列化为对象 RedisData redisData = JSONUtil.toBean(json, RedisData.class); R r = JSONUtil.toBean((JSONObject) redisData.getData(), type); LocalDateTime expireTime = redisData.getExpireTime(); // 5.判断是否过期 if(expireTime.isAfter(LocalDateTime.now())) { // 5.1.未过期，直接返回店铺信息 return r; } // 5.2.已过期，需要缓存重建 // 6.缓存重建 // 6.1.获取互斥锁 String lockKey = LOCK_SHOP_KEY + id; boolean isLock = tryLock(lockKey); // 6.2.判断是否获取锁成功 if (isLock){ // 6.3.成功，开启独立线程，实现缓存重建 CACHE_REBUILD_EXECUTOR.submit(() -\u003e { try { // 查询数据库 R newR = dbFallback.apply(id); // 重建缓存 this.setWithLogicalExpire(key, newR, time, unit); } catch (Exception e) { throw new RuntimeException(e); }finally { // 释放锁 unlock(lockKey); } }); } // 6.4.返回过期的商铺信息 return r; } public \u003cR, ID\u003e R queryWithMutex( String keyPrefix, ID id, Class\u003cR\u003e type, Function\u003cID, R\u003e dbFallback, Long time, TimeUnit unit) { String key = keyPrefix + id; // 1.从redis查询商铺缓存 String shopJson = stringRedisTemplate.opsForValue().get(key); // 2.判断是否存在 if (StrUtil.isNotBlank(shopJson)) { // 3.存在，直接返回 return JSONUtil.toBean(shopJson, type); } // 判断命中的是否是空值 if (shopJson != null) { // 返回一个错误信息 return null; } // 4.实现缓存重建 // 4.1.获取互斥锁 String lockKey = LOCK_SHOP_KEY + id; R r = null; try { boolean isLock = tryLock(lockKey); // 4.2.判断是否获取成功 if (!isLock) { // 4.3.获取锁失败，休眠并重试 Thread.sleep(50); return queryWithMutex(keyPrefix, id, type, dbFallback, time, unit); } // 4.4.获取锁成功，根据id查询数据库 r = dbFallback.apply(id); // 5.不存在，返回错误 if (r == null) { // 将空值写入redis stringRedisTemplate.opsForValue().set(key, \"\", CACHE_NULL_TTL, TimeUnit.MINUTES); // 返回错误信息 return null; } // 6.存在，写入redis this.set(key, r, time, unit); } catch (InterruptedException e) { throw new RuntimeException(e); }finally { // 7.释放锁 unlock(lockKey); } // 8.返回 return r; } private boolean tryLock(String key) { Boolean flag = stringRedisTemplate.opsForValue().setIfAbsent(key, \"1\", 10, TimeUnit.SECONDS); return BooleanUtil.isTrue(flag); } private void unlock(String key) { stringRedisTemp","date":"2023-01-09","objectID":"/redis%E5%BA%94%E7%94%A8/:3:11","tags":[],"title":"Redis应用","uri":"/redis%E5%BA%94%E7%94%A8/"},{"categories":["redis"],"content":"3、优惠卷秒杀 ","date":"2023-01-09","objectID":"/redis%E5%BA%94%E7%94%A8/:4:0","tags":[],"title":"Redis应用","uri":"/redis%E5%BA%94%E7%94%A8/"},{"categories":["redis"],"content":"3.1 全局唯一ID 每个店铺都可以发布优惠券： 当用户抢购时，就会生成订单并保存到tb_voucher_order这张表中，而订单表如果使用数据库自增ID就存在一些问题： id的规律性太明显 受单表数据量的限制 场景分析：如果我们的id具有太明显的规则，用户或者说商业对手很容易猜测出来我们的一些敏感信息，比如商城在一天时间内，卖出了多少单，这明显不合适。 场景分析二：随着我们商城规模越来越大，mysql的单表的容量不宜超过500W，数据量过大之后，我们要进行拆库拆表，但拆分表了之后，他们从逻辑上讲他们是同一张表，所以他们的id是不能一样的， 于是乎我们需要保证id的唯一性。 全局ID生成器，是一种在分布式系统下用来生成全局唯一ID的工具，一般要满足下列特性： 为了增加ID的安全性，我们可以不直接使用Redis自增的数值，而是拼接一些其它信息： ID的组成部分：符号位：1bit，永远为0 时间戳：31bit，以秒为单位，可以使用69年 序列号：32bit，秒内的计数器，支持每秒产生2^32个不同ID ","date":"2023-01-09","objectID":"/redis%E5%BA%94%E7%94%A8/:4:1","tags":[],"title":"Redis应用","uri":"/redis%E5%BA%94%E7%94%A8/"},{"categories":["redis"],"content":"3.2 Redis实现全局唯一Id @Component public class RedisIdWorker { /** * 开始时间戳 */ private static final long BEGIN_TIMESTAMP = 1640995200L; /** * 序列号的位数 */ private static final int COUNT_BITS = 32; private StringRedisTemplate stringRedisTemplate; public RedisIdWorker(StringRedisTemplate stringRedisTemplate) { this.stringRedisTemplate = stringRedisTemplate; } public long nextId(String keyPrefix) { // 1.生成时间戳 LocalDateTime now = LocalDateTime.now(); long nowSecond = now.toEpochSecond(ZoneOffset.UTC); long timestamp = nowSecond - BEGIN_TIMESTAMP; // 2.生成序列号 // 2.1.获取当前日期，精确到天 String date = now.format(DateTimeFormatter.ofPattern(\"yyyy:MM:dd\")); // 2.2.自增长 long count = stringRedisTemplate.opsForValue().increment(\"icr:\" + keyPrefix + \":\" + date); // 3.拼接并返回 return timestamp \u003c\u003c COUNT_BITS | count; } } 测试类 知识小贴士：关于countdownlatch countdownlatch名为信号枪：主要的作用是同步协调在多线程的等待于唤醒问题 我们如果没有CountDownLatch ，那么由于程序是异步的，当异步程序没有执行完时，主线程就已经执行完了，然后我们期望的是分线程全部走完之后，主线程再走，所以我们此时需要使用到CountDownLatch CountDownLatch 中有两个最重要的方法 1、countDown 2、await await 方法 是阻塞方法，我们担心分线程没有执行完时，main线程就先执行，所以使用await可以让main线程阻塞，那么什么时候main线程不再阻塞呢？当CountDownLatch 内部维护的 变量变为0时，就不再阻塞，直接放行，那么什么时候CountDownLatch 维护的变量变为0 呢，我们只需要调用一次countDown ，内部变量就减少1，我们让分线程和变量绑定， 执行完一个分线程就减少一个变量，当分线程全部走完，CountDownLatch 维护的变量就是0，此时await就不再阻塞，统计出来的时间也就是所有分线程执行完后的时间。 @Test void testIdWorker() throws InterruptedException { CountDownLatch latch = new CountDownLatch(300); Runnable task = () -\u003e { for (int i = 0; i \u003c 100; i++) { long id = redisIdWorker.nextId(\"order\"); System.out.println(\"id = \" + id); } latch.countDown(); }; long begin = System.currentTimeMillis(); for (int i = 0; i \u003c 300; i++) { es.submit(task); } latch.await(); long end = System.currentTimeMillis(); System.out.println(\"time = \" + (end - begin)); } ","date":"2023-01-09","objectID":"/redis%E5%BA%94%E7%94%A8/:4:2","tags":[],"title":"Redis应用","uri":"/redis%E5%BA%94%E7%94%A8/"},{"categories":["redis"],"content":"3.3 添加优惠卷 每个店铺都可以发布优惠券，分为平价券和特价券。平价券可以任意购买，而特价券需要秒杀抢购： tb_voucher：优惠券的基本信息，优惠金额、使用规则等 tb_seckill_voucher：优惠券的库存、开始抢购时间，结束抢购时间。特价优惠券才需要填写这些信息 平价卷由于优惠力度并不是很大，所以是可以任意领取 而代金券由于优惠力度大，所以像第二种卷，就得限制数量，从表结构上也能看出，特价卷除了具有优惠卷的基本信息以外，还具有库存，抢购时间，结束时间等等字段 **新增普通卷代码： **VoucherController @PostMapping public Result addVoucher(@RequestBody Voucher voucher) { voucherService.save(voucher); return Result.ok(voucher.getId()); } 新增秒杀卷代码： VoucherController @PostMapping(\"seckill\") public Result addSeckillVoucher(@RequestBody Voucher voucher) { voucherService.addSeckillVoucher(voucher); return Result.ok(voucher.getId()); } VoucherServiceImpl @Override @Transactional public void addSeckillVoucher(Voucher voucher) { // 保存优惠券 save(voucher); // 保存秒杀信息 SeckillVoucher seckillVoucher = new SeckillVoucher(); seckillVoucher.setVoucherId(voucher.getId()); seckillVoucher.setStock(voucher.getStock()); seckillVoucher.setBeginTime(voucher.getBeginTime()); seckillVoucher.setEndTime(voucher.getEndTime()); seckillVoucherService.save(seckillVoucher); // 保存秒杀库存到Redis中 stringRedisTemplate.opsForValue().set(SECKILL_STOCK_KEY + voucher.getId(), voucher.getStock().toString()); } ","date":"2023-01-09","objectID":"/redis%E5%BA%94%E7%94%A8/:4:3","tags":[],"title":"Redis应用","uri":"/redis%E5%BA%94%E7%94%A8/"},{"categories":["redis"],"content":"3.4 实现秒杀下单 下单核心思路：当我们点击抢购时，会触发右侧的请求，我们只需要编写对应的controller即可 秒杀下单应该思考的内容： 下单时需要判断两点： 秒杀是否开始或结束，如果尚未开始或已经结束则无法下单 库存是否充足，不足则无法下单 下单核心逻辑分析： 当用户开始进行下单，我们应当去查询优惠卷信息，查询到优惠卷信息，判断是否满足秒杀条件 比如时间是否充足，如果时间充足，则进一步判断库存是否足够，如果两者都满足，则扣减库存，创建订单，然后返回订单id，如果有一个条件不满足则直接结束。 VoucherOrderServiceImpl @Override public Result seckillVoucher(Long voucherId) { // 1.查询优惠券 SeckillVoucher voucher = seckillVoucherService.getById(voucherId); // 2.判断秒杀是否开始 if (voucher.getBeginTime().isAfter(LocalDateTime.now())) { // 尚未开始 return Result.fail(\"秒杀尚未开始！\"); } // 3.判断秒杀是否已经结束 if (voucher.getEndTime().isBefore(LocalDateTime.now())) { // 尚未开始 return Result.fail(\"秒杀已经结束！\"); } // 4.判断库存是否充足 if (voucher.getStock() \u003c 1) { // 库存不足 return Result.fail(\"库存不足！\"); } //5，扣减库存 boolean success = seckillVoucherService.update() .setSql(\"stock= stock -1\") .eq(\"voucher_id\", voucherId).update(); if (!success) { //扣减库存 return Result.fail(\"库存不足！\"); } //6.创建订单 VoucherOrder voucherOrder = new VoucherOrder(); // 6.1.订单id long orderId = redisIdWorker.nextId(\"order\"); voucherOrder.setId(orderId); // 6.2.用户id Long userId = UserHolder.getUser().getId(); voucherOrder.setUserId(userId); // 6.3.代金券id voucherOrder.setVoucherId(voucherId); save(voucherOrder); return Result.ok(orderId); } ","date":"2023-01-09","objectID":"/redis%E5%BA%94%E7%94%A8/:4:4","tags":[],"title":"Redis应用","uri":"/redis%E5%BA%94%E7%94%A8/"},{"categories":["redis"],"content":"3.5 库存超卖问题分析 有关超卖问题分析：在我们原有代码中是这么写的 if (voucher.getStock() \u003c 1) { // 库存不足 return Result.fail(\"库存不足！\"); } //5，扣减库存 boolean success = seckillVoucherService.update() .setSql(\"stock= stock -1\") .eq(\"voucher_id\", voucherId).update(); if (!success) { //扣减库存 return Result.fail(\"库存不足！\"); } 假设线程1过来查询库存，判断出来库存大于1，正准备去扣减库存，但是还没有来得及去扣减，此时线程2过来，线程2也去查询库存，发现这个数量一定也大于1，那么这两个线程都会去扣减库存，最终多个线程相当于一起去扣减库存，此时就会出现库存的超卖问题。 超卖问题是典型的多线程安全问题，针对这一问题的常见解决方案就是加锁：而对于加锁，我们通常有两种解决方案：见下图： 悲观锁： 悲观锁可以实现对于数据的串行化执行，比如syn，和lock都是悲观锁的代表，同时，悲观锁中又可以再细分为公平锁，非公平锁，可重入锁，等等 乐观锁： 乐观锁：会有一个版本号，每次操作数据会对版本号+1，再提交回数据时，会去校验是否比之前的版本大1 ，如果大1 ，则进行操作成功，这套机制的核心逻辑在于，如果在操作过程中，版本号只比原来大1 ，那么就意味着操作过程中没有人对他进行过修改，他的操作就是安全的，如果不大1，则数据被修改过，当然乐观锁还有一些变种的处理方式比如cas 乐观锁的典型代表：就是cas，利用cas进行无锁化机制加锁，var5 是操作前读取的内存值，while中的var1+var2 是预估值，如果预估值 == 内存值，则代表中间没有被人修改过，此时就将新值去替换 内存值 其中do while 是为了在操作失败时，再次进行自旋操作，即把之前的逻辑再操作一次。==\u003e一种自旋锁实现 int var5; do { var5 = this.getIntVolatile(var1, var2); } while(!this.compareAndSwapInt(var1, var2, var5, var5 + var4)); return var5; 课程中的使用方式： 课程中的使用方式是没有像cas一样带自旋的操作，也没有对version的版本号+1 ，他的操作逻辑是在操作时，对版本号进行+1 操作，然后要求version 如果是1 的情况下，才能操作，那么第一个线程在操作后，数据库中的version变成了2，但是他自己满足version=1 ，所以没有问题，此时线程2执行，线程2 最后也需要加上条件version =1 ，但是现在由于线程1已经操作过了，所以线程2，操作时就不满足version=1 的条件了，所以线程2无法执行成功 ","date":"2023-01-09","objectID":"/redis%E5%BA%94%E7%94%A8/:4:5","tags":[],"title":"Redis应用","uri":"/redis%E5%BA%94%E7%94%A8/"},{"categories":["redis"],"content":"3.6 乐观锁解决超卖问题 修改代码方案一、 VoucherOrderServiceImpl 在扣减库存时，改为： boolean success = seckillVoucherService.update() .setSql(\"stock= stock -1\") //set stock = stock -1 .eq(\"voucher_id\", voucherId).eq(\"stock\",voucher.getStock()).update(); //where id = ？ and stock = ? # 等于先前的库存再做扣减 以上逻辑的核心含义是：只要我扣减库存时的库存和之前我查询到的库存是一样的，就意味着没有人在中间修改过库存，那么此时就是安全的，但是以上这种方式通过测试发现会有很多失败的情况，失败的原因在于：在使用乐观锁过程中假设100个线程同时都拿到了100的库存，然后大家一起去进行扣减，但是100个人中只有1个人能扣减成功，其他的人在处理时，他们在扣减时，库存已经被修改过了，所以此时其他线程都会失败。显然这样的乐观锁实现不能出现在实际生产环境中。 修改代码方案二、 之前的方式要修改前后都保持一致，但是这样我们分析过，成功的概率太低，所以我们的乐观锁需要变一下，改成stock大于0 即可 boolean success = seckillVoucherService.update() .setSql(\"stock= stock -1\") .eq(\"voucher_id\", voucherId).update().gt(\"stock\",0); //where id = ? and stock \u003e 0 解决方案二的实现解释：库存只要剩余，就可以扣减。假设一个场景：当前库存剩余1个，现在有三个线程同时发起请求，同时到达数据库，那么只会有一个线程获得记录的行锁，去执行库存扣减（写锁），其他线程不论是读还是写都会被阻塞。这样库存扣减成功过后，行锁得到释放；其他线程再来更新就会发现当前库存已经清空，就会失败返回。所以归根结底，方案二的实现依赖了MySQL默认存储引擎InnoDB的行锁机制，因为是update语句，MySQL会默认加上行锁，但是有其他非主键或非唯一索引列，行锁会退化成粒度更大的gap lock 知识小扩展： 针对cas中的自旋压力过大，我们可以使用Longaddr这个类去解决 Java8 提供的一个对AtomicLong改进后的一个类，LongAdder 大量线程并发更新一个原子性的时候，天然的问题就是自旋，会导致并发性问题，当然这也比我们直接使用syn来的好 所以利用这么一个类，LongAdder来进行优化 如果获取某个值，则会对cell和base的值进行递增，最后返回一个完整的值 ","date":"2023-01-09","objectID":"/redis%E5%BA%94%E7%94%A8/:4:6","tags":[],"title":"Redis应用","uri":"/redis%E5%BA%94%E7%94%A8/"},{"categories":["redis"],"content":"3.6 优惠券秒杀-一人一单 需求：修改秒杀业务，要求同一个优惠券，一个用户只能下一单 现在的问题在于： 优惠卷是为了引流，但是目前的情况是，一个人可以无限制的抢这个优惠卷，所以我们应当增加一层逻辑，让一个用户只能下一个单，而不是让一个用户下多个单 具体操作逻辑如下：比如时间是否充足，如果时间充足，则进一步判断库存是否足够，然后再根据优惠卷id和用户id查询是否已经下过这个订单，如果下过这个订单， 不再下单，否则进行下单 VoucherOrderServiceImpl 初步代码：增加一人一单逻辑 @Override public Result seckillVoucher(Long voucherId) { // 1.查询优惠券 SeckillVoucher voucher = seckillVoucherService.getById(voucherId); // 2.判断秒杀是否开始 if (voucher.getBeginTime().isAfter(LocalDateTime.now())) { // 尚未开始 return Result.fail(\"秒杀尚未开始！\"); } // 3.判断秒杀是否已经结束 if (voucher.getEndTime().isBefore(LocalDateTime.now())) { // 尚未开始 return Result.fail(\"秒杀已经结束！\"); } // 4.判断库存是否充足 if (voucher.getStock() \u003c 1) { // 库存不足 return Result.fail(\"库存不足！\"); } // 5.一人一单逻辑 // 5.1.用户id Long userId = UserHolder.getUser().getId(); int count = query().eq(\"user_id\", userId).eq(\"voucher_id\", voucherId).count(); // 5.2.判断是否存在 if (count \u003e 0) { // 用户已经购买过了 return Result.fail(\"用户已经购买过一次！\"); } //6，扣减库存 boolean success = seckillVoucherService.update() .setSql(\"stock= stock -1\") .eq(\"voucher_id\", voucherId).update(); if (!success) { //扣减库存 return Result.fail(\"库存不足！\"); } //7.创建订单 VoucherOrder voucherOrder = new VoucherOrder(); // 7.1.订单id long orderId = redisIdWorker.nextId(\"order\"); voucherOrder.setId(orderId); voucherOrder.setUserId(userId); // 7.3.代金券id voucherOrder.setVoucherId(voucherId); save(voucherOrder); return Result.ok(orderId); } **存在问题：**现在的问题还是和之前一样，并发过来（可能用户贪图优惠劵，会利用科技在一瞬间发起大量并发请求，以获得大量优惠订单），查询数据库，一开始都不存在订单（MySQL查询默认都是不加锁的），所以我们还是需要加锁，但是乐观锁比较适合更新数据，而现在是插入数据，所以我们需要使用悲观锁操作 **注意：**在这里提到了非常多的问题，我们需要慢慢的来思考，首先我们的初始方案是封装了一个createVoucherOrder方法，同时为了确保他线程安全，在方法上添加了一把synchronized 锁。 当然经过分析，我们发现并发问题主要出现在查询订单这里，我们可以考虑这样两种处理方案： 代码层面：在出现并发问题根源的关键代码片段处，加上互斥锁，这样加锁的粒度正好合适 数据库层面：关键语句上我们使用数据库的锁，来进行阻塞其他线程并发请求带来的问题 个人认为，最好是数据库层面进行加锁，因为数据库层面的死锁问题会直接报错返回，但是程序层面的死锁问题，服务可能会直接崩溃掉 @Transactional public synchronized Result createVoucherOrder(Long voucherId) { Long userId = UserHolder.getUser().getId(); // 5.1.查询订单 int count = query().eq(\"user_id\", userId).eq(\"voucher_id\", voucherId).count(); // 5.2.判断是否存在 if (count \u003e 0) { // 用户已经购买过了 return Result.fail(\"用户已经购买过一次！\"); } // 6.扣减库存 boolean success = seckillVoucherService.update() .setSql(\"stock = stock - 1\") // set stock = stock - 1 .eq(\"voucher_id\", voucherId).gt(\"stock\", 0) // where id = ? and stock \u003e 0 .update(); if (!success) { // 扣减失败 return Result.fail(\"库存不足！\"); } // 7.创建订单 VoucherOrder voucherOrder = new VoucherOrder(); // 7.1.订单id long orderId = redisIdWorker.nextId(\"order\"); voucherOrder.setId(orderId); // 7.2.用户id voucherOrder.setUserId(userId); // 7.3.代金券id voucherOrder.setVoucherId(voucherId); save(voucherOrder); // 7.返回订单id return Result.ok(orderId); } ，但是这样添加锁，锁的粒度太粗了，在使用锁过程中，控制锁粒度 是一个非常重要的事情，因为如果锁的粒度太大，会导致每个线程进来都会锁住，所以我们需要去控制锁的粒度，以下这段代码需要修改为： intern() 这个方法是从常量池中拿到数据，如果我们直接使用userId.toString() 他拿到的对象实际上是不同的对象，new出来的对象，我们使用锁必须保证锁必须是同一把，所以我们需要使用intern()方法 @Transactional public Result createVoucherOrder(Long voucherId) { Long userId = UserHolder.getUser().getId(); synchronized(userId.toString().intern()){ // 5.1.查询订单 int count = query().eq(\"user_id\", userId).eq(\"voucher_id\", voucherId).count(); // 5.2.判断是否存在 if (count \u003e 0) { // 用户已经购买过了 return Result.fail(\"用户已经购买过一次！\"); } // 6.扣减库存 boolean success = seckillVoucherService.update() .setSql(\"stock = stock - 1\") // set stock = stock - 1 .eq(\"voucher_id\", voucherId).gt(\"stock\", 0) // where id = ? and stock \u003e 0 .update(); if (!success) { // 扣减失败 return Result.fail(\"库存不足！\"); } // 7.创建订单 VoucherOrder voucherOrder = new VoucherOrder(); // 7.1.订单id long orderId = redisIdWorker.nextId(\"order\"); voucherOrder.setId(orderId); // 7.2.用户id voucherOrder.setUserId(userId); // 7.3.代金券id voucherOrder.setVoucherId(voucherId); save(voucherOrder); // 7.返回订单id return Result.ok(orderId); } } 但是以上代码还是存在问题，问题的原因在于当前方法被spring的事务控制，如果你在方法内部加锁，可能会导致当前方法事务还没有提交，但是锁已经释放也会导致问题，所以我们选择将当前方法整体包裹起来，确保事务不会出现问题：如下： 在seckillVoucher 方法中，添加以下逻辑，这样就能保证事务的特性，同时也控制了锁的粒度 但是以上做法依然有问题，因为你调用的方法，其实是this.的方式调用的，事务想要生效，还得利用代理来生效，所以这个地方，我们需要获得原始的事务对象， 来操作事务 ","date":"2023-01-09","objectID":"/redis%E5%BA%94%E7%94%A8/:4:7","tags":[],"title":"Redis应用","uri":"/redis%E5%BA%94%E7%94%A8/"},{"categories":["redis"],"content":"3.7 集群环境下的并发问题 通过加锁可以解决在单机情况下的一人一单安全问题，但是在集群模式下就不行了。 1、我们将服务启动两份，端口分别为8081和8082： 2、然后修改nginx的conf目录下的nginx.conf文件，配置反向代理和负载均衡： 具体操作(略) 有关锁失效原因分析 由于现在我们部署了多个tomcat，每个tomcat都有一个属于自己的jvm，那么假设在服务器A的tomcat内部，有两个线程，这两个线程由于使用的是同一份代码，那么他们的锁对象是同一个，是可以实现互斥的，但是如果现在是服务器B的tomcat内部，又有两个线程，但是他们的锁对象写的虽然和服务器A一样，但是锁对象却不是同一个，所以线程3和线程4可以实现互斥，但是却无法和线程1和线程2实现互斥，这就是集群环境下，syn锁失效的原因，在这种情况下，我们就需要使用分布式锁来解决这个问题。 ","date":"2023-01-09","objectID":"/redis%E5%BA%94%E7%94%A8/:4:8","tags":[],"title":"Redis应用","uri":"/redis%E5%BA%94%E7%94%A8/"},{"categories":["redis"],"content":"4、分布式锁 ","date":"2023-01-09","objectID":"/redis%E5%BA%94%E7%94%A8/:5:0","tags":[],"title":"Redis应用","uri":"/redis%E5%BA%94%E7%94%A8/"},{"categories":["redis"],"content":"4.1 基本原理和实现方式对比 分布式锁：满足分布式系统或集群模式下多进程可见并且互斥的锁。 分布式锁的核心思想就是让大家（不同的线程之间，包括跨服务器级的不同线程）都使用同一把锁，只要大家使用的是同一把锁，那么我们就能锁住当前线程，让其他线程进行自旋等待，让程序串行执行，这就是分布式锁的核心思路。虽然降低了程序的并发性能，但是为了数据安全，这是必要的 那么分布式锁他应该满足一些什么样的条件呢？ 可见性：多个线程都能看到相同的结果，注意：这个地方说的可见性并不是并发编程中指的内存可见性，只是说多个进程之间都能感知到变化的意思 互斥：互斥是分布式锁的最基本的条件，使得程序串行执行 高可用：程序不易崩溃，时时刻刻都保证较高的可用性 高性能：由于加锁本身就让性能降低，所有对于分布式锁本身需要他就较高的加锁性能和释放锁性能 安全性：安全也是程序中必不可少的一环 常见的分布式锁有三种 Mysql：mysql本身就带有锁机制，但是由于mysql性能本身一般，所以采用分布式锁的情况下，其实使用mysql作为分布式锁比较少见 Redis：redis作为分布式锁是非常常见的一种使用方式，现在企业级开发中基本都使用redis或者zookeeper作为分布式锁，利用setnx这个方法，如果插入key成功，则表示获得到了锁，如果有人插入成功，其他人插入失败则表示无法获得到锁，利用这套逻辑来实现分布式锁 Zookeeper：zookeeper也是企业级开发中较好的一个实现分布式锁的方案，由于本套视频并不讲解zookeeper的原理和分布式锁的实现，所以不过多阐述 ","date":"2023-01-09","objectID":"/redis%E5%BA%94%E7%94%A8/:5:1","tags":[],"title":"Redis应用","uri":"/redis%E5%BA%94%E7%94%A8/"},{"categories":["redis"],"content":"4.2 Redis分布式锁的实现核心思路 实现分布式锁时需要实现的两个基本方法： 获取锁： 互斥：确保只能有一个线程获取锁 非阻塞：尝试一次，成功返回true，失败返回false 释放锁： 手动释放 超时释放：获取锁时添加一个超时时间 核心思路： 我们利用 redis 的 setNx 方法，当有多个线程进入时，我们就利用该方法，第一个线程进入时，redis 中就有这个key 了，返回了1，如果结果是1，则表示他抢到了锁，那么他去执行业务，然后再删除锁，退出锁逻辑，没有抢到锁的哥们，等待一定时间后重试即可 ","date":"2023-01-09","objectID":"/redis%E5%BA%94%E7%94%A8/:5:2","tags":[],"title":"Redis应用","uri":"/redis%E5%BA%94%E7%94%A8/"},{"categories":["redis"],"content":"4.3 实现分布式锁版本一 加锁逻辑 锁的基本接口 SimpleRedisLock 利用setnx方法进行加锁，同时增加过期时间，防止死锁，此方法可以保证加锁和增加过期时间具有原子性 private static final String KEY_PREFIX=\"lock:\" @Override public boolean tryLock(long timeoutSec) { // 获取线程标示 String threadId = Thread.currentThread().getId(); // 获取锁 Boolean success = stringRedisTemplate.opsForValue() .setIfAbsent(KEY_PREFIX + name, threadId + \"\", timeoutSec, TimeUnit.SECONDS); return Boolean.TRUE.equals(success); } 释放锁逻辑 SimpleRedisLock 释放锁 public void unlock() { //通过del删除锁 stringRedisTemplate.delete(KEY_PREFIX + name); } 修改业务代码 @Override public Result seckillVoucher(Long voucherId) { // 1.查询优惠券 SeckillVoucher voucher = seckillVoucherService.getById(voucherId); // 2.判断秒杀是否开始 if (voucher.getBeginTime().isAfter(LocalDateTime.now())) { // 尚未开始 return Result.fail(\"秒杀尚未开始！\"); } // 3.判断秒杀是否已经结束 if (voucher.getEndTime().isBefore(LocalDateTime.now())) { // 尚未开始 return Result.fail(\"秒杀已经结束！\"); } // 大量进程同时涌入时，由于默认MySQL的InnoDB没有对查询做互斥操作，会导致并发读出相同的那一个数据， // 假设这时库存刚好还剩一件商品，那么后续的业务就会存在超卖问题 // 解决的方案：1.将MySQL数据库的InnoDB存储引擎默认隔离级别REAPETABLE改成串行化级别，永久解决并发问题。 // 但是这种方案带来的并发性能会骤降，而且也会将一些其他业务的并发性能受到阻塞，所以一般都不会采用这种方案 // 2.还有一种方案，就是局部加锁。我们在执行这个关键操作时，为了保证不能大量进程并发地读，需要在读的时候加上锁。 // 当然上锁的方案很多：比如在业务代码层面上锁，或者数据库层面上锁（for update），当然为了控制这个上锁的粒度 // ，最好选择在数据库层面上锁，可以控制上锁的粒度在那一条sql语句上，同时，死锁问题就不需要我们操心了 // 4.判断库存是否充足 if (voucher.getStock() \u003c 1) { // 库存不足 return Result.fail(\"库存不足！\"); } Long userId = UserHolder.getUser().getId(); //创建锁对象(新增代码) SimpleRedisLock lock = new SimpleRedisLock(\"order:\" + userId, stringRedisTemplate); //获取锁对象 boolean isLock = lock.tryLock(1200); //加锁失败 if (!isLock) { return Result.fail(\"不允许重复下单\"); } try { //获取代理对象(事务) IVoucherOrderService proxy = (IVoucherOrderService) AopContext.currentProxy(); return proxy.createVoucherOrder(voucherId); } finally { //释放锁 lock.unlock(); } } ","date":"2023-01-09","objectID":"/redis%E5%BA%94%E7%94%A8/:5:3","tags":[],"title":"Redis应用","uri":"/redis%E5%BA%94%E7%94%A8/"},{"categories":["redis"],"content":"4.4 Redis分布式锁误删情况说明 逻辑说明： 持有锁的线程在锁的内部出现了阻塞，导致他的锁超时自动释放，这时其他线程，线程2来尝试获得锁，就拿到了这把锁，然后线程2在持有锁执行过程中，线程1反应过来，继续执行，而线程1执行过程中，走到了删除锁逻辑，此时就会把本应该属于线程2的锁进行删除，这就是误删别人锁的情况说明 解决方案：解决方案就是在每个线程释放锁的时候，去判断一下当前这把锁是否属于自己，如果不属于自己，则不进行锁的删除，假设还是上边的情况，线程1卡顿，锁自动释放，线程2进入到锁的内部执行逻辑，此时线程1反应过来，然后删除锁，但是线程1，一看当前这把锁不是属于自己，于是不进行删除锁逻辑，当线程2走到删除锁逻辑时，如果没有卡过自动释放锁的时间点，则判断当前这把锁是属于自己的，于是删除这把锁。 ","date":"2023-01-09","objectID":"/redis%E5%BA%94%E7%94%A8/:5:4","tags":[],"title":"Redis应用","uri":"/redis%E5%BA%94%E7%94%A8/"},{"categories":["redis"],"content":"4.5 解决Redis分布式锁误删问题 需求：修改之前的分布式锁实现，满足：在获取锁时存入线程标示（可以用UUID表示） 在释放锁时先获取锁中的线程标示，判断是否与当前线程标示一致。当然线程的标识应当满足在分布式的服务器架构下线程标识唯一， 如果一致则释放锁 如果不一致则不释放锁 核心逻辑：在存入锁时，放入自己线程的标识，在删除锁时，判断当前这把锁的标识是不是自己存入的，如果是，则进行删除，如果不是，则不进行删除。 具体代码如下：加锁 private static final String ID_PREFIX = UUID.randomUUID().toString(true) + \"-\"; @Override public boolean tryLock(long timeoutSec) { // 获取线程标示 String threadId = ID_PREFIX + Thread.currentThread().getId(); // 获取锁 Boolean success = stringRedisTemplate.opsForValue() .setIfAbsent(KEY_PREFIX + name, threadId, timeoutSec, TimeUnit.SECONDS); return Boolean.TRUE.equals(success); } 释放锁 public void unlock() { // 获取线程标示 String threadId = ID_PREFIX + Thread.currentThread().getId(); // 获取锁中的标示 String id = stringRedisTemplate.opsForValue().get(KEY_PREFIX + name); // 判断标示是否一致 if(threadId.equals(id)) { // 释放锁 stringRedisTemplate.delete(KEY_PREFIX + name); } } 有关代码实操说明： 在我们修改完此处代码后，我们重启工程，然后启动两个线程，第一个线程持有锁后，手动释放锁，第二个线程此时进入到锁内部，再放行第一个线程，此时第一个线程由于锁的value值并非是自己，所以不能释放锁，也就无法删除别人的锁，此时第二个线程能够正确释放锁，通过这个案例初步说明我们解决了锁误删的问题。 ","date":"2023-01-09","objectID":"/redis%E5%BA%94%E7%94%A8/:5:5","tags":[],"title":"Redis应用","uri":"/redis%E5%BA%94%E7%94%A8/"},{"categories":["redis"],"content":"4.6 分布式锁的原子性问题 更为极端的误删逻辑说明： 线程1现在持有锁之后，在执行业务逻辑过程中，他正准备删除锁，而且已经走到了条件判断的过程中，比如他已经拿到了当前这把锁确实是属于他自己的，正准备删除锁，但是此时线程1突然阻塞了，那么此时线程2进来，但是线程1他会接着往后执行，当他阻塞结束后，他直接就会执行删除锁那行代码，相当于条件判断并没有起到作用，这就是删锁时的原子性问题，之所以有这个问题，是因为线程1的拿锁，比锁，删锁，实际上并不是原子性的，我们要防止刚才的情况发生， ","date":"2023-01-09","objectID":"/redis%E5%BA%94%E7%94%A8/:5:6","tags":[],"title":"Redis应用","uri":"/redis%E5%BA%94%E7%94%A8/"},{"categories":["redis"],"content":"4.7 Lua脚本解决多条命令原子性问题 Redis提供了Lua脚本功能，在一个脚本中编写多条Redis命令，确保多条命令执行时的原子性。Lua是一种编程语言，它的基本语法大家可以参考网站：https://www.runoob.com/lua/lua-tutorial.html，这里重点介绍Redis提供的调用函数，我们可以使用lua去操作redis，又能保证他的原子性，这样就可以实现拿锁比锁删锁是一个原子性动作了，作为Java程序员这一块并不作一个简单要求，并不需要大家过于精通，只需要知道他有什么作用即可。 这里重点介绍Redis提供的调用函数，语法如下： redis.call('命令名称', 'key', '其它参数', ...) 例如，我们要执行set name jack，则脚本是这样： -- 执行 set name jack redis.call('set', 'name', 'jack') 例如，我们要先执行set name Rose，再执行get name，则脚本如下： -- 先执行 set name jack redis.call('set', 'name', 'Rose') -- 再执行 get name local name = redis.call('get', 'name') -- 返回 return name 写好脚本以后，需要用Redis命令来调用脚本，调用脚本的常见命令如下： 例如，我们要执行 redis.call('set', 'name', 'jack') 这个脚本，语法如下： 如果脚本中的key、value不想写死，可以作为参数传递。key类型参数会放入KEYS数组，其它参数会放入ARGV数组，在脚本中可以从KEYS和ARGV数组获取这些参数： 接下来我们来回一下我们释放锁的逻辑： 释放锁的业务流程是这样的 ​ 1、获取锁中的线程标示 ​ 2、判断是否与指定的标示（当前线程标示）一致 ​ 3、如果一致则释放锁（删除） ​ 4、如果不一致则什么都不做 如果用Lua脚本来表示则是这样的： 最终我们操作redis的拿锁比锁删锁的lua脚本就会变成这样 -- 这里的 KEYS[1] 就是锁的key，这里的ARGV[1] 就是当前线程标示 -- 获取锁中的标示，判断是否与当前线程标示一致 if (redis.call('GET', KEYS[1]) == ARGV[1]) then -- 一致，则删除锁 return redis.call('DEL', KEYS[1]) end -- 不一致，则直接返回 return 0 ","date":"2023-01-09","objectID":"/redis%E5%BA%94%E7%94%A8/:5:7","tags":[],"title":"Redis应用","uri":"/redis%E5%BA%94%E7%94%A8/"},{"categories":["redis"],"content":"4.8 利用Java代码调用Lua脚本改造分布式锁 lua脚本本身并不需要大家花费太多时间去研究，只需要知道如何调用，大致是什么意思即可，所以在笔记中并不会详细的去解释这些lua表达式的含义。 我们的RedisTemplate中，可以利用execute方法去执行lua脚本，参数对应关系就如下图股 Java代码 private static final DefaultRedisScript\u003cLong\u003e UNLOCK_SCRIPT; static { UNLOCK_SCRIPT = new DefaultRedisScript\u003c\u003e(); UNLOCK_SCRIPT.setLocation(new ClassPathResource(\"unlock.lua\")); UNLOCK_SCRIPT.setResultType(Long.class); } public void unlock() { // 调用lua脚本 stringRedisTemplate.execute( UNLOCK_SCRIPT, Collections.singletonList(KEY_PREFIX + name), ID_PREFIX + Thread.currentThread().getId()); } 经过以上代码改造后，我们就能够实现 拿锁比锁删锁的原子性动作了~ 小总结： 基于Redis的分布式锁实现思路： 利用set nx ex获取锁，并设置过期时间，保存线程标示 释放锁时先判断线程标示是否与自己一致，一致则删除锁 特性： 利用set nx满足互斥性 利用set ex保证故障时锁依然能释放，避免死锁，提高安全性 利用Redis集群保证高可用和高并发特性 笔者总结：我们一路走来，利用添加过期时间，防止死锁问题的发生，但是有了过期时间之后，可能出现误删别人锁的问题，这个问题我们开始是利用删之前 通过拿锁，比锁，删锁这个逻辑来解决的，也就是删之前判断一下当前这把锁是否是属于自己的，但是现在还有原子性问题，也就是我们没法保证拿锁比锁删锁是一个原子性的动作，最后通过lua表达式来解决这个问题 但是目前还剩下一个问题锁不住（其实就是极端情况，上锁之前都所有线程阻塞了，那么这会导致没有一个线程拿到锁，也就是锁不住），什么是锁不住呢，你想一想，如果当过期时间到了之后，我们可以给他续期一下，比如续个30s，就好像是网吧上网，网费到了之后，然后说，来，网管，再给我来10块的，是不是后边的问题都不会发生了，那么续期问题怎么解决呢，可以依赖于我们接下来要学习redission啦 测试逻辑： 第一个线程进来，得到了锁，手动删除锁，模拟锁超时了，其他线程会执行lua来抢锁，当第一个线程利用lua删除锁时，lua能保证他不能删除他的锁，第二个线程删除锁时，利用lua同样可以保证不会删除别人的锁，同时还能保证原子性。 ","date":"2023-01-09","objectID":"/redis%E5%BA%94%E7%94%A8/:5:8","tags":[],"title":"Redis应用","uri":"/redis%E5%BA%94%E7%94%A8/"},{"categories":["redis"],"content":"5、分布式锁-redission ","date":"2023-01-09","objectID":"/redis%E5%BA%94%E7%94%A8/:6:0","tags":[],"title":"Redis应用","uri":"/redis%E5%BA%94%E7%94%A8/"},{"categories":["redis"],"content":"5.1 分布式锁-redission功能介绍 基于setnx实现的分布式锁存在下面的问题： 重入问题：重入问题是指 获得锁的线程可以再次进入到相同的锁的代码块中，可重入锁的意义在于防止死锁，比如HashTable这样的代码中，他的方法都是使用synchronized修饰的，假如他在一个方法内，调用另一个方法，那么此时如果是不可重入的，不就死锁了吗？所以可重入锁他的主要意义是防止死锁，我们的synchronized和Lock锁都是可重入的。 不可重试：是指目前的分布式只能尝试一次，我们认为合理的情况是：当线程在获得锁失败后，他应该能再次尝试获得锁。 **超时释放：**我们在加锁时增加了过期时间，这样的我们可以防止死锁，但是如果卡顿的时间超长，虽然我们采用了lua表达式防止删锁的时候，误删别人的锁，但是毕竟没有锁住，有安全隐患 主从一致性： 如果Redis提供了主从集群，当我们向集群写数据时，主机需要异步的将数据同步给从机，而万一在同步过去之前，主机宕机了，就会出现死锁问题。 那么什么是Redission呢 Redisson是一个在Redis的基础上实现的Java驻内存数据网格（In-Memory Data Grid）。它不仅提供了一系列的分布式的Java常用对象，还提供了许多分布式服务，其中就包含了各种分布式锁的实现。 Redission提供了分布式锁的多种多样的功能 ","date":"2023-01-09","objectID":"/redis%E5%BA%94%E7%94%A8/:6:1","tags":[],"title":"Redis应用","uri":"/redis%E5%BA%94%E7%94%A8/"},{"categories":["redis"],"content":"5.2 分布式锁-Redission快速入门 引入依赖： \u003cdependency\u003e \u003cgroupId\u003eorg.redisson\u003c/groupId\u003e \u003cartifactId\u003eredisson\u003c/artifactId\u003e \u003cversion\u003e3.13.6\u003c/version\u003e \u003c/dependency\u003e 配置Redisson客户端： @Configuration public class RedissonConfig { @Bean public RedissonClient redissonClient(){ // 配置 Config config = new Config(); config.useSingleServer().setAddress(\"redis://192.168.150.101:6379\") .setPassword(\"123321\"); // 创建RedissonClient对象 return Redisson.create(config); } } 如何使用Redission的分布式锁 @Resource private RedissionClient redissonClient; @Test void testRedisson() throws Exception{ //获取锁(可重入)，指定锁的名称 RLock lock = redissonClient.getLock(\"anyLock\"); //尝试获取锁，参数分别是：获取锁的最大等待时间(期间会重试)，锁自动释放时间，时间单位 boolean isLock = lock.tryLock(1,10,TimeUnit.SECONDS); //判断获取锁成功 if(isLock){ try{ System.out.println(\"执行业务\"); }finally{ //释放锁 lock.unlock(); } } } 在 VoucherOrderServiceImpl 注入RedissonClient @Resource private RedissonClient redissonClient; @Override public Result seckillVoucher(Long voucherId) { // 1.查询优惠券 SeckillVoucher voucher = seckillVoucherService.getById(voucherId); // 2.判断秒杀是否开始 if (voucher.getBeginTime().isAfter(LocalDateTime.now())) { // 尚未开始 return Result.fail(\"秒杀尚未开始！\"); } // 3.判断秒杀是否已经结束 if (voucher.getEndTime().isBefore(LocalDateTime.now())) { // 尚未开始 return Result.fail(\"秒杀已经结束！\"); } // 4.判断库存是否充足 if (voucher.getStock() \u003c 1) { // 库存不足 return Result.fail(\"库存不足！\"); } Long userId = UserHolder.getUser().getId(); //创建锁对象 这个代码不用了，因为我们现在要使用分布式锁 //SimpleRedisLock lock = new SimpleRedisLock(\"order:\" + userId, stringRedisTemplate); RLock lock = redissonClient.getLock(\"lock:order:\" + userId); //获取锁对象 boolean isLock = lock.tryLock(); //加锁失败 if (!isLock) { return Result.fail(\"不允许重复下单\"); } try { //获取代理对象(事务) IVoucherOrderService proxy = (IVoucherOrderService) AopContext.currentProxy(); return proxy.createVoucherOrder(voucherId); } finally { //释放锁 lock.unlock(); } } ","date":"2023-01-09","objectID":"/redis%E5%BA%94%E7%94%A8/:6:2","tags":[],"title":"Redis应用","uri":"/redis%E5%BA%94%E7%94%A8/"},{"categories":["redis"],"content":"5.3 分布式锁-redission可重入锁原理 在Lock锁中，他是借助于底层的一个voaltile的一个state变量来记录重入的状态的，比如当前没有人持有这把锁，那么state=0，假如有人持有这把锁，那么state=1，如果持有这把锁的人再次持有这把锁，那么state就会 +1 ，如果是对于synchronized而言，他在c语言代码中会有一个count，原理和state类似，也是重入一次就加一，释放一次就-1 ，直到减少成 0 时，表示当前这把锁没有被人持有。 在redission中，也支持可重入锁 在分布式锁中，他采用hash结构用来存储锁，其中大key表示这把锁是否存在，用小key表示当前这把锁被哪个线程持有，所以接下来我们一起分析一下当前的这个lua表达式 -- 锁是否存在，不存在就初始化这个锁 if (redis.call('exists', KEYS[1]) == 0) then redis.call('hset', KEYS[1], ARGV[2], 1); redis.call('pexpire', KEYS[1], ARGV[1]); return nil; end; -- 如果锁存在，就要判断锁的线程是不是当前线程标识（是否属于自己）。 -- 如果是，就将锁的值+1，然后设置过期值成功返回；如果不是就失败，返回这把锁的失效时间，用于之后的重试 if (redis.call('hexists', KEYS[1], ARGV[2]) == 1) then redis.call('hincrby', KEYS[1], ARGV[2], 1); redis.call('pexpire', KEYS[1], ARGV[1]); return nil; end; return redis.call('pttl', KEYS[1]); end; 这个地方一共有3个参数 KEYS[1]：锁名称 ARGV[1]：锁失效时间 ARGV[2]：id+\":\"+threadId =\u003e 锁的小key exists：判断lock是否存在,如果==0，就表示当前这把锁不存在 redis.call('hset', KEYS[1], ARGV[2], 1)：此时他就开始往redis里边去写数据 ，写成一个hash结构 Lock{ id+\":\"+threadId : 1 } 如果当前这把锁存在，则第一个条件不满足，再判断 redis.call('hexists', KEYS[1], ARGV[2]) == 1 此时需要通过大key+小key，判断当前这把锁是否是属于自己的，如果是自己的，则进行 redis.call('hincrby', KEYS[1], ARGV[2], 1) 将当前这个锁的value进行+1 ，redis.call('pexpire', KEYS[1], ARGV[1])，然后再对其设置过期时间，如果以上两个条件都不满足，则表示当前这把锁抢锁失败，最后返回pttl，即为当前这把锁的失效时间 如果小伙帮们看了前边的源码， 你会发现他会去判断当前这个方法的返回值是否为null，如果是null，则对应则前两个if对应的条件，退出抢锁逻辑，如果返回的不是null，即走了第三个分支，在源码处会进行while(true)的自旋抢锁。 ","date":"2023-01-09","objectID":"/redis%E5%BA%94%E7%94%A8/:6:3","tags":[],"title":"Redis应用","uri":"/redis%E5%BA%94%E7%94%A8/"},{"categories":["redis"],"content":"5.4 分布式锁-redission锁重试和WatchDog机制 说明：由于课程中已经说明了有关tryLock的源码解析以及其看门狗原理，所以笔者在这里给大家分析lock()方法的源码解析，希望大家在学习过程中，能够掌握更多的知识 抢锁过程中，获得当前线程，通过tryAcquire进行抢锁，该抢锁逻辑和之前逻辑相同 1、先判断当前这把锁是否存在，如果不存在，插入一把锁，返回null 2、判断当前这把锁是否是属于当前线程，如果是，则返回null 所以如果返回是null，则代表着当前这哥们已经抢锁完毕，或者可重入完毕，但是如果以上两个条件都不满足，则进入到第三个条件，返回的是锁的失效时间，同学们可以自行往下翻一点点，你能发现有个while( true) 再次进行tryAcquire进行抢锁 long threadId = Thread.currentThread().getId(); Long ttl = tryAcquire(-1, leaseTime, unit, threadId); // lock acquired if (ttl == null) { return; } 接下来会有一个条件分支，因为lock方法有重载方法，一个是带参数，一个是不带参数，如果带带参数传入的值是-1，如果传入参数，则leaseTime是他本身，所以如果传入了参数，此时leaseTime != -1 则会进去抢锁，抢锁的逻辑就是之前说的那三个逻辑 if (leaseTime != -1) { return tryLockInnerAsync(waitTime, leaseTime, unit, threadId, RedisCommands.EVAL_LONG); } 如果是没有传入时间，则此时也会进行抢锁， 而且抢锁时间是默认看门狗时间 commandExecutor.getConnectionManager().getCfg().getLockWatchdogTimeout() ttlRemainingFuture.onComplete((ttlRemaining, e) 这句话相当于对以上抢锁进行了监听，也就是说当上边抢锁完毕后，此方法会被调用，具体调用的逻辑就是去后台开启一个线程，进行续约逻辑，也就是看门狗线程 RFuture\u003cLong\u003e ttlRemainingFuture = tryLockInnerAsync(waitTime, commandExecutor.getConnectionManager().getCfg().getLockWatchdogTimeout(), TimeUnit.MILLISECONDS, threadId, RedisCommands.EVAL_LONG); ttlRemainingFuture.onComplete((ttlRemaining, e) -\u003e { if (e != null) { return; } // lock acquired if (ttlRemaining == null) { scheduleExpirationRenewal(threadId); } }); return ttlRemainingFuture; 此逻辑就是续约逻辑，注意看commandExecutor.getConnectionManager().newTimeout（） 此方法 Method( new TimerTask() {},参数2 ，参数3 ) 指的是：通过参数2，参数3 去描述什么时候去做参数1的事情，现在的情况是：10s之后去做参数一的事情 因为锁的失效时间是30s，当10s之后，此时这个timeTask 就触发了，他就去进行续约，把当前这把锁续约成30s，如果操作成功，那么此时就会递归调用自己，再重新设置一个timeTask()，于是再过10s后又再设置一个timerTask，完成不停的续约 那么大家可以想一想，假设我们的线程出现了宕机他还会续约吗？当然不会，因为没有人再去调用renewExpiration这个方法，所以等到时间之后自然就释放了。 private void renewExpiration() { ExpirationEntry ee = EXPIRATION_RENEWAL_MAP.get(getEntryName()); if (ee == null) { return; } Timeout task = commandExecutor.getConnectionManager().newTimeout(new TimerTask() { @Override public void run(Timeout timeout) throws Exception { ExpirationEntry ent = EXPIRATION_RENEWAL_MAP.get(getEntryName()); if (ent == null) { return; } Long threadId = ent.getFirstThreadId(); if (threadId == null) { return; } RFuture\u003cBoolean\u003e future = renewExpirationAsync(threadId); future.onComplete((res, e) -\u003e { if (e != null) { log.error(\"Can't update lock \" + getName() + \" expiration\", e); return; } if (res) { // reschedule itself renewExpiration(); } }); } }, internalLockLeaseTime / 3, TimeUnit.MILLISECONDS); ee.setTimeout(task); } ","date":"2023-01-09","objectID":"/redis%E5%BA%94%E7%94%A8/:6:4","tags":[],"title":"Redis应用","uri":"/redis%E5%BA%94%E7%94%A8/"},{"categories":["redis"],"content":"5.5 分布式锁-redission锁的MutiLock原理 为了提高redis的可用性，我们会搭建集群或者主从，现在以主从为例: 此时我们去写命令，写在主机上，主机会将数据同步给从机，但是假设在主机还没有来得及把数据写入到从机去的时候，此时主机宕机，哨兵会发现主机宕机，并且选举一个slave变成master，而此时新的master中实际上并没有锁信息，此时锁信息就已经丢掉了。 为了解决这个问题，redission提出来了MutiLock锁，使用这把锁咱们就不使用主从了，每个节点的地位都是一样的， 这把锁加锁的逻辑需要写入到每一个主丛节点上，只有所有的服务器都写入成功，此时才是加锁成功，假设现在某个节点挂了，那么他去获得锁的时候，只要有一个节点拿不到，都不能算是加锁成功，就保证了加锁的可靠性。 虽然说出来容易，但是技术实现上还是存在一定的困难：如何保证redis所有节点之间锁数据的强一致性，不同节点大概都位于不同的服务器上，那么要实现进程隔离级别的数据一致性，可以考虑TCC分布式事务方案。当然也可以使用其他方案，如MultiLock的设计 那么 MutiLock 加锁原理是什么呢？笔者画了一幅图来说明 当我们去设置了多个锁时，redission会将多个锁添加到一个集合中，然后用while循环不停地去尝试拿锁，但是会有一个总共的加锁时间，这个时间是用需要加锁的个数 * 1500ms ，假设有3个锁，那么时间就是4500ms，假设在这4500ms内，所有的锁都加锁成功，那么此时才算是加锁成功，如果在4500ms有线程加锁失败，则会再次去进行重试 ","date":"2023-01-09","objectID":"/redis%E5%BA%94%E7%94%A8/:6:5","tags":[],"title":"Redis应用","uri":"/redis%E5%BA%94%E7%94%A8/"},{"categories":["redis"],"content":"6、秒杀优化 ","date":"2023-01-09","objectID":"/redis%E5%BA%94%E7%94%A8/:7:0","tags":[],"title":"Redis应用","uri":"/redis%E5%BA%94%E7%94%A8/"},{"categories":["redis"],"content":"6.1 秒杀优化-异步秒杀思路 我们来回顾一下下单流程 当用户发起请求，此时会请求nginx，nginx会访问到tomcat，而tomcat中的程序，会进行串行操作，分成如下几个步骤 1、查询优惠卷 2、判断秒杀库存是否足够 3、查询订单 4、校验是否是一人一单 5、扣减库存 6、创建订单 在这六步操作中，又有很多操作是要去操作数据库的，而且还是一个线程串行执行， 这样就会导致我们的程序执行的很慢，所以我们需要异步程序执行，那么如何加速呢？ 在这里笔者想给大家分享一下课程内没有的思路，看看有没有小伙伴这么想，比如，我们可以不可以使用异步编排来做，或者说我开启N多线程，N多个线程，一个线程执行查询优惠卷，一个执行判断扣减库存，一个去创建订单等等，然后再统一做返回，这种做法和课程中有哪种好呢？答案是课程中的好，因为如果你采用我刚说的方式，如果访问的人很多，那么线程池中的线程可能一下子就被消耗完了，而且你使用上述方案，最大的特点在于，你觉得时效性会非常重要，但是你想想是吗？并不是，比如我只要确定他能做这件事，然后我后边慢慢做就可以了，我并不需要他一口气做完这件事，所以我们应当采用的是课程中，类似消息队列的方式来完成我们的需求，而不是使用线程池或者是异步编排的方式来完成这个需求 优化方案：我们将耗时比较短的逻辑判断放入到redis中，比如是否库存足够，比如是否一人一单，这样的操作，只要这种逻辑可以完成，就意味着我们是一定可以下单完成的，我们只需要进行快速的逻辑判断，根本就不用等下单逻辑走完，我们直接给用户返回成功，再在后台开一个线程，后台线程慢慢的去执行queue里边的消息，这样程序不就超级快了吗？而且也不用担心线程池消耗殆尽的问题，因为这里我们的程序中并没有手动使用任何线程池，当然这里边有两个难点 第一个难点是我们怎么在redis中去快速校验一人一单，还有库存判断 第二个难点是由于我们校验和tomct下单是两个线程，那么我们如何知道到底哪个单他最后是否成功，或者是下单完成，为了完成这件事我们在redis操作完之后，我们会将一些信息返回给前端，同时也会把这些信息丢到异步queue中去，后续操作中，可以通过这个id来查询我们tomcat中的下单逻辑是否完成了。 我们现在来看看整体思路：当用户下单之后，判断库存是否充足只需要到redis中去根据key找对应的value是否大于0即可，如果不充足，则直接结束，如果充足，继续在redis中判断用户是否可以下单，如果set集合中没有这条数据，说明他可以下单，如果set集合中没有这条记录，则将userId和优惠卷存入到redis中，并且返回0，整个过程需要保证是原子性的，我们可以使用lua来操作 当以上判断逻辑走完之后，我们可以判断当前redis中返回的结果是否是0，如果是0，则表示可以下单，则将之前说的信息存入到到queue中去，然后返回，然后再来个线程异步的下单，前端可以通过返回的订单id来判断是否下单成功。 ","date":"2023-01-09","objectID":"/redis%E5%BA%94%E7%94%A8/:7:1","tags":[],"title":"Redis应用","uri":"/redis%E5%BA%94%E7%94%A8/"},{"categories":["redis"],"content":"6.2 秒杀优化-Redis完成秒杀资格判断 需求： 新增秒杀优惠券的同时，将优惠券信息保存到Redis中 基于Lua脚本，判断秒杀库存、一人一单，决定用户是否抢购成功 如果抢购成功，将优惠券id和用户id封装后存入阻塞队列 开启线程任务，不断从阻塞队列中获取信息，实现异步下单功能 VoucherServiceImpl @Override @Transactional public void addSeckillVoucher(Voucher voucher) { // 保存优惠券 save(voucher); // 保存秒杀信息 SeckillVoucher seckillVoucher = new SeckillVoucher(); seckillVoucher.setVoucherId(voucher.getId()); seckillVoucher.setStock(voucher.getStock()); seckillVoucher.setBeginTime(voucher.getBeginTime()); seckillVoucher.setEndTime(voucher.getEndTime()); seckillVoucherService.save(seckillVoucher); // 保存秒杀库存到Redis中 //SECKILL_STOCK_KEY 这个变量定义在RedisConstans中 //private static final String SECKILL_STOCK_KEY =\"seckill:stock:\" stringRedisTemplate.opsForValue().set(SECKILL_STOCK_KEY + voucher.getId(), voucher.getStock().toString()); } 完整lua表达式 -- 1.参数列表 -- 1.1.优惠券id local voucherId = ARGV[1] -- 1.2.用户id local userId = ARGV[2] -- 1.3.订单id local orderId = ARGV[3] -- 2.数据key -- 2.1.库存key local stockKey = 'seckill:stock:' .. voucherId -- 2.2.订单key local orderKey = 'seckill:order:' .. voucherId -- 3.脚本业务 -- 3.1.判断库存是否充足 get stockKey if(tonumber(redis.call('get', stockKey)) \u003c= 0) then -- 3.2.库存不足，返回1 return 1 end -- 3.2.判断用户是否下单 SISMEMBER orderKey userId if(redis.call('sismember', orderKey, userId) == 1) then -- 3.3.存在，说明是重复下单，返回2 return 2 end -- 3.4.扣库存 incrby stockKey -1 redis.call('incrby', stockKey, -1) -- 3.5.下单（保存用户）sadd orderKey userId redis.call('sadd', orderKey, userId) -- 3.6.发送消息到队列中， XADD stream.orders * k1 v1 k2 v2 ... redis.call('xadd', 'stream.orders', '*', 'userId', userId, 'voucherId', voucherId, 'id', orderId) return 0 当以上lua表达式执行完毕后，剩下的就是根据步骤3,4来执行我们接下来的任务了 VoucherOrderServiceImpl @Override public Result seckillVoucher(Long voucherId) { //获取用户 Long userId = UserHolder.getUser().getId(); long orderId = redisIdWorker.nextId(\"order\"); // 1.执行lua脚本 Long result = stringRedisTemplate.execute( SECKILL_SCRIPT, Collections.emptyList(), voucherId.toString(), userId.toString(), String.valueOf(orderId) ); int r = result.intValue(); // 2.判断结果是否为0 if (r != 0) { // 2.1.不为0 ，代表没有购买资格 return Result.fail(r == 1 ? \"库存不足\" : \"不能重复下单\"); } //TODO 保存阻塞队列 // 3.返回订单id return Result.ok(orderId); } ","date":"2023-01-09","objectID":"/redis%E5%BA%94%E7%94%A8/:7:2","tags":[],"title":"Redis应用","uri":"/redis%E5%BA%94%E7%94%A8/"},{"categories":["redis"],"content":"6.3 秒杀优化-基于阻塞队列实现秒杀优化 VoucherOrderServiceImpl 修改下单动作，现在我们去下单时，是通过lua表达式去原子执行判断逻辑，如果判断我出来不为0 ，则要么是库存不足，要么是重复下单，返回错误信息，如果是0，则把下单的逻辑保存到队列中去，然后异步执行 //异步处理线程池 private static final ExecutorService SECKILL_ORDER_EXECUTOR = Executors.newSingleThreadExecutor(); //在类初始化之后执行，因为当这个类初始化好了之后，随时都是有可能要执行的 @PostConstruct private void init() { SECKILL_ORDER_EXECUTOR.submit(new VoucherOrderHandler()); } // 用于线程池处理的任务 // 当初始化完毕后，就会去从对列中去拿信息 private class VoucherOrderHandler implements Runnable{ @Override public void run() { while (true){ try { // 1.获取队列中的订单信息 VoucherOrder voucherOrder = orderTasks.take(); // 2.创建订单 handleVoucherOrder(voucherOrder); } catch (Exception e) { log.error(\"处理订单异常\", e); } } } private void handleVoucherOrder(VoucherOrder voucherOrder) { //1.获取用户 Long userId = voucherOrder.getUserId(); // 2.创建锁对象 RLock redisLock = redissonClient.getLock(\"lock:order:\" + userId); // 3.尝试获取锁 boolean isLock = redisLock.lock(); // 4.判断是否获得锁成功 if (!isLock) { // 获取锁失败，直接返回失败或者重试 log.error(\"不允许重复下单！\"); return; } try { //注意：由于是spring的事务是放在threadLocal中，此时的是多线程，事务会失效 proxy.createVoucherOrder(voucherOrder); } finally { // 释放锁 redisLock.unlock(); } } //a private BlockingQueue\u003cVoucherOrder\u003e orderTasks = new ArrayBlockingQueue\u003c\u003e(1024 * 1024); @Override public Result seckillVoucher(Long voucherId) { Long userId = UserHolder.getUser().getId(); long orderId = redisIdWorker.nextId(\"order\"); // 1.执行lua脚本 Long result = stringRedisTemplate.execute( SECKILL_SCRIPT, Collections.emptyList(), voucherId.toString(), userId.toString(), String.valueOf(orderId) ); int r = result.intValue(); // 2.判断结果是否为0 if (r != 0) { // 2.1.不为0 ，代表没有购买资格 return Result.fail(r == 1 ? \"库存不足\" : \"不能重复下单\"); } VoucherOrder voucherOrder = new VoucherOrder(); // 2.3.订单id long orderId = redisIdWorker.nextId(\"order\"); voucherOrder.setId(orderId); // 2.4.用户id voucherOrder.setUserId(userId); // 2.5.代金券id voucherOrder.setVoucherId(voucherId); // 2.6.放入阻塞队列 orderTasks.add(voucherOrder); //3.获取代理对象 proxy = (IVoucherOrderService)AopContext.currentProxy(); //4.返回订单id return Result.ok(orderId); } @Transactional public void createVoucherOrder(VoucherOrder voucherOrder) { Long userId = voucherOrder.getUserId(); // 5.1.查询订单 int count = query().eq(\"user_id\", userId).eq(\"voucher_id\", voucherOrder.getVoucherId()).count(); // 5.2.判断是否存在 if (count \u003e 0) { // 用户已经购买过了 log.error(\"用户已经购买过了\"); return ; } // 6.扣减库存 boolean success = seckillVoucherService.update() .setSql(\"stock = stock - 1\") // set stock = stock - 1 .eq(\"voucher_id\", voucherOrder.getVoucherId()).gt(\"stock\", 0) // where id = ? and stock \u003e 0 .update(); if (!success) { // 扣减失败 log.error(\"库存不足\"); return ; } save(voucherOrder); } ","date":"2023-01-09","objectID":"/redis%E5%BA%94%E7%94%A8/:7:3","tags":[],"title":"Redis应用","uri":"/redis%E5%BA%94%E7%94%A8/"},{"categories":["redis"],"content":"总结优缺点： 秒杀业务的优化思路是什么？ 先利用Redis完成库存余量、一人一单判断，完成抢单业务（即是将“决策逻辑”业务抽离出来决策此业务是否成功，如果成功就将剩余的操作逻辑和所有“决策逻辑”塞入消息队列里去异步执行，自己就直接返回成功了） 再将下单业务放入阻塞队列，利用独立线程异步下单 基于阻塞队列的异步秒杀存在哪些问题？ 内存限制问题 数据安全问题 还有最大的问题：虽然我们使用了消息队列来对冗杂业务做异步处理，使之运行得更快，但是还是存在一些问题：返回成功，但是异步执行失败。这样就可能导致库存为负得情况出现。 有问题，也就有解决之法：我们认为异步任务是必须成功的，不允许失败，那么我们可以提供一个重试机制，来使其最终成功 ","date":"2023-01-09","objectID":"/redis%E5%BA%94%E7%94%A8/:7:4","tags":[],"title":"Redis应用","uri":"/redis%E5%BA%94%E7%94%A8/"},{"categories":["redis"],"content":"7、Redis消息队列 ","date":"2023-01-09","objectID":"/redis%E5%BA%94%E7%94%A8/:8:0","tags":[],"title":"Redis应用","uri":"/redis%E5%BA%94%E7%94%A8/"},{"categories":["redis"],"content":"7.1 Redis消息队列-认识消息队列 什么是消息队列：字面意思就是存放消息的队列。最简单的消息队列模型包括3个角色： 消息队列：存储和管理消息，也被称为消息代理（Message Broker） 生产者：发送消息到消息队列 消费者：从消息队列获取消息并处理消息 使用队列的好处在于 **解耦：**所谓解耦，举一个生活中的例子就是：快递员(生产者)把快递放到快递柜里边(Message Queue)去，我们(消费者)从快递柜里边去拿东西，这就是一个异步，如果耦合，那么这个快递员相当于直接把快递交给你，这事固然好，但是万一你不在家，那么快递员就会一直等你，这就浪费了快递员的时间，所以这种思想在我们日常开发中，是非常有必要的。 这种场景在我们秒杀中就变成了：我们下单之后，利用redis去进行校验下单条件，再通过队列把消息发送出去，然后再启动一个线程去消费这个消息，完成解耦，同时也加快我们的响应速度。 这里我们可以使用一些现成的mq，比如kafka，rabbitmq等等，但是呢，如果没有安装mq，我们也可以直接使用redis提供的mq方案，降低我们的部署和学习成本。 ","date":"2023-01-09","objectID":"/redis%E5%BA%94%E7%94%A8/:8:1","tags":[],"title":"Redis应用","uri":"/redis%E5%BA%94%E7%94%A8/"},{"categories":["redis"],"content":"7.2 Redis消息队列-基于List实现消息队列 消息队列（Message Queue），字面意思就是存放消息的队列。而Redis的list数据结构是一个双向链表，很容易模拟出队列效果。 队列是入口和出口不在一边，因此我们可以利用：LPUSH 结合 RPOP、或者 RPUSH 结合 LPOP来实现。 不过要注意的是，当队列中没有消息时RPOP或LPOP操作会返回null，并不像JVM的阻塞队列那样会阻塞并等待消息。因此这里应该使用BRPOP或者BLPOP来实现阻塞效果。 基于List的消息队列有哪些优缺点？ 优点： 利用Redis存储，不受限于JVM内存上限 基于Redis的持久化机制，数据安全性有保证 可以满足消息有序性 缺点： 无法避免消息丢失 只支持单消费者，因为只有一个queue ","date":"2023-01-09","objectID":"/redis%E5%BA%94%E7%94%A8/:8:2","tags":[],"title":"Redis应用","uri":"/redis%E5%BA%94%E7%94%A8/"},{"categories":["redis"],"content":"7.3 Redis消息队列-基于PubSub的消息队列 PubSub（发布订阅）是Redis2.0版本引入的消息传递模型。顾名思义，消费者可以订阅一个或多个channel，生产者向对应channel发送消息后，所有订阅者都能收到相关消息。 SUBSCRIBE channel [channel]：订阅一个或多个频道 PUBLISH channel msg：向一个频道发送消息 PSUBSCRIBE pattern[pattern]：订阅与pattern格式匹配的所有频道 基于PubSub的消息队列有哪些优缺点？ 优点： 采用发布订阅模型，支持多生产、多消费 缺点： 不支持数据持久化 无法避免消息丢失 消息堆积有上限，超出时数据丢失 ","date":"2023-01-09","objectID":"/redis%E5%BA%94%E7%94%A8/:8:3","tags":[],"title":"Redis应用","uri":"/redis%E5%BA%94%E7%94%A8/"},{"categories":["redis"],"content":"7.4 Redis消息队列-基于Stream的消息队列 Stream 是 Redis 5.0 引入的一种新数据类型，可以实现一个功能非常完善的消息队列。 发送消息的命令： 例如： 读取消息的方式之一：XREAD 例如，使用XREAD读取第一个消息： XREAD阻塞方式，读取最新的消息： 在业务开发中，我们可以循环的调用XREAD阻塞方式来查询最新消息，从而实现持续监听队列的效果，伪代码如下 注意：当我们指定起始ID为$时，代表读取最新的消息，如果我们处理一条消息的过程中，又有超过1条以上的消息加入队列，则下次获取时也只能获取到最新的一条，会出现漏读消息的问题 STREAM类型消息队列的XREAD命令特点： 消息可回溯 一个消息可以被多个消费者读取，也支持消费者组 可以阻塞读取 有消息漏读的风险 ","date":"2023-01-09","objectID":"/redis%E5%BA%94%E7%94%A8/:8:4","tags":[],"title":"Redis应用","uri":"/redis%E5%BA%94%E7%94%A8/"},{"categories":["redis"],"content":"7.5 Redis消息队列-基于Stream的消息队列-消费者组 消费者组（Consumer Group）：将多个消费者划分到一个组中，监听同一个队列。 具备下列特点： 创建消费者组： key：队列名称 groupName：消费者组名称 ID：起始ID标示，$代表队列中最后一个消息，0则代表队列中第一个消息 MKSTREAM：队列不存在时自动创建队列 其它常见命令： 删除指定的消费者组 XGROUP DESTORY key groupName 给指定的消费者组添加消费者 XGROUP CREATECONSUMER key groupname consumername 删除消费者组中的指定消费者 XGROUP DELCONSUMER key groupname consumername 从消费者组读取消息： XREADGROUP GROUP group consumer [COUNT count] [BLOCK milliseconds] [NOACK] STREAMS key [key ...] ID [ID ...] group：消费组名称 consumer：消费者名称，如果消费者不存在，会自动创建一个消费者 count：本次查询的最大数量 BLOCK milliseconds：当没有消息时最长等待时间 NOACK：无需手动ACK，获取到消息后自动确认 STREAMS key：指定队列名称 ID：获取消息的起始ID： “\u003e\"：从下一个未消费的消息开始 其它：根据指定id从pending-list中获取已消费但未确认的消息，例如0，是从pending-list中的第一个消息开始 消费者监听消息的基本思路： STREAM类型消息队列的XREADGROUP命令特点： 消息可回溯 可以多消费者争抢消息，加快消费速度 可以阻塞读取 没有消息漏读的风险 有消息确认机制，保证消息至少被消费一次 最后我们来个小对比 ","date":"2023-01-09","objectID":"/redis%E5%BA%94%E7%94%A8/:8:5","tags":[],"title":"Redis应用","uri":"/redis%E5%BA%94%E7%94%A8/"},{"categories":["redis"],"content":"7.6 基于Redis的Stream结构作为消息队列，实现异步秒杀下单 需求： 创建一个Stream类型的消息队列，名为stream.orders 修改之前的秒杀下单Lua脚本，在认定有抢购资格后，直接向stream.orders中添加消息，内容包含voucherId、userId、orderId 项目启动时，开启一个线程任务，尝试获取stream.orders中的消息，完成下单 修改lua表达式,新增3.6 VoucherOrderServiceImpl private class VoucherOrderHandler implements Runnable { @Override public void run() { while (true) { try { // 1.获取消息队列中的订单信息 XREADGROUP GROUP g1 c1 COUNT 1 BLOCK 2000 STREAMS s1 \u003e List\u003cMapRecord\u003cString, Object, Object\u003e\u003e list = stringRedisTemplate.opsForStream().read( Consumer.from(\"g1\", \"c1\"), StreamReadOptions.empty().count(1).block(Duration.ofSeconds(2)), StreamOffset.create(\"stream.orders\", ReadOffset.lastConsumed()) ); // 2.判断订单信息是否为空 if (list == null || list.isEmpty()) { // 如果为null，说明没有消息，继续下一次循环 continue; } // 解析数据 MapRecord\u003cString, Object, Object\u003e record = list.get(0); Map\u003cObject, Object\u003e value = record.getValue(); VoucherOrder voucherOrder = BeanUtil.fillBeanWithMap(value, new VoucherOrder(), true); // 3.创建订单 createVoucherOrder(voucherOrder); // 4.确认消息 XACK stringRedisTemplate.opsForStream().acknowledge(\"s1\", \"g1\", record.getId()); } catch (Exception e) { log.error(\"处理订单异常\", e); //处理异常消息 handlePendingList(); } } } private void handlePendingList() { while (true) { try { // 1.获取pending-list中的订单信息 XREADGROUP GROUP g1 c1 COUNT 1 BLOCK 2000 STREAMS s1 0 List\u003cMapRecord\u003cString, Object, Object\u003e\u003e list = stringRedisTemplate.opsForStream().read( Consumer.from(\"g1\", \"c1\"), StreamReadOptions.empty().count(1), StreamOffset.create(\"stream.orders\", ReadOffset.from(\"0\")) ); // 2.判断订单信息是否为空 if (list == null || list.isEmpty()) { // 如果为null，说明没有异常消息，结束循环 break; } // 解析数据 MapRecord\u003cString, Object, Object\u003e record = list.get(0); Map\u003cObject, Object\u003e value = record.getValue(); VoucherOrder voucherOrder = BeanUtil.fillBeanWithMap(value, new VoucherOrder(), true); // 3.创建订单 createVoucherOrder(voucherOrder); // 4.确认消息 XACK stringRedisTemplate.opsForStream().acknowledge(\"s1\", \"g1\", record.getId()); } catch (Exception e) { log.error(\"处理pendding订单异常\", e); try{ Thread.sleep(20); }catch(Exception e){ e.printStackTrace(); } } } } } ","date":"2023-01-09","objectID":"/redis%E5%BA%94%E7%94%A8/:8:6","tags":[],"title":"Redis应用","uri":"/redis%E5%BA%94%E7%94%A8/"},{"categories":["redis"],"content":"8、达人探店 ","date":"2023-01-09","objectID":"/redis%E5%BA%94%E7%94%A8/:9:0","tags":[],"title":"Redis应用","uri":"/redis%E5%BA%94%E7%94%A8/"},{"categories":["redis"],"content":"8.1、达人探店-发布探店笔记 发布探店笔记 探店笔记类似点评网站的评价，往往是图文结合。对应的表有两个： tb_blog：探店笔记表，包含笔记中的标题、文字、图片等 tb_blog_comments：其他用户对探店笔记的评价 具体发布流程 上传接口 @Slf4j @RestController @RequestMapping(\"upload\") public class UploadController { @PostMapping(\"blog\") public Result uploadImage(@RequestParam(\"file\") MultipartFile image) { try { // 获取原始文件名称 String originalFilename = image.getOriginalFilename(); // 生成新文件名 String fileName = createNewFileName(originalFilename); // 保存文件 image.transferTo(new File(SystemConstants.IMAGE_UPLOAD_DIR, fileName)); // 返回结果 log.debug(\"文件上传成功，{}\", fileName); return Result.ok(fileName); } catch (IOException e) { throw new RuntimeException(\"文件上传失败\", e); } } } 注意：同学们在操作时，需要修改SystemConstants.IMAGE_UPLOAD_DIR 自己图片所在的地址，在实际开发中图片一般会放在nginx上或者是云存储上。 BlogController @RestController @RequestMapping(\"/blog\") public class BlogController { @Resource private IBlogService blogService; @PostMapping public Result saveBlog(@RequestBody Blog blog) { //获取登录用户 UserDTO user = UserHolder.getUser(); blog.setUpdateTime(user.getId()); //保存探店博文 blogService.saveBlog(blog); //返回id return Result.ok(blog.getId()); } } ","date":"2023-01-09","objectID":"/redis%E5%BA%94%E7%94%A8/:9:1","tags":[],"title":"Redis应用","uri":"/redis%E5%BA%94%E7%94%A8/"},{"categories":["redis"],"content":"8.2 达人探店-查看探店笔记 实现查看发布探店笔记的接口 实现代码： BlogServiceImpl @Override public Result queryBlogById(Long id) { // 1.查询blog Blog blog = getById(id); if (blog == null) { return Result.fail(\"笔记不存在！\"); } // 2.查询blog有关的用户 queryBlogUser(blog); return Result.ok(blog); } ","date":"2023-01-09","objectID":"/redis%E5%BA%94%E7%94%A8/:9:2","tags":[],"title":"Redis应用","uri":"/redis%E5%BA%94%E7%94%A8/"},{"categories":["redis"],"content":"8.3 达人探店-点赞功能 初始代码 @GetMapping(\"/likes/{id}\") public Result queryBlogLikes(@PathVariable(\"id\") Long id) { //修改点赞数量 blogService.update().setSql(\"liked = liked +1 \").eq(\"id\",id).update(); return Result.ok(); } 问题分析：这种方式会导致一个用户无限点赞，明显是不合理的 造成这个问题的原因是，我们现在的逻辑，发起请求只是给数据库+1，所以才会出现这个问题 完善点赞功能 需求： 同一个用户只能点赞一次，再次点击则取消点赞 如果当前用户已经点赞，则点赞按钮高亮显示（前端已实现，判断字段Blog类的isLike属性） 实现步骤： 给Blog类中添加一个isLike字段，标示是否被当前用户点赞 修改点赞功能，利用Redis的set集合判断是否点赞过，未点赞过则点赞数+1，已点赞过则点赞数-1 修改根据id查询Blog的业务，判断当前登录用户是否点赞过，赋值给isLike字段 修改分页查询Blog业务，判断当前登录用户是否点赞过，赋值给isLike字段 为什么采用set集合： 因为我们的数据是不能重复的，当用户操作过之后，无论他怎么操作，都是 具体步骤： 1、在Blog 添加一个字段 @TableField(exist = false) private Boolean isLike; 2、修改代码 @Override public Result likeBlog(Long id){ // 1.获取登录用户 Long userId = UserHolder.getUser().getId(); // 2.判断当前登录用户是否已经点赞 String key = BLOG_LIKED_KEY + id; Boolean isMember = stringRedisTemplate.opsForSet().isMember(key, userId.toString()); if(BooleanUtil.isFalse(isMember)){ //3.如果未点赞，可以点赞 //3.1 数据库点赞数+1 boolean isSuccess = update().setSql(\"liked = liked + 1\").eq(\"id\", id).update(); //3.2 保存用户到Redis的set集合 if(isSuccess){ stringRedisTemplate.opsForSet().add(key,userId.toString()); } }else{ //4.如果已点赞，取消点赞 //4.1 数据库点赞数-1 boolean isSuccess = update().setSql(\"liked = liked - 1\").eq(\"id\", id).update(); //4.2 把用户从Redis的set集合移除 if(isSuccess){ stringRedisTemplate.opsForSet().remove(key,userId.toString()); } } ","date":"2023-01-09","objectID":"/redis%E5%BA%94%E7%94%A8/:9:3","tags":[],"title":"Redis应用","uri":"/redis%E5%BA%94%E7%94%A8/"},{"categories":["redis"],"content":"8.4 达人探店-点赞排行榜 在探店笔记的详情页面，应该把给该笔记点赞的人显示出来，比如最早点赞的TOP5，形成点赞排行榜： 之前的点赞是放到set集合，但是set集合是不能排序的，所以这个时候，咱们可以采用一个可以排序的set集合，就是咱们的sortedSet 我们接下来来对比一下这些集合的区别是什么 所有点赞的人，需要是唯一的，所以我们应当使用set或者是sortedSet 其次我们需要排序，就可以直接锁定使用sortedSet啦 修改代码 BlogServiceImpl 点赞逻辑代码 @Override public Result likeBlog(Long id) { // 1.获取登录用户 Long userId = UserHolder.getUser().getId(); // 2.判断当前登录用户是否已经点赞 String key = BLOG_LIKED_KEY + id; Double score = stringRedisTemplate.opsForZSet().score(key, userId.toString()); if (score == null) { // 3.如果未点赞，可以点赞 // 3.1.数据库点赞数 + 1 boolean isSuccess = update().setSql(\"liked = liked + 1\").eq(\"id\", id).update(); // 3.2.保存用户到Redis的set集合 zadd key value score if (isSuccess) { stringRedisTemplate.opsForZSet().add(key, userId.toString(), System.currentTimeMillis()); } } else { // 4.如果已点赞，取消点赞 // 4.1.数据库点赞数 -1 boolean isSuccess = update().setSql(\"liked = liked - 1\").eq(\"id\", id).update(); // 4.2.把用户从Redis的set集合移除 if (isSuccess) { stringRedisTemplate.opsForZSet().remove(key, userId.toString()); } } return Result.ok(); } private void isBlogLiked(Blog blog) { // 1.获取登录用户 UserDTO user = UserHolder.getUser(); if (user == null) { // 用户未登录，无需查询是否点赞 return; } Long userId = user.getId(); // 2.判断当前登录用户是否已经点赞 String key = \"blog:liked:\" + blog.getId(); Double score = stringRedisTemplate.opsForZSet().score(key, userId.toString()); blog.setIsLike(score != null); } 点赞列表查询列表 BlogController @GetMapping(\"/likes/{id}\") public Result queryBlogLikes(@PathVariable(\"id\") Long id) { return blogService.queryBlogLikes(id); } BlogService @Override public Result queryBlogLikes(Long id) { String key = BLOG_LIKED_KEY + id; // 1.查询top5的点赞用户 zrange key 0 4 Set\u003cString\u003e top5 = stringRedisTemplate.opsForZSet().range(key, 0, 4); if (top5 == null || top5.isEmpty()) { return Result.ok(Collections.emptyList()); } // 2.解析出其中的用户id List\u003cLong\u003e ids = top5.stream().map(Long::valueOf).collect(Collectors.toList()); String idStr = StrUtil.join(\",\", ids); // 3.根据用户id查询用户 WHERE id IN ( 5 , 1 ) ORDER BY FIELD(id, 5, 1) List\u003cUserDTO\u003e userDTOS = userService.query() .in(\"id\", ids).last(\"ORDER BY FIELD(id,\" + idStr + \")\").list() .stream() .map(user -\u003e BeanUtil.copyProperties(user, UserDTO.class)) .collect(Collectors.toList()); // 4.返回 return Result.ok(userDTOS); } ","date":"2023-01-09","objectID":"/redis%E5%BA%94%E7%94%A8/:9:4","tags":[],"title":"Redis应用","uri":"/redis%E5%BA%94%E7%94%A8/"},{"categories":["redis"],"content":"9、好友关注 ","date":"2023-01-09","objectID":"/redis%E5%BA%94%E7%94%A8/:10:0","tags":[],"title":"Redis应用","uri":"/redis%E5%BA%94%E7%94%A8/"},{"categories":["redis"],"content":"9.1 好友关注-关注和取消关注 针对用户的操作：可以对用户进行关注和取消关注功能。 实现思路： 需求：基于该表数据结构，实现两个接口： 关注和取关接口 判断是否关注的接口 关注是User之间的关系，是博主与粉丝的关系，数据库中有一张tb_follow表来标示： 注意: 这里需要把主键修改为自增长，简化开发。 FollowController //关注 @PutMapping(\"/{id}/{isFollow}\") public Result follow(@PathVariable(\"id\") Long followUserId, @PathVariable(\"isFollow\") Boolean isFollow) { return followService.follow(followUserId, isFollow); } //取消关注 @GetMapping(\"/or/not/{id}\") public Result isFollow(@PathVariable(\"id\") Long followUserId) { return followService.isFollow(followUserId); } FollowService 取消关注service @Override public Result isFollow(Long followUserId) { // 1.获取登录用户 Long userId = UserHolder.getUser().getId(); // 2.查询是否关注 select count(*) from tb_follow where user_id = ? and follow_user_id = ? Integer count = query().eq(\"user_id\", userId).eq(\"follow_user_id\", followUserId).count(); // 3.判断 return Result.ok(count \u003e 0); } 关注service @Override public Result follow(Long followUserId, Boolean isFollow) { // 1.获取登录用户 Long userId = UserHolder.getUser().getId(); String key = \"follows:\" + userId; // 1.判断到底是关注还是取关 if (isFollow) { // 2.关注，新增数据 Follow follow = new Follow(); follow.setUserId(userId); follow.setFollowUserId(followUserId); boolean isSuccess = save(follow); } else { // 3.取关，删除 delete from tb_follow where user_id = ? and follow_user_id = ? remove(new QueryWrapper\u003cFollow\u003e() .eq(\"user_id\", userId).eq(\"follow_user_id\", followUserId)); } return Result.ok(); } ","date":"2023-01-09","objectID":"/redis%E5%BA%94%E7%94%A8/:10:1","tags":[],"title":"Redis应用","uri":"/redis%E5%BA%94%E7%94%A8/"},{"categories":["redis"],"content":"9.2 好友关注-共同关注 想要去看共同关注的好友，需要首先进入到这个页面，这个页面会发起两个请求 1、去查询用户的详情 2、去查询用户的笔记 以上两个功能和共同关注没有什么关系，大家可以自行将笔记中的代码拷贝到idea中就可以实现这两个功能了，我们的重点在于共同关注功能。 // UserController 根据id查询用户 @GetMapping(\"/{id}\") public Result queryUserById(@PathVariable(\"id\") Long userId){ // 查询详情 User user = userService.getById(userId); if (user == null) { return Result.ok(); } UserDTO userDTO = BeanUtil.copyProperties(user, UserDTO.class); // 返回 return Result.ok(userDTO); } // BlogController 根据id查询博主的探店笔记 @GetMapping(\"/of/user\") public Result queryBlogByUserId( @RequestParam(value = \"current\", defaultValue = \"1\") Integer current, @RequestParam(\"id\") Long id) { // 根据用户查询 Page\u003cBlog\u003e page = blogService.query() .eq(\"user_id\", id).page(new Page\u003c\u003e(current, SystemConstants.MAX_PAGE_SIZE)); // 获取当前页数据 List\u003cBlog\u003e records = page.getRecords(); return Result.ok(records); } 接下来我们来看看共同关注如何实现： 需求：利用Redis中恰当的数据结构，实现共同关注功能。在博主个人页面展示出当前用户与博主的共同关注呢。 当然是使用我们之前学习过的set集合咯，在set集合中，有交集并集补集的api，我们可以把两人的关注的人分别放入到一个set集合中，然后再通过api去查看这两个set集合中的交集数据。 我们先来改造当前的关注列表 改造原因是因为我们需要在用户关注了某位用户后，需要将数据放入到set集合中，方便后续进行共同关注，同时当取消关注时，也需要从set集合中进行删除 FollowServiceImpl @Override public Result follow(Long followUserId, Boolean isFollow) { // 1.获取登录用户 Long userId = UserHolder.getUser().getId(); String key = \"follows:\" + userId; // 1.判断到底是关注还是取关 if (isFollow) { // 2.关注，新增数据 Follow follow = new Follow(); follow.setUserId(userId); follow.setFollowUserId(followUserId); boolean isSuccess = save(follow); if (isSuccess) { // 把关注用户的id，放入redis的set集合 sadd userId followerUserId stringRedisTemplate.opsForSet().add(key, followUserId.toString()); } } else { // 3.取关，删除 delete from tb_follow where user_id = ? and follow_user_id = ? boolean isSuccess = remove(new QueryWrapper\u003cFollow\u003e() .eq(\"user_id\", userId).eq(\"follow_user_id\", followUserId)); if (isSuccess) { // 把关注用户的id从Redis集合中移除 stringRedisTemplate.opsForSet().remove(key, followUserId.toString()); } } return Result.ok(); } 具体的关注代码： FollowServiceImpl @Override public Result followCommons(Long id) { // 1.获取当前用户 Long userId = UserHolder.getUser().getId(); String key = \"follows:\" + userId; // 2.求交集 String key2 = \"follows:\" + id; Set\u003cString\u003e intersect = stringRedisTemplate.opsForSet().intersect(key, key2); if (intersect == null || intersect.isEmpty()) { // 无交集 return Result.ok(Collections.emptyList()); } // 3.解析id集合 List\u003cLong\u003e ids = intersect.stream().map(Long::valueOf).collect(Collectors.toList()); // 4.查询用户 List\u003cUserDTO\u003e users = userService.listByIds(ids) .stream() .map(user -\u003e BeanUtil.copyProperties(user, UserDTO.class)) .collect(Collectors.toList()); return Result.ok(users); } ","date":"2023-01-09","objectID":"/redis%E5%BA%94%E7%94%A8/:10:2","tags":[],"title":"Redis应用","uri":"/redis%E5%BA%94%E7%94%A8/"},{"categories":["redis"],"content":"9.3 好友关注-Feed流实现方案 当我们关注了用户后，这个用户发了动态，那么我们应该把这些数据推送给用户，这个需求，其实我们又把他叫做Feed流，关注推送也叫做Feed流，直译为投喂。为用户持续的提供“沉浸式”的体验，通过无限下拉刷新获取新的信息。 对于传统的模式的内容解锁：我们是需要用户去通过搜索引擎或者是其他的方式去解锁想要看的内容 对于新型的Feed流的的效果：不需要我们用户再去推送信息，而是系统分析用户到底想要什么，然后直接把内容推送给用户，从而使用户能够更加的节约时间，不用主动去寻找。 Feed流的实现有两种模式： Feed流产品有两种常见模式： Timeline：不做内容筛选，简单的按照内容发布时间排序，常用于好友或关注。例如朋友圈 优点：信息全面，不会有缺失。并且实现也相对简单 缺点：信息噪音较多，用户不一定感兴趣，内容获取效率低 智能排序：利用智能算法屏蔽掉违规的、用户不感兴趣的内容。推送用户感兴趣信息来吸引用户 优点：投喂用户感兴趣信息，用户粘度很高，容易沉迷 缺点：如果算法不精准，可能起到反作用 本例中的个人页面，是基于关注的好友来做Feed流，因此采用Timeline的模式。该模式的实现方案有三种： 我们本次针对好友的操作，采用的就是Timeline的方式，只需要拿到我们关注用户的信息，然后按照时间排序即可 ，因此采用Timeline的模式。该模式的实现方案有三种： 拉模式 推模式 推拉结合 拉模式：也叫做读扩散 该模式的核心含义就是：当张三和李四和王五发了消息后，都会保存在自己的邮箱中，假设赵六要读取信息，那么他会从读取他自己的收件箱，此时系统会从他关注的人群中，把他关注人的信息全部都进行拉取，然后在进行排序 优点：比较节约空间，因为赵六在读信息时，并没有重复读取，而且读取完之后可以把他的收件箱进行清楚。 缺点：比较延迟，当用户读取数据时才去关注的人里边去读取数据，假设用户关注了大量的用户，那么此时就会拉取海量的内容，对服务器压力巨大。 推模式：也叫做写扩散 推模式是没有写邮箱的，当张三写了一个内容，此时会主动的把张三写的内容发送到他的粉丝收件箱中去，假设此时李四再来读取，就不用再去临时拉取了 优点：时效快，不用临时拉取 缺点：内存压力大，假设一个大V写信息，很多人关注他， 就会写很多分数据到粉丝那边去 推拉结合模式：也叫做读写混合 推拉模式是一个折中的方案，站在发件人这一段，如果是个普通的人，那么我们采用写扩散的方式，直接把数据写入到他的粉丝中去，因为普通的人他的粉丝关注量比较小，所以这样做没有压力，如果是大V，那么他是直接将数据先写入到一份到发件箱里边去，然后再直接写一份到活跃粉丝收件箱里边去，现在站在收件人这端来看，如果是活跃粉丝，那么大V和普通的人发的都会直接写入到自己收件箱里边来，而如果是普通的粉丝，由于他们上线不是很频繁，所以等他们上线时，再从发件箱里边去拉信息。 ","date":"2023-01-09","objectID":"/redis%E5%BA%94%E7%94%A8/:10:3","tags":[],"title":"Redis应用","uri":"/redis%E5%BA%94%E7%94%A8/"},{"categories":["redis"],"content":"9.4 好友关注-推送到粉丝收件箱 需求： 修改新增探店笔记的业务，在保存blog到数据库的同时，推送到粉丝的收件箱 收件箱满足可以根据时间戳排序，必须用Redis的数据结构实现 查询收件箱数据时，可以实现分页查询 Feed流中的数据会不断更新，所以数据的角标也在变化，因此不能采用传统的分页模式。 传统了分页在feed流是不适用的，因为我们的数据会随时发生变化 假设在t1 时刻，我们去读取第一页，此时page = 1 ，size = 5 ，那么我们拿到的就是10~6 这几条记录，假设现在t2时候又发布了一条记录，此时t3 时刻，我们来读取第二页，读取第二页传入的参数是page=2 ，size=5 ，那么此时读取到的第二页实际上是从6 开始，然后是6~2 ，那么我们就读取到了重复的数据，所以feed流的分页，不能采用原始方案来做。 Feed流的滚动分页 我们需要记录每次操作的最后一条，然后从这个位置开始去读取数据 举个例子：我们从t1时刻开始，拿第一页数据，拿到了10~6，然后记录下当前最后一次拿取的记录，就是6，t2时刻发布了新的记录，此时这个11放到最顶上，但是不会影响我们之前记录的6，此时t3时刻来拿第二页，第二页这个时候拿数据，还是从6后一点的5去拿，就拿到了5-1的记录。我们这个地方可以采用sortedSet来做，可以进行范围查询，并且还可以记录当前获取数据时间戳最小值，就可以实现滚动分页了 核心的意思：就是我们在保存完探店笔记后，获得到当前笔记的粉丝，然后把数据推送到粉丝的redis中去。 @Override public Result saveBlog(Blog blog) { // 1.获取登录用户 UserDTO user = UserHolder.getUser(); blog.setUserId(user.getId()); // 2.保存探店笔记 boolean isSuccess = save(blog); if(!isSuccess){ return Result.fail(\"新增笔记失败!\"); } // 3.查询笔记作者的所有粉丝 select * from tb_follow where follow_user_id = ? List\u003cFollow\u003e follows = followService.query().eq(\"follow_user_id\", user.getId()).list(); // 4.推送笔记id给所有粉丝 for (Follow follow : follows) { // 4.1.获取粉丝id Long userId = follow.getUserId(); // 4.2.推送 String key = FEED_KEY + userId; stringRedisTemplate.opsForZSet().add(key, blog.getId().toString(), System.currentTimeMillis()); } // 5.返回id return Result.ok(blog.getId()); } ","date":"2023-01-09","objectID":"/redis%E5%BA%94%E7%94%A8/:10:4","tags":[],"title":"Redis应用","uri":"/redis%E5%BA%94%E7%94%A8/"},{"categories":["redis"],"content":"9.5 好友关注-实现分页查询收邮箱 需求：在个人主页的“关注”卡片中，查询并展示推送的Blog信息： 具体操作如下： 1、每次查询完成后，我们要分析出查询出数据的最小时间戳，这个值会作为下一次查询的条件 2、我们需要找到与上一次查询相同的查询个数作为偏移量，下次查询时，跳过这些查询过的数据，拿到我们需要的数据 综上：我们的请求参数中就需要携带 lastId：上一次查询的最小时间戳 和偏移量这两个参数。 这两个参数第一次会由前端来指定，以后的查询就根据后台结果作为条件，再次传递到后台。 一、定义出来具体的返回值实体类 @Data public class ScrollResult { private List\u003c?\u003e list; private Long minTime; private Integer offset; } BlogController 注意：RequestParam 表示接受url地址栏传参的注解，当方法上参数的名称和url地址栏不相同时，可以通过RequestParam 来进行指定 @GetMapping(\"/of/follow\") public Result queryBlogOfFollow( @RequestParam(\"lastId\") Long max, @RequestParam(value = \"offset\", defaultValue = \"0\") Integer offset){ return blogService.queryBlogOfFollow(max, offset); } BlogServiceImpl @Override public Result queryBlogOfFollow(Long max, Integer offset) { // 1.获取当前用户 Long userId = UserHolder.getUser().getId(); // 2.查询收件箱 ZREVRANGEBYSCORE key Max Min LIMIT offset count String key = FEED_KEY + userId; Set\u003cZSetOperations.TypedTuple\u003cString\u003e\u003e typedTuples = stringRedisTemplate.opsForZSet() .reverseRangeByScoreWithScores(key, 0, max, offset, 2); // 3.非空判断 if (typedTuples == null || typedTuples.isEmpty()) { return Result.ok(); } // 4.解析数据：blogId、minTime（时间戳）、offset List\u003cLong\u003e ids = new ArrayList\u003c\u003e(typedTuples.size()); long minTime = 0; // 2 int os = 1; // 2 for (ZSetOperations.TypedTuple\u003cString\u003e tuple : typedTuples) { // 5 4 4 2 2 // 4.1.获取id ids.add(Long.valueOf(tuple.getValue())); // 4.2.获取分数(时间戳） long time = tuple.getScore().longValue(); if(time == minTime){ os++; }else{ minTime = time; os = 1; } } os = minTime == max ? os : os + offset; // 5.根据id查询blog String idStr = StrUtil.join(\",\", ids); List\u003cBlog\u003e blogs = query().in(\"id\", ids).last(\"ORDER BY FIELD(id,\" + idStr + \")\").list(); for (Blog blog : blogs) { // 5.1.查询blog有关的用户 queryBlogUser(blog); // 5.2.查询blog是否被点赞 isBlogLiked(blog); } // 6.封装并返回 ScrollResult r = new ScrollResult(); r.setList(blogs); r.setOffset(os); r.setMinTime(minTime); return Result.ok(r); } ","date":"2023-01-09","objectID":"/redis%E5%BA%94%E7%94%A8/:10:5","tags":[],"title":"Redis应用","uri":"/redis%E5%BA%94%E7%94%A8/"},{"categories":["redis"],"content":"10、附近商户 ","date":"2023-01-09","objectID":"/redis%E5%BA%94%E7%94%A8/:11:0","tags":[],"title":"Redis应用","uri":"/redis%E5%BA%94%E7%94%A8/"},{"categories":["redis"],"content":"10.1、附近商户-GEO数据结构的基本用法 GEO就是Geolocation的简写形式，代表地理坐标。Redis在3.2版本中加入了对GEO的支持，允许存储地理坐标信息，帮助我们根据经纬度来检索数据。常见的命令有： GEOADD：添加一个地理空间信息，包含：经度（longitude）、纬度（latitude）、值（member） GEODIST：计算指定的两个点之间的距离并返回 GEOHASH：将指定member的坐标转为hash字符串形式并返回 GEOPOS：返回指定member的坐标 GEORADIUS：指定圆心、半径，找到该圆内包含的所有member，并按照与圆心之间的距离排序后返回。6.以后已废弃 GEOSEARCH：在指定范围内搜索member，并按照与指定点之间的距离排序后返回。范围可以是圆形或矩形。6.2.新功能 GEOSEARCHSTORE：与GEOSEARCH功能一致，不过可以把结果存储到一个指定的key。 6.2.新功能 ","date":"2023-01-09","objectID":"/redis%E5%BA%94%E7%94%A8/:11:1","tags":[],"title":"Redis应用","uri":"/redis%E5%BA%94%E7%94%A8/"},{"categories":["redis"],"content":"10.2 附近商户-导入店铺数据到GEO 具体场景说明： 当我们点击美食之后，会出现一系列的商家，商家中可以按照多种排序方式，我们此时关注的是距离，这个地方就需要使用到我们的GEO，向后台传入当前app收集的地址(我们此处是写死的) ，以当前坐标作为圆心，同时绑定相同的店家类型type，以及分页信息，把这几个条件传入后台，后台查询出对应的数据再返回。 我们要做的事情是：将数据库表中的数据导入到redis中去，redis中的GEO，GEO在redis中就一个menber和一个经纬度，我们把x和y轴传入到redis做的经纬度位置去，但我们不能把所有的数据都放入到menber中去，毕竟作为redis是一个内存级数据库，如果存海量数据，redis还是力不从心，所以我们在这个地方存储他的id即可。 但是这个时候还有一个问题，就是在redis中并没有存储type，所以我们无法根据type来对数据进行筛选，所以我们可以按照商户类型做分组，类型相同的商户作为同一组，以typeId为key存入同一个GEO集合中即可 代码 HmDianPingApplicationTests @Test void loadShopData() { // 1.查询店铺信息 List\u003cShop\u003e list = shopService.list(); // 2.把店铺分组，按照typeId分组，typeId一致的放到一个集合 Map\u003cLong, List\u003cShop\u003e\u003e map = list.stream().collect(Collectors.groupingBy(Shop::getTypeId)); // 3.分批完成写入Redis for (Map.Entry\u003cLong, List\u003cShop\u003e\u003e entry : map.entrySet()) { // 3.1.获取类型id Long typeId = entry.getKey(); String key = SHOP_GEO_KEY + typeId; // 3.2.获取同类型的店铺的集合 List\u003cShop\u003e value = entry.getValue(); List\u003cRedisGeoCommands.GeoLocation\u003cString\u003e\u003e locations = new ArrayList\u003c\u003e(value.size()); // 3.3.写入redis GEOADD key 经度 纬度 member for (Shop shop : value) { // stringRedisTemplate.opsForGeo().add(key, new Point(shop.getX(), shop.getY()), shop.getId().toString()); locations.add(new RedisGeoCommands.GeoLocation\u003c\u003e( shop.getId().toString(), new Point(shop.getX(), shop.getY()) )); } stringRedisTemplate.opsForGeo().add(key, locations); } } ","date":"2023-01-09","objectID":"/redis%E5%BA%94%E7%94%A8/:11:2","tags":[],"title":"Redis应用","uri":"/redis%E5%BA%94%E7%94%A8/"},{"categories":["redis"],"content":"10.3 附近商户-实现附近商户功能 SpringDataRedis的2.3.9版本并不支持Redis 6.2提供的GEOSEARCH命令，因此我们需要提示其版本，修改自己的POM 第一步：导入pom \u003cdependency\u003e \u003cgroupId\u003eorg.springframework.boot\u003c/groupId\u003e \u003cartifactId\u003espring-boot-starter-data-redis\u003c/artifactId\u003e \u003cexclusions\u003e \u003cexclusion\u003e \u003cartifactId\u003espring-data-redis\u003c/artifactId\u003e \u003cgroupId\u003eorg.springframework.data\u003c/groupId\u003e \u003c/exclusion\u003e \u003cexclusion\u003e \u003cartifactId\u003elettuce-core\u003c/artifactId\u003e \u003cgroupId\u003eio.lettuce\u003c/groupId\u003e \u003c/exclusion\u003e \u003c/exclusions\u003e \u003c/dependency\u003e \u003cdependency\u003e \u003cgroupId\u003eorg.springframework.data\u003c/groupId\u003e \u003cartifactId\u003espring-data-redis\u003c/artifactId\u003e \u003cversion\u003e2.6.2\u003c/version\u003e \u003c/dependency\u003e \u003cdependency\u003e \u003cgroupId\u003eio.lettuce\u003c/groupId\u003e \u003cartifactId\u003elettuce-core\u003c/artifactId\u003e \u003cversion\u003e6.1.6.RELEASE\u003c/version\u003e \u003c/dependency\u003e 第二步： ShopController @GetMapping(\"/of/type\") public Result queryShopByType( @RequestParam(\"typeId\") Integer typeId, @RequestParam(value = \"current\", defaultValue = \"1\") Integer current, @RequestParam(value = \"x\", required = false) Double x, @RequestParam(value = \"y\", required = false) Double y ) { return shopService.queryShopByType(typeId, current, x, y); } ShopServiceImpl @Override public Result queryShopByType(Integer typeId, Integer current, Double x, Double y) { // 1.判断是否需要根据坐标查询 if (x == null || y == null) { // 不需要坐标查询，按数据库查询 Page\u003cShop\u003e page = query() .eq(\"type_id\", typeId) .page(new Page\u003c\u003e(current, SystemConstants.DEFAULT_PAGE_SIZE)); // 返回数据 return Result.ok(page.getRecords()); } // 2.计算分页参数 int from = (current - 1) * SystemConstants.DEFAULT_PAGE_SIZE; int end = current * SystemConstants.DEFAULT_PAGE_SIZE; // 3.查询redis、按照距离排序、分页。结果：shopId、distance String key = SHOP_GEO_KEY + typeId; GeoResults\u003cRedisGeoCommands.GeoLocation\u003cString\u003e\u003e results = stringRedisTemplate.opsForGeo() // GEOSEARCH key BYLONLAT x y BYRADIUS 10 WITHDISTANCE .search( key, GeoReference.fromCoordinate(x, y), new Distance(5000), RedisGeoCommands.GeoSearchCommandArgs.newGeoSearchArgs().includeDistance().limit(end) ); // 4.解析出id if (results == null) { return Result.ok(Collections.emptyList()); } List\u003cGeoResult\u003cRedisGeoCommands.GeoLocation\u003cString\u003e\u003e\u003e list = results.getContent(); if (list.size() \u003c= from) { // 没有下一页了，结束 return Result.ok(Collections.emptyList()); } // 4.1.截取 from ~ end的部分 List\u003cLong\u003e ids = new ArrayList\u003c\u003e(list.size()); Map\u003cString, Distance\u003e distanceMap = new HashMap\u003c\u003e(list.size()); list.stream().skip(from).forEach(result -\u003e { // 4.2.获取店铺id String shopIdStr = result.getContent().getName(); ids.add(Long.valueOf(shopIdStr)); // 4.3.获取距离 Distance distance = result.getDistance(); distanceMap.put(shopIdStr, distance); }); // 5.根据id查询Shop String idStr = StrUtil.join(\",\", ids); List\u003cShop\u003e shops = query().in(\"id\", ids).last(\"ORDER BY FIELD(id,\" + idStr + \")\").list(); for (Shop shop : shops) { shop.setDistance(distanceMap.get(shop.getId().toString()).getValue()); } // 6.返回 return Result.ok(shops); } ","date":"2023-01-09","objectID":"/redis%E5%BA%94%E7%94%A8/:11:3","tags":[],"title":"Redis应用","uri":"/redis%E5%BA%94%E7%94%A8/"},{"categories":["redis"],"content":"11、用户签到 11.1、用户签到-BitMap功能演示 我们针对签到功能完全可以通过mysql来完成，比如说以下这张表 用户一次签到，就是一条记录，假如有1000万用户，平均每人每年签到次数为10次，则这张表一年的数据量为 1亿条 每签到一次需要使用（8 + 8 + 1 + 1 + 3 + 1）共22 字节的内存，一个月则最多需要600多字节 我们如何能够简化一点呢？其实可以考虑小时候一个挺常见的方案，就是小时候，咱们准备一张小小的卡片，你只要签到就打上一个勾，我最后判断你是否签到，其实只需要到小卡片上看一看就知道了 我们可以采用类似这样的方案来实现我们的签到需求。 我们按月来统计用户签到信息，签到记录为1，未签到则记录为0. 把每一个bit位对应当月的每一天，形成了映射关系。用0和1标示业务状态，这种思路就称为位图（BitMap）。这样我们就用极小的空间，来实现了大量数据的表示 Redis中是利用string类型数据结构实现BitMap，因此最大上限是512M，转换为bit则是 2^32个bit位。 BitMap的操作命令有： SETBIT：向指定位置（offset）存入一个0或1 GETBIT ：获取指定位置（offset）的bit值 BITCOUNT ：统计BitMap中值为1的bit位的数量 BITFIELD ：操作（查询、修改、自增）BitMap中bit数组中的指定位置（offset）的值 BITFIELD_RO ：获取BitMap中bit数组，并以十进制形式返回 BITOP ：将多个BitMap的结果做位运算（与 、或、异或） BITPOS ：查找bit数组中指定范围内第一个0或1出现的位置 11.2 用户签到-实现签到功能 需求：实现签到接口，将当前用户当天签到信息保存到Redis中 思路：我们可以把年和月作为bitMap的key，然后保存到一个bitMap中，每次签到就到对应的位上把数字从0变成1，只要对应是1，就表明说明这一天已经签到了，反之则没有签到。 我们通过接口文档发现，此接口并没有传递任何的参数，没有参数怎么确实是哪一天签到呢？这个很容易，可以通过后台代码直接获取即可，然后到对应的地址上去修改bitMap。 代码 UserController @PostMapping(\"/sign\") public Result sign(){ return userService.sign(); } UserServiceImpl @Override public Result sign() { // 1.获取当前登录用户 Long userId = UserHolder.getUser().getId(); // 2.获取日期 LocalDateTime now = LocalDateTime.now(); // 3.拼接key String keySuffix = now.format(DateTimeFormatter.ofPattern(\":yyyyMM\")); String key = USER_SIGN_KEY + userId + keySuffix; // 4.获取今天是本月的第几天 int dayOfMonth = now.getDayOfMonth(); // 5.写入Redis SETBIT key offset 1 stringRedisTemplate.opsForValue().setBit(key, dayOfMonth - 1, true); return Result.ok(); } 11.3 用户签到-签到统计 **问题1：**什么叫做连续签到天数？ 从最后一次签到开始向前统计，直到遇到第一次未签到为止，计算总的签到次数，就是连续签到天数。 Java逻辑代码：获得当前这个月的最后一次签到数据，定义一个计数器，然后不停的向前统计，直到获得第一个非0的数字即可，每得到一个非0的数字计数器+1，直到遍历完所有的数据，就可以获得当前月的签到总天数了 **问题2：**如何得到本月到今天为止的所有签到数据？ BITFIELD key GET u[dayOfMonth] 0 假设今天是10号，那么我们就可以从当前月的第一天开始，获得到当前这一天的位数，是10号，那么就是10位，去拿这段时间的数据，就能拿到所有的数据了，那么这10天里边签到了多少次呢？统计有多少个1即可。 问题3：如何从后向前遍历每个bit位？ 注意：bitMap返回的数据是10进制，哪假如说返回一个数字8，那么我哪儿知道到底哪些是0，哪些是1呢？我们只需要让得到的10进制数字和1做与运算就可以了，因为1只有遇见1 才是1，其他数字都是0 ，我们把签到结果和1进行与操作，每与一次，就把签到结果向右移动一位，依次内推，我们就能完成逐个遍历的效果了。 需求：实现下面接口，统计当前用户截止当前时间在本月的连续签到天数 有用户有时间我们就可以组织出对应的key，此时就能找到这个用户截止这天的所有签到记录，再根据这套算法，就能统计出来他连续签到的次数了 代码 UserController @GetMapping(\"/sign/count\") public Result signCount(){ return userService.signCount(); } UserServiceImpl @Override public Result signCount() { // 1.获取当前登录用户 Long userId = UserHolder.getUser().getId(); // 2.获取日期 LocalDateTime now = LocalDateTime.now(); // 3.拼接key String keySuffix = now.format(DateTimeFormatter.ofPattern(\":yyyyMM\")); String key = USER_SIGN_KEY + userId + keySuffix; // 4.获取今天是本月的第几天 int dayOfMonth = now.getDayOfMonth(); // 5.获取本月截止今天为止的所有的签到记录，返回的是一个十进制的数字 BITFIELD sign:5:202203 GET u14 0 List\u003cLong\u003e result = stringRedisTemplate.opsForValue().bitField( key, BitFieldSubCommands.create() .get(BitFieldSubCommands.BitFieldType.unsigned(dayOfMonth)).valueAt(0) ); if (result == null || result.isEmpty()) { // 没有任何签到结果 return Result.ok(0); } Long num = result.get(0); if (num == null || num == 0) { return Result.ok(0); } // 6.循环遍历 int count = 0; while (true) { // 6.1.让这个数字与1做与运算，得到数字的最后一个bit位 // 判断这个bit位是否为0 if ((num \u0026 1) == 0) { // 如果为0，说明未签到，结束 break; }else { // 如果不为0，说明已签到，计数器+1 count++; } // 把数字右移一位，抛弃最后一个bit位，继续下一个bit位 num \u003e\u003e\u003e= 1; } return Result.ok(count); } 11.4 额外加餐-关于使用bitmap来解决缓存穿透的方案 回顾缓存穿透： 发起了一个数据库不存在的，redis里边也不存在的数据，通常你可以把他看成一个攻击 解决方案： 判断id\u003c0 如果数据库是空，那么就可以直接往redis里边把这个空数据缓存起来 第一种解决方案：遇到的问题是如果用户访问的是id不存在的数据，则此时就无法生效 第二种解决方案：遇到的问题是：如果是不同的id那就可以防止下次过来直击数据 所以我们如何解决呢？ 我们可以将数据库的数据，所对应的id写入到一个list集合中，当用户过来访问的时候，我们直接去判断list中是否包含当前的要查询的数据，如果说用户要查询的id数据并不在list集合中，则直接返回，如果list中包含对应查询的id数据，则说明不是一次缓存穿透数据，则直接放行。 现在的问题是这个主键其实并没有那么短，而是很长的一个 主键 哪怕你单独去提取这个主键，但是在11年左右，淘宝的商品总量就已经超过10亿个 所以如果采用以上方案，这个list也会很大，所以我们可以使用bitmap来减少list的存储空间 我们可以把list数据抽象成一个非常大的bitmap，我们不再使用list，而是将db中的id数据利用哈希思想，比如： id % bitmap.size = 算出当前这个id对应应该落在bitmap的哪个索引上，然后将这个值从0变成1，然后当用户来查询数据时，此时已经没有了list，让用户用他查询的id去用相同的哈希算法， 算出来当前这个id应当落在bitmap的哪一位，然后判断这一位是0，还是1，如果是0则表明这一位上的数据一定不存在， 采用这种方式来处理，需要重点考虑一个事情，就是误差率，所谓的误差率就是指当发生哈希冲突的时候，产生的误差。 ","date":"2023-01-09","objectID":"/redis%E5%BA%94%E7%94%A8/:12:0","tags":[],"title":"Redis应用","uri":"/redis%E5%BA%94%E7%94%A8/"},{"categories":["redis"],"content":"12、UV统计 ","date":"2023-01-09","objectID":"/redis%E5%BA%94%E7%94%A8/:13:0","tags":[],"title":"Redis应用","uri":"/redis%E5%BA%94%E7%94%A8/"},{"categories":["redis"],"content":"12.1 UV统计-HyperLogLog 首先我们搞懂两个概念： UV：全称Unique Visitor，也叫独立访客量，是指通过互联网访问、浏览这个网页的自然人。1天内同一个用户多次访问该网站，只记录1次。 PV：全称Page View，也叫页面访问量或点击量，用户每访问网站的一个页面，记录1次PV，用户多次打开页面，则记录多次PV。往往用来衡量网站的流量。 通常来说UV会比PV大很多，所以衡量同一个网站的访问量，我们需要综合考虑很多因素，所以我们只是单纯的把这两个值作为一个参考值 UV统计在服务端做会比较麻烦，因为要判断该用户是否已经统计过了，需要将统计过的用户信息保存。但是如果每个访问的用户都保存到Redis中，数据量会非常恐怖，那怎么处理呢？ Hyperloglog(HLL)是从Loglog算法派生的概率算法，用于确定非常大的集合的基数，而不需要存储其所有值。相关算法原理大家可以参考：https://juejin.cn/post/6844903785744056333#heading-0 Redis中的HLL是基于string结构实现的，单个HLL的内存永远小于16kb，内存占用低的令人发指！作为代价，其测量结果是概率性的，有小于0.81％的误差。不过对于UV统计来说，这完全可以忽略。 ","date":"2023-01-09","objectID":"/redis%E5%BA%94%E7%94%A8/:13:1","tags":[],"title":"Redis应用","uri":"/redis%E5%BA%94%E7%94%A8/"},{"categories":["redis"],"content":"12.2 UV统计-测试百万数据的统计 测试思路：我们直接利用单元测试，向HyperLogLog中添加100万条数据，看看内存占用和统计效果如何 经过测试：我们会发生他的误差是在允许范围内，并且内存占用极小 ","date":"2023-01-09","objectID":"/redis%E5%BA%94%E7%94%A8/:13:2","tags":[],"title":"Redis应用","uri":"/redis%E5%BA%94%E7%94%A8/"},{"categories":["golang"],"content":"彻底搞懂GOROOT、GOPATH、PATH、mod管理和gopath管理项目的区别 ","date":"2023-01-09","objectID":"/mod%E5%92%8Cgopath%E4%BE%9D%E8%B5%96%E7%AE%A1%E7%90%86/:0:0","tags":["go编程技巧"],"title":"Mod和gopath依赖管理","uri":"/mod%E5%92%8Cgopath%E4%BE%9D%E8%B5%96%E7%AE%A1%E7%90%86/"},{"categories":["golang"],"content":"1、GOPATH 和 GOROOT 不同于其他语言，go中没有项目的说法，只有包, 其中有两个重要的路径，GOROOT 和 GOPATH Go开发相关的环境变量如下： GOROOT：GOROOT就是Go的安装目录，（类似于java的JDK） GOPATH：GOPATH是我们的工作空间,保存go项目代码和第三方依赖包 GOPATH可以设置多个，其中，第一个将会是默认的包目录，使用 go get 下载的包都会在第一个path中的src目录下，使用 go install时，在哪个GOPATH中找到了这个包，就会在哪个GOPATH下的bin目录生成可执行文件 ","date":"2023-01-09","objectID":"/mod%E5%92%8Cgopath%E4%BE%9D%E8%B5%96%E7%AE%A1%E7%90%86/:1:0","tags":["go编程技巧"],"title":"Mod和gopath依赖管理","uri":"/mod%E5%92%8Cgopath%E4%BE%9D%E8%B5%96%E7%AE%A1%E7%90%86/"},{"categories":["golang"],"content":"2、修改 GOPATH 和 GOROOT GOROOT GOROOT是Go的安装路径。GOROOT在绝大多数情况下都不需要修改 Mac中安装Go会自动配置好GOROOT，路径为/usr/local/go。Win中默认的GOROOT是在 C:\\Go中，也可自己指定 【如下图所示则我的GORROT为：D:\\development\\go】，以下是GOROOT目录的内容： 可以看到GOROOT下有bin，doc和src目录。bin目录下有我们熟悉的go和gofmt工具。可以认为GOOROOT和Java里的JDK目录类似。 GOPATH GOPATH是开发时的工作目录。用于： 保存编译后的二进制文件。 go get和go install命令会下载go代码到GOPATH。 import包时的搜索路径 使用GOPATH时，GO会在以下目录中搜索包： GOROOT/src：该目录保存了Go标准库代码。 GOPATH/src：该目录保存了应用自身的代码和第三方依赖的代码。 假设程序中引入了如下的包： import \"Go-Player/src/chapter17/models\" 第一步：Go会先去GOROOT的scr目录中查找，很显然它不是标准库的包，没找到。 第二步：继续在GOPATH的src目录去找，准确说是GOPATH/src/Go-Player/src/chapter17/models这个目录。如果该目录不存在，会报错找不到package。在使用GOPATH管理项目时，需要按照GO寻找package的规范来合理地保存和组织Go代码。 ","date":"2023-01-09","objectID":"/mod%E5%92%8Cgopath%E4%BE%9D%E8%B5%96%E7%AE%A1%E7%90%86/:2:0","tags":["go编程技巧"],"title":"Mod和gopath依赖管理","uri":"/mod%E5%92%8Cgopath%E4%BE%9D%E8%B5%96%E7%AE%A1%E7%90%86/"},{"categories":["golang"],"content":"3、HelloWord——GOPATH版 ","date":"2023-01-09","objectID":"/mod%E5%92%8Cgopath%E4%BE%9D%E8%B5%96%E7%AE%A1%E7%90%86/:3:0","tags":["go编程技巧"],"title":"Mod和gopath依赖管理","uri":"/mod%E5%92%8Cgopath%E4%BE%9D%E8%B5%96%E7%AE%A1%E7%90%86/"},{"categories":["golang"],"content":"（1）设置并查看GOPATH和GOROOT环境变量 安装go SKD目录：D:\\development\\go go项目存放目录：D:\\development\\jetbrains\\goland\\workspace，并且此目录下含有bin、src、pkg三个文件夹，src文件夹用来存放项目代码 当引入module时，首先在GOROOT的src目录下查找，然后再GPOPATH的src目录下查找 ","date":"2023-01-09","objectID":"/mod%E5%92%8Cgopath%E4%BE%9D%E8%B5%96%E7%AE%A1%E7%90%86/:3:1","tags":["go编程技巧"],"title":"Mod和gopath依赖管理","uri":"/mod%E5%92%8Cgopath%E4%BE%9D%E8%B5%96%E7%AE%A1%E7%90%86/"},{"categories":["golang"],"content":"（2）GOLand环境配置 在D:\\development\\jetbrains\\goland\\workspace\\src目录下新建项目GO-Player bin：存放编译后的exe文件 pkg：存放自定义包的目录 src：存放项目源文件的目录 按如下指令进行配置 可在Settings中选择SDK和添加GOPATH ","date":"2023-01-09","objectID":"/mod%E5%92%8Cgopath%E4%BE%9D%E8%B5%96%E7%AE%A1%E7%90%86/:3:2","tags":["go编程技巧"],"title":"Mod和gopath依赖管理","uri":"/mod%E5%92%8Cgopath%E4%BE%9D%E8%B5%96%E7%AE%A1%E7%90%86/"},{"categories":["golang"],"content":"（3）测试 models：Student.go main：hello.go package main import ( //\"./models\" //相对路径 \"Go-Player/src/ademo/models\" //根据GOPATH找 //根据GOPATH：D:\\development\\jetbrains\\goland\\workspace，在其src目录下查找 //即GOPATH/src/Go-Player/src/ademo/models \"fmt\" ) func main() { stu := models.Student{ Name: \"张三\", } fmt.Println(stu) } 此篇文章仅介绍网上大部分GOPATH版本。Go语言Hello World都只简单地介绍了GOPATH版本。但是从Go的1.11版本之后，已不再推荐使用GOPATH来构建应用了。也就是说GOPATH被认为是废弃的，错误的做法。 ","date":"2023-01-09","objectID":"/mod%E5%92%8Cgopath%E4%BE%9D%E8%B5%96%E7%AE%A1%E7%90%86/:3:3","tags":["go编程技巧"],"title":"Mod和gopath依赖管理","uri":"/mod%E5%92%8Cgopath%E4%BE%9D%E8%B5%96%E7%AE%A1%E7%90%86/"},{"categories":["golang"],"content":"4、一些踩坑经验 当你开启了GO111MODULE，仍然使用GOPATH模式的方法，在引入自定义模块时会报错。go mod具体使用将在下一篇介绍 GO111MODULE 有三个值：off, on和auto（默认值）。 GO111MODULE=off，go命令行将不会支持module功能，寻找依赖包的方式将会沿用旧版本那种通过vendor目录或者GOPATH模式来查找。 GO111MODULE=on，go命令行会使用modules，而一点也不会去GOPATH目录下查找。 GO111MODULE=auto，默认值，go命令行将会根据当前目录来决定是否启用module功能。这种情况下可以分为两种情形： 当前目录在GOPATH/src之外且该目录包含go.mod文件，即使用go mod对项目的第三方依赖进行管理，不再使用gopath的方式 当前文件在包含go.mod文件的目录下面。 当modules 功能启用时，依赖包的存放位置变更为$GOPATH/pkg，允许同一个package多个版本并存，且多个项目可以共享缓存的 module。 ","date":"2023-01-09","objectID":"/mod%E5%92%8Cgopath%E4%BE%9D%E8%B5%96%E7%AE%A1%E7%90%86/:4:0","tags":["go编程技巧"],"title":"Mod和gopath依赖管理","uri":"/mod%E5%92%8Cgopath%E4%BE%9D%E8%B5%96%E7%AE%A1%E7%90%86/"},{"categories":["golang"],"content":"（1）使用了了相对路径：import “./models” 报错：build command-line-arguments: cannot find module for path _/D_/dev这里后面一堆本地路径 这是因为在go module下 你源码中 impot …/ 这样的引入形式不支持了， 应该改成 impot 模块名/ 。 这样就ok了 ","date":"2023-01-09","objectID":"/mod%E5%92%8Cgopath%E4%BE%9D%E8%B5%96%E7%AE%A1%E7%90%86/:4:1","tags":["go编程技巧"],"title":"Mod和gopath依赖管理","uri":"/mod%E5%92%8Cgopath%E4%BE%9D%E8%B5%96%E7%AE%A1%E7%90%86/"},{"categories":["golang"],"content":"（2）使用结合了GOPATH的形式：import \"Go-Player/src/ademo/models\" 于是我们把上面的import改成了结合GOPATH的如上形式 报错：package Go-Player/src/ademo/models is not in GOROOT D:/development/go/src/GPlayer/src/ademo/models ","date":"2023-01-09","objectID":"/mod%E5%92%8Cgopath%E4%BE%9D%E8%B5%96%E7%AE%A1%E7%90%86/:4:2","tags":["go编程技巧"],"title":"Mod和gopath依赖管理","uri":"/mod%E5%92%8Cgopath%E4%BE%9D%E8%B5%96%E7%AE%A1%E7%90%86/"},{"categories":["golang"],"content":"（3）彻底解决方法：用go env -u 恢复初始设置 不再使用go mod： go env -w GO111MODULE=off 或者 go env -w GO111MODULE=auto go env -u GO111MODULE 区别在于，如果GO111MODULE=on或者auto，在go get下载包时候，会下载到GOPATH/pkg/mod，引入时也是同样的从这个目录开始。如果这行了上述命令，那么在go get下载包时候，会下载到GOPATH/src 目录下 ","date":"2023-01-09","objectID":"/mod%E5%92%8Cgopath%E4%BE%9D%E8%B5%96%E7%AE%A1%E7%90%86/:4:3","tags":["go编程技巧"],"title":"Mod和gopath依赖管理","uri":"/mod%E5%92%8Cgopath%E4%BE%9D%E8%B5%96%E7%AE%A1%E7%90%86/"},{"categories":["golang"],"content":"4、最佳实践 如果在GOPATH目录下的src文件下编写项目代码，可以不使用mod管理方式，直接沿用旧版本的GOPATH管理； 如果在GOPATH以外的任意目录下要编译并运行项目代码，为了方便管理第三方依赖，使用go mod进行管理。 #初始化并创建以“当前项目根目录”为moudles名称的go.mod文件 go mod init 当前项目根目录 #剔除无用包，拿取有用包，准备代码所需环境（第三方依赖） go mod tidy #编译运行即可 #注意：当前mod文件里的moudles名称必须与当前项目目录名保持一致，否则会报错！ ","date":"2023-01-09","objectID":"/mod%E5%92%8Cgopath%E4%BE%9D%E8%B5%96%E7%AE%A1%E7%90%86/:5:0","tags":["go编程技巧"],"title":"Mod和gopath依赖管理","uri":"/mod%E5%92%8Cgopath%E4%BE%9D%E8%B5%96%E7%AE%A1%E7%90%86/"},{"categories":["数据结构与算法"],"content":"leetcode每日一题","date":"2023-01-08","objectID":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/","tags":["leetcode"],"title":"Leetcode每日一题","uri":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/"},{"categories":["数据结构与算法"],"content":"Leetcode刷题笔记 [toc] ","date":"2023-01-08","objectID":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/:0:0","tags":["leetcode"],"title":"Leetcode每日一题","uri":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/"},{"categories":["数据结构与算法"],"content":"day01 ","date":"2023-01-08","objectID":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/:1:0","tags":["leetcode"],"title":"Leetcode每日一题","uri":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/"},{"categories":["数据结构与算法"],"content":"移除元素 题目 给你一个数组 nums 和一个值val，你需要 原地 移除所有数值等于val的元素，并返回移除后数组的新长度。 不要使用额外的数组空间，你必须仅使用 O(1) 额外空间并 原地 修改输入数组。 元素的顺序可以改变。你不需要考虑数组中超出新长度后面的元素。 暴力题解 这个题目暴力的解法就是两层for循环，一个for循环遍历数组元素 ，第二个for循环更新数组。 class Solution { public int removeElement(int[] nums, int val) { int j=0; int i=0; int len=nums.length; for(i=0;i\u003clen;i++){ if(val==nums[i]){ //将之后的元素全部往前挪移覆盖一位 for(j=i+1;j\u003clen;j++){ nums[j-1]=nums[j]; } //将数组索引也往前挪 i--; len--; } } return len; } } 双指针法求解 由于题目要求删除数组中等于val的元素，因此输出数组的长度一定小于等于输入数组的长度，我们可以把输出的数组直接写在输入数组上。可以使用双指针：右指针right 指向当前将要处理的元素，左指针 left 指向下一个将要赋值的位置。 如果右指针指向的元素不等于 val，它一定是输出数组的一个元素，我们就将右指针指向的元素复制到左指针位置，然后将左右指针同时右移； 如果右指针指向的元素等于 val，它不能在输出数组里，此时左指针不动，右指针右移一位。 整个过程保持不变的性质是：区间 [0,left)中的元素都不等于 val。当左右指针遍历完输入数组以后，left 的值就是输出数组的长度。 class Solution { public int removeElement(int[] nums, int val) { int n = nums.length; int left = 0;//指输出元素索引 for (int right = 0; right \u003c n; right++){ if (nums[right] != val) { //不等于删除值，则将这个元素添加至输出元素 nums[left] = nums[right]; left++; } } return left; } } ","date":"2023-01-08","objectID":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/:1:1","tags":["leetcode"],"title":"Leetcode每日一题","uri":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/"},{"categories":["数据结构与算法"],"content":"day02 ","date":"2023-01-08","objectID":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/:2:0","tags":["leetcode"],"title":"Leetcode每日一题","uri":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/"},{"categories":["数据结构与算法"],"content":"最大子数组和 题目 给你一个整数数组 nums ，请你找出一个具有最大和的连续子数组（子数组最少包含一个元素），返回其最大和。子数组 是数组中的一个连续部分。 暴力破解 找规律 对于含有正数的序列而言,最大子序列肯定是正数,所以头尾肯定都是正数.我们可以从第一个正数开始算起,每往后加一个数便更新一次和的最大值;当当前和成为负数时,则表明此前序列无法为后面提供最大子序列和,因此必须重新确定序列首项 class Solution { public int maxSubArray(int[] nums) { int maxSum = Integer.MIN_VALUE;//最大和 int thisSum = 0;//当前和 int len = nums.length; for(int i = 0; i \u003c len; i++) { thisSum += nums[i]; if(maxSum \u003c thisSum) { maxSum = thisSum; } //如果当前和小于0则归零，因为对于后面的元素来说这些是减小的。于是归零，意即从此处算开始最大和 if(thisSum \u003c 0) { thisSum = 0; } } return maxSum; } } ","date":"2023-01-08","objectID":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/:2:1","tags":["leetcode"],"title":"Leetcode每日一题","uri":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/"},{"categories":["数据结构与算法"],"content":"加一 题目 给定一个由 整数 组成的 非空 数组所表示的非负整数，在该数的基础上加一。最高位数字存放在数组的首位， 数组中每个元素只存储单个数字。你可以假设除了整数 0 之外，这个整数不会以零开头。 找规律 将这个看作是两个数组相加，辅助数组始终要比给定数组多一位，用来承载可能两数之和比给定数组多一位。两数相加，先从末尾相加，依次往前，满十进一 class Solution { public int[] plusOne(int[] digits) { int len1=digits.length; int len2=len1+1; int i,j; int[] arr=new int[len2]; arr[len2-1]=1; for(i=len1-1,j=len2-1;;i--,j--){ int sum=digits[i]+arr[j]; if(sum\u003e9){ //进1 arr[j-1]++; //取余 arr[j]=(sum)%10; }else{ arr[j]=sum; } //跳出条件放到此处，防止少算一次 if (i==0){ break; } } //根据首位判断是否输出首位 if (arr[0]==0){ for (i=1,j=0;i\u003clen2;){ digits[j++]=arr[i++]; } return digits; } return arr; } } ","date":"2023-01-08","objectID":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/:2:2","tags":["leetcode"],"title":"Leetcode每日一题","uri":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/"},{"categories":["数据结构与算法"],"content":"day03 ","date":"2023-01-08","objectID":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/:3:0","tags":["leetcode"],"title":"Leetcode每日一题","uri":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/"},{"categories":["数据结构与算法"],"content":"合并两个有序数组 题目 给你两个按 非递减顺序 排列的整数数组 nums1 和 nums2，另有两个整数 m 和 n ，分别表示 nums1 和 nums2 中的元素数目。请你 合并 nums2 到 nums1 中，使合并后的数组同样按 非递减顺序 排列。 注意：最终，合并后数组不应由函数返回，而是存储在数组 nums1 中。为了应对这种情况，nums1 的初始长度为 m + n，其中前 m 个元素表示应合并的元素，后 n 个元素为 0 ，应忽略。nums2 的长度为 n 。 方法一：先合并再排序（缺点，没有利用两数组已经排序的条件） class Solution { public void merge(int[] nums1, int m, int[] nums2, int n) { //先合并末尾数据 for (int i = m, j = 0; i \u003c nums1.length \u0026\u0026 j \u003c n; i++, j++) { nums1[i] = nums2[j]; } //按递增排序(希尔排序) for (int gap = nums1.length / 2; gap \u003e 0; gap /= 2) {//步长 //对每组进行直接插入排序 for (int i = gap; i \u003c nums1.length; i++) { int index = i; int value = nums1[i]; while (index - gap \u003e= 0 \u0026\u0026 nums1[index - gap] \u003e value) { nums1[index] = nums1[index - gap]; index -= gap; } nums1[index] = value; } } } } 方法二：双指针 方法一没有利用数组nums1与 nums2已经被排序的性质。为了利用这一性质，我们可以使用双指针方法。这一方法将两个数组看作队列，每次从两个数组头部取出比较小的数字放到结果中。如下面的动画所示： 我们为两个数组分别设置一个指针p1与p2来作为队列的头部指针。代码实现如下： class Solution { public void merge(int[] nums1, int m, int[] nums2, int n) { int p1 = 0, p2 = 0; int[] sorted = new int[m + n]; int cur; while (p1 \u003c m || p2 \u003c n) { if (p1 == m) { cur = nums2[p2++]; } else if (p2 == n) { cur = nums1[p1++]; } else if (nums1[p1] \u003c nums2[p2]) { cur = nums1[p1++]; } else { cur = nums2[p2++]; } sorted[p1 + p2 - 1] = cur; } for (int i = 0; i != m + n; ++i) { nums1[i] = sorted[i]; } } } ","date":"2023-01-08","objectID":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/:3:1","tags":["leetcode"],"title":"Leetcode每日一题","uri":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/"},{"categories":["数据结构与算法"],"content":"day04 ","date":"2023-01-08","objectID":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/:4:0","tags":["leetcode"],"title":"Leetcode每日一题","uri":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/"},{"categories":["数据结构与算法"],"content":"59. 螺旋矩阵 II 题目 给你一个正整数 n ，生成一个包含 1 到 n2 所有元素，且元素按顺时针顺序螺旋排列的 n x n 正方形矩阵 matrix 。 示例 1： 输入：n = 3 输出：[[1,2,3],[8,9,4],[7,6,5]] 示例 2： 输入：n = 1 输出：[[1]] 提示： 1 \u003c= n \u003c= 20 方法一：暴力破解，环形遍历–\u003e也即按层模拟 package other; import java.util.Arrays; class Solution5 { public static void main(String[] args) { System.out.println(Arrays.deepToString(generateMatrix(4))); } public static int[][] generateMatrix(int n) { int[][] arr = new int[n][n]; int val = 1;//表示填入值 int i = 0, j = 0;//arr数组索引，i为行标，j列标 int tempN1 = n;//表示每列或每行在某环的行走最大值 int tempN2 = 0;//表示每列或每行在某环的行走最小值 for (int l = 0; l \u003c n - n / 2; l++) {//环数 while (j \u003c tempN1) { arr[i][j] = val++; j++; } j--;//还原j值 i++;//下一个数 while (i \u003c tempN1) { arr[i][j] = val++; i++; } i--;//还原i值 j--;//下一个数 while (j \u003e= tempN2) { arr[i][j] = val++; j--; } j++;//还原j值 i--;//下一个数 while (i \u003e tempN2) { arr[i][j] = val++; i--; } i++;//还原i数 j++;//指向下一个数字 tempN1--; tempN2++; } return arr; } } ","date":"2023-01-08","objectID":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/:4:1","tags":["leetcode"],"title":"Leetcode每日一题","uri":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/"},{"categories":["数据结构与算法"],"content":"day05 ","date":"2023-01-08","objectID":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/:5:0","tags":["leetcode"],"title":"Leetcode每日一题","uri":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/"},{"categories":["数据结构与算法"],"content":"209. 长度最小的子数组 题目 给定一个含有 n 个正整数的数组和一个正整数 target 。 找出该数组中满足其和 ≥ target 的长度最小的 连续子数组 [numsl, numsl+1, …, numsr-1, numsr] ，并返回其长度。如果不存在符合条件的子数组，返回 0 。 方法一：暴力破解（超时） 题目要求求出和大于等于target的长度最小的连续子数组，于是乎，就使用一个“窗口”从长度最小的1开始到整个数组的长度遍历，每次遍历都要“框出”子数组的和并比较，满足要求就返回。 public int minSubArrayLen(int target, int[] nums) { int left=0;//每个窗口的左索引 int sum=0; for(int cap=1;cap\u003c=nums.length;cap++){//滑动窗口大小 for(left=0;left\u003cnums.length-cap+1;left++){//左索引位置 sum=0; //计算窗口内的和 for(int i=left;i\u003cleft+cap;i++){ sum+=nums[i]; } if (sum\u003e=target) return cap; } } return 0; } 方法二：滑动窗口 使用左右指针 left 和 right，left 和 right 之间的长度即为滑动窗口的大小（即连续数组的大小）。如果滑动窗口内的值 sum \u003e= target，维护连续数组最短长度，left 向右移动，缩小滑动窗口。如果滑动窗口内的值 sum \u003c target，则 right 向有移动，扩大滑动窗口。 public int minSubArrayLen(int target, int[] nums) { int left=0; int right=0; int sum=0; int len=Integer.MAX_VALUE; while(right\u003cnums.length){ sum+=nums[right]; while(sum\u003e=target){ if(len\u003eright-left+1) len=right-left+1;//与上次的进行比较，保留最短结果 sum-=nums[left++]; } right++; } if (len==Integer.MAX_VALUE) return 0; return len; } ","date":"2023-01-08","objectID":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/:5:1","tags":["leetcode"],"title":"Leetcode每日一题","uri":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/"},{"categories":["数据结构与算法"],"content":"day06 ","date":"2023-01-08","objectID":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/:6:0","tags":["leetcode"],"title":"Leetcode每日一题","uri":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/"},{"categories":["数据结构与算法"],"content":"977. 有序数组的平方 题目 给你一个按 非递减顺序 排序的整数数组 nums，返回 每个数字的平方 组成的新数组，要求也按 非递减顺序 排序。 示例 1： 输入：nums = [-4,-1,0,3,10] 输出：[0,1,9,16,100] 解释：平方后，数组变为 [16,1,0,9,100] 排序后，数组变为 [0,1,9,16,100] 示例2： 输入：nums = [-7,-3,2,3,11] 输出：[4,9,9,49,121] 方法一：直接排序 最简单的方法就是将数组nums中的数平方后直接排序。 class Solution { public int[] sortedSquares(int[] nums) { int[] ans = new int[nums.length]; for (int i = 0; i \u003c nums.length; ++i) { ans[i] = nums[i] * nums[i]; } Arrays.sort(ans); return ans; } } 方法二：双指针 同样地，我们可以使用两个指针分别指向位置 0 和 n-1，每次比较两个指针对应的数，选择较大的那个逆序放入答案并移动指针。这种方法无需处理某一指针移动至边界的情况，读者可以仔细思考其精髓所在。 class Solution { public int[] sortedSquares(int[] nums) { int n = nums.length; int[] ans = new int[n]; for (int i = 0, j = n - 1, pos = n - 1; i \u003c= j;) { if (nums[i] * nums[i] \u003e nums[j] * nums[j]) { ans[pos] = nums[i] * nums[i]; ++i; } else { ans[pos] = nums[j] * nums[j]; --j; } --pos; } return ans; } } ","date":"2023-01-08","objectID":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/:6:1","tags":["leetcode"],"title":"Leetcode每日一题","uri":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/"},{"categories":["数据结构与算法"],"content":"day07 ","date":"2023-01-08","objectID":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/:7:0","tags":["leetcode"],"title":"Leetcode每日一题","uri":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/"},{"categories":["数据结构与算法"],"content":"19. 删除链表的倒数第 N 个结点 题目 给你一个链表，删除链表的倒数第 n 个结点，并且返回链表的头结点。 示例 1： 输入：head = [1,2,3,4,5], n = 2 输出：[1,2,3,5] 示例 2： 输入：head = [1], n = 1 输出：[] 示例 3： 输入：head = [1,2], n = 1 输出：[1] 提示： 链表中结点的数目为 sz 1 \u003c= sz \u003c= 30 0 \u003c= Node.val \u003c= 100 1 \u003c= n \u003c= sz **进阶：**你能尝试使用一趟扫描实现吗？ 解决思路 前言 在对链表进行操作时，一种常用的技巧是添加一个哑节点（dummy node），它的 next 指针指向链表的头节点。这样一来，我们就不需要对头节点进行特殊的判断了。 例如，在本题中，如果我们要删除节点 y，我们需要知道节点 y 的前驱节点 x，并将 x 的指针指向 y 的后继节点。但由于头节点不存在前驱节点，因此我们需要在删除头节点时进行特殊判断。但如果我们添加了哑节点，那么头节点的前驱节点就是哑节点本身，此时我们就只需要考虑通用的情况即可。（注意：此处头节点存放了有意义的数据） 特别地，在某些语言中，由于需要自行对内存进行管理。因此在实际的面试中，对于「是否需要释放被删除节点对应的空间」这一问题，我们需要和面试官进行积极的沟通以达成一致。下面的代码中默认不释放空间。 方法一：计算链表长度 思路与算法 一种容易想到的方法是，我们首先从头节点开始对链表进行一次遍历，得到链表的长度 LL。随后我们再从头节点开始对链表进行一次遍历，当遍历到第 L-n+1个节点时，它就是我们需要删除的节点。 为了与题目中的 n 保持一致，节点的编号从 1 开始，头节点为编号 1 的节点。 为了方便删除操作，我们可以从哑节点开始遍历 L-n+1个节点。当遍历到第 L-n+1 个节点时，它的下一个节点就是我们需要删除的节点，这样我们只需要修改一次指针，就能完成删除操作。 代码 class Solution { public ListNode removeNthFromEnd(ListNode head, int n) { ListNode dummy = new ListNode(0, head); int length = getLength(head); ListNode cur = dummy; for (int i = 1; i \u003c length - n + 1; ++i) { cur = cur.next; } cur.next = cur.next.next; ListNode ans = dummy.next; return ans; } public int getLength(ListNode head) { int length = 0; while (head != null) { ++length; head = head.next; } return length; } } /** * Definition for singly-linked list. * type ListNode struct { * Val int * Next *ListNode * } */ func removeNthFromEnd(head *ListNode, n int) *ListNode { l:=0 heads:=\u0026ListNode{Next:head} tmp:=heads for tmp!=nil{ l++ tmp=tmp.Next } tmp = heads no:=0 for tmp!=nil { // 找到了n节点 if no==l-n-1 { if tmp.Next!=nil{ tmp.Next = tmp.Next.Next }else{ tmp.Next = nil } } no++ tmp=tmp.Next } return heads.Next } 复杂度分析 时间复杂度：O(L)，其中 L是链表的长度。 空间复杂度：O(1)。 方法二：栈 思路与算法 我们也可以在遍历链表的同时将所有节点依次入栈。根据栈「先进后出」的原则，我们弹出栈的第 n 个节点就是需要删除的节点，并且目前栈顶的节点就是待删除节点的前驱节点。这样一来，删除操作就变得十分方便了。 代码 class Solution { public ListNode removeNthFromEnd(ListNode head, int n) { ListNode dummy = new ListNode(0, head); Deque\u003cListNode\u003e stack = new LinkedList\u003cListNode\u003e(); ListNode cur = dummy; while (cur != null) { stack.push(cur); cur = cur.next; } for (int i = 0; i \u003c n; ++i) { stack.pop(); } ListNode prev = stack.peek(); prev.next = prev.next.next; ListNode ans = dummy.next; return ans; } } /** * Definition for singly-linked list. * type ListNode struct { * Val int * Next *ListNode * } */ func removeNthFromEnd(head *ListNode, n int) *ListNode { heads:=\u0026ListNode{Next:head} stack := []*ListNode{} // 先所有节点入栈 tmp:=heads for tmp!=nil { stack = append(stack,tmp) tmp = tmp.Next } var lastN *ListNode for n\u003e0{ lastN = stack[len(stack)-1] stack = stack[:len(stack)-1] n-- } stack[len(stack)-1].Next = lastN.Next return heads.Next } 复杂度分析 时间复杂度：O(L)，其中 L 是链表的长度。 空间复杂度：O(L)，其中 L 是链表的长度。主要为栈的开销。 方法三：双指针 思路与算法 我们也可以在不预处理出链表的长度，以及使用常数空间的前提下解决本题。 由于我们需要找到倒数第 n 个节点，因此我们可以使用两个指针first 和 second 同时对链表进行遍历，并且 first 比 second 超前 n 个节点。当 first 遍历到链表的末尾时，second 就恰好处于倒数第 n 个节点。 具体地，初始时 first 和second 均指向头节点。我们首先使用first 对链表进行遍历，遍历的次数为 n。此时，first 和 second 之间间隔了 n-1个节点，即first 比 second 超前了 n个节点。 在这之后，我们同时使用 first和 second 对链表进行遍历。当first 遍历到链表的末尾（即first 为空指针）时，second 恰好指向倒数第 n 个节点。 根据方法一和方法二，如果我们能够得到的是倒数第 n 个节点的前驱节点而不是倒数第 n 个节点的话，删除操作会更加方便。因此我们可以考虑在初始时将 second 指向哑节点，其余的操作步骤不变。这样一来，当 first 遍历到链表的末尾时，second 的下一个节点就是我们需要删除的节点。 代码 class Solution { public ListNode removeNthFromEnd(ListNode head, int n) { ListNode dummy = new ListNode(0, head); ListNode first = head; ListNode second = dummy; for (int i = 0; i \u003c n; ++i) { first = first.next; } while (first != null) { first = first.next; second = second.next; } second.next = second.next.next; ListNode ans = dummy.next; return ans; } } /** * Definition for singly-linked list. * type ListNode struct { * Val int * Next *ListNode * } */ func removeNthFromEnd(head *ListNode, n int) *ListNode { heads:=\u0026ListNode{Next:head} first:=heads second:=heads for n\u003e0 { first = first.Next n-- } for first.Next!=nil{ first=first.Next second = second.Next } // fmt.Println(second.Val) second.Next = second.Next.Next return heads.Next } 复杂度分析 时间复杂度：O(L)，","date":"2023-01-08","objectID":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/:7:1","tags":["leetcode"],"title":"Leetcode每日一题","uri":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/"},{"categories":["数据结构与算法"],"content":"day08 ","date":"2023-01-08","objectID":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/:8:0","tags":["leetcode"],"title":"Leetcode每日一题","uri":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/"},{"categories":["数据结构与算法"],"content":"24. 两两交换链表中的节点 题目 给你一个链表，两两交换其中相邻的节点，并返回交换后链表的头节点。你必须在不修改节点内部的值的情况下完成本题（即，只能进行节点交换）。 示例 1： 输入：head = [1,2,3,4] 输出：[2,1,4,3] 示例 2： 输入：head = [] 输出：[] 示例 3： 输入：head = [1] 输出：[1] 提示： 链表中节点的数目在范围 [0, 100] 内 0 \u003c= Node.val \u003c= 100 方法 方法一：递归 思路与算法 可以通过递归的方式实现两两交换链表中的节点。 递归的终止条件是链表中没有节点，或者链表中只有一个节点，此时无法进行交换。 如果链表中至少有两个节点，则在两两交换链表中的节点之后，原始链表的头节点变成新的链表的第二个节点，原始链表的第二个节点变成新的链表的头节点。链表中的其余节点的两两交换可以递归地实现。在对链表中的其余节点递归地两两交换之后，更新节点之间的指针关系，即可完成整个链表的两两交换。 用 head 表示原始链表的头节点，新的链表的第二个节点，用 newHead 表示新的链表的头节点，原始链表的第二个节点，则原始链表中的其余节点的头节点是 newHead.next。令 head.next = swapPairs(newHead.next)，表示将其余节点进行两两交换，交换后的新的头节点为 head 的下一个节点。然后令 newHead.next = head，即完成了所有节点的交换。最后返回新的链表的头节点 newHead。 代码 class Solution { public ListNode swapPairs(ListNode head) { if (head == null || head.next == null) { return head; } ListNode newHead = head.next; head.next = swapPairs(newHead.next); newHead.next = head; return newHead; } } 复杂度分析 时间复杂度：O(n)，其中 n 是链表的节点数量。需要对每个节点进行更新指针的操作。 空间复杂度：O(n)，其中 n 是链表的节点数量。空间复杂度主要取决于递归调用的栈空间。 方法二：迭代 思路与算法 也可以通过迭代的方式实现两两交换链表中的节点。 创建哑结点 dummyHead，令 dummyHead.next = head。令 temp 表示当前到达的节点，初始时 temp = dummyHead。每次需要交换 temp 后面的两个节点。 如果 temp 的后面没有节点或者只有一个节点，则没有更多的节点需要交换，因此结束交换。否则，获得 temp 后面的两个节点 node1 和 node2，通过更新节点的指针关系实现两两交换节点。 具体而言，交换之前的节点关系是 temp -\u003e node1 -\u003e node2，交换之后的节点关系要变成 temp -\u003e node2 -\u003e node1，因此需要进行如下操作。 temp.next = node2 node1.next = node2.next node2.next = node1 完成上述操作之后，节点关系即变成 temp -\u003e node2 -\u003e node1。再令 temp = node1，对链表中的其余节点进行两两交换，直到全部节点都被两两交换。 两两交换链表中的节点之后，新的链表的头节点是 dummyHead.next，返回新的链表的头节点即可。 代码 class Solution { public ListNode swapPairs(ListNode head) { ListNode dummyHead = new ListNode(0); dummyHead.next = head; ListNode temp = dummyHead; while (temp.next != null \u0026\u0026 temp.next.next != null) { ListNode node1 = temp.next; ListNode node2 = temp.next.next; temp.next = node2; node1.next = node2.next; node2.next = node1; temp = node1; } return dummyHead.next; } } //the other /** * Definition for singly-linked list. * public class ListNode { * int val; * ListNode next; * ListNode() {} * ListNode(int val) { this.val = val; } * ListNode(int val, ListNode next) { this.val = val; this.next = next; } * } */ class Solution { public ListNode swapPairs(ListNode head) { //哑节点 ListNode dummyNode = new ListNode(-1,head); //每两个节点的遍历链表，当不足两个时则不交换 ListNode temp=dummyNode; ListNode pre=temp.next; if(temp.next==null){ return dummyNode.next; } ListNode post=temp.next.next; while(pre!=null\u0026\u0026post!=null){ temp.next=post; pre.next=post.next; post.next=pre; //移动指针 temp=pre; pre=temp.next; if(temp.next!=null){ post=temp.next.next; } } return dummyNode.next; } } 复杂度分析 时间复杂度：O(n)，其中 n 是链表的节点数量。需要对每个节点进行更新指针的操作。 空间复杂度：O(1)。 ","date":"2023-01-08","objectID":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/:8:1","tags":["leetcode"],"title":"Leetcode每日一题","uri":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/"},{"categories":["数据结构与算法"],"content":"day09 ","date":"2023-01-08","objectID":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/:9:0","tags":["leetcode"],"title":"Leetcode每日一题","uri":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/"},{"categories":["数据结构与算法"],"content":"25. K 个一组翻转链表 题目 给你链表的头节点 head ，每 k 个节点一组进行翻转，请你返回修改后的链表。 k 是一个正整数，它的值小于或等于链表的长度。如果节点总数不是 k 的整数倍，那么请将最后剩余的节点保持原有顺序。 你不能只是单纯的改变节点内部的值，而是需要实际进行节点交换。 示例 1： 输入：head = [1,2,3,4,5], k = 2 输出：[2,1,4,3,5] 示例 2： 输入：head = [1,2,3,4,5], k = 3 输出：[3,2,1,4,5] 提示： 链表中的节点数目为 n 1 \u003c= k \u003c= n \u003c= 5000 0 \u003c= Node.val \u003c= 1000 **进阶：**你可以设计一个只用 O(1) 额外内存空间的算法解决此问题吗？ 方法 方法一：模拟 思路与算法 本题的目标非常清晰易懂，不涉及复杂的算法，但是实现过程中需要考虑的细节比较多，容易写出冗长的代码。主要考查面试者设计的能力。 我们需要把链表节点按照 k 个一组分组，所以可以使用一个指针 head 依次指向每组的头节点。这个指针每次向前移动 k 步，直至链表结尾。对于每个分组，我们先判断它的长度是否大于等于 k。若是，我们就翻转这部分链表，否则不需要翻转。 接下来的问题就是如何翻转一个分组内的子链表。翻转一个链表并不难，过程可以参考「206. 反转链表」。但是对于一个子链表，除了翻转其本身之外，还需要将子链表的头部与上一个子链表连接，以及子链表的尾部与下一个子链表连接。如下图所示： 因此，在翻转子链表的时候，我们不仅需要子链表头节点 head，还需要有 head 的上一个节点 pre，以便翻转完后把子链表再接回 pre。 但是对于第一个子链表，它的头节点 head 前面是没有节点 pre 的。太麻烦了！难道只能特判了吗？答案是否定的。没有条件，我们就创造条件；没有节点，我们就创建一个节点。我们新建一个节点（哑节点），把它接到链表的头部，让它作为 pre 的初始值，这样 head 前面就有了一个节点，我们就可以避开链表头部的边界条件。这么做还有一个好处，下面我们会看到。 反复移动指针 head 与 pre，对 head 所指向的子链表进行翻转，直到结尾，我们就得到了答案。下面我们该返回函数值了。 有的同学可能发现这又是一件麻烦事：链表翻转之后，链表的头节点发生了变化，那么应该返回哪个节点呢？照理来说，前 k 个节点翻转之后，链表的头节点应该是第 k 个节点。那么要在遍历过程中记录第 k 个节点吗？但是如果链表里面没有 k 个节点，答案又还是原来的头节点。我们又多了一大堆循环和判断要写，太崩溃了！ 等等！还记得我们创建了节点 pre 吗？这个节点一开始被连接到了头节点的前面，而无论之后链表有没有翻转，它的 next 指针都会指向正确的头节点。那么我们只要返回它的下一个节点就好了。至此，问题解决。 class Solution { public ListNode reverseKGroup(ListNode head, int k) { ListNode hair = new ListNode(0); hair.next = head; ListNode pre = hair; while (head != null) { ListNode tail = pre; // 查看剩余部分长度是否大于等于 k for (int i = 0; i \u003c k; ++i) { tail = tail.next; if (tail == null) { return hair.next; } } ListNode nex = tail.next; ListNode[] reverse = myReverse(head, tail); head = reverse[0]; tail = reverse[1]; // 把子链表重新接回原链表 pre.next = head; tail.next = nex; pre = tail; head = tail.next; } return hair.next; } public ListNode[] myReverse(ListNode head, ListNode tail) { ListNode prev = tail.next; ListNode p = head; while (prev != tail) { ListNode nex = p.next; p.next = prev; prev = p; p = nex; } return new ListNode[]{tail, head}; } } 复杂度分析 时间复杂度：O(n)，其中 n 为链表的长度。head 指针会在 $O(\\lfloor \\frac{n}{k} \\rfloor) $个节点上停留，每次停留需要进行一次 O(k) 的翻转操作。 空间复杂度：O(1)，我们只需要建立常数个变量。 ","date":"2023-01-08","objectID":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/:9:1","tags":["leetcode"],"title":"Leetcode每日一题","uri":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/"},{"categories":["数据结构与算法"],"content":"day10 ","date":"2023-01-08","objectID":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/:10:0","tags":["leetcode"],"title":"Leetcode每日一题","uri":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/"},{"categories":["数据结构与算法"],"content":"141. 环形链表 题目 给你一个链表的头节点 head ，判断链表中是否有环。 如果链表中有某个节点，可以通过连续跟踪 next 指针再次到达，则链表中存在环。 为了表示给定链表中的环，评测系统内部使用整数 pos 来表示链表尾连接到链表中的位置（索引从 0 开始）。注意：pos 不作为参数进行传递 。仅仅是为了标识链表的实际情况。 如果链表中存在环 ，则返回 true 。 否则，返回 false 。 示例 1： 输入：head = [3,2,0,-4], pos = 1 输出：true 解释：链表中有一个环，其尾部连接到第二个节点。 示例 2： 输入：head = [1,2], pos = 0 输出：true 解释：链表中有一个环，其尾部连接到第一个节点。 示例 3： 输入：head = [1], pos = -1 输出：false 解释：链表中没有环。 提示： 链表中节点的数目范围是 [0, 104] -105 \u003c= Node.val \u003c= 105 pos 为 -1 或者链表中的一个 有效索引 。 **进阶：**你能用 O(1)（即，常量）内存解决此问题吗？ 解法 方法一：哈希表 思路及算法 最容易想到的方法是遍历所有节点，每次遍历到一个节点时，判断该节点此前是否被访问过。 具体地，我们可以使用哈希表来存储所有已经访问过的节点。每次我们到达一个节点，如果该节点已经存在于哈希表中，则说明该链表是环形链表，否则就将该节点加入哈希表中。重复这一过程，直到我们遍历完整个链表即可。 代码 public class Solution { public boolean hasCycle(ListNode head) { Set\u003cListNode\u003e seen = new HashSet\u003cListNode\u003e(); while (head != null) { if (!seen.add(head)) { return true; } head = head.next; } return false; } } 复杂度分析 时间复杂度：O(N)，其中 N 是链表中的节点数。最坏情况下我们需要遍历每个节点一次。 空间复杂度：O(N)，其中 N 是链表中的节点数。主要为哈希表的开销，最坏情况下我们需要将每个节点插入到哈希表中一次。 方法二：快慢指针 思路及算法 本方法需要读者对「Floyd 判圈算法」（又称龟兔赛跑算法）有所了解。 假想「乌龟」和「兔子」在链表上移动，「兔子」跑得快，「乌龟」跑得慢。当「乌龟」和「兔子」从链表上的同一个节点开始移动时，如果该链表中没有环，那么「兔子」将一直处于「乌龟」的前方；如果该链表中有环，那么「兔子」会先于「乌龟」进入环，并且一直在环内移动。等到「乌龟」进入环时，由于「兔子」的速度快，它一定会在某个时刻与乌龟相遇，即套了「乌龟」若干圈。 我们可以根据上述思路来解决本题。具体地，我们定义两个指针，一快一满。慢指针每次只移动一步，而快指针每次移动两步。初始时，慢指针在位置 head，而快指针在位置 head.next。这样一来，如果在移动的过程中，快指针反过来追上慢指针，就说明该链表为环形链表。否则快指针将到达链表尾部，该链表不为环形链表。 细节 为什么我们要规定初始时慢指针在位置 head，快指针在位置 head.next，而不是两个指针都在位置 head（即与「乌龟」和「兔子」中的叙述相同）？ 观察下面的代码，我们使用的是 while 循环，循环条件先于循环体。由于循环条件一定是判断快慢指针是否重合，如果我们将两个指针初始都置于 head，那么 while 循环就不会执行。因此，我们可以假想一个在 head 之前的虚拟节点，慢指针从虚拟节点移动一步到达 head，快指针从虚拟节点移动两步到达 head.next，这样我们就可以使用 while 循环了。 当然，我们也可以使用 do-while 循环。此时，我们就可以把快慢指针的初始值都置为 head。 代码 public class Solution { public boolean hasCycle(ListNode head) { if (head == null || head.next == null) { return false; } ListNode slow = head; ListNode fast = head.next; while (slow != fast) { if (fast == null || fast.next == null) { return false; } slow = slow.next; fast = fast.next.next; } return true; } } 复杂度分析 时间复杂度：O(N)，其中 N 是链表中的节点数。 当链表中不存在环时，快指针将先于慢指针到达链表尾部，链表中每个节点至多被访问两次。 当链表中存在环时，每一轮移动后，快慢指针的距离将减小一。而初始距离为环的长度，因此至多移动 N 轮。 空间复杂度：O(1)。我们只使用了两个指针的额外空间。 ","date":"2023-01-08","objectID":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/:10:1","tags":["leetcode"],"title":"Leetcode每日一题","uri":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/"},{"categories":["数据结构与算法"],"content":"day11 ","date":"2023-01-08","objectID":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/:11:0","tags":["leetcode"],"title":"Leetcode每日一题","uri":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/"},{"categories":["数据结构与算法"],"content":"142. 环形链表 II 题目 给定一个链表的头节点 head ，返回链表开始入环的第一个节点。 如果链表无环，则返回 null。 如果链表中有某个节点，可以通过连续跟踪 next 指针再次到达，则链表中存在环。 为了表示给定链表中的环，评测系统内部使用整数 pos 来表示链表尾连接到链表中的位置（索引从 0 开始）。如果 pos 是 -1，则在该链表中没有环。注意：pos 不作为参数进行传递，仅仅是为了标识链表的实际情况。 不允许修改 链表。 示例 1： 输入：head = [3,2,0,-4], pos = 1 输出：返回索引为 1 的链表节点 解释：链表中有一个环，其尾部连接到第二个节点。 示例 2： 输入：head = [1,2], pos = 0 输出：返回索引为 0 的链表节点 解释：链表中有一个环，其尾部连接到第一个节点。 示例 3： 输入：head = [1], pos = -1 输出：返回 null 解释：链表中没有环。 提示： 链表中节点的数目范围在范围 [0, 104] 内 -105 \u003c= Node.val \u003c= 105 pos 的值为 -1 或者链表中的一个有效索引 **进阶：**你是否可以使用 O(1) 空间解决此题？ 方法 方法一：哈希表 思路与算法 一个非常直观的思路是：我们遍历链表中的每个节点，并将它记录下来；一旦遇到了此前遍历过的节点，就可以判定链表中存在环。借助哈希表可以很方便地实现。 代码 public class Solution { public ListNode detectCycle(ListNode head) { ListNode pos = head; Set\u003cListNode\u003e visited = new HashSet\u003cListNode\u003e(); while (pos != null) { if (visited.contains(pos)) { return pos; } else { visited.add(pos); } pos = pos.next; } return null; } } 复杂度分析 时间复杂度：O(N)，其中 N 为链表中节点的数目。我们恰好需要访问链表中的每一个节点。 空间复杂度：O(N)，其中 N 为链表中节点的数目。我们需要将链表中的每个节点都保存在哈希表当中。 方法二：快慢指针 思路与算法 我们使用两个指针，fast 与 slow。它们起始都位于链表的头部。随后，slow 指针每次向后移动一个位置，而 fast 指针向后移动两个位置。如果链表中存在环，则 fast 指针最终将再次与 slow 指针在环中相遇。 如下图所示，设链表中环外部分的长度为 a。slow 指针进入环后，又走了 b 的距离与 fast 相遇。此时，fast 指针已经走完了环的 n 圈，因此它走过的总距离为 a+n*(b+c)+b = a+(n+1)*b+n*c。 根据题意，任意时刻，fast 指针走过的距离都为 slow 指针的 2 倍。因此，我们有 `a+(n+1)*b+n*c = 2*(a+b) ==\u003e a = c+(n-1)*(b+c)` 有了 a=c+(n-1)(b+c) 的等量关系，我们会发现：从相遇点到入环点的距离加上 n-1 圈的环长，恰好等于从链表头部到入环点的距离。 因此，当发现 slow 与 fast 相遇时，我们再额外使用一个指针 ptr。起始，它指向链表头部；随后，它和 slow 每次向后移动一个位置。最终，它们会在入环点相遇。 public class Solution { public ListNode detectCycle(ListNode head) { if (head == null) { return null; } ListNode slow = head, fast = head; while (fast != null) { slow = slow.next; if (fast.next != null) { fast = fast.next.next; } else { return null; } if (fast == slow) { ListNode ptr = head; while (ptr != slow) { ptr = ptr.next; slow = slow.next; } return ptr; } } return null; } } 复杂度分析 时间复杂度：O(N)，其中 N 为链表中节点的数目。在最初判断快慢指针是否相遇时，slow 指针走过的距离不会超过链表的总长度；随后寻找入环点时，走过的距离也不会超过链表的总长度。因此，总的执行时间为 O(N)+O(N)=O(N)。 空间复杂度：O(1)。我们只使用了 slow,fast,ptr 三个指针。 ","date":"2023-01-08","objectID":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/:11:1","tags":["leetcode"],"title":"Leetcode每日一题","uri":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/"},{"categories":["数据结构与算法"],"content":"day12 ","date":"2023-01-08","objectID":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/:12:0","tags":["leetcode"],"title":"Leetcode每日一题","uri":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/"},{"categories":["数据结构与算法"],"content":"160. 相交链表 题目 给你两个单链表的头节点 headA 和 headB ，请你找出并返回两个单链表相交的起始节点。如果两个链表不存在相交节点，返回 null 。 图示两个链表在节点 c1 开始相交**：** 题目数据 保证 整个链式结构中不存在环。 注意，函数返回结果后，链表必须 保持其原始结构 。 自定义评测： 评测系统 的输入如下（你设计的程序 不适用 此输入）： intersectVal - 相交的起始节点的值。如果不存在相交节点，这一值为 0 listA - 第一个链表 listB - 第二个链表 skipA - 在 listA 中（从头节点开始）跳到交叉节点的节点数 skipB - 在 listB 中（从头节点开始）跳到交叉节点的节点数 评测系统将根据这些输入创建链式数据结构，并将两个头节点 headA 和 headB 传递给你的程序。如果程序能够正确返回相交节点，那么你的解决方案将被 视作正确答案 。 示例 1： 输入：intersectVal = 8, listA = [4,1,8,4,5], listB = [5,6,1,8,4,5], skipA = 2, skipB = 3 输出：Intersected at '8' 解释：相交节点的值为 8 （注意，如果两个链表相交则不能为 0）。 从各自的表头开始算起，链表 A 为 [4,1,8,4,5]，链表 B 为 [5,6,1,8,4,5]。 在 A 中，相交节点前有 2 个节点；在 B 中，相交节点前有 3 个节点。 示例 2： 输入：intersectVal = 2, listA = [1,9,1,2,4], listB = [3,2,4], skipA = 3, skipB = 1 输出：Intersected at '2' 解释：相交节点的值为 2 （注意，如果两个链表相交则不能为 0）。 从各自的表头开始算起，链表 A 为 [1,9,1,2,4]，链表 B 为 [3,2,4]。 在 A 中，相交节点前有 3 个节点；在 B 中，相交节点前有 1 个节点。 示例 3： 输入：intersectVal = 0, listA = [2,6,4], listB = [1,5], skipA = 3, skipB = 2 输出：null 解释：从各自的表头开始算起，链表 A 为 [2,6,4]，链表 B 为 [1,5]。 由于这两个链表不相交，所以 intersectVal 必须为 0，而 skipA 和 skipB 可以是任意值。 这两个链表不相交，因此返回 null 。 提示： listA 中节点数目为 m listB 中节点数目为 n 1 \u003c= m, n \u003c= 3 * 104 1 \u003c= Node.val \u003c= 105 0 \u003c= skipA \u003c= m 0 \u003c= skipB \u003c= n 如果 listA 和 listB 没有交点，intersectVal 为 0 如果 listA 和 listB 有交点，intersectVal == listA[skipA] == listB[skipB] **进阶：**你能否设计一个时间复杂度 O(m + n) 、仅用 O(1) 内存的解决方案？ 方法 方法一：哈希集合 思路和算法 判断两个链表是否相交，可以使用哈希集合存储链表节点。 首先遍历链表 headA，并将链表 headA 中的每个节点加入哈希集合中。然后遍历链表 headB，对于遍历到的每个节点，判断该节点是否在哈希集合中： 如果当前节点不在哈希集合中，则继续遍历下一个节点； 如果当前节点在哈希集合中，则后面的节点都在哈希集合中，即从当前节点开始的所有节点都在两个链表的相交部分，因此在链表 headB 中遍历到的第一个在哈希集合中的节点就是两个链表相交的节点，返回该节点。 如果链表 headB 中的所有节点都不在哈希集合中，则两个链表不相交，返回 null。 代码 public class Solution { public ListNode getIntersectionNode(ListNode headA, ListNode headB) { Set\u003cListNode\u003e visited = new HashSet\u003cListNode\u003e(); ListNode temp = headA; while (temp != null) { visited.add(temp); temp = temp.next; } temp = headB; while (temp != null) { if (visited.contains(temp)) { return temp; } temp = temp.next; } return null; } } 复杂度分析 时间复杂度：O(m+n)，其中 m 和 n 是分别是链表 headA 和 headB 的长度。需要遍历两个链表各一次。 空间复杂度：O(m)，其中 m 是链表 headA 的长度。需要使用哈希集合存储链表 headA 中的全部节点。 方法二：双指针 思路和算法 使用双指针的方法，可以将空间复杂度降至 O(1)O(1)。 只有当链表 headA 和 headB 都不为空时，两个链表才可能相交。因此首先判断链表 headA 和 headB 是否为空，如果其中至少有一个链表为空，则两个链表一定不相交，返回 null。 当链表 headA 和 headB 都不为空时，创建两个指针 pA 和 pB*，初始时分别指向两个链表的头节点 headA 和 headB，然后将两个指针依次遍历两个链表的每个节点。具体做法如下： 每步操作需要同时更新指针 pA 和 pB。 如果指针 pA 不为空，则将指针 pA 移到下一个节点；如果指针 pB 不为空，则将指针 pB 移到下一个节点。 如果指针 pA 为空，则将指针 pA 移到链表 headB 的头节点；如果指针 pB 为空，则将指针 pB 移到链表 headA 的头节点。 当指针 pA 和 pB 指向同一个节点或者都为空时，返回它们指向的节点或者 null。 证明 下面提供双指针方法的正确性证明。考虑两种情况，第一种情况是两个链表相交，第二种情况是两个链表不相交。 情况一：两个链表相交 链表 headA 和 headB 的长度分别是 m 和 n*。假设链表 headA 的不相交部分有 a 个节点，链表 headB 的不相交部分有 b 个节点，两个链表相交的部分有 c 个节点，则有 a+c=m，b+c=n。 如果 a=b，则两个指针会同时到达两个链表相交的节点，此时返回相交的节点； 如果 a!=b，则指针 pA 会遍历完链表 headA，指针 pB 会遍历完链表 headB，两个指针不会同时到达链表的尾节点，然后指针 pA 移到链表 headB 的头节点，指针 pB 移到链表 headA 的头节点，然后两个指针继续移动，在指针 pA 移动了 a+c+b 次、指针 pB 移动了 b+c+a 次之后，两个指针会同时到达两个链表相交的节点，该节点也是两个指针第一次同时指向的节点，此时返回相交的节点。 情况二：两个链表不相交 链表 headA 和 headB 的长度分别是 m 和 n。考虑当 m=n 和 m != n 时，两个指针分别会如何移动： 如果 m=n，则两个指针会同时到达两个链表的尾节点，然后同时变成空值 null，此时返回 null； 如果 m != n，则由于两个链表没有公共节点，两个指针也不会同时到达两个链表的尾节点，因此两个指针都会遍历完两个链表，在指针 pA 移动了 m+n 次、指针 pB 移动了 n+m 次之后，两个指针会同时变成空值 null，此时返回 null。 代码 public class Solution { public ListNode getIntersectionNode(ListNode headA, ListNode headB) { if (headA == null || headB == null) { return null; } ListNode pA = headA, pB = headB; while (pA != pB) { pA = pA == null ? headB : pA.next; pB = pB == null ? headA : pB.next; } return pA; } } 复杂度分析 时间复杂度：O(m+n)，其中 m 和 n 是分别是链表 headA 和 headB 的长度。两个指针同时遍历两个链表，每个指针遍历两个链表各一次。 空间复杂度：O(1)。 ","date":"2023-01-08","objectID":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/:12:1","tags":["leetcode"],"title":"Leetcode每日一题","uri":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/"},{"categories":["数据结构与算法"],"content":"day13 ","date":"2023-01-08","objectID":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/:13:0","tags":["leetcode"],"title":"Leetcode每日一题","uri":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/"},{"categories":["数据结构与算法"],"content":"206. 反转链表 题目 给你单链表的头节点 head ，请你反转链表，并返回反转后的链表。 示例 1： 输入：head = [1,2,3,4,5] 输出：[5,4,3,2,1] 示例 2： 输入：head = [1,2] 输出：[2,1] 示例 3： 输入：head = [] 输出：[] 提示： 链表中节点的数目范围是 [0, 5000] -5000 \u003c= Node.val \u003c= 5000 解法 方法一：迭代 假设存在链表 1–\u003e2–\u003e3–\u003e∅，我们想要把它改成 ∅\u003c–1\u003c–2\u003c–3。 在遍历列表时，将当前节点的 next 指针改为指向前一个元素。由于节点没有引用其上一个节点，因此必须事先存储其前一个元素。在更改引用之前，还需要另一个指针来存储下一个节点。不要忘记在最后返回新的头引用！ class Solution { public ListNode reverseList(ListNode head) { ListNode prev = null; ListNode curr = head; while (curr != null) { ListNode nextTemp = curr.next; curr.next = prev; prev = curr; curr = nextTemp; } return prev; } } 复杂度分析 时间复杂度：O(n)，假设 n 是列表的长度，时间复杂度是 O(n)。 空间复杂度：O(1)。 方法二：递归 递归版本稍微复杂一些，其关键在于反向工作。假设列表的其余部分已经被反转，现在我们应该如何反转它前面的部分？ 假设列表为： n~1~--\u003e…--\u003en~k−1~--\u003en~k~--\u003en~k+1~--\u003e…--\u003en~m~--\u003e∅ 若从节点 n~k+1~ 到 n~k~ n~1~--\u003e…--\u003en~k−1~--\u003en~k~--\u003en~k+1~\u003c--…\u003c--n~m~ 我们希望 n~k+1~ 的下一个节点指向 n~k~ 所以，n~k~.next.next*=*n~k~ 要小心的是 n~1~ 的下一个必须指向 ∅ 。如果你忽略了这一点，你的链表中可能会产生循环。如果使用大小为 22 的链表测试代码，则可能会捕获此错误。 class Solution { public ListNode reverseList(ListNode head) { if (head == null || head.next == null) { return head; } ListNode p = reverseList(head.next); head.next.next = head; head.next = null; return p; } } 复杂度分析 时间复杂度：O(n)，假设 n 是列表的长度，那么时间复杂度为 O(n)。 空间复杂度：O(n)，由于使用递归，将会使用隐式栈空间。递归深度可能会达到 n 层。 ","date":"2023-01-08","objectID":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/:13:1","tags":["leetcode"],"title":"Leetcode每日一题","uri":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/"},{"categories":["数据结构与算法"],"content":"day14 ","date":"2023-01-08","objectID":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/:14:0","tags":["leetcode"],"title":"Leetcode每日一题","uri":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/"},{"categories":["数据结构与算法"],"content":"20. 有效的括号 题目 给定一个只包括 '('，')'，'{'，'}'，'['，']' 的字符串 s ，判断字符串是否有效。 有效字符串需满足： 左括号必须用相同类型的右括号闭合。 左括号必须以正确的顺序闭合。 示例 1： 输入：s = \"()\" 输出：true 示例 2： 输入：s = \"()[]{}\" 输出：true 示例 3： 输入：s = \"(]\" 输出：false 示例 4： 输入：s = \"([)]\" 输出：false 示例 5： 输入：s = \"{[]}\" 输出：true 提示： 1 \u003c= s.length \u003c= 104 s 仅由括号 '()[]{}' 组成 题解 方法一：栈 判断括号的有效性可以使用「栈」这一数据结构来解决。 我们遍历给定的字符串 s。当我们遇到一个左括号时，我们会期望在后续的遍历中，有一个相同类型的右括号将其闭合。由于后遇到的左括号要先闭合，因此我们可以将这个左括号放入栈顶。 当我们遇到一个右括号时，我们需要将一个相同类型的左括号闭合。此时，我们可以取出栈顶的左括号并判断它们是否是相同类型的括号。如果不是相同的类型，或者栈中并没有左括号，那么字符串 s 无效，返回 False。为了快速判断括号的类型，我们可以使用哈希表存储每一种括号。哈希表的键为右括号，值为相同类型的左括号。 在遍历结束后，如果栈中没有左括号，说明我们将字符串 s 中的所有左括号闭合，返回 True，否则返回 False。 注意到有效字符串的长度一定为偶数，因此如果字符串的长度为奇数，我们可以直接返回 False，省去后续的遍历判断过程。 class Solution { public boolean isValid(String s) { int n = s.length(); if (n % 2 == 1) { return false; } Map\u003cCharacter, Character\u003e pairs = new HashMap\u003cCharacter, Character\u003e() {{ put(')', '('); put(']', '['); put('}', '{'); }}; Deque\u003cCharacter\u003e stack = new LinkedList\u003cCharacter\u003e(); for (int i = 0; i \u003c n; i++) { char ch = s.charAt(i); if (pairs.containsKey(ch)) { if (stack.isEmpty() || stack.peek() != pairs.get(ch)) { return false; } stack.pop(); } else { stack.push(ch); } } return stack.isEmpty(); } } func isValid(s string) bool { stack := []byte{} leftOk:=func(ch byte) bool{ return ch=='('||ch=='['||ch=='{' } check :=func(topc,c byte) bool { return (c==')'\u0026\u0026topc=='(')||(c==']'\u0026\u0026topc=='[')||(c=='}'\u0026\u0026topc=='{') } // 遇到左括号就压入栈中，遇到右括号就从栈顶取出括号是否配对，如果不配对就返回false，配对继续遍历 for i:=0;i\u003clen(s);i++{ if leftOk(s[i]){ stack = append(stack,s[i]) }else{ if len(stack)!=0\u0026\u0026check(stack[len(stack)-1],s[i]){ stack = stack[:len(stack)-1] }else{ return false } } } // fmt.Println(string(stack)) return len(stack)==0 } 复杂度分析 时间复杂度：O(n)，其中 n 是字符串 s 的长度。 空间复杂度：O*(*n+∣Σ∣)，其中 Σ 表示字符集，本题中字符串只包含 66 种括号，∣Σ∣=6。栈中的字符数量为 O(n)，而哈希表使用的空间为 O(∣Σ∣)，相加即可得到总空间复杂度。 ","date":"2023-01-08","objectID":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/:14:1","tags":["leetcode"],"title":"Leetcode每日一题","uri":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/"},{"categories":["数据结构与算法"],"content":"day15 ","date":"2023-01-08","objectID":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/:15:0","tags":["leetcode"],"title":"Leetcode每日一题","uri":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/"},{"categories":["数据结构与算法"],"content":"225. 用队列实现栈 题目 请你仅使用两个队列实现一个后入先出（LIFO）的栈，并支持普通栈的全部四种操作（push、top、pop 和 empty）。 实现 MyStack 类： void push(int x) 将元素 x 压入栈顶。 int pop() 移除并返回栈顶元素。 int top() 返回栈顶元素。 boolean empty() 如果栈是空的，返回 true ；否则，返回 false 。 注意： 你只能使用队列的基本操作 —— 也就是 push to back、peek/pop from front、size 和 is empty 这些操作。 你所使用的语言也许不支持队列。 你可以使用 list （列表）或者 deque（双端队列）来模拟一个队列 , 只要是标准的队列操作即可。 示例： 输入： [\"MyStack\", \"push\", \"push\", \"top\", \"pop\", \"empty\"] [[], [1], [2], [], [], []] 输出： [null, null, null, 2, 2, false] 解释： MyStack myStack = new MyStack(); myStack.push(1); myStack.push(2); myStack.top(); // 返回 2 myStack.pop(); // 返回 2 myStack.empty(); // 返回 False 提示： 1 \u003c= x \u003c= 9 最多调用100 次 push、pop、top 和 empty 每次调用 pop 和 top 都保证栈不为空 **进阶：**你能否仅用一个队列来实现栈。 方法 方法一：两个队列 为了满足栈的特性，即最后入栈的元素最先出栈，在使用队列实现栈时，应满足队列前端的元素是最后入栈的元素。可以使用两个队列实现栈的操作，其中 queue1 用于存储栈内的元素，queue2 作为入栈操作的辅助队列。 入栈操作时，首先将元素入队到 queue2，然后将 queue1 的全部元素依次出队并入队到 queue2，此时 queue2 的前端的元素即为新入栈的元素，再将 queue1 和 queue2 互换，则 queue1 的元素即为栈内的元素，queue1 的前端和后端分别对应栈顶和栈底。 由于每次入栈操作都确保 queue1 的前端元素为栈顶元素，因此出栈操作和获得栈顶元素操作都可以简单实现。出栈操作只需要移除 queue1 的前端元素并返回即可，获得栈顶元素操作只需要获得 queue1 的前端元素并返回即可（不移除元素）。 由于 queue1 用于存储栈内的元素，判断栈是否为空时，只需要判断 queue1 是否为空即可。 class MyStack { Queue\u003cInteger\u003e queue1; Queue\u003cInteger\u003e queue2; /** Initialize your data structure here. */ public MyStack() { queue1 = new LinkedList\u003cInteger\u003e(); queue2 = new LinkedList\u003cInteger\u003e(); } /** Push element x onto stack. */ public void push(int x) { queue2.offer(x); while (!queue1.isEmpty()) { queue2.offer(queue1.poll()); } Queue\u003cInteger\u003e temp = queue1; queue1 = queue2; queue2 = temp; } /** Removes the element on top of the stack and returns that element. */ public int pop() { return queue1.poll(); } /** Get the top element. */ public int top() { return queue1.peek(); } /** Returns whether the stack is empty. */ public boolean empty() { return queue1.isEmpty(); } } 复杂度分析 时间复杂度：入栈操作 O(n)，其余操作都是 O(1)，其中 n 是栈内的元素个数。 入栈操作需要将 queue1 中的 n 个元素出队，并入队 n+1 个元素到 queue2，共有 2n+1 次操作，每次出队和入队操作的时间复杂度都是 O(1)，因此入栈操作的时间复杂度是 O(n)。 出栈操作对应将 queue1 的前端元素出队，时间复杂度是 O(1)。 获得栈顶元素操作对应获得 queue1 的前端元素，时间复杂度是 O(1)。 判断栈是否为空操作只需要判断 queue1 是否为空，时间复杂度是 O(1)。 空间复杂度：O(n)，其中 n 是栈内的元素个数。需要使用两个队列存储栈内的元素。 方法二：一个队列 方法一使用了两个队列实现栈的操作，也可以使用一个队列实现栈的操作。 使用一个队列时，为了满足栈的特性，即最后入栈的元素最先出栈，同样需要满足队列前端的元素是最后入栈的元素。 入栈操作时，首先获得入栈前的元素个数 n，然后将元素入队到队列，再将队列中的前 n 个元素（即除了新入栈的元素之外的全部元素）依次出队并入队到队列，此时队列的前端的元素即为新入栈的元素，且队列的前端和后端分别对应栈顶和栈底。 由于每次入栈操作都确保队列的前端元素为栈顶元素，因此出栈操作和获得栈顶元素操作都可以简单实现。出栈操作只需要移除队列的前端元素并返回即可，获得栈顶元素操作只需要获得队列的前端元素并返回即可（不移除元素）。 由于队列用于存储栈内的元素，判断栈是否为空时，只需要判断队列是否为空即可。 class MyStack { Queue\u003cInteger\u003e queue; /** Initialize your data structure here. */ public MyStack() { queue = new LinkedList\u003cInteger\u003e(); } /** Push element x onto stack. */ public void push(int x) { int n = queue.size(); queue.offer(x); for (int i = 0; i \u003c n; i++) { queue.offer(queue.poll()); } } /** Removes the element on top of the stack and returns that element. */ public int pop() { return queue.poll(); } /** Get the top element. */ public int top() { return queue.peek(); } /** Returns whether the stack is empty. */ public boolean empty() { return queue.isEmpty(); } } 复杂度分析 时间复杂度：入栈操作 O(n)，其余操作都是 O(1)，其中 n 是栈内的元素个数。 入栈操作需要将队列中的 n 个元素出队，并入队 n+1 个元素到队列，共有 2n+1 次操作，每次出队和入队操作的时间复杂度都是 O(1)，因此入栈操作的时间复杂度是 O(n)。 出栈操作对应将队列的前端元素出队，时间复杂度是 O(1)。 获得栈顶元素操作对应获得队列的前端元素，时间复杂度是 O(1)。 判断栈是否为空操作只需要判断队列是否为空，时间复杂度是 O(1)。 空间复杂度：O(n)，其中 n 是栈内的元素个数。需要使用一个队列存储栈内的元素。 ","date":"2023-01-08","objectID":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/:15:1","tags":["leetcode"],"title":"Leetcode每日一题","uri":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/"},{"categories":["数据结构与算法"],"content":"day16 ","date":"2023-01-08","objectID":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/:16:0","tags":["leetcode"],"title":"Leetcode每日一题","uri":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/"},{"categories":["数据结构与算法"],"content":"232. 用栈实现队列 题目 请你仅使用两个栈实现先入先出队列。队列应当支持一般队列支持的所有操作（push、pop、peek、empty）： 实现 MyQueue 类： void push(int x) 将元素 x 推到队列的末尾 int pop() 从队列的开头移除并返回元素 int peek() 返回队列开头的元素 boolean empty() 如果队列为空，返回 true ；否则，返回 false 说明： 你 只能 使用标准的栈操作 —— 也就是只有 push to top, peek/pop from top, size, 和 is empty 操作是合法的。 你所使用的语言也许不支持栈。你可以使用 list 或者 deque（双端队列）来模拟一个栈，只要是标准的栈操作即可。 示例 1： 输入： [\"MyQueue\", \"push\", \"push\", \"peek\", \"pop\", \"empty\"] [[], [1], [2], [], [], []] 输出： [null, null, null, 1, 1, false] 解释： MyQueue myQueue = new MyQueue(); myQueue.push(1); // queue is: [1] myQueue.push(2); // queue is: [1, 2] (leftmost is front of the queue) myQueue.peek(); // return 1 myQueue.pop(); // return 1, queue is [2] myQueue.empty(); // return false 提示： 1 \u003c= x \u003c= 9 最多调用 100 次 push、pop、peek 和 empty 假设所有操作都是有效的 （例如，一个空的队列不会调用 pop 或者 peek 操作） 进阶： 你能否实现每个操作均摊时间复杂度为 O(1) 的队列？换句话说，执行 n 个操作的总时间复杂度为 O(n) ，即使其中一个操作可能花费较长时间。 方法 https://leetcode.cn/problems/implement-queue-using-stacks/solution/yong-zhan-shi-xian-dui-lie-by-leetcode/ 代码 class MyQueue { Stack\u003cInteger\u003e s1; Stack\u003cInteger\u003e s2; public MyQueue() { this.s1=new Stack\u003c\u003e(); this.s2=new Stack\u003c\u003e(); } public void push(int x) { s1.push(x); } public int pop() { while(s1.size()!=0){ s2.push(s1.pop()); } int res=s2.pop(); while(s2.size()!=0){ s1.push(s2.pop()); } return res; } public int peek() { while(s1.size()!=0){ s2.push(s1.pop()); } int res=s2.peek(); while(s2.size()!=0){ s1.push(s2.pop()); } return res; } public boolean empty() { return s1.empty(); } } ","date":"2023-01-08","objectID":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/:16:1","tags":["leetcode"],"title":"Leetcode每日一题","uri":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/"},{"categories":["数据结构与算法"],"content":"day17 ","date":"2023-01-08","objectID":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/:17:0","tags":["leetcode"],"title":"Leetcode每日一题","uri":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/"},{"categories":["数据结构与算法"],"content":"239. 滑动窗口最大值 题目 给你一个整数数组 nums，有一个大小为 k 的滑动窗口从数组的最左侧移动到数组的最右侧。你只可以看到在滑动窗口内的 k 个数字。滑动窗口每次只向右移动一位。 返回 滑动窗口中的最大值 。 示例 1： 输入：nums = [1,3,-1,-3,5,3,6,7], k = 3 输出：[3,3,5,5,6,7] 解释： 滑动窗口的位置 最大值 --------------- ----- [1 3 -1] -3 5 3 6 7 3 1 [3 -1 -3] 5 3 6 7 3 1 3 [-1 -3 5] 3 6 7 5 1 3 -1 [-3 5 3] 6 7 5 1 3 -1 -3 [5 3 6] 7 6 1 3 -1 -3 5 [3 6 7] 7 示例 2： 输入：nums = [1], k = 1 输出：[1] 提示： 1 \u003c= nums.length \u003c= 105 -104 \u003c= nums[i] \u003c= 104 1 \u003c= k \u003c= nums.length 方法一（超时） 暴力解法，依次移动窗口，不断寻找最大值，当然可以取巧：每一次只滑动一个索引位置，因此，每一次滑动窗口内的数已经知道了最大的数。如果最大的数还在下一次滑动窗口内，那么就可以利用这个属性，只需要比较上次滑动窗口的最大值和这次滑动窗口的末尾索引处的值，就可以得出下一次滑动窗口的最大值了，而不用傻乎乎的重复比较。当然，还是超时了… class Solution { public int[] maxSlidingWindow(int[] nums, int k) { int len=nums.length; int max=Integer.MIN_VALUE; int index=-1; int[] arr = new int[len-k+1]; for(int i=0;i\u003clen-k+1;i++){ if(index!=0){ if(max\u003cnums[i+k-1]){ max=nums[i+k-1]; } } max=Integer.MIN_VALUE; index=-1; for(int j=i;j\u003ci+k;j++){ if(max\u003cnums[j]){ max=nums[j]; index=j; } } arr[i]=max; } return arr; } } /** 已经知道上一个最大的数了， 记录这个最大的数的位置如果不是窗口的第一位，显然具有利用价值： 无需比较上次窗口的最后一位，只需要比较窗口的下一位即可。 */ 其他方法 优先队列、单调队列（双端队列的特殊情况） https://leetcode.cn/problems/sliding-window-maximum/solution/hua-dong-chuang-kou-zui-da-zhi-by-leetco-ki6m/ ","date":"2023-01-08","objectID":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/:17:1","tags":["leetcode"],"title":"Leetcode每日一题","uri":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/"},{"categories":["数据结构与算法"],"content":"day18 ","date":"2023-01-08","objectID":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/:18:0","tags":["leetcode"],"title":"Leetcode每日一题","uri":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/"},{"categories":["数据结构与算法"],"content":"1047. 删除字符串中的所有相邻重复项 题目 给出由小写字母组成的字符串 S，重复项删除操作会选择两个相邻且相同的字母，并删除它们。 在 S 上反复执行重复项删除操作，直到无法继续删除。 在完成所有重复项删除操作后返回最终的字符串。答案保证唯一。 示例： 输入：\"abbaca\" 输出：\"ca\" 解释： 例如，在 \"abbaca\" 中，我们可以删除 \"bb\" 由于两字母相邻且相同，这是此时唯一可以执行删除操作的重复项。之后我们得到字符串 \"aaca\"，其中又只有 \"aa\" 可以执行重复项删除操作，所以最后的字符串为 \"ca\"。 提示： 1 \u003c= S.length \u003c= 20000 S 仅由小写英文字母组成。 解法 方法一：栈 充分理解题意后，我们可以发现，当字符串中同时有多组相邻重复项时，我们无论是先删除哪一个，都不会影响最终的结果。因此我们可以从左向右顺次处理该字符串。 而消除一对相邻重复项可能会导致新的相邻重复项出现，如从字符串 abba 中删除 bb 会导致出现新的相邻重复项 aa 出现。因此我们需要保存当前还未被删除的字符。一种显而易见的数据结构呼之欲出：栈。我们只需要遍历该字符串，如果当前字符和栈顶字符相同，我们就贪心地将其消去，否则就将其入栈即可。 代码 class Solution { public String removeDuplicates(String s) { StringBuffer stack = new StringBuffer(); int top = -1; for (int i = 0; i \u003c s.length(); ++i) { char ch = s.charAt(i); if (top \u003e= 0 \u0026\u0026 stack.charAt(top) == ch) { stack.deleteCharAt(top); --top; } else { stack.append(ch); ++top; } } return stack.toString(); } } 复杂度分析 时间复杂度：O(n)，其中 n 是字符串的长度。我们只需要遍历该字符串一次。 空间复杂度：O(n) 或 O(1))，取决于使用的语言提供的字符串类是否提供了类似「入栈」和「出栈」的接口。注意返回值不计入空间复杂度。 ","date":"2023-01-08","objectID":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/:18:1","tags":["leetcode"],"title":"Leetcode每日一题","uri":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/"},{"categories":["数据结构与算法"],"content":"day19 ","date":"2023-01-08","objectID":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/:19:0","tags":["leetcode"],"title":"Leetcode每日一题","uri":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/"},{"categories":["数据结构与算法"],"content":"151. 颠倒字符串中的单词 题目 给你一个字符串 s ，颠倒字符串中 单词 的顺序。 单词 是由非空格字符组成的字符串。s 中使用至少一个空格将字符串中的 单词 分隔开。 返回 单词 顺序颠倒且 单词 之间用单个空格连接的结果字符串。 **注意：**输入字符串 s中可能会存在前导空格、尾随空格或者单词间的多个空格。返回的结果字符串中，单词间应当仅用单个空格分隔，且不包含任何额外的空格。 示例 1： 输入：s = \"the sky is blue\" 输出：\"blue is sky the\" 示例 2： 输入：s = \" hello world \" 输出：\"world hello\" 解释：颠倒后的字符串中不能存在前导空格和尾随空格。 示例 3： 输入：s = \"a good example\" 输出：\"example good a\" 解释：如果两个单词间有多余的空格，颠倒后的字符串需要将单词间的空格减少到仅有一个。 提示： 1 \u003c= s.length \u003c= 104 s 包含英文大小写字母、数字和空格 ' ' s 中 至少存在一个 单词 方法一 思路： 去除首部空格 将空格之间的子串追加到字符串累加器并附带一个空格 追加完过后，再以空格分隔字符串 倒序数组，追加空格即可 代码：（空间复杂度较高） class Solution { public String reverseWords(String s) { char[] chs = s.toCharArray(); char prev = 0; StringBuilder stringBuilder = new StringBuilder(); int index=-1; for (int i = 0; i \u003c s.length(); i++) { if (chs[i]!=' '){ index=i; break; } } for (int i = index; i \u003c chs.length; i++) { if (chs[i] == ' ' \u0026\u0026 prev == ' ') {//如果当前字符和上一个字符为' ', continue; } stringBuilder.append(chs[i]); prev = chs[i]; } String str = stringBuilder.toString(); String[] split = str.split(\" \"); stringBuilder = new StringBuilder(); for (int i = split.length - 1; i \u003e 0; i--) { stringBuilder.append(split[i]).append(' '); } stringBuilder.append(split[0]); return stringBuilder.toString(); } } 方法二 https://leetcode.cn/problems/reverse-words-in-a-string/solution/fan-zhuan-zi-fu-chuan-li-de-dan-ci-by-leetcode-sol/ ","date":"2023-01-08","objectID":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/:19:1","tags":["leetcode"],"title":"Leetcode每日一题","uri":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/"},{"categories":["数据结构与算法"],"content":"day20 ","date":"2023-01-08","objectID":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/:20:0","tags":["leetcode"],"title":"Leetcode每日一题","uri":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/"},{"categories":["数据结构与算法"],"content":"541. 反转字符串 II 题目 给定一个字符串 s 和一个整数 k，从字符串开头算起，每计数至 2k 个字符，就反转这 2k 字符中的前 k 个字符。 如果剩余字符少于 k 个，则将剩余字符全部反转。 如果剩余字符小于 2k 但大于或等于 k 个，则反转前 k 个字符，其余字符保持原样。 示例 1： 输入：s = \"abcdefg\", k = 2 输出：\"bacdfeg\" 示例 2： 输入：s = \"abcd\", k = 2 输出：\"bacd\" 提示： 1 \u003c= s.length \u003c= 104 s 仅由小写英文组成 1 \u003c= k \u003c= 104 方法 方法一：模拟 我们直接按题意进行模拟：反转每个下标从 2k2k 的倍数开始的，长度为 kk 的子串。若该子串长度不足 kk，则反转整个子串。 class Solution { public String reverseStr(String s, int k) { int n = s.length(); char[] arr = s.toCharArray(); for (int i = 0; i \u003c n; i += 2 * k) { reverse(arr, i, Math.min(i + k, n) - 1); } return new String(arr); } public void reverse(char[] arr, int left, int right) { while (left \u003c right) { char temp = arr[left]; arr[left] = arr[right]; arr[right] = temp; left++; right--; } } } 复杂度分析 时间复杂度：O(n)，其中 n 是字符串 s 的长度。 空间复杂度：O(1) 或 O(n)，取决于使用的语言中字符串类型的性质。如果字符串是可修改的，那么我们可以直接在原字符串上修改，空间复杂度为 O(1)，否则需要使用 O(n) 的空间将字符串临时转换为可以修改的数据结构（例如数组），空间复杂度为 O(n)。 ","date":"2023-01-08","objectID":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/:20:1","tags":["leetcode"],"title":"Leetcode每日一题","uri":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/"},{"categories":["数据结构与算法"],"content":"day21 ","date":"2023-01-08","objectID":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/:21:0","tags":["leetcode"],"title":"Leetcode每日一题","uri":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/"},{"categories":["数据结构与算法"],"content":"1. 两数之和 题目 给定一个整数数组 nums 和一个整数目标值 target，请你在该数组中找出 和为目标值 target 的那 两个 整数，并返回它们的数组下标。 你可以假设每种输入只会对应一个答案。但是，数组中同一个元素在答案里不能重复出现。 你可以按任意顺序返回答案。 示例 1： 输入：nums = [2,7,11,15], target = 9 输出：[0,1] 解释：因为 nums[0] + nums[1] == 9 ，返回 [0, 1] 。 示例 2： 输入：nums = [3,2,4], target = 6 输出：[1,2] 示例 3： 输入：nums = [3,3], target = 6 输出：[0,1] 提示： 2 \u003c= nums.length \u003c= 104 -109 \u003c= nums[i] \u003c= 109 -109 \u003c= target \u003c= 109 只会存在一个有效答案 **进阶：**你可以想出一个时间复杂度小于 O(n2) 的算法吗？ 方法 方法一：暴力枚举 思路及算法 最容易想到的方法是枚举数组中的每一个数 x，寻找数组中是否存在 target - x。当我们使用遍历整个数组的方式寻找 target - x 时，需要注意到每一个位于 x 之前的元素都已经和 x 匹配过，因此不需要再进行匹配。而每一个元素不能被使用两次，所以我们只需要在 x 后面的元素中寻找 target - x。 代码 class Solution { public int[] twoSum(int[] nums, int target) { int n = nums.length; for (int i = 0; i \u003c n; ++i) { for (int j = i + 1; j \u003c n; ++j) { if (nums[i] + nums[j] == target) { return new int[]{i, j}; } } } return new int[0]; } } 复杂度分析 时间复杂度：O(N^2^)，其中 N 是数组中的元素数量。最坏情况下数组中任意两个数都要被匹配一次。 空间复杂度：O(1)。 方法二：哈希表 思路及算法 此题，如果使用方法一，显然时间复杂度过高。结合题意：在一堆值里找一个值，应该考虑转化为hash的数据结构，hash的时间复杂度为O(1) 注意到方法一的时间复杂度较高的原因是寻找 target - x 的时间复杂度过高。因此，我们需要一种更优秀的方法，能够快速寻找数组中是否存在目标元素。如果存在，我们需要找出它的索引。使用哈希表，可以将寻找 target - x 的时间复杂度降低到从 O(N) 降低到 O(1)。这样我们创建一个哈希表，对于每一个 x，我们首先查询哈希表中是否存在 target - x，然后将 x 插入到哈希表中，即可保证不会让 x 和自己匹配。 代码 class Solution { public int[] twoSum(int[] nums, int target) { Map\u003cInteger, Integer\u003e hashtable = new HashMap\u003cInteger, Integer\u003e(); for (int i = 0; i \u003c nums.length; ++i) { // 将此处的n次循环变为1次即可得到结果，这就是hash的好处 if (hashtable.containsKey(target - nums[i])) { return new int[]{hashtable.get(target - nums[i]), i}; } hashtable.put(nums[i], i); } return new int[0]; } } 复杂度分析 时间复杂度：O(N)，其中 N 是数组中的元素数量。对于每一个元素 x，我们可以 O(1) 地寻找 target - x。 空间复杂度：O(N)，其中 N 是数组中的元素数量。主要为哈希表的开销。 ","date":"2023-01-08","objectID":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/:21:1","tags":["leetcode"],"title":"Leetcode每日一题","uri":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/"},{"categories":["数据结构与算法"],"content":"day22 15. 三数之和 题目 给你一个包含 n 个整数的数组 nums，判断 nums 中是否存在三个元素 *a，b，c ，*使得 a + b + c = 0 ？请你找出所有和为 0 且不重复的三元组。 **注意：**答案中不可以包含重复的三元组。 示例 1： 输入：nums = [-1,0,1,2,-1,-4] 输出：[[-1,-1,2],[-1,0,1]] 示例 2： 输入：nums = [] 输出：[] 示例 3： 输入：nums = [0] 输出：[] 提示： 0 \u003c= nums.length \u003c= 3000 -105 \u003c= nums[i] \u003c= 105 解法 方法一：排序 + 双指针 题目中要求找到所有「不重复」且和为 0 的三元组，这个「不重复」的要求使得我们无法简单地使用三重循环枚举所有的三元组。这是因为在最坏的情况下，数组中的元素全部为 0，即 [0, 0, 0, 0, 0, ..., 0, 0, 0] 任意一个三元组的和都为 00。如果我直接使用三重循环枚举三元组，会得到 O(N^3^) 个满足题目要求的三元组（其中 N 是数组的长度）时间复杂度至少为 O(N^3^)。在这之后，我们还需要使用哈希表进行去重操作，得到不包含重复三元组的最终答案，又消耗了大量的空间。这个做法的时间复杂度和空间复杂度都很高，因此我们要换一种思路来考虑这个问题。 「不重复」的本质是什么？我们保持三重循环的大框架不变，只需要保证： 第二重循环枚举到的元素不小于当前第一重循环枚举到的元素； 第三重循环枚举到的元素不小于当前第二重循环枚举到的元素。 也就是说，我们枚举的三元组 (a, b, c) 满足 a≤b≤c，保证了只有 (a, b, c) 这个顺序会被枚举到，而 (b, a, c)、(c, b, a) 等等这些不会，这样就减少了重复。要实现这一点，我们可以将数组中的元素从小到大进行排序，随后使用普通的三重循环就可以满足上面的要求。 同时，对于每一重循环而言，相邻两次枚举的元素不能相同，否则也会造成重复。举个例子，如果排完序的数组为 [0, 1, 2, 2, 2, 3] ^ ^ ^ 我们使用三重循环枚举到的第一个三元组为 (0, 1, 2)，如果第三重循环继续枚举下一个元素，那么仍然是三元组 (0, 1, 2)，产生了重复。因此我们需要将第三重循环「跳到」下一个不相同的元素，即数组中的最后一个元素 3，枚举三元组 (0, 1, 3)。 下面给出了改进的方法的伪代码实现： nums.sort() for first = 0 .. n-1 // 只有和上一次枚举的元素不相同，我们才会进行枚举 if first == 0 or nums[first] != nums[first-1] then for second = first+1 .. n-1 if second == first+1 or nums[second] != nums[second-1] then for third = second+1 .. n-1 if third == second+1 or nums[third] != nums[third-1] then // 判断是否有 a+b+c==0 check(first, second, third) 这种方法的时间复杂度仍然为 O(N^3^)，毕竟我们还是没有跳出三重循环的大框架。然而它是很容易继续优化的，可以发现，如果我们固定了前两重循环枚举到的元素 a 和 b，那么只有唯一的 c 满足 a+b+c=0。当第二重循环往后枚举一个元素 b’ 时，由于 b’ \u003e b，那么满足 a+b’+c’=0 的 c’ 一定有 c’ \u003c c，即 c’ 在数组中一定出现在 c 的左侧。也就是说，我们可以从小到大枚举 b，同时从大到小枚举 c，即第二重循环和第三重循环实际上是并列的关系。 有了这样的发现，我们就可以保持第二重循环不变，而将第三重循环变成一个从数组最右端开始向左移动的指针，从而得到下面的伪代码： nums.sort() for first = 0 .. n-1 if first == 0 or nums[first] != nums[first-1] then // 第三重循环对应的指针 third = n-1 for second = first+1 .. n-1 if second == first+1 or nums[second] != nums[second-1] then // 向左移动指针，直到 a+b+c 不大于 0 while nums[first]+nums[second]+nums[third] \u003e 0 third = third-1 // 判断是否有 a+b+c==0 check(first, second, third) 这个方法就是我们常说的「双指针」，当我们需要枚举数组中的两个元素时，如果我们发现随着第一个元素的递增，第二个元素是递减的，那么就可以使用双指针的方法，将枚举的时间复杂度从 O(N^2)O(N2) 减少至 O(N)。为什么是 O(N) 呢？这是因为在枚举的过程每一步中，「左指针」会向右移动一个位置（也就是题目中的 b），而「右指针」会向左移动若干个位置，这个与数组的元素有关，但我们知道它一共会移动的位置数为 O(N)，均摊下来，每次也向左移动一个位置，因此时间复杂度为 O(N)。 注意到我们的伪代码中还有第一重循环，时间复杂度为 O(N)O(N)，因此枚举的总时间复杂度为 O(N^2^)。由于排序的时间复杂度为 O(NlogN)，在渐进意义下小于前者，因此算法的总时间复杂度为 O(N^2^)。 上述的伪代码中还有一些细节需要补充，例如我们需要保持左指针一直在右指针的左侧（即满足 b≤c），具体可以参考下面的代码，均给出了详细的注释。 代码： class Solution { public List\u003cList\u003cInteger\u003e\u003e threeSum(int[] nums) { int n = nums.length; Arrays.sort(nums); List\u003cList\u003cInteger\u003e\u003e ans = new ArrayList\u003cList\u003cInteger\u003e\u003e(); // 枚举 a for (int first = 0; first \u003c n; ++first) { // 需要和上一次枚举的数不相同 if (first \u003e 0 \u0026\u0026 nums[first] == nums[first - 1]) { continue; } // c 对应的指针初始指向数组的最右端 int third = n - 1; int target = -nums[first]; // 枚举 b for (int second = first + 1; second \u003c n; ++second) { // 需要和上一次枚举的数不相同 if (second \u003e first + 1 \u0026\u0026 nums[second] == nums[second - 1]) { continue; } // 需要保证 b 的指针在 c 的指针的左侧 while (second \u003c third \u0026\u0026 nums[second] + nums[third] \u003e target) { --third; } // 如果指针重合，随着 b 后续的增加 // 就不会有满足 a+b+c=0 并且 b\u003cc 的 c 了，可以退出循环 if (second == third) { break; } if (nums[second] + nums[third] == target) { List\u003cInteger\u003e list = new ArrayList\u003cInteger\u003e(); list.add(nums[first]); list.add(nums[second]); list.add(nums[third]); ans.add(list); } } } return ans; } } 复杂度分析 时间复杂度：O(N^2^)，其中 N 是数组 nums 的长度。 空间复杂度：O(logN)。我们忽略存储答案的空间，额外的排序的空间复杂度为 O(logN)。然而我们修改了输入的数组 nums，在实际情况下不一定允许，因此也可以看成使用了一个额外的数组存储了 nums 的副本并进行排序，空间复杂度为 O(N)。 ","date":"2023-01-08","objectID":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/:22:0","tags":["leetcode"],"title":"Leetcode每日一题","uri":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/"},{"categories":["数据结构与算法"],"content":"day23 202. 快乐数 题目 编写一个算法来判断一个数 n 是不是快乐数。 「快乐数」 定义为： 对于一个正整数，每一次将该数替换为它每个位置上的数字的平方和。 然后重复这个过程直到这个数变为 1，也可能是 无限循环 但始终变不到 1。 如果这个过程 结果为 1，那么这个数就是快乐数。 如果 n 是 快乐数 就返回 true ；不是，则返回 false 。 示例 1： 输入：n = 19 输出：true 解释： 12 + 92 = 82 82 + 22 = 68 62 + 82 = 100 12 + 02 + 02 = 1 示例 2： 输入：n = 2 输出：false 提示： 1 \u003c= n \u003c= 231 - 1 解法 暴力错误且超时 class Solution { public boolean isHappy(int n) { int tmp=n; while(true){ tmp=get(tmp); if(tmp==n) return false; else if(tmp==1) return true; } } public int get(int n) { int tmp=n; int sum=0; int r=0; while(tmp!=0){ r = tmp%10; sum = r*r+sum; tmp = tmp/10; } return sum; } } 超时的原因在于：条件判断错误，不是一定要出现到初始数字才会陷入死循环，有可能是中途的某个节点出现循环，例如：数字116 显然，需要使用弗洛伊德判圈算法（龟兔赛跑算法）–\u003e 快慢指针法：一个快指针和一个慢指针同时出发，如果快指针追上了慢指针说明是一个死循环，就返回false 快慢指针法 class Solution { public boolean isHappy(int n) { int slowRunner = n; int fastRunner = getNext(n); while (fastRunner != 1 \u0026\u0026 slowRunner != fastRunner) { slowRunner = getNext(slowRunner); fastRunner = getNext(getNext(fastRunner)); } return fastRunner == 1; } public int getNext(int n) { int tmp=n; int sum=0; int r=0; while(tmp!=0){ r = tmp%10; sum = r*r+sum; tmp = tmp/10; } return sum; } } ","date":"2023-01-08","objectID":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/:23:0","tags":["leetcode"],"title":"Leetcode每日一题","uri":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/"},{"categories":["数据结构与算法"],"content":"day24 454. 四数相加 II 题目 给你四个整数数组 nums1、nums2、nums3 和 nums4 ，数组长度都是 n ，请你计算有多少个元组 (i, j, k, l) 能满足： 0 \u003c= i, j, k, l \u003c n nums1[i] + nums2[j] + nums3[k] + nums4[l] == 0 示例 1： 输入：nums1 = [1,2], nums2 = [-2,-1], nums3 = [-1,2], nums4 = [0,2] 输出：2 解释： 两个元组如下： 1. (0, 0, 0, 1) -\u003e nums1[0] + nums2[0] + nums3[0] + nums4[1] = 1 + (-2) + (-1) + 2 = 0 2. (1, 1, 0, 0) -\u003e nums1[1] + nums2[1] + nums3[0] + nums4[0] = 2 + (-1) + (-1) + 0 = 0 示例 2： 输入：nums1 = [0], nums2 = [0], nums3 = [0], nums4 = [0] 输出：1 提示： n == nums1.length n == nums2.length n == nums3.length n == nums4.length 1 \u003c= n \u003c= 200 -228 \u003c= nums1[i], nums2[i], nums3[i], nums4[i] \u003c= 228 解法：分组+hash表 思路与算法 我们可以将四个数组分成两部分，A 和 B 为一组，C 和 D 为另外一组。 对于 A 和 B，我们使用二重循环对它们进行遍历，得到所有 A[i]+B[j] 的值并存入哈希映射中。对于哈希映射中的每个键值对，每个键表示一种 A[i]+B[j]，对应的值为 A[i]+B[j] 出现的次数。 对于 CC 和 DD，我们同样使用二重循环对它们进行遍历。当遍历到 C[k]+D[l] 时，如果 -(C[k]+D[l]) 出现在哈希映射中，那么将 -(C[k]+D[l]) 对应的值累加进答案中。 最终即可得到满足 A[i]+B[j]+C[k]+D[l]=0 的四元组数目。 代码 class Solution { public int fourSumCount(int[] A, int[] B, int[] C, int[] D) { Map\u003cInteger, Integer\u003e countAB = new HashMap\u003cInteger, Integer\u003e(); for (int u : A) { for (int v : B) { countAB.put(u + v, countAB.getOrDefault(u + v, 0) + 1); } } int ans = 0; for (int u : C) { for (int v : D) { if (countAB.containsKey(-u - v)) { ans += countAB.get(-u - v); } } } return ans; } } 复杂度分析 时间复杂度：O(n^2^)。我们使用了两次二重循环，时间复杂度均为 O(n^2^)。在循环中对哈希映射进行的修改以及查询操作的期望时间复杂度均为 O(1)，因此总时间复杂度为 O(n^2^)。 空间复杂度：O(n^2^)，即为哈希映射需要使用的空间。在最坏的情况下，A[i]+B[j] 的值均不相同，因此值的个数为 n^2^，也就需要 O(n^2^) 的空间。 ","date":"2023-01-08","objectID":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/:24:0","tags":["leetcode"],"title":"Leetcode每日一题","uri":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/"},{"categories":["数据结构与算法"],"content":"day25 100. 相同的树 题目 给你两棵二叉树的根节点 p 和 q ，编写一个函数来检验这两棵树是否相同。 如果两个树在结构上相同，并且节点具有相同的值，则认为它们是相同的。 示例 1： 输入：p = [1,2,3], q = [1,2,3] 输出：true 示例 2： 输入：p = [1,2], q = [1,null,2] 输出：false 示例 3： 输入：p = [1,2,1], q = [1,1,2] 输出：false 提示： 两棵树上的节点数目都在范围 [0, 100] 内 -104 \u003c= Node.val \u003c= 104 解法 两个二叉树相同，当且仅当两个二叉树的结构完全相同，且所有对应节点的值相同。因此，可以通过搜索的方式判断两个二叉树是否相同。 方法一：深度优先搜索 如果两个二叉树都为空，则两个二叉树相同。如果两个二叉树中有且只有一个为空，则两个二叉树一定不相同。 如果两个二叉树都不为空，那么首先判断它们的根节点的值是否相同，若不相同则两个二叉树一定不同，若相同，再分别判断两个二叉树的左子树是否相同以及右子树是否相同。这是一个递归的过程，因此可以使用深度优先搜索，递归地判断两个二叉树是否相同。 class Solution { public boolean isSameTree(TreeNode p, TreeNode q) { if (p == null \u0026\u0026 q == null) { return true; } else if (p == null || q == null) { return false; } else if (p.val != q.val) { return false; } else { return isSameTree(p.left, q.left) \u0026\u0026 isSameTree(p.right, q.right); } } } 复杂度分析 时间复杂度：O(min(m,n))，其中 m 和 n 分别是两个二叉树的节点数。对两个二叉树同时进行深度优先搜索，只有当两个二叉树中的对应节点都不为空时才会访问到该节点，因此被访问到的节点数不会超过较小的二叉树的节点数。 空间复杂度：O(min(m,n))，其中 m 和 n 分别是两个二叉树的节点数。空间复杂度取决于递归调用的层数，递归调用的层数不会超过较小的二叉树的最大高度，最坏情况下，二叉树的高度等于节点数。 方法二：广度优先搜索 也可以通过广度优先搜索判断两个二叉树是否相同。同样首先判断两个二叉树是否为空，如果两个二叉树都不为空，则从两个二叉树的根节点开始广度优先搜索。 使用两个队列分别存储两个二叉树的节点。初始时将两个二叉树的根节点分别加入两个队列。每次从两个队列各取出一个节点，进行如下比较操作。 比较两个节点的值，如果两个节点的值不相同则两个二叉树一定不同； 如果两个节点的值相同，则判断两个节点的子节点是否为空，如果只有一个节点的左子节点为空，或者只有一个节点的右子节点为空，则两个二叉树的结构不同，因此两个二叉树一定不同； 如果两个节点的子节点的结构相同，则将两个节点的非空子节点分别加入两个队列，子节点加入队列时需要注意顺序，如果左右子节点都不为空，则先加入左子节点，后加入右子节点。 如果搜索结束时两个队列同时为空，则两个二叉树相同。如果只有一个队列为空，则两个二叉树的结构不同，因此两个二叉树不同。 class Solution { public boolean isSameTree(TreeNode p, TreeNode q) { if (p == null \u0026\u0026 q == null) { return true; } else if (p == null || q == null) { return false; } Queue\u003cTreeNode\u003e queue1 = new LinkedList\u003cTreeNode\u003e(); Queue\u003cTreeNode\u003e queue2 = new LinkedList\u003cTreeNode\u003e(); queue1.offer(p); queue2.offer(q); while (!queue1.isEmpty() \u0026\u0026 !queue2.isEmpty()) { TreeNode node1 = queue1.poll(); TreeNode node2 = queue2.poll(); if (node1.val != node2.val) { return false; } TreeNode left1 = node1.left, right1 = node1.right, left2 = node2.left, right2 = node2.right; if (left1 == null ^ left2 == null) { return false; } if (right1 == null ^ right2 == null) { return false; } if (left1 != null) { queue1.offer(left1); } if (right1 != null) { queue1.offer(right1); } if (left2 != null) { queue2.offer(left2); } if (right2 != null) { queue2.offer(right2); } } return queue1.isEmpty() \u0026\u0026 queue2.isEmpty(); } } 复杂度分析 时间复杂度：O(min(m,n))，其中 m 和 n 分别是两个二叉树的节点数。对两个二叉树同时进行广度优先搜索，只有当两个二叉树中的对应节点都不为空时才会访问到该节点，因此被访问到的节点数不会超过较小的二叉树的节点数。 空间复杂度：O(min(m,n))，其中 m 和 n 分别是两个二叉树的节点数。空间复杂度取决于队列中的元素个数，队列中的元素个数不会超过较小的二叉树的节点数。 ","date":"2023-01-08","objectID":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/:25:0","tags":["leetcode"],"title":"Leetcode每日一题","uri":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/"},{"categories":["数据结构与算法"],"content":"day26 101. 对称二叉树 题目 给你一个二叉树的根节点 root ， 检查它是否轴对称。 示例 1： 输入：root = [1,2,2,3,4,4,3] 输出：true 示例 2： 输入：root = [1,2,2,null,3,null,3] 输出：false 提示： 树中节点数目在范围 [1, 1000] 内 -100 \u003c= Node.val \u003c= 100 **进阶：**你可以运用递归和迭代两种方法解决这个问题吗？ 解法 方法一：递归 思路和算法 如果一个树的左子树与右子树镜像对称，那么这个树是对称的。 因此，该问题可以转化为：两个树在什么情况下互为镜像？ 如果同时满足下面的条件，两个树互为镜像： 它们的两个根结点具有相同的值 每个树的右子树都与另一个树的左子树镜像对称 我们可以实现这样一个递归函数，通过「同步移动」两个指针的方法来遍历这棵树，p 指针和 q 指针一开始都指向这棵树的根，随后 p 右移时，q 左移，p 左移时，q 右移。每次检查当前 p 和 q 节点的值是否相等，如果相等再判断左右子树是否对称。 代码如下： class Solution { public boolean isSymmetric(TreeNode root) { return check(root.left, root.right); } public boolean check(TreeNode p, TreeNode q) { if (p == null \u0026\u0026 q == null) { return true; }else if (p == null || q == null) { return false; }else if (p.val != q.val ){ return false; } return check(p.left, q.right) \u0026\u0026 check(p.right, q.left); // 就是看左子树和右子树节点是否相同 } } 复杂度分析 假设树上一共 n 个节点。 时间复杂度：这里遍历了这棵树，渐进时间复杂度为 O(n)。 空间复杂度：这里的空间复杂度和递归使用的栈空间有关，这里递归层数不超过 n，故渐进空间复杂度为 O(n)。 方法二：迭代 思路和算法 「方法一」中我们用递归的方法实现了对称性的判断，那么如何用迭代的方法实现呢？首先我们引入一个队列，这是把递归程序改写成迭代程序的常用方法。初始化时我们把根节点入队两次。每次提取两个结点并比较它们的值（队列中每两个连续的结点应该是相等的，而且它们的子树互为镜像），然后将两个结点的左右子结点按相反的顺序插入队列中。当队列为空时，或者我们检测到树不对称（即从队列中取出两个不相等的连续结点）时，该算法结束。 class Solution { public boolean isSymmetric(TreeNode root) { return check(root, root); } public boolean check(TreeNode u, TreeNode v) { Queue\u003cTreeNode\u003e q = new LinkedList\u003cTreeNode\u003e(); q.offer(u); q.offer(v); while (!q.isEmpty()) { u = q.poll(); v = q.poll(); if (u == null \u0026\u0026 v == null) { continue; } if ((u == null || v == null) || (u.val != v.val)) { return false; } // 核心代码 q.offer(u.left); q.offer(v.right); q.offer(u.right); q.offer(v.left); } return true; } } 复杂度分析 时间复杂度：O(n)，同「方法一」。 空间复杂度：这里需要用一个队列来维护节点，每个节点最多进队一次，出队一次，队列中最多不会超过 n 个点，故渐进空间复杂度为 O(n)。 ","date":"2023-01-08","objectID":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/:26:0","tags":["leetcode"],"title":"Leetcode每日一题","uri":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/"},{"categories":["数据结构与算法"],"content":"day27 104. 二叉树的最大深度 题目 给定一个二叉树，找出其最大深度。 二叉树的深度为根节点到最远叶子节点的最长路径上的节点数。 说明: 叶子节点是指没有子节点的节点。 示例： 给定二叉树 [3,9,20,null,null,15,7]， 3 / \\ 9 20 / \\ 15 7 返回它的最大深度 3 。 解法 方法一：深度优先搜索 思路与算法 如果我们知道了左子树和右子树的最大深度 l 和 r，那么该二叉树的最大深度即为max(l,r)+1 而左子树和右子树的最大深度又可以以同样的方式进行计算。因此我们可以用「深度优先搜索」的方法来计算二叉树的最大深度。具体而言，在计算当前二叉树的最大深度时，可以先递归计算出其左子树和右子树的最大深度，然后在 O(1) 时间内计算出当前二叉树的最大深度。递归在访问到空节点时退出。 class Solution { public int maxDepth(TreeNode root) { if (root == null) { return 0; } else { int leftHeight = maxDepth(root.left); int rightHeight = maxDepth(root.right); return Math.max(leftHeight, rightHeight) + 1; } } } 复杂度分析 时间复杂度：O(n)，其中 n 为二叉树节点的个数。每个节点在递归中只被遍历一次。 空间复杂度：O(height)，其中 height 表示二叉树的高度。递归函数需要栈空间，而栈空间取决于递归的深度，因此空间复杂度等价于二叉树的高度。 方法二：广度优先搜索 思路与算法 我们也可以用「广度优先搜索」的方法来解决这道题目，但我们需要对其进行一些修改，此时我们广度优先搜索的队列里存放的是「当前层的所有节点」。每次拓展下一层的时候，不同于广度优先搜索的每次只从队列里拿出一个节点，我们需要将队列里的所有节点都拿出来进行拓展，这样能保证每次拓展完的时候队列里存放的是当前层的所有节点，即我们是一层一层地进行拓展，最后我们用一个变量 ans 来维护拓展的次数，该二叉树的最大深度即为 ans。 class Solution { public int maxDepth(TreeNode root) { if (root == null) { return 0; } Queue\u003cTreeNode\u003e queue = new LinkedList\u003cTreeNode\u003e(); queue.offer(root); int ans = 0; while (!queue.isEmpty()) { int size = queue.size(); while (size \u003e 0) { TreeNode node = queue.poll(); if (node.left != null) { queue.offer(node.left); } if (node.right != null) { queue.offer(node.right); } size--; } ans++; } return ans; } } /** * Definition for a binary tree node. * type TreeNode struct { * Val int * Left *TreeNode * Right *TreeNode * } */ func maxDepth(root *TreeNode) int { if root==nil { return 0 } depth:=0 queue:=[]*TreeNode{root} for len(queue)!=0{ n:=len(queue) for i:=0;i\u003cn;i++{ node:=queue[i] // fmt.Println(node.Val) if node.Left!=nil{ queue=append(queue,node.Left) } if node.Right!=nil{ queue=append(queue,node.Right) } } queue=queue[n:] depth++ } return depth } 复杂度分析 时间复杂度：O(n)，其中 n 为二叉树的节点个数。与方法一同样的分析，每个节点只会被访问一次。 空间复杂度：此方法空间的消耗取决于队列存储的元素数量，其在最坏情况下会达到 O(n)。 ","date":"2023-01-08","objectID":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/:27:0","tags":["leetcode"],"title":"Leetcode每日一题","uri":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/"},{"categories":["数据结构与算法"],"content":"day28 111. 二叉树的最小深度 题目 给定一个二叉树，找出其最小深度。 最小深度是从根节点到最近叶子节点的最短路径上的节点数量。 **说明：**叶子节点是指没有子节点的节点。 示例 1： 输入：root = [3,9,20,null,null,15,7] 输出：2 示例 2： 输入：root = [2,null,3,null,4,null,5,null,6] 输出：5 提示： 树中节点数的范围在 [0, 105] 内 -1000 \u003c= Node.val \u003c= 1000 解法 方法一：深度优先搜索 思路及解法 首先可以想到使用深度优先搜索的方法，遍历整棵树，记录最小深度。 对于每一个非叶子节点，我们只需要分别计算其左右子树的最小叶子节点深度。这样就将一个大问题转化为了小问题，可以递归地解决该问题。 代码 class Solution { public int minDepth(TreeNode root) { if (root==null) return 0; int l=Integer.MAX_VALUE; int r=Integer.MAX_VALUE; if (root.left==null\u0026\u0026root.right==null) return 1; if (root.left!=null){ l=minDepth(root.left)+1; } if(root.right!=null){ // System.out.println(root.val); r=minDepth(root.right)+1; } return l\u003er?r:l; } } 复杂度分析 时间复杂度：O(N)，其中 N 是树的节点数。对每个节点访问一次。 空间复杂度：O(H)，其中 H 是树的高度。空间复杂度主要取决于递归时栈空间的开销，最坏情况下，树呈现链状，空间复杂度为 O(N)。平均情况下树的高度与节点数的对数正相关，空间复杂度为 O(log N)。 方法二：广度优先搜索 思路及解法 同样，我们可以想到使用广度优先搜索的方法，遍历整棵树。 当我们找到一个叶子节点时，直接返回这个叶子节点的深度。广度优先搜索的性质保证了最先搜索到的叶子节点的深度一定最小。 代码 class Solution { public int minDepth(TreeNode root) { if (root==null) return 0; if (root.left==null\u0026\u0026root.right==null) return 1; int layerNum=1; Queue\u003cTreeNode\u003e queue=new LinkedList\u003c\u003e(); queue.offer(root); while(!queue.isEmpty()){ int size = queue.size(); while(size\u003e0){ TreeNode node = queue.poll(); if(node.left==null\u0026\u0026node.right==null) return layerNum; if(node.left!=null) queue.offer(node.left); if(node.right!=null) queue.offer(node.right); size--; } layerNum++; } return layerNum; } } 复杂度分析 时间复杂度：O(N)，其中 N 是树的节点数。对每个节点访问一次。 空间复杂度：O(N)，其中 N 是树的节点数。空间复杂度主要取决于队列的开销，队列中的元素个数不会超过树的节点数。 ","date":"2023-01-08","objectID":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/:28:0","tags":["leetcode"],"title":"Leetcode每日一题","uri":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/"},{"categories":["数据结构与算法"],"content":"day29 112. 路径总和 题目 给你二叉树的根节点 root 和一个表示目标和的整数 targetSum 。判断该树中是否存在 根节点到叶子节点 的路径，这条路径上所有节点值相加等于目标和 targetSum 。如果存在，返回 true ；否则，返回 false 。 叶子节点 是指没有子节点的节点。 示例 1： 输入：root = [5,4,8,11,null,13,4,7,2,null,null,null,1], targetSum = 22 输出：true 解释：等于目标和的根节点到叶节点路径如上图所示。 示例 2： 输入：root = [1,2,3], targetSum = 5 输出：false 解释：树中存在两条根节点到叶子节点的路径： (1 --\u003e 2): 和为 3 (1 --\u003e 3): 和为 4 不存在 sum = 5 的根节点到叶子节点的路径。 示例 3： 输入：root = [], targetSum = 0 输出：false 解释：由于树是空的，所以不存在根节点到叶子节点的路径。 提示： 树中节点的数目在范围 [0, 5000] 内 -1000 \u003c= Node.val \u003c= 1000 -1000 \u003c= targetSum \u003c= 1000 解法 写在前面 注意到本题的要求是，询问是否有从「根节点」到某个「叶子节点」经过的路径上的节点之和等于目标和。核心思想是对树进行一次遍历，在遍历时记录从根节点到当前节点的路径和，以防止重复计算。 需要特别注意的是，给定的 root 可能为空。 方法一：广度优先搜索 思路及算法 首先我们可以想到使用广度优先搜索的方式，记录从根节点到当前节点的路径和，以防止重复计算。 这样我们使用两个队列，分别存储将要遍历的节点，以及根节点到这些节点的路径和即可。 代码 class Solution { public boolean hasPathSum(TreeNode root, int sum) { if (root == null) { return false; } Queue\u003cTreeNode\u003e queNode = new LinkedList\u003c\u003e(); Queue\u003cInteger\u003e queVal = new LinkedList\u003c\u003e(); queNode.offer(root); queVal.offer(root.val); while (!queNode.isEmpty()) { TreeNode now = queNode.poll(); int temp = queVal.poll(); if (now.left == null \u0026\u0026 now.right == null) { if (temp == sum) { return true; } continue; } if (now.left != null) { queNode.offer(now.left); queVal.offer(now.left.val + temp); } if (now.right != null) { queNode.offer(now.right); queVal.offer(now.right.val + temp); } } return false; } } 复杂度分析 时间复杂度：O(N)，其中 N 是树的节点数。对每个节点访问一次。 空间复杂度：O(N)，其中 N 是树的节点数。空间复杂度主要取决于队列的开销，队列中的元素个数不会超过树的节点数。 方法二：递归 思路及算法 观察要求我们完成的函数，我们可以归纳出它的功能：询问是否存在从当前节点 root 到叶子节点的路径，满足其路径和为 sum。 假定从根节点到当前节点的值之和为 val，我们可以将这个大问题转化为一个小问题：是否存在从当前节点的子节点到叶子的路径，满足其路径和为 sum - val。 不难发现这满足递归的性质，若当前节点就是叶子节点，那么我们直接判断 sum 是否等于 val 即可（因为路径和已经确定，就是当前节点的值，我们只需要判断该路径和是否满足条件）。若当前节点不是叶子节点，我们只需要递归地询问它的子节点是否能满足条件即可。 代码 class Solution { public boolean hasPathSum(TreeNode root, int sum) { if (root == null) { return false; } if (root.left == null \u0026\u0026 root.right == null) { return sum == root.val; } return hasPathSum(root.left, sum - root.val) || hasPathSum(root.right, sum - root.val); } } // 或者：正向思维，遍历二叉树，将到达叶子节点的路径和累加与 targetSum 比较，如果相等就返回true class Solution { public boolean hasPathSum(TreeNode root, int targetSum) { return doSome(root,targetSum,0); } public boolean doSome(TreeNode root,int targetSum,int tmp){ if (root==null) return false; // 叶子节点 if(root.left==null\u0026\u0026root.right==null) { if(tmp+root.val==targetSum){ return true; } else { tmp=0; return false; } } if(root.left!=null) { boolean l =doSome(root.left,targetSum,tmp+root.val); if (l) return true; } if(root.right!=null) { boolean r = doSome(root.right,targetSum,tmp+root.val); if (r) return true; } return false; } } 复杂度分析 时间复杂度：O(N)，其中 N 是树的节点数。对每个节点访问一次。 空间复杂度：O(H)，其中 H 是树的高度。空间复杂度主要取决于递归时栈空间的开销，最坏情况下，树呈现链状，空间复杂度为 O(N)。平均情况下树的高度与节点数的对数正相关，空间复杂度为 O(log N)。 ","date":"2023-01-08","objectID":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/:29:0","tags":["leetcode"],"title":"Leetcode每日一题","uri":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/"},{"categories":["数据结构与算法"],"content":"day30 222. 完全二叉树的节点个数 题目 给你一棵 完全二叉树 的根节点 root ，求出该树的节点个数。 完全二叉树 的定义如下：在完全二叉树中，除了最底层节点可能没填满外，其余每层节点数都达到最大值，并且最下面一层的节点都集中在该层最左边的若干位置。若最底层为第 h 层，则该层包含 1~ 2h 个节点。 示例 1： 输入：root = [1,2,3,4,5,6] 输出：6 示例 2： 输入：root = [] 输出：0 示例 3： 输入：root = [1] 输出：1 提示： 树中节点的数目范围是[0, 5 * 104] 0 \u003c= Node.val \u003c= 5 * 104 题目数据保证输入的树是 完全二叉树 **进阶：**遍历树来统计节点是一种时间复杂度为 O(n) 的简单解决方案。你可以设计一个更快的算法吗？ 解法 方法一：深度优先遍历 class Solution { public int countNodes(TreeNode root) { if(root == null) { return 0; } int left = countNodes(root.left); int right = countNodes(root.right); return left+right+1; } } 方法二：广度优先遍历 一如既往，广度优先貌似硬是要比深度优先慢一些 class Solution { public int countNodes(TreeNode root) { if (root==null) return 0; Queue\u003cTreeNode\u003e queue=new LinkedList\u003c\u003e(); int count=0; queue.offer(root); while(!queue.isEmpty()){ int size = queue.size(); while(size\u003e0){ TreeNode n = queue.poll(); if (n!=null) count++; if (n.left!=null) queue.offer(n.left); if (n.right!=null) queue.offer(n.right); size--; } } return count; } } 方法三：二分查找 + 位运算 对于任意二叉树，都可以通过广度优先搜索或深度优先搜索计算节点个数，时间复杂度和空间复杂度都是 O(n)，其中 n 是二叉树的节点个数。这道题规定了给出的是完全二叉树，因此可以利用完全二叉树的特性计算节点个数。 规定根节点位于第 0 层，完全二叉树的最大层数为 h。根据完全二叉树的特性可知，完全二叉树的最左边的节点一定位于最底层，因此从根节点出发，每次访问左子节点，直到遇到叶子节点，该叶子节点即为完全二叉树的最左边的节点，经过的路径长度即为最大层数 h。 当 0≤i\u003ch 时，第 i 层包含 2^i^ 个节点，最底层包含的节点数最少为 1，最多为 2^h^。 当最底层包含 1 个节点时，完全二叉树的节点个数是 $$ \\sum_{i=0}^{h-1}2^i+1=2^h $$ 当最底层包含 2^h^ 个节点时，完全二叉树的节点个数是 $$ \\sum_{i=0}^{h}2^i=2^{h+1}-1 $$ 因此对于最大层数为 h 的完全二叉树，节点个数一定在 [2^h^,2^h+1^-1] 的范围内，可以在该范围内通过二分查找的方式得到完全二叉树的节点个数。 具体做法是，根据节点个数范围的上下界得到当前需要判断的节点个数 k，如果第 k 个节点存在，则节点个数一定大于或等于 k，如果第 k 个节点不存在，则节点个数一定小于 k，由此可以将查找的范围缩小一半，直到得到节点个数。 如何判断第 k 个节点是否存在呢？如果第 k 个节点位于第 h 层，则 k 的二进制表示包含 h+1 位，其中最高位是 1，其余各位从高到低表示从根节点到第 k 个节点的路径，0 表示移动到左子节点，1 表示移动到右子节点。通过位运算得到第 k 个节点对应的路径，判断该路径对应的节点是否存在，即可判断第 k 个节点是否存在。 class Solution { public int countNodes(TreeNode root) { if (root == null) { return 0; } int level = 0; TreeNode node = root; while (node.left != null) { level++; node = node.left; } int low = 1 \u003c\u003c level, high = (1 \u003c\u003c (level + 1)) - 1; while (low \u003c high) { int mid = (high - low + 1) / 2 + low; if (exists(root, level, mid)) { low = mid; } else { high = mid - 1; } } return low; } public boolean exists(TreeNode root, int level, int k) { int bits = 1 \u003c\u003c (level - 1); TreeNode node = root; while (node != null \u0026\u0026 bits \u003e 0) { if ((bits \u0026 k) == 0) { node = node.left; } else { node = node.right; } bits \u003e\u003e= 1; } return node != null; } } 复杂度分析 时间复杂度：O(log^2^n)，其中 n 是完全二叉树的节点数。 首先需要 O(h) 的时间得到完全二叉树的最大层数，其中 h 是完全二叉树的最大层数。 使用二分查找确定节点个数时，需要查找的次数为 O(log 2^h^)=O(h)，每次查找需要遍历从根节点开始的一条长度为 h 的路径，需要 O(h) 的时间，因此二分查找的总时间复杂度是 O(h^2^)。 因此总时间复杂度是 O(h^2^)。由于完全二叉树满足 2^h^≤n\u003c2^h+1^，因此有 O(h)=O(log n)，O(h^2^)=O(log^2^ n)。 空间复杂度：O(1)。只需要维护有限的额外空间。 ","date":"2023-01-08","objectID":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/:30:0","tags":["leetcode"],"title":"Leetcode每日一题","uri":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/"},{"categories":["数据结构与算法"],"content":"day31 226. 翻转二叉树 题目 给你一棵二叉树的根节点 root ，翻转这棵二叉树，并返回其根节点。 示例 1： 输入：root = [4,2,7,1,3,6,9] 输出：[4,7,2,9,6,3,1] 示例 2： 输入：root = [2,1,3] 输出：[2,3,1] 示例 3： 输入：root = [] 输出：[] 提示： 树中节点数目范围在 [0, 100] 内 -100 \u003c= Node.val \u003c= 100 解法 方法一：深度优先遍历 思路与算法 这是一道很经典的二叉树问题。显然，我们从根节点开始，递归地对树进行遍历，并从叶子节点先开始翻转。如果当前遍历到的节点 root 的左右两棵子树都已经翻转，那么我们只需要交换两棵子树的位置，即可完成以 root 为根节点的整棵子树的翻转。 代码 /** * Definition for a binary tree node. * public class TreeNode { * int val; * TreeNode left; * TreeNode right; * TreeNode() {} * TreeNode(int val) { this.val = val; } * TreeNode(int val, TreeNode left, TreeNode right) { * this.val = val; * this.left = left; * this.right = right; * } * } */ class Solution { public TreeNode invertTree(TreeNode root) { if(root==null) return null; // 先交换一下左右子树 TreeNode tmp = root.right; root.right = root.left; root.left = tmp; // 此时已经交换过子树了，所以分别遍历左右子树 root.left = invertTree(root.left); root.right = invertTree(root.right); return root; } } 复杂度分析 时间复杂度：O(N)，其中 N 为二叉树节点的数目。我们会遍历二叉树中的每一个节点，对每个节点而言，我们在常数时间内交换其两棵子树。 空间复杂度：O(N)。使用的空间由递归栈的深度决定，它等于当前节点在二叉树中的高度。在平均情况下，二叉树的高度与节点个数为对数关系，即 O*(logN)。而在最坏情况下，树形成链状，空间复杂度为 O(N)。 方法二：广度优先遍历 public TreeNode invertTree(TreeNode root) { if (root == null) return root; Queue\u003cTreeNode\u003e queue = new LinkedList\u003c\u003e(); queue.add(root);//相当于把数据加入到队列尾部 while (!queue.isEmpty()) { //poll方法相当于移除队列头部的元素 TreeNode node = queue.poll(); //先交换子节点 TreeNode left = node.left; node.left = node.right; node.right = left; if (node.left != null) queue.add(node.left); if (node.right != null) queue.add(node.right); } return root; } ","date":"2023-01-08","objectID":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/:31:0","tags":["leetcode"],"title":"Leetcode每日一题","uri":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/"},{"categories":["数据结构与算法"],"content":"day32 236. 二叉树的最近公共祖先 题目 给定一个二叉树, 找到该树中两个指定节点的最近公共祖先。 百度百科中最近公共祖先的定义为：“对于有根树 T 的两个节点 p、q，最近公共祖先表示为一个节点 x，满足 x 是 p、q 的祖先且 x 的深度尽可能大（一个节点也可以是它自己的祖先）。” 示例 1： 输入：root = [3,5,1,6,2,0,8,null,null,7,4], p = 5, q = 1 输出：3 解释：节点 5 和节点 1 的最近公共祖先是节点 3 。 示例 2： 输入：root = [3,5,1,6,2,0,8,null,null,7,4], p = 5, q = 4 输出：5 解释：节点 5 和节点 4 的最近公共祖先是节点 5 。因为根据定义最近公共祖先节点可以为节点本身。 示例 3： 输入：root = [1,2], p = 1, q = 2 输出：1 提示： 树中节点数目在范围 [2, 105] 内。 -109 \u003c= Node.val \u003c= 109 所有 Node.val 互不相同 。 p != q p 和 q 均存在于给定的二叉树中。 解法 方法零：两层递归 class Solution { public TreeNode lowestCommonAncestor(TreeNode root, TreeNode p, TreeNode q) { if (root==null) return null; //左子树是否含有这两个节点，如果有，就继续递归，直到左子树不再同时含有两个节点 if (doSome(root.left,p)\u0026\u0026doSome(root.left,q)) { return lowestCommonAncestor(root.left,p,q); } if (doSome(root.right,p)\u0026\u0026doSome(root.right,q)){ return lowestCommonAncestor(root.right,p,q); } return root; } // 检查 node子树里是否含有n节点 public boolean doSome(TreeNode node,TreeNode n){ if (node==null)return false; if (node.val==n.val){ return true; } boolean l = doSome(node.left,n); boolean r = doSome(node.right,n); return l||r; } } 方法一：递归 思路和算法 我们递归遍历整棵二叉树，定义 f~x~ 表示 x 节点的子树中是否包含 p 节点或 q 节点，如果包含为 true，否则为 false。那么符合条件的最近公共祖先 x 一定满足如下条件：(f^lson^ \u0026\u0026 f^rson^) ∣∣ ((x = p ∣∣ x = q) \u0026\u0026 (f^lson^ ∣∣ f^rson^)) 其中 lson 和 rson 分别代表 x 节点的左孩子和右孩子。初看可能会感觉条件判断有点复杂，我们来一条条看，f^lson^ \u0026\u0026 f^rson^` 说明左子树和右子树均包含 p 节点或 q 节点，如果左子树包含的是 p 节点，那么右子树只能包含 q 节点，反之亦然，因为 p 节点和 q 节点都是不同且唯一的节点，因此如果满足这个判断条件即可说明 x 就是我们要找的最近公共祖先。再来看第二条判断条件，这个判断条件即是考虑了 x 恰好是 p 节点或 q 节点且它的左子树或右子树有一个包含了另一个节点的情况，因此如果满足这个判断条件亦可说明 x 就是我们要找的最近公共祖先。 你可能会疑惑这样找出来的公共祖先深度是否是最大的。其实是最大的，因为我们是自底向上从叶子节点开始更新的，所以在所有满足条件的公共祖先中一定是深度最大的祖先先被访问到，且由于 f~x~ 本身的定义很巧妙，在找到最近公共祖先 x 以后，f~x~ 按定义被设置为 true ，即假定了这个子树中只有一个 p 节点或 q 节点，因此其他公共祖先不会再被判断为符合条件。 class Solution { private TreeNode ans; public Solution() { this.ans = null; } private boolean dfs(TreeNode root, TreeNode p, TreeNode q) { if (root == null) return false; boolean lson = dfs(root.left, p, q); boolean rson = dfs(root.right, p, q); if ((lson \u0026\u0026 rson) || ((root.val == p.val || root.val == q.val) \u0026\u0026 (lson || rson))) { ans = root; } return lson || rson || (root.val == p.val || root.val == q.val); } public TreeNode lowestCommonAncestor(TreeNode root, TreeNode p, TreeNode q) { this.dfs(root, p, q); return this.ans; } } 复杂度分析 时间复杂度：O(N) ，其中 N 是二叉树的节点数。二叉树的所有节点有且只会被访问一次，因此时间复杂度为 O(N)。 空间复杂度：O(N) ，其中 N 是二叉树的节点数。递归调用的栈深度取决于二叉树的高度，二叉树最坏情况下为一条链，此时高度为 N ，因此空间复杂度为 O(N)。 方法二：存储父节点 思路 我们可以用哈希表存储所有节点的父节点，然后我们就可以利用节点的父节点信息从 p 结点开始不断往上跳，并记录已经访问过的节点，再从 q 节点开始不断往上跳，如果碰到已经访问过的节点，那么这个节点就是我们要找的最近公共祖先。 算法 从根节点开始遍历整棵二叉树，用哈希表记录每个节点的父节点指针。 从 p 节点开始不断往它的祖先移动，并用数据结构记录已经访问过的祖先节点。 同样，我们再从 q 节点开始不断往它的祖先移动，如果有祖先已经被访问过，即意味着这是 p 和 q 的深度最深的公共祖先，即 LCA 节点。 class Solution { Map\u003cInteger, TreeNode\u003e parent = new HashMap\u003cInteger, TreeNode\u003e(); Set\u003cInteger\u003e visited = new HashSet\u003cInteger\u003e(); public void dfs(TreeNode root) { if (root.left != null) { parent.put(root.left.val, root); dfs(root.left); } if (root.right != null) { parent.put(root.right.val, root); dfs(root.right); } } public TreeNode lowestCommonAncestor(TreeNode root, TreeNode p, TreeNode q) { dfs(root); while (p != null) { visited.add(p.val); p = parent.get(p.val); } while (q != null) { if (visited.contains(q.val)) { return q; } q = parent.get(q.val); } return null; } } 复杂度分析 时间复杂度：O(N)，其中 N 是二叉树的节点数。二叉树的所有节点有且只会被访问一次，从 p 和 q 节点往上跳经过的祖先节点个数不会超过 N，因此总的时间复杂度为 O(N)。 空间复杂度：O(N) ，其中 N 是二叉树的节点数。递归调用的栈深度取决于二叉树的高度，二叉树最坏情况下为一条链，此时高度为 N，因此空间复杂度为 O(N)，哈希表存储每个节点的父节点也需要 O(N) 的空间复杂度，因此最后总的空间复杂度为 O(N)。 ","date":"2023-01-08","objectID":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/:32:0","tags":["leetcode"],"title":"Leetcode每日一题","uri":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/"},{"categories":["数据结构与算法"],"content":"day33 257. 二叉树的所有路径 题目 给你一个二叉树的根节点 root ，按 任意顺序 ，返回所有从根节点到叶子节点的路径。 叶子节点 是指没有子节点的节点。 示例 1： 输入：root = [1,2,3,null,5] 输出：[\"1-\u003e2-\u003e5\",\"1-\u003e3\"] 示例 2： 输入：root = [1] 输出：[\"1\"] 提示： 树中节点的数目在范围 [1, 100] 内 -100 \u003c= Node.val \u003c= 100 解法 方法一：深度优先搜索 思路与算法 最直观的方法是使用深度优先搜索。在深度优先搜索遍历二叉树时，我们需要考虑当前的节点以及它的孩子节点。 如果当前节点不是叶子节点，则在当前的路径末尾添加该节点，并继续递归遍历该节点的每一个孩子节点。 如果当前节点是叶子节点，则在当前路径末尾添加该节点后我们就得到了一条从根节点到叶子节点的路径，将该路径加入到答案即可。 如此，当遍历完整棵二叉树以后我们就得到了所有从根节点到叶子节点的路径。当然，深度优先搜索也可以使用非递归的方式实现，这里不再赘述。 代码 class Solution { public List\u003cString\u003e binaryTreePaths(TreeNode root) { List\u003cString\u003e paths = new ArrayList\u003cString\u003e(); constructPaths(root, \"\", paths); return paths; } public void constructPaths(TreeNode root, String path, List\u003cString\u003e paths) { if (root != null) { StringBuffer pathSB = new StringBuffer(path); pathSB.append(Integer.toString(root.val)); if (root.left == null \u0026\u0026 root.right == null) { // 当前节点是叶子节点 paths.add(pathSB.toString()); // 把路径加入到答案中 } else { pathSB.append(\"-\u003e\"); // 当前节点不是叶子节点，继续递归遍历 constructPaths(root.left, pathSB.toString(), paths); constructPaths(root.right, pathSB.toString(), paths); } } } } 复杂度分析 时间复杂度：O(N^2^)，其中 N 表示节点数目。在深度优先搜索中每个节点会被访问一次且只会被访问一次，每一次会对 path 变量进行拷贝构造，时间代价为 O(N) ，故时间复杂度为 O(N^2^)。 空间复杂度：O(N^2^)，其中 N 表示节点数目。除答案数组外我们需要考虑递归调用的栈空间。在最坏情况下，当二叉树中每个节点只有一个孩子节点时，即整棵二叉树呈一个链状，此时递归的层数为 N，此时每一层的 path 变量的空间代价的总和为 $$ O(\\sum_{i = 1}^{N} i) = O(N^2) $$ 空间复杂度为 O(N^2^)。最好情况下，当二叉树为平衡二叉树时，它的高度为 log N ，此时空间复杂度为 $$ O((\\log {N})^2) $$ 。 方法二：广度优先搜索 思路与算法 我们也可以用广度优先搜索来实现。我们维护一个队列，存储节点以及根到该节点的路径。一开始这个队列里只有根节点。在每一步迭代中，我们取出队列中的首节点，如果它是叶子节点，则将它对应的路径加入到答案中。如果它不是叶子节点，则将它的所有孩子节点加入到队列的末尾。当队列为空时广度优先搜索结束，我们即能得到答案。 代码 class Solution { public List\u003cString\u003e binaryTreePaths(TreeNode root) { List\u003cString\u003e paths = new ArrayList\u003cString\u003e(); if (root == null) { return paths; } Queue\u003cTreeNode\u003e nodeQueue = new LinkedList\u003cTreeNode\u003e(); Queue\u003cString\u003e pathQueue = new LinkedList\u003cString\u003e(); nodeQueue.offer(root); pathQueue.offer(Integer.toString(root.val)); while (!nodeQueue.isEmpty()) { TreeNode node = nodeQueue.poll(); String path = pathQueue.poll(); if (node.left == null \u0026\u0026 node.right == null) { paths.add(path); } else { if (node.left != null) { nodeQueue.offer(node.left); pathQueue.offer(new StringBuffer(path).append(\"-\u003e\").append(node.left.val).toString()); } if (node.right != null) { nodeQueue.offer(node.right); pathQueue.offer(new StringBuffer(path).append(\"-\u003e\").append(node.right.val).toString()); } } } return paths; } } 复杂度分析 时间复杂度：O(N^2^)，其中 N 表示节点数目。分析同方法一。 空间复杂度：O(N^2^)，其中 N 表示节点数目。在最坏情况下，队列中会存在 N 个节点，保存字符串的队列中每个节点的最大长度为 N，故空间复杂度为 O(N^2^)。 ","date":"2023-01-08","objectID":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/:33:0","tags":["leetcode"],"title":"Leetcode每日一题","uri":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/"},{"categories":["数据结构与算法"],"content":"day34 404. 左叶子之和 题目 给定二叉树的根节点 root ，返回所有左叶子之和。 示例 1： 输入: root = [3,9,20,null,null,15,7] 输出: 24 解释: 在这个二叉树中，有两个左叶子，分别是 9 和 15，所以返回 24 示例 2: 输入: root = [1] 输出: 0 提示: 节点数在 [1, 1000] 范围内 -1000 \u003c= Node.val \u003c= 1000 解法 一个节点为「左叶子」节点，当且仅当它是某个节点的左子节点，并且它是一个叶子结点。因此我们可以考虑对整棵树进行遍历，当我们遍历到节点 node 时，如果它的左子节点是一个叶子结点，那么就将它的左子节点的值累加计入答案。 遍历整棵树的方法有深度优先搜索和广度优先搜索，下面分别给出了实现代码。 方法一：深度优先搜索 class Solution { public int sumOfLeftLeaves(TreeNode root) { return root != null ? dfs(root) : 0; } public int dfs(TreeNode node) { int ans = 0; if (node.left != null) { ans += isLeafNode(node.left) ? node.left.val : dfs(node.left); } if (node.right != null \u0026\u0026 !isLeafNode(node.right)) { ans += dfs(node.right); } return ans; } public boolean isLeafNode(TreeNode node) { return node.left == null \u0026\u0026 node.right == null; } } 复杂度分析 时间复杂度：O(n)，其中 n 是树中的节点个数。 空间复杂度：O(n)。空间复杂度与深度优先搜索使用的栈的最大深度相关。在最坏的情况下，树呈现链式结构，深度为 O(n)，对应的空间复杂度也为 O(n)。 方法二：广度优先搜索 class Solution { public int sumOfLeftLeaves(TreeNode root) { if (root == null) { return 0; } Queue\u003cTreeNode\u003e queue = new LinkedList\u003cTreeNode\u003e(); queue.offer(root); int ans = 0; while (!queue.isEmpty()) { TreeNode node = queue.poll(); if (node.left != null) { if (isLeafNode(node.left)) { ans += node.left.val; } else { queue.offer(node.left); } } if (node.right != null) { if (!isLeafNode(node.right)) { queue.offer(node.right); } } } return ans; } public boolean isLeafNode(TreeNode node) { return node.left == null \u0026\u0026 node.right == null; } } 复杂度分析 时间复杂度：O(n) ，其中 n 是树中的节点个数。 空间复杂度：O(n) 。空间复杂度与广度优先搜索使用的队列需要的容量相关，为 O(n)。 ","date":"2023-01-08","objectID":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/:34:0","tags":["leetcode"],"title":"Leetcode每日一题","uri":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/"},{"categories":["数据结构与算法"],"content":"day35 513. 找树左下角的值 题目 给定一个二叉树的 根节点 root，请找出该二叉树的 最底层 最左边 节点的值。 假设二叉树中至少有一个节点。 示例 1: 输入: root = [2,1,3] 输出: 1 示例 2: 输入: [1,2,3,4,null,5,6,null,null,7] 输出: 7 提示: 二叉树的节点个数的范围是 [1,104] -231 \u003c= Node.val \u003c= 231 - 1 解法 方法一：深度优先搜索 使用 height 记录遍历到的节点的高度，curVal 记录高度在 curHeight 的最左节点的值。在深度优先搜索时，我们先搜索当前节点的左子节点，再搜索当前节点的右子节点，然后判断当前节点的高度 height 是否大于 curHeight，如果是，那么将 curVal 设置为当前结点的值,curHeight 设置为 height。 因为我们先遍历左子树，然后再遍历右子树，所以对同一高度的所有节点，最左节点肯定是最先被遍历到的。 class Solution { int curVal = 0; int curHeight = 0; public int findBottomLeftValue(TreeNode root) { int curHeight = 0; dfs(root, 0); return curVal; } public void dfs(TreeNode root, int height) { if (root == null) { return; } height++; dfs(root.left, height); dfs(root.right, height); // 相当于后续遍历麻，最先遍历的一定是左子节点，然后是右、根子节点 if (height \u003e curHeight) { curHeight = height; curVal = root.val; } } } 复杂度分析 时间复杂度：O(n)，其中 n 是二叉树的节点数目。需要遍历 n 个节点。 空间复杂度：O(n)。递归栈需要占用 O(n) 的空间。 方法二：广度优先搜索 使用广度优先搜索遍历每一层的节点。在遍历一个节点时，需要先把它的非空右子节点放入队列，然后再把它的非空左子节点放入队列，这样才能保证从右到左遍历每一层的节点。广度优先搜索所遍历的最后一个节点的值就是最底层最左边节点的值。 class Solution { public int findBottomLeftValue(TreeNode root) { int ret = 0; Queue\u003cTreeNode\u003e queue = new ArrayDeque\u003cTreeNode\u003e(); queue.offer(root); while (!queue.isEmpty()) { TreeNode p = queue.poll(); if (p.right != null) { queue.offer(p.right); } if (p.left != null) { queue.offer(p.left); } ret = p.val; } return ret; } } 复杂度分析 时间复杂度：O(n)，其中 n 是二叉树的节点数目。 空间复杂度：O(n)。如果二叉树是满完全二叉树，那么队列 q 最多保存 2/n 个节点。 ","date":"2023-01-08","objectID":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/:35:0","tags":["leetcode"],"title":"Leetcode每日一题","uri":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/"},{"categories":["数据结构与算法"],"content":"day36 559. N 叉树的最大深度 题目 给定一个 N 叉树，找到其最大深度。 最大深度是指从根节点到最远叶子节点的最长路径上的节点总数。 N 叉树输入按层序遍历序列化表示，每组子节点由空值分隔（请参见示例）。 示例 1： 输入：root = [1,null,3,2,4,null,5,6] 输出：3 示例 2： 输入：root = [1,null,2,3,4,5,null,null,6,7,null,8,null,9,10,null,null,11,null,12,null,13,null,null,14] 输出：5 提示： 树的深度不会超过 1000 。 树的节点数目位于 [0, 104] 之间。 解法 方法一：深度优先搜索 如果根节点有 N 个子节点，则这 N 个子节点对应 N 个子树。记 N 个子树的最大深度中的最大值为 maxChildDepth，则该 N 叉树的最大深度为 maxChildDepth+1。 每个子树的最大深度又可以以同样的方式进行计算。因此我们可以用「深度优先搜索」的方法计算 N 叉树的最大深度。具体而言，在计算当前 N 叉树的最大深度时，可以先递归计算出其每个子树的最大深度，然后在 O(1) 的时间内计算出当前 N 叉树的最大深度。递归在访问到空节点时退出。 class Solution { public int maxDepth(Node root) { if (root == null) { return 0; } int maxChildDepth = 0; List\u003cNode\u003e children = root.children; for (Node child : children) { int childDepth = maxDepth(child); maxChildDepth = Math.max(maxChildDepth, childDepth); } return maxChildDepth + 1; } } 复杂度分析 时间复杂度：O(n)，其中 n 为 N 叉树节点的个数。每个节点在递归中只被遍历一次。 空间复杂度：O(height)，其中 height 表示 N 叉树的高度。递归函数需要栈空间，而栈空间取决于递归的深度，因此空间复杂度等价于 N 叉树的高度。 方法二：广度优先搜索 我们也可以用「广度优先搜索」的方法来解决这道题目，但我们需要对其进行一些修改，此时我们广度优先搜索的队列里存放的是「当前层的所有节点」。每次拓展下一层的时候，不同于广度优先搜索的每次只从队列里拿出一个节点，我们需要将队列里的所有节点都拿出来进行拓展，这样能保证每次拓展完的时候队列里存放的是当前层的所有节点，即我们是一层一层地进行拓展。最后我们用一个变量 ans 来维护拓展的次数，该 N 叉树的最大深度即为 ans。 class Solution { public int maxDepth(Node root) { if (root == null) { return 0; } Queue\u003cNode\u003e queue = new LinkedList\u003cNode\u003e(); queue.offer(root); int ans = 0; while (!queue.isEmpty()) { int size = queue.size(); while (size \u003e 0) { Node node = queue.poll(); List\u003cNode\u003e children = node.children; for (Node child : children) { queue.offer(child); } size--; } ans++; } return ans; } } 复杂度分析 时间复杂度：O(n)，其中 n 为 N 叉树的节点个数。与方法一同样的分析，每个节点只会被访问一次。 空间复杂度：此方法空间的消耗取决于队列存储的元素数量，其在最坏情况下会达到 O(n)。 ","date":"2023-01-08","objectID":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/:36:0","tags":["leetcode"],"title":"Leetcode每日一题","uri":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/"},{"categories":["数据结构与算法"],"content":"day37 617. 合并二叉树 题目 给你两棵二叉树： root1 和 root2 。 想象一下，当你将其中一棵覆盖到另一棵之上时，两棵树上的一些节点将会重叠（而另一些不会）。你需要将这两棵树合并成一棵新二叉树。合并的规则是：如果两个节点重叠，那么将这两个节点的值相加作为合并后节点的新值；否则，不为 null 的节点将直接作为新二叉树的节点。 返回合并后的二叉树。 注意: 合并过程必须从两个树的根节点开始。 示例 1： 输入：root1 = [1,3,2,5], root2 = [2,1,3,null,4,null,7] 输出：[3,4,5,5,4,null,7] 示例 2： 输入：root1 = [1], root2 = [1,2] 输出：[2,2] 提示： 两棵树中的节点数目在范围 [0, 2000] 内 -104 \u003c= Node.val \u003c= 104 解法 方法一：深度优先搜索 可以使用深度优先搜索合并两个二叉树。从根节点开始同时遍历两个二叉树，并将对应的节点进行合并。 两个二叉树的对应节点可能存在以下三种情况，对于每种情况使用不同的合并方式。 如果两个二叉树的对应节点都为空，则合并后的二叉树的对应节点也为空； 如果两个二叉树的对应节点只有一个为空，则合并后的二叉树的对应节点为其中的非空节点； 如果两个二叉树的对应节点都不为空，则合并后的二叉树的对应节点的值为两个二叉树的对应节点的值之和，此时需要显性合并两个节点。 对一个节点进行合并之后，还要对该节点的左右子树分别进行合并。这是一个递归的过程。 class Solution { public TreeNode mergeTrees(TreeNode root1, TreeNode root2) { TreeNode node; // 定义一个局部变量用于递归时，本层节点记录 if(root1==null\u0026\u0026root2==null){// 当两个根节点都是null，直接返回null return null; }else if(root1==null){// 当两个根节点有一个为空时，也直接返回非空节点，没必要加 return root2; }else if(root2==null){// 当两个根节点有一个为空时，也直接返回非空节点，没必要加 return root1; }else{// 当两个根节点都不为空的时候，先记录合并后的根节点 node = new TreeNode(root1.val+root2.val); } // 然后再递归的遍历左右子树 TreeNode left = mergeTrees(root1.left,root2.left); TreeNode right = mergeTrees(root1.right,root2.right); // 将左右子树的结果挂在新的根节点上 node.left = left; node.right = right; return node; } } 复杂度分析 时间复杂度：O(min(m,n))，其中 m 和 n 分别是两个二叉树的节点个数。对两个二叉树同时进行深度优先搜索，只有当两个二叉树中的对应节点都不为空时才会对该节点进行显性合并操作，因此被访问到的节点数不会超过较小的二叉树的节点数。 空间复杂度：O(min(m,n))，其中 m 和 n 分别是两个二叉树的节点个数。空间复杂度取决于递归调用的层数，递归调用的层数不会超过较小的二叉树的最大高度，最坏情况下，二叉树的高度等于节点数。 方法二：广度优先搜索 也可以使用广度优先搜索合并两个二叉树。首先判断两个二叉树是否为空，如果两个二叉树都为空，则合并后的二叉树也为空，如果只有一个二叉树为空，则合并后的二叉树为另一个非空的二叉树。 如果两个二叉树都不为空，则首先计算合并后的根节点的值，然后从合并后的二叉树与两个原始二叉树的根节点开始广度优先搜索，从根节点开始同时遍历每个二叉树，并将对应的节点进行合并。 使用三个队列分别存储合并后的二叉树的节点以及两个原始二叉树的节点。初始时将每个二叉树的根节点分别加入相应的队列。每次从每个队列中取出一个节点，判断两个原始二叉树的节点的左右子节点是否为空。如果两个原始二叉树的当前节点中至少有一个节点的左子节点不为空，则合并后的二叉树的对应节点的左子节点也不为空。对于右子节点同理。 如果合并后的二叉树的左子节点不为空，则需要根据两个原始二叉树的左子节点计算合并后的二叉树的左子节点以及整个左子树。考虑以下两种情况： 如果两个原始二叉树的左子节点都不为空，则合并后的二叉树的左子节点的值为两个原始二叉树的左子节点的值之和，在创建合并后的二叉树的左子节点之后，将每个二叉树中的左子节点都加入相应的队列； 如果两个原始二叉树的左子节点有一个为空，即有一个原始二叉树的左子树为空，则合并后的二叉树的左子树即为另一个原始二叉树的左子树，此时也不需要对非空左子树继续遍历，因此不需要将左子节点加入队列。 对于右子节点和右子树，处理方法与左子节点和左子树相同。 class Solution { public TreeNode mergeTrees(TreeNode t1, TreeNode t2) { if (t1 == null) { return t2; } if (t2 == null) { return t1; } TreeNode merged = new TreeNode(t1.val + t2.val); Queue\u003cTreeNode\u003e queue = new LinkedList\u003cTreeNode\u003e(); Queue\u003cTreeNode\u003e queue1 = new LinkedList\u003cTreeNode\u003e(); Queue\u003cTreeNode\u003e queue2 = new LinkedList\u003cTreeNode\u003e(); queue.offer(merged); queue1.offer(t1); queue2.offer(t2); while (!queue1.isEmpty() \u0026\u0026 !queue2.isEmpty()) { TreeNode node = queue.poll(), node1 = queue1.poll(), node2 = queue2.poll(); TreeNode left1 = node1.left, left2 = node2.left, right1 = node1.right, right2 = node2.right; if (left1 != null || left2 != null) { if (left1 != null \u0026\u0026 left2 != null) { TreeNode left = new TreeNode(left1.val + left2.val); node.left = left; queue.offer(left); queue1.offer(left1); queue2.offer(left2); } else if (left1 != null) { node.left = left1; } else if (left2 != null) { node.left = left2; } } if (right1 != null || right2 != null) { if (right1 != null \u0026\u0026 right2 != null) { TreeNode right = new TreeNode(right1.val + right2.val); node.right = right; queue.offer(right); queue1.offer(right1); queue2.offer(right2); } else if (right1 != null) { node.right = right1; } else { node.right = right2; } } } return merged; } } 复杂度分析 时间复杂度：O(min(m,n))，其中 m 和 n 分别是两个二叉树的节点个数。对两个二叉树同时进行广度优先搜索，只有当两个二叉树中的对应节点都不为空时才会访问到该节点，因此被访问到的节点数不会超过较小的二叉树的节点数。 空间复杂度：O(min(m,n))，其中 m 和 n 分别是两个二叉树的节点个数。空间复杂度取决于队列中的元素个数，队列中的元素个数不会超过较小的二叉树的节点数。 ","date":"2023-01-08","objectID":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/:37:0","tags":["leetcode"],"title":"Leetcode每日一题","uri":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/"},{"categories":["数据结构与算法"],"content":"day38 654. 最大二叉树 题目 给定一个不重复的整数数组 nums 。 最大二叉树 可以用下面的算法从 nums 递归地构建: 创建一个根节点，其值为 nums 中的最大值。 递归地在最大值 左边 的 子数组前缀上 构建左子树。 递归地在最大值 右边 的 子数组后缀上 构建右子树。 返回 nums 构建的 *最大二叉树* 。 示例 1： 输入：nums = [3,2,1,6,0,5] 输出：[6,3,5,null,2,0,null,null,1] 解释：递归调用如下所示： - [3,2,1,6,0,5] 中的最大值是 6 ，左边部分是 [3,2,1] ，右边部分是 [0,5] 。 - [3,2,1] 中的最大值是 3 ，左边部分是 [] ，右边部分是 [2,1] 。 - 空数组，无子节点。 - [2,1] 中的最大值是 2 ，左边部分是 [] ，右边部分是 [1] 。 - 空数组，无子节点。 - 只有一个元素，所以子节点是一个值为 1 的节点。 - [0,5] 中的最大值是 5 ，左边部分是 [0] ，右边部分是 [] 。 - 只有一个元素，所以子节点是一个值为 0 的节点。 - 空数组，无子节点。 示例 2： 输入：nums = [3,2,1] 输出：[3,null,2,null,1] 提示： 1 \u003c= nums.length \u003c= 1000 0 \u003c= nums[i] \u003c= 1000 nums 中的所有整数 互不相同 解法 方法一：递归 思路与算法 最简单的方法是直接按照题目描述进行模拟。 我们用递归函数 construct(nums,left,right) 表示对数组 nums 中从 nums 到 的元素构建一棵树。我们首先找到这一区间中的最大值，记为 nums 中从 nums[best]，这样就确定了根节点的值。随后我们就可以进行递归： 左子树为 construct(nums, left, best-1)； 右子树为 construct(nums,best+1,right)。 当递归到一个无效的区间（即 left*\u003e*right）时，便可以返回一棵空的树。 代码 class Solution { public TreeNode constructMaximumBinaryTree(int[] nums) { return construct(nums, 0, nums.length - 1); } public TreeNode construct(int[] nums, int left, int right) { if (left \u003e right) { return null; } int best = left; for (int i = left + 1; i \u003c= right; ++i) { if (nums[i] \u003e nums[best]) { best = i; } } TreeNode node = new TreeNode(nums[best]); node.left = construct(nums, left, best - 1); node.right = construct(nums, best + 1, right); return node; } } 复杂度分析 时间复杂度：O(n^2^)，其中 n 是数组 nums 的长度。在最坏的情况下，数组严格递增或递减，需要递归 n 层，第 i (0≤i\u003cn) 层需要遍历 n-i 个元素以找出最大值，总时间复杂度为 O(n^2^)。 空间复杂度：O(n)，即为最坏情况下需要使用的栈空间。 方法二：单调栈 思路与算法 我们可以将题目中构造树的过程等价转换为下面的构造过程： 初始时，我们只有一个根节点，其中存储了整个数组； 在每一步操作中，我们可以「任选」一个存储了超过一个数的节点，找出其中的最大值并存储在该节点。最大值左侧的数组部分下放到该节点的左子节点，右侧的数组部分下放到该节点的右子节点； 如果所有的节点都恰好存储了一个数，那么构造结束。 由于最终构造出的是一棵树，因此无需按照题目的要求「递归」地进行构造，而是每次可以「任选」一个节点进行构造。这里可以类比一棵树的「深度优先搜索」和「广度优先搜索」，二者都可以起到遍历整棵树的效果。 既然可以任意进行选择，那么我们不妨每次选择数组中最大值最大的那个节点进行构造。这样一来，我们就可以保证按照数组中元素降序排序的顺序依次构造每个节点。因此： 当我们选择的节点中数组的最大值为 nums[i] 时，所有大于 nums[i] 的元素已经被构造过（即被单独放入某一个节点中），所有小于 nums[i] 的元素还没有被构造过。 这就说明： 在最终构造出的树上，以 nums[i] 为根节点的子树，在原数组中对应的区间，左边界为 nums[i] 左侧第一个比它大的元素所在的位置，右边界为 nums[i] 右侧第一个比它大的元素所在的位置。左右边界均为开边界。 如果某一侧边界不存在，则那一侧边界为数组的边界。如果两侧边界均不存在，说明其为最大值，即根节点。 并且： nums[i] 的父结点是两个边界中较小的那个元素对应的节点。 因此，我们的任务变为：找出每一个元素左侧和右侧第一个比它大的元素所在的位置。这就是一个经典的单调栈问题了，可以参考 503. 下一个更大元素 II。如果左侧的元素较小，那么该元素就是左侧元素的右子节点；如果右侧的元素较小，那么该元素就是右侧元素的左子节点。 代码 class Solution { public TreeNode constructMaximumBinaryTree(int[] nums) { int n = nums.length; Deque\u003cInteger\u003e stack = new ArrayDeque\u003cInteger\u003e(); int[] left = new int[n]; int[] right = new int[n]; Arrays.fill(left, -1); Arrays.fill(right, -1); TreeNode[] tree = new TreeNode[n]; for (int i = 0; i \u003c n; ++i) { tree[i] = new TreeNode(nums[i]); while (!stack.isEmpty() \u0026\u0026 nums[i] \u003e nums[stack.peek()]) { right[stack.pop()] = i; } if (!stack.isEmpty()) { left[i] = stack.peek(); } stack.push(i); } TreeNode root = null; for (int i = 0; i \u003c n; ++i) { if (left[i] == -1 \u0026\u0026 right[i] == -1) { root = tree[i]; } else if (right[i] == -1 || (left[i] != -1 \u0026\u0026 nums[left[i]] \u003c nums[right[i]])) { tree[left[i]].right = tree[i]; } else { tree[right[i]].left = tree[i]; } } return root; } } 我们还可以把最后构造树的过程放进单调栈求解的步骤中，省去用来存储左右边界的数组。下面的代码理解起来较为困难，同一个节点的左右子树会被多次赋值，读者可以仔细品味其妙处所在。 class Solution { public TreeNode constructMaximumBinaryTree(int[] nums) { int n = nums.length; List\u003cInteger\u003e stack = new ArrayList\u003cInteger\u003e(); TreeNode[] tree = new TreeNode[n]; for (int i = 0; i \u003c n; ++i) { tree[i] = new TreeNode(nums[i]); while (!stack.isEmpty() \u0026\u0026 nums[i] \u003e nums[stack.get(stack.size() - 1)]) { tree[i].left = tree[stack.get(stack.size() - 1)]; stack.remove(stack.size() - 1); } if (!stack.isEmpty()) { tree[stack.get(stack.size() - 1)].right = tree[i]; } stack.add(i); } return tree[stack.get(0)]; } } 复杂度分析 时间复杂度：O(n)，其中 n 是数组 nums 的长度。单调栈求解左右边界和构造树均需要 O(n) 的时间。 空间复杂度：O(n)，即为单调栈和数组 tree 需要使用的空间。 ","date":"2023-01-08","objectID":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/:38:0","tags":["leetcode"],"title":"Leetcode每日一题","uri":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/"},{"categories":["数据结构与算法"],"content":"day39 144. 二叉树的前序遍历 题目 给你二叉树的根节点 root ，返回它节点值的 前序 遍历。 示例 1： 输入：root = [1,null,2,3] 输出：[1,2,3] 示例 2： 输入：root = [] 输出：[] 示例 3： 输入：root = [1] 输出：[1] 示例 4： 输入：root = [1,2] 输出：[1,2] 示例 5： 输入：root = [1,null,2] 输出：[1,2] 提示： 树中节点数目在范围 [0, 100] 内 -100 \u003c= Node.val \u003c= 100 **进阶：**递归算法很简单，你可以通过迭代算法完成吗？ 解法 方法一：递归 思路与算法 首先我们需要了解什么是二叉树的前序遍历：按照访问根节点——左子树——右子树的方式遍历这棵树，而在访问左子树或者右子树的时候，我们按照同样的方式遍历，直到遍历完整棵树。因此整个遍历过程天然具有递归的性质，我们可以直接用递归函数来模拟这一过程。 定义 preorder(root) 表示当前遍历到 root 节点的答案。按照定义，我们只要首先将 root 节点的值加入答案，然后递归调用 preorder(root.left) 来遍历 root 节点的左子树，最后递归调用 preorder(root.right) 来遍历 root 节点的右子树即可，递归终止的条件为碰到空节点。 代码 class Solution { public List\u003cInteger\u003e preorderTraversal(TreeNode root) { List\u003cInteger\u003e res = new ArrayList\u003cInteger\u003e(); preorder(root, res); return res; } public void preorder(TreeNode root, List\u003cInteger\u003e res) { if (root == null) { return; } res.add(root.val); preorder(root.left, res); preorder(root.right, res); } } 复杂度分析 时间复杂度：O(n)，其中 n 是二叉树的节点数。每一个节点恰好被遍历一次。 空间复杂度：O(n)，为递归过程中栈的开销，平均情况下为 O(log n)，最坏情况下树呈现链状，为 O(n)。 方法二：迭代 思路与算法 我们也可以用迭代的方式实现方法一的递归函数，两种方式是等价的，区别在于递归的时候隐式地维护了一个栈，而我们在迭代的时候需要显式地将这个栈模拟出来，其余的实现与细节都相同，具体可以参考下面的代码。 初始化维护一个栈，将根节点入栈。 当栈不为空时 弹出栈顶元素 node，将节点值加入结果数组中。 若 node 的右子树不为空，右子树入栈。 若 node 的左子树不为空，左子树入栈。 代码 class Solution { public List\u003cInteger\u003e preorderTraversal(TreeNode root) { List\u003cInteger\u003e res = new ArrayList\u003cInteger\u003e(); if (root == null) { return res; } Deque\u003cTreeNode\u003e stack = new LinkedList\u003cTreeNode\u003e(); TreeNode node = root; while (!stack.isEmpty() || node != null) { while (node != null) { res.add(node.val); stack.push(node); node = node.left; } node = stack.pop(); node = node.right; } return res; } } 复杂度分析 时间复杂度：O(n)，其中 n 是二叉树的节点数。每一个节点恰好被遍历一次。 空间复杂度：O(n)，为迭代过程中显式栈的开销，平均情况下为 O(log n)，最坏情况下树呈现链状，为 O(n)。 方法三：Morris 遍历 思路与算法 有一种巧妙的方法可以在线性时间内，只占用常数空间来实现前序遍历。这种方法由 J. H. Morris 在 1979 年的论文「Traversing Binary Trees Simply and Cheaply」中首次提出，因此被称为 Morris 遍历。 Morris 遍历的核心思想是利用树的大量空闲指针，实现空间开销的极限缩减。其前序遍历规则总结如下： 新建临时节点，令该节点为 root； 如果当前节点的左子节点为空，将当前节点加入答案，并遍历当前节点的右子节点； 如果当前节点的左子节点不为空，在当前节点的左子树中找到当前节点在中序遍历下的前驱节点： 如果前驱节点的右子节点为空，将前驱节点的右子节点设置为当前节点。然后将当前节点加入答案，并将前驱节点的右子节点更新为当前节点。当前节点更新为当前节点的左子节点。 如果前驱节点的右子节点为当前节点，将它的右子节点重新设为空。当前节点更新为当前节点的右子节点。 重复步骤 2 和步骤 3，直到遍历结束。 这样我们利用 Morris 遍历的方法，前序遍历该二叉树，即可实现线性时间与常数空间的遍历。 代码 class Solution { public List\u003cInteger\u003e preorderTraversal(TreeNode root) { List\u003cInteger\u003e res = new ArrayList\u003cInteger\u003e(); if (root == null) { return res; } TreeNode p1 = root, p2 = null; while (p1 != null) { p2 = p1.left; if (p2 != null) { while (p2.right != null \u0026\u0026 p2.right != p1) { p2 = p2.right; } if (p2.right == null) { res.add(p1.val); p2.right = p1; p1 = p1.left; continue; } else { p2.right = null; } } else { res.add(p1.val); } p1 = p1.right; } return res; } } 复杂度分析 时间复杂度：O(n)，其中 n 是二叉树的节点数。没有左子树的节点只被访问一次，有左子树的节点被访问两次。 空间复杂度：O(1)。只操作已经存在的指针（树的空闲指针），因此只需要常数的额外空间。 ","date":"2023-01-08","objectID":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/:39:0","tags":["leetcode"],"title":"Leetcode每日一题","uri":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/"},{"categories":["数据结构与算法"],"content":"day40 94. 二叉树的中序遍历 题目 给定一个二叉树的根节点 root ，返回 它的 中序 遍历 。 示例 1： 输入：root = [1,null,2,3] 输出：[1,3,2] 示例 2： 输入：root = [] 输出：[] 示例 3： 输入：root = [1] 输出：[1] 提示： 树中节点数目在范围 [0, 100] 内 -100 \u003c= Node.val \u003c= 100 进阶: 递归算法很简单，你可以通过迭代算法完成吗？ 解法 方法一：递归 思路与算法 首先我们需要了解什么是二叉树的中序遍历：按照访问左子树——根节点——右子树的方式遍历这棵树，而在访问左子树或者右子树的时候我们按照同样的方式遍历，直到遍历完整棵树。因此整个遍历过程天然具有递归的性质，我们可以直接用递归函数来模拟这一过程。 定义 inorder(root) 表示当前遍历到 root 节点的答案，那么按照定义，我们只要递归调用 inorder(root.left) 来遍历 root 节点的左子树，然后将 root 节点的值加入答案，再递归调用inorder(root.right) 来遍历 root 节点的右子树即可，递归终止的条件为碰到空节点。 代码 class Solution { public List\u003cInteger\u003e inorderTraversal(TreeNode root) { List\u003cInteger\u003e res = new ArrayList\u003cInteger\u003e(); inorder(root, res); return res; } public void inorder(TreeNode root, List\u003cInteger\u003e res) { if (root == null) { return; } inorder(root.left, res); res.add(root.val); inorder(root.right, res); } } 复杂度分析 时间复杂度：O(n)，其中 n 为二叉树节点的个数。二叉树的遍历中每个节点会被访问一次且只会被访问一次。 空间复杂度：O(n)。空间复杂度取决于递归的栈深度，而栈深度在二叉树为一条链的情况下会达到 O(n) 的级别。 方法二：迭代 思路与算法 方法一的递归函数我们也可以用迭代的方式实现，两种方式是等价的，区别在于递归的时候隐式地维护了一个栈，而我们在迭代的时候需要显式地将这个栈模拟出来，其他都相同，具体实现可以看下面的代码。 初始化一个空栈。 当【根节点不为空】或者【栈不为空】时，从根节点开始 若当前节点有左子树，一直遍历左子树，每次将当前节点压入栈中。 若当前节点无左子树，从栈中弹出该节点，尝试访问该节点的右子树。 代码 class Solution { public List\u003cInteger\u003e inorderTraversal(TreeNode root) { List\u003cInteger\u003e res = new ArrayList\u003cInteger\u003e(); Deque\u003cTreeNode\u003e stk = new LinkedList\u003cTreeNode\u003e(); while (root != null || !stk.isEmpty()) { while (root != null) { stk.push(root); root = root.left; } root = stk.pop(); res.add(root.val); root = root.right; } return res; } } 复杂度分析 时间复杂度：O(n)，其中 n 为二叉树节点的个数。二叉树的遍历中每个节点会被访问一次且只会被访问一次。 空间复杂度：O(n)。空间复杂度取决于栈深度，而栈深度在二叉树为一条链的情况下会达到 O(n) 的级别。 方法三：Morris 中序遍历 思路与算法 Morris 遍历算法是另一种遍历二叉树的方法，它能将非递归的中序遍历空间复杂度降为 O(1)。 Morris 遍历算法整体步骤如下（假设当前遍历到的节点为 x）： 如果 x 无左孩子，先将 x 的值加入答案数组，再访问 x 的右孩子，即x*=*x.right。 如果 x 有左孩子，则找到 x 左子树上最右的节点（即左子树中序遍历的最后一个节点，x 在中序遍历中的前驱节点），我们记为predecessor. 根据predecessor的右孩子是否为空，进行如下操作。 如果 predecessor 的右孩子为空，则将其右孩子指向 x，然后访问 x 的左孩子，即 x = x.left。 如果 predecessor 的右孩子不为空，则此时其右孩子指向 x，说明我们已经遍历完 x 的左子树，我们将 predecessor 的右孩子置空，将 x 的值加入答案数组，然后访问 x 的右孩子，即 x*=*x.right。 重复上述操作，直至访问完整棵树。 其实整个过程我们就多做一步：假设当前遍历到的节点为 x，将 x 的左子树中最右边的节点的右孩子指向 x，这样在左子树遍历完成后我们通过这个指向走回了 x，且能通过这个指向知晓我们已经遍历完成了左子树，而不用再通过栈来维护，省去了栈的空间复杂度。 代码 class Solution { public List\u003cInteger\u003e inorderTraversal(TreeNode root) { List\u003cInteger\u003e res = new ArrayList\u003cInteger\u003e(); TreeNode predecessor = null; while (root != null) { if (root.left != null) { // predecessor 节点就是当前 root 节点向左走一步，然后一直向右走至无法走为止 predecessor = root.left; while (predecessor.right != null \u0026\u0026 predecessor.right != root) { predecessor = predecessor.right; } // 让 predecessor 的右指针指向 root，继续遍历左子树 if (predecessor.right == null) { predecessor.right = root; root = root.left; } // 说明左子树已经访问完了，我们需要断开链接 else { res.add(root.val); predecessor.right = null; root = root.right; } } // 如果没有左孩子，则直接访问右孩子 else { res.add(root.val); root = root.right; } } return res; } } 复杂度分析 时间复杂度：O(n)，其中 n 为二叉搜索树的节点个数。Morris 遍历中每个节点会被访问两次，因此总时间复杂度为 O(2n)=O(n)。 空间复杂度：O(1)。 ","date":"2023-01-08","objectID":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/:40:0","tags":["leetcode"],"title":"Leetcode每日一题","uri":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/"},{"categories":["数据结构与算法"],"content":"day41 105. 从前序与中序遍历序列构造二叉树 题目 给定两个整数数组 preorder 和 inorder ，其中 preorder 是二叉树的先序遍历， inorder 是同一棵树的中序遍历，请构造二叉树并返回其根节点。 示例 1: 输入: preorder = [3,9,20,15,7], inorder = [9,3,15,20,7] 输出: [3,9,20,null,null,15,7] 示例 2: 输入: preorder = [-1], inorder = [-1] 输出: [-1] 提示: 1 \u003c= preorder.length \u003c= 3000 inorder.length == preorder.length -3000 \u003c= preorder[i], inorder[i] \u003c= 3000 preorder 和 inorder 均 无重复 元素 inorder 均出现在 preorder preorder 保证 为二叉树的前序遍历序列 inorder 保证 为二叉树的中序遍历序列 解法 二叉树前序遍历的顺序为： 先遍历根节点； 随后递归地遍历左子树； 最后递归地遍历右子树。 二叉树中序遍历的顺序为： 先递归地遍历左子树； 随后遍历根节点； 最后递归地遍历右子树。 在「递归」地遍历某个子树的过程中，我们也是将这颗子树看成一颗全新的树，按照上述的顺序进行遍历。挖掘「前序遍历」和「中序遍历」的性质，我们就可以得出本题的做法。 方法一：递归 思路 对于任意一颗树而言，前序遍历的形式总是 [ 根节点, [左子树的前序遍历结果], [右子树的前序遍历结果] ] 即根节点总是前序遍历中的第一个节点。而中序遍历的形式总是 [ [左子树的中序遍历结果], 根节点, [右子树的中序遍历结果] ] 只要我们在中序遍历中定位到根节点，那么我们就可以分别知道左子树和右子树中的节点数目。由于同一颗子树的前序遍历和中序遍历的长度显然是相同的，因此我们就可以对应到前序遍历的结果中，对上述形式中的所有左右括号进行定位。 这样以来，我们就知道了左子树的前序遍历和中序遍历结果，以及右子树的前序遍历和中序遍历结果，我们就可以递归地对构造出左子树和右子树，再将这两颗子树接到根节点的左右位置。 细节 在中序遍历中对根节点进行定位时，一种简单的方法是直接扫描整个中序遍历的结果并找出根节点，但这样做的时间复杂度较高。我们可以考虑使用哈希表来帮助我们快速地定位根节点。对于哈希映射中的每个键值对，键表示一个元素（节点的值），值表示其在中序遍历中的出现位置。在构造二叉树的过程之前，我们可以对中序遍历的列表进行一遍扫描，就可以构造出这个哈希映射。在此后构造二叉树的过程中，我们就只需要 O(1) 的时间对根节点进行定位了。 下面的代码给出了详细的注释。 class Solution { private Map\u003cInteger, Integer\u003e indexMap; public TreeNode myBuildTree(int[] preorder, int[] inorder, int preorder_left, int preorder_right, int inorder_left, int inorder_right) { if (preorder_left \u003e preorder_right) { return null; } // 前序遍历中的第一个节点就是根节点 int preorder_root = preorder_left; // 在中序遍历中定位根节点 int inorder_root = indexMap.get(preorder[preorder_root]); // 先把根节点建立出来 TreeNode root = new TreeNode(preorder[preorder_root]); // 得到左子树中的节点数目 int size_left_subtree = inorder_root - inorder_left; // 递归地构造左子树，并连接到根节点 // 先序遍历中「从 左边界+1 开始的 size_left_subtree」个元素就对应了中序遍历中「从 左边界 开始到 根节点定位-1」的元素 root.left = myBuildTree(preorder, inorder, preorder_left + 1, preorder_left + size_left_subtree, inorder_left, inorder_root - 1); // 递归地构造右子树，并连接到根节点 // 先序遍历中「从 左边界+1+左子树节点数目 开始到 右边界」的元素就对应了中序遍历中「从 根节点定位+1 到 右边界」的元素 root.right = myBuildTree(preorder, inorder, preorder_left + size_left_subtree + 1, preorder_right, inorder_root + 1, inorder_right); return root; } public TreeNode buildTree(int[] preorder, int[] inorder) { int n = preorder.length; // 构造哈希映射，帮助我们快速定位根节点 indexMap = new HashMap\u003cInteger, Integer\u003e(); for (int i = 0; i \u003c n; i++) { indexMap.put(inorder[i], i); } return myBuildTree(preorder, inorder, 0, n - 1, 0, n - 1); } } /** * Definition for a binary tree node. * type TreeNode struct { * Val int * Left *TreeNode * Right *TreeNode * } */ func buildTree(preorder []int, inorder []int) *TreeNode { return dfs(preorder,inorder) } func dfs(preorder,inorder []int)*TreeNode{ if len(preorder)==0{ return nil } // fmt.Println(preorder[prel]) root:=\u0026TreeNode{Val:preorder[0]}// 创建根节点 inRootIdx:=0//取中序遍历根节点索引 for ;inRootIdx\u003clen(inorder);inRootIdx++{ if inorder[inRootIdx]==preorder[0]{ break } } root.Left = dfs(preorder[1:len(inorder[:inRootIdx])+1],inorder[:inRootIdx]) root.Right = dfs(preorder[len(inorder[:inRootIdx])+1:],inorder[inRootIdx+1:]) return root } 复杂度分析 时间复杂度：O(n)，其中 n 是树中的节点个数。 空间复杂度：O(n)，除去返回的答案需要的 O(n) 空间之外，我们还需要使用 O(n) 的空间存储哈希映射，以及 O(h)（其中 h 是树的高度）的空间表示递归时栈空间。这里 h \u003c n，所以总空间复杂度为 O(n)。 方法二：迭代 思路 迭代法是一种非常巧妙的实现方法。 对于前序遍历中的任意两个连续节点 u 和 v，根据前序遍历的流程，我们可以知道 u 和 v 只有两种可能的关系： v 是 u 的左儿子。这是因为在遍历到 u 之后，下一个遍历的节点就是 u 的左儿子，即 v； u 没有左儿子，并且 v 是 u 的某个祖先节点（或者 u 本身）的右儿子。如果 u 没有左儿子，那么下一个遍历的节点就是 u 的右儿子。如果 u 没有右儿子，我们就会向上回溯，直到遇到第一个有右儿子（且 u 不在它的右儿子的子树中）的节点 u_a，那么 v 就是 u_a 的右儿子。 第二种关系看上去有些复杂。我们举一个例子来说明其正确性，并在例子中给出我们的迭代算法。 例子 我们以树 3 / \\ 9 20 / / \\ 8 15 7 / \\ 5 10 / 4 为例，它的前序遍历和中序遍历分别为 preorder = [3, 9, 8, 5, 4, 10, 20, 15, 7] inorder = [4, 5, 8, 10, 9, 3, 15, 20, 7] 我们用一个栈 stack 来维护「当前节点的所有还没有考虑过右儿子的祖先节点」，栈顶就是当前节点。也就是说，只有在栈中的节点才可能连接一个新的右儿子。同时，我们用一个指针 index 指向中序遍历的某个位置，初始值为 0。index 对应的节点是「当前节点不断往左走达到的最终节点」，这也是符合中序遍历的，它的作用在下面的过程中会有所体现。 首先我们将根节点 3 入栈，再初始化 index 所指向的节点为 4，随后对于前序遍历中的每个节点，我们依次判断它是栈顶节点的左儿子，还是栈中某个节点的右儿子。 我们遍历 9。9 一定是栈顶节点 3 的左儿子。我们使用反证法，假设 9 是 3 的右儿子，那么 3 没有左儿子，index 应该恰好指向 3，但实际上为 4","date":"2023-01-08","objectID":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/:41:0","tags":["leetcode"],"title":"Leetcode每日一题","uri":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/"},{"categories":["数据结构与算法"],"content":"day42 106. 从中序与后序遍历序列构造二叉树 题目 给定两个整数数组 inorder 和 postorder ，其中 inorder 是二叉树的中序遍历， postorder 是同一棵树的后序遍历，请你构造并返回这颗 二叉树 。 示例 1: 输入：inorder = [9,3,15,20,7], postorder = [9,15,7,20,3] 输出：[3,9,20,null,null,15,7] 示例 2: 输入：inorder = [-1], postorder = [-1] 输出：[-1] 提示: 1 \u003c= inorder.length \u003c= 3000 postorder.length == inorder.length -3000 \u003c= inorder[i], postorder[i] \u003c= 3000 inorder 和 postorder 都由 不同 的值组成 postorder 中每一个值都在 inorder 中 inorder 保证是树的中序遍历 postorder 保证是树的后序遍历 解法 方法一：递归 我们可以发现后序遍历的数组最后一个元素代表的即为根节点。知道这个性质后，我们可以利用已知的根节点信息在中序遍历的数组中找到根节点所在的下标，然后根据其将中序遍历的数组分成左右两部分，左边部分即左子树，右边部分为右子树，针对每个部分可以用同样的方法继续递归下去构造。 算法 为了高效查找根节点元素在中序遍历数组中的下标，我们选择创建哈希表来存储中序序列，即建立一个（元素，下标）键值对的哈希表。 定义递归函数 helper(in_left, in_right) 表示当前递归到中序序列中当前子树的左右边界，递归入口为helper(0, n - 1) ： 如果 in_left \u003e in_right，说明子树为空，返回空节点。 选择后序遍历的最后一个节点作为根节点。 利用哈希表 O(1) 查询当根节点在中序遍历中下标为 index。从 in_left 到 index - 1 属于左子树，从 index + 1 到 in_right 属于右子树。 根据后序遍历逻辑，递归创建右子树 helper(index + 1, in_right) 和左子树 helper(in_left, index - 1)。注意这里有需要先创建右子树，再创建左子树的依赖关系。可以理解为在后序遍历的数组中整个数组是先存储左子树的节点，再存储右子树的节点，最后存储根节点，如果按每次选择「后序遍历的最后一个节点」为根节点，则先被构造出来的应该为右子树。 返回根节点 root 代码 class Solution { int post_idx; int[] postorder; int[] inorder; Map\u003cInteger, Integer\u003e idx_map = new HashMap\u003cInteger, Integer\u003e(); public TreeNode helper(int in_left, int in_right) { // 如果这里没有节点构造二叉树了，就结束 if (in_left \u003e in_right) { return null; } // 选择 post_idx 位置的元素作为当前子树根节点 int root_val = postorder[post_idx]; TreeNode root = new TreeNode(root_val); // 根据 root 所在位置分成左右两棵子树 int index = idx_map.get(root_val); // 下标减一 post_idx--; // 构造右子树 root.right = helper(index + 1, in_right); // 构造左子树 root.left = helper(in_left, index - 1); return root; } public TreeNode buildTree(int[] inorder, int[] postorder) { this.postorder = postorder; this.inorder = inorder; // 从后序遍历的最后一个元素开始 post_idx = postorder.length - 1; // 建立（元素，下标）键值对的哈希表 int idx = 0; for (Integer val : inorder) { idx_map.put(val, idx++); } return helper(0, inorder.length - 1); } } // 或者 复杂度分析 时间复杂度：O(n)，其中 n 是树中的节点个数。 空间复杂度：O(n)。我们需要使用 O(n) 的空间存储哈希表，以及 O(h)（其中 h 是树的高度）的空间表示递归时栈空间。这里 h \u003c n，所以总空间复杂度为 O(n)。 方法二：迭代 思路 迭代法是一种非常巧妙的实现方法。迭代法的实现基于以下两点发现。 如果将中序遍历反序，则得到反向的中序遍历，即每次遍历右孩子，再遍历根节点，最后遍历左孩子。 如果将后序遍历反序，则得到反向的前序遍历，即每次遍历根节点，再遍历右孩子，最后遍历左孩子。 「反向」的意思是交换遍历左孩子和右孩子的顺序，即反向的遍历中，右孩子在左孩子之前被遍历。 因此可以使用和「105. 从前序与中序遍历序列构造二叉树」的迭代方法类似的方法构造二叉树。 对于后序遍历中的任意两个连续节点 u 和 v（在后序遍历中，u 在 v 的前面），根据后序遍历的流程，我们可以知道 u 和 v 只有两种可能的关系： u 是 v 的右儿子。这是因为在遍历到 u 之后，下一个遍历的节点就是 u 的双亲节点，即 v； v 没有右儿子，并且 u 是 v 的某个祖先节点（或者 v 本身）的左儿子。如果 v 没有右儿子，那么上一个遍历的节点就是 v 的左儿子。如果 v 没有左儿子，则从 v 开始向上遍历 v 的祖先节点，直到遇到一个有左儿子（且 v 不在它的左儿子的子树中）的节点 v_a，那么 u 就是 v_a 的左儿子。 第二种关系看上去有些复杂。我们举一个例子来说明其正确性，并在例子中给出我们的迭代算法。 例子 我们以树 3 / \\ 9 20 / \\ \\ 15 10 7 / \\ 5 8 \\ 4 为例，它的中序遍历和后序遍历分别为 inorder = [15, 9, 10, 3, 20, 5, 7, 8, 4] postorder = [15, 10, 9, 5, 4, 8, 7, 20, 3] 我们用一个栈 stack 来维护「当前节点的所有还没有考虑过左儿子的祖先节点」，栈顶就是当前节点。也就是说，只有在栈中的节点才可能连接一个新的左儿子。同时，我们用一个指针 index 指向中序遍历的某个位置，初始值为 n - 1，其中 n 为数组的长度。index 对应的节点是「当前节点不断往右走达到的最终节点」，这也是符合反向中序遍历的，它的作用在下面的过程中会有所体现。 首先我们将根节点 3 入栈，再初始化 index 所指向的节点为 4，随后对于后序遍历中的每个节点，我们依次判断它是栈顶节点的右儿子，还是栈中某个节点的左儿子。 我们遍历 20。20 一定是栈顶节点 3 的右儿子。我们使用反证法，假设 20 是 3 的左儿子，因为 20 和 3 中间不存在其他的节点，那么 3 没有右儿子，index 应该恰好指向 3，但实际上为 4，因此产生了矛盾。所以我们将 20 作为 3 的右儿子，并将 20 入栈。 stack = [3, 20] index -\u003e inorder[8] = 4 我们遍历 7，8 和 4。同理可得它们都是上一个节点（栈顶节点）的右儿子，所以它们会依次入栈。 stack = [3, 20, 7, 8, 4] index -\u003e inorder[8] = 4 我们遍历 5，这时情况就不一样了。我们发现 index 恰好指向当前的栈顶节点 4，也就是说 4 没有右儿子，那么 5 必须为栈中某个节点的左儿子。那么如何找到这个节点呢？栈中的节点的顺序和它们在反向前序遍历中出现的顺序是一致的，而且每一个节点的左儿子都还没有被遍历过，那么这些节点的顺序和它们在反向中序遍历中出现的顺序一定是相反的。 这是因为栈中的任意两个相邻的节点，前者都是后者的某个祖先。并且我们知道，栈中的任意一个节点的左儿子还没有被遍历过，说明后者一定是前者右儿子的子树中的节点，那么后者就先于前者出现在反向中序遍历中。 因此我们可以把 index 不断向左移动，并与栈顶节点进行比较。如果 index 对应的元素恰好等于栈顶节点，那么说明我们在反向中序遍历中找到了栈顶节点，所以将 index 减少 1 并弹出栈顶节点，直到 index 对应的元素不等于栈顶节点。按照这样的过程，我们弹出的最后一个节点 x 就是 5 的双亲节点，这是因为 5 出现在了 x 与 x 在栈中的下一个节点的反向中序遍历之间，因此 5 就是 x 的左儿子。 回到我们的例子，我们会依次从栈顶弹出 4，8 和 7，并且将 index 向左移动了三次。我们将 5 作为最后弹出的节点 7 的左儿子，并将 5 入栈。 stack = [3, 20, 5] index -\u003e inorder[5] = 5 我们遍历 9。同理，index 恰好指向当前栈顶节点 5，那么我们会依次从栈顶弹出 5，20 和 3，并且将 index 向左移动了三次。我们将 9 作为最后弹出的节点 3 的左儿子，并将 9 入栈。 ","date":"2023-01-08","objectID":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/:42:0","tags":["leetcode"],"title":"Leetcode每日一题","uri":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/"},{"categories":["数据结构与算法"],"content":"day43 145. 二叉树的后序遍历 题目 给你一棵二叉树的根节点 root ，返回其节点值的 后序遍历 。 示例 1： 输入：root = [1,null,2,3] 输出：[3,2,1] 示例 2： 输入：root = [] 输出：[] 示例 3： 输入：root = [1] 输出：[1] 提示： 树中节点的数目在范围 [0, 100] 内 -100 \u003c= Node.val \u003c= 100 **进阶：**递归算法很简单，你可以通过迭代算法完成吗？ 解法 方法一：递归 思路与算法 首先我们需要了解什么是二叉树的后序遍历：按照访问左子树——右子树——根节点的方式遍历这棵树，而在访问左子树或者右子树的时候，我们按照同样的方式遍历，直到遍历完整棵树。因此整个遍历过程天然具有递归的性质，我们可以直接用递归函数来模拟这一过程。 定义 postorder(root) 表示当前遍历到 root 节点的答案。按照定义，我们只要递归调用 postorder(root-\u003eleft) 来遍历 root 节点的左子树，然后递归调用 postorder(root-\u003eright) 来遍历 root 节点的右子树，最后将 root 节点的值加入答案即可，递归终止的条件为碰到空节点。 代码 class Solution { public List\u003cInteger\u003e postorderTraversal(TreeNode root) { List\u003cInteger\u003e res = new ArrayList\u003cInteger\u003e(); postorder(root, res); return res; } public void postorder(TreeNode root, List\u003cInteger\u003e res) { if (root == null) { return; } postorder(root.left, res); postorder(root.right, res); res.add(root.val); } } 复杂度分析 时间复杂度：O(n)，其中 n 是二叉搜索树的节点数。每一个节点恰好被遍历一次。 空间复杂度：O(n)，为递归过程中栈的开销，平均情况下为 O(log n)，最坏情况下树呈现链状，为 O(n)。 方法二：迭代 思路与算法 我们也可以用迭代的方式实现方法一的递归函数，两种方式是等价的，区别在于递归的时候隐式地维护了一个栈，而我们在迭代的时候需要显式地将这个栈模拟出来，其余的实现与细节都相同，具体可以参考下面的代码。 代码 class Solution { public List\u003cInteger\u003e postorderTraversal(TreeNode root) { List\u003cInteger\u003e res = new ArrayList\u003cInteger\u003e(); if (root == null) { return res; } Deque\u003cTreeNode\u003e stack = new LinkedList\u003cTreeNode\u003e(); TreeNode prev = null; while (root != null || !stack.isEmpty()) { while (root != null) { stack.push(root); root = root.left; } root = stack.pop(); if (root.right == null || root.right == prev) { res.add(root.val); prev = root; root = null; } else { stack.push(root); root = root.right; } } return res; } } 复杂度分析 时间复杂度：O(n)，其中 n 是二叉搜索树的节点数。每一个节点恰好被遍历一次。 空间复杂度：O(n)，为迭代过程中显式栈的开销，平均情况下为 O(log n)，最坏情况下树呈现链状，为 O(n)。 方法三：Morris 遍历 思路与算法 有一种巧妙的方法可以在线性时间内，只占用常数空间来实现后序遍历。这种方法由 J. H. Morris 在 1979 年的论文「Traversing Binary Trees Simply and Cheaply」中首次提出，因此被称为 Morris 遍历。 Morris 遍历的核心思想是利用树的大量空闲指针，实现空间开销的极限缩减。其后序遍历规则总结如下： 新建临时节点，令该节点为 root； 如果当前节点的左子节点为空，则遍历当前节点的右子节点； 如果当前节点的左子节点不为空，在当前节点的左子树中找到当前节点在中序遍历下的前驱节点； 如果前驱节点的右子节点为空，将前驱节点的右子节点设置为当前节点，当前节点更新为当前节点的左子节点。 如果前驱节点的右子节点为当前节点，将它的右子节点重新设为空。倒序输出从当前节点的左子节点到该前驱节点这条路径上的所有节点。当前节点更新为当前节点的右子节点。 重复步骤 2 和步骤 3，直到遍历结束。 这样我们利用 Morris 遍历的方法，后序遍历该二叉搜索树，即可实现线性时间与常数空间的遍历。 代码 class Solution { public List\u003cInteger\u003e postorderTraversal(TreeNode root) { List\u003cInteger\u003e res = new ArrayList\u003cInteger\u003e(); if (root == null) { return res; } TreeNode p1 = root, p2 = null; while (p1 != null) { p2 = p1.left; if (p2 != null) { while (p2.right != null \u0026\u0026 p2.right != p1) { p2 = p2.right; } if (p2.right == null) { p2.right = p1; p1 = p1.left; continue; } else { p2.right = null; addPath(res, p1.left); } } p1 = p1.right; } addPath(res, root); return res; } public void addPath(List\u003cInteger\u003e res, TreeNode node) { int count = 0; while (node != null) { ++count; res.add(node.val); node = node.right; } int left = res.size() - count, right = res.size() - 1; while (left \u003c right) { int temp = res.get(left); res.set(left, res.get(right)); res.set(right, temp); left++; right--; } } } 复杂度分析 时间复杂度：O(n)，其中 n 是二叉树的节点数。没有左子树的节点只被访问一次，有左子树的节点被访问两次。 空间复杂度：O(1)。只操作已经存在的指针（树的空闲指针），因此只需要常数的额外空间。 ","date":"2023-01-08","objectID":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/:43:0","tags":["leetcode"],"title":"Leetcode每日一题","uri":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/"},{"categories":["数据结构与算法"],"content":"day44 102. 二叉树的层序遍历 题目 给你二叉树的根节点 root ，返回其节点值的 层序遍历 。 （即逐层地，从左到右访问所有节点）。 示例 1： 输入：root = [3,9,20,null,null,15,7] 输出：[[3],[9,20],[15,7]] 示例 2： 输入：root = [1] 输出：[[1]] 示例 3： 输入：root = [] 输出：[] 提示： 树中节点数目在范围 [0, 2000] 内 -1000 \u003c= Node.val \u003c= 1000 解法 解法一：深度优先 采用递归方法来找出层序序列。思路：我们知道，遍历递归地遍历二叉树是往深处走，我们只需要在往深处走的时候，将相同深度的节点放到一起即可。 class Solution { public List\u003cList\u003cInteger\u003e\u003e levelOrder(TreeNode root) { if (root==null) return new ArrayList\u003c\u003e(); List\u003cList\u003cInteger\u003e\u003e lls =new ArrayList\u003c\u003e(); doSome(root,1,lls); return lls; } // 递归层序遍历的秘诀：深度相同的节点放到同一个list public void doSome(TreeNode node,int depth,List\u003cList\u003cInteger\u003e\u003e lls){ // 当前深度下没有对应的“层list”，就创建 if(lls.size()\u003cdepth) { lls.add(new ArrayList\u003cInteger\u003e()); } //在同一深度，把节点加上去 lls.get(depth-1).add(node.val); if(node.left!=null) doSome(node.left,depth+1,lls); if(node.right!=null) doSome(node.right,depth+1,lls); } } /** * Definition for a binary tree node. * type TreeNode struct { * Val int * Left *TreeNode * Right *TreeNode * } */ func levelOrder(root *TreeNode) [][]int { ans := [][]int{} if root==nil { return ans } var dfs func(root *TreeNode,depth int) dfs = func(root *TreeNode,depth int){ if root==nil{ return } if len(ans)\u003cdepth+1{ ans=append(ans,[]int{}) } ans[depth]=append(ans[depth],root.Val) dfs(root.Left,depth+1) dfs(root.Right,depth+1) return } dfs(root,0) return ans } 解法二：广度优先搜索 解题思路 本文将会讲解为什么这道题适合用广度优先搜索（BFS），以及 BFS 适用于什么样的场景。 DFS（深度优先搜索）和 BFS（广度优先搜索）就像孪生兄弟，提到一个总是想起另一个。然而在实际使用中，我们用 DFS 的时候远远多于 BFS。那么，是不是 BFS 就没有什么用呢？ 如果我们使用 DFS/BFS 只是为了遍历一棵树、一张图上的所有结点的话，那么 DFS 和 BFS 的能力没什么差别，我们当然更倾向于更方便写、空间复杂度更低的 DFS 遍历。不过，某些使用场景是 DFS 做不到的，只能使用 BFS 遍历。这就是本文要介绍的两个场景：「层序遍历」、「最短路径」。 本文包括以下内容： DFS 与 BFS 的特点比较 BFS 的适用场景 如何用 BFS 进行层序遍历 如何用 BFS 求解最短路径问题 DFS 与 BFS 让我们先看看在二叉树上进行 DFS 遍历和 BFS 遍历的代码比较。 DFS 遍历使用递归： void dfs(TreeNode root) { if (root == null) { return; } dfs(root.left); dfs(root.right); } BFS 遍历使用队列数据结构： void bfs(TreeNode root) { Queue\u003cTreeNode\u003e queue = new ArrayDeque\u003c\u003e(); queue.add(root); while (!queue.isEmpty()) { TreeNode node = queue.poll(); // Java 的 pop 写作 poll() if (node.left != null) { queue.add(node.left); } if (node.right != null) { queue.add(node.right); } } } 只是比较两段代码的话，最直观的感受就是：DFS 遍历的代码比 BFS 简洁太多了！这是因为递归的方式隐含地使用了系统的 栈，我们不需要自己维护一个数据结构。如果只是简单地将二叉树遍历一遍，那么 DFS 显然是更方便的选择。 虽然 DFS 与 BFS 都是将二叉树的所有结点遍历了一遍，但它们遍历结点的顺序不同。 这个遍历顺序也是 BFS 能够用来解「层序遍历」、「最短路径」问题的根本原因。下面，我们结合几道例题来讲讲 BFS 是如何求解层序遍历和最短路径问题的。 BFS 的应用一：层序遍历 BFS 的层序遍历应用就是本题了： LeetCode 102. Binary Tree Level Order Traversal 二叉树的层序遍历（Medium） 给定一个二叉树，返回其按层序遍历得到的节点值。层序遍历即逐层地、从左到右访问所有结点。 什么是层序遍历呢？简单来说，层序遍历就是把二叉树分层，然后每一层从左到右遍历： 乍一看来，这个遍历顺序和 BFS 是一样的，我们可以直接用 BFS 得出层序遍历结果。然而，层序遍历要求的输入结果和 BFS 是不同的。层序遍历要求我们区分每一层，也就是返回一个二维数组。而 BFS 的遍历结果是一个一维数组，无法区分每一层。 那么，怎么给 BFS 遍历的结果分层呢？我们首先来观察一下 BFS 遍历的过程中，结点进队列和出队列的过程： 截取 BFS 遍历过程中的某个时刻： 可以看到，此时队列中的结点是 3、4、5，分别来自第 1 层和第 2 层。这个时候，第 1 层的结点还没出完，第 2 层的结点就进来了，而且两层的结点在队列中紧挨在一起，我们无法区分队列中的结点来自哪一层。 因此，我们需要稍微修改一下代码，在每一层遍历开始前，先记录队列中的结点数量 n（也就是这一层的结点数量），然后一口气处理完这一层的 n 个结点。 // 二叉树的层序遍历 void bfs(TreeNode root) { Queue\u003cTreeNode\u003e queue = new ArrayDeque\u003c\u003e(); queue.add(root); while (!queue.isEmpty()) { int n = queue.size(); for (int i = 0; i \u003c n; i++) { // 变量 i 无实际意义，只是为了循环 n 次 TreeNode node = queue.poll(); if (node.left != null) { queue.add(node.left); } if (node.right != null) { queue.add(node.right); } } } } 这样，我们就将 BFS 遍历改造成了层序遍历。在遍历的过程中，结点进队列和出队列的过程为： 可以看到，在 while 循环的每一轮中，都是将当前层的所有结点出队列，再将下一层的所有结点入队列，这样就实现了层序遍历。 最终我们得到的题解代码为： public List\u003cList\u003cInteger\u003e\u003e levelOrder(TreeNode root) { List\u003cList\u003cInteger\u003e\u003e res = new ArrayList\u003c\u003e(); Queue\u003cTreeNode\u003e queue = new ArrayDeque\u003c\u003e(); if (root != null) { queue.add(root); } while (!queue.isEmpty()) { int n = queue.size(); List\u003cInteger\u003e level = new ArrayList\u003c\u003e(); for (int i = 0; i \u003c n; i++) { TreeNode node = queue.poll(); level.add(node.val); if (node.left != null) { queue.add(node.left); } if (node.right != null) { queue.add(node.right); } } res.add(level); } return res; } /** * Definition for a binary tree node. * type TreeNode struct { * Val int * Left *TreeNode * Right *TreeNode * } */ func levelOrder","date":"2023-01-08","objectID":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/:44:0","tags":["leetcode"],"title":"Leetcode每日一题","uri":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/"},{"categories":["数据结构与算法"],"content":"day45 107. 二叉树的层序遍历 II 题目 给你二叉树的根节点 root ，返回其节点值 自底向上的层序遍历 。 （即按从叶子节点所在层到根节点所在的层，逐层从左向右遍历） 示例 1： 输入：root = [3,9,20,null,null,15,7] 输出：[[15,7],[9,20],[3]] 示例 2： 输入：root = [1] 输出：[[1]] 示例 3： 输入：root = [] 输出：[] 提示： 树中节点数目在范围 [0, 2000] 内 -1000 \u003c= Node.val \u003c= 1000 解法 解法一：无脑广度优先 在前面一题的思路上，转换一下：只需要在添加当层遍历list时，插入头部即可。 题解 解法二：不优雅的深度优先遍历 将上题设深度优先产生的序列反序即可。 class Solution { void level(TreeNode root, int index, List\u003cList\u003cInteger\u003e\u003e res) { // 当前行对应的列表不存在，加一个空列表 if(res.size() \u003c index) { res.add(new ArrayList\u003cInteger\u003e()); } // 将当前节点的值加入当前行的 res 中 res.get(index-1).add(root.val); // 递归处理左子树 if(root.left != null) { level(root.left, index+1, res); } // 递归处理右子树 if(root.right != null) { level(root.right, index+1, res); } } public List\u003cList\u003cInteger\u003e\u003e levelOrderBottom(TreeNode root) { if(root == null) { return new ArrayList\u003cList\u003cInteger\u003e\u003e(); } List\u003cList\u003cInteger\u003e\u003e list = new ArrayList\u003cList\u003cInteger\u003e\u003e(); List\u003cList\u003cInteger\u003e\u003e res = new ArrayList\u003cList\u003cInteger\u003e\u003e(); level(root, 1, list); for(int i = list.size()-1; i \u003e= 0; i--){ res.add(list.get(i)); } return res; } } ","date":"2023-01-08","objectID":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/:45:0","tags":["leetcode"],"title":"Leetcode每日一题","uri":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/"},{"categories":["数据结构与算法"],"content":"day46 199. 二叉树的右视图 题目 给定一个二叉树的 根节点 root，想象自己站在它的右侧，按照从顶部到底部的顺序，返回从右侧所能看到的节点值。 示例 1: 输入: [1,2,3,null,5,null,4] 输出: [1,3,4] 示例 2: 输入: [1,null,3] 输出: [1,3] 示例 3: 输入: [] 输出: [] 提示: 二叉树的节点个数的范围是 [0,100] -100 \u003c= Node.val \u003c= 100 解法 BFS 思路： 利用 BFS 进行层次遍历，记录下每层的最后一个元素。 时间复杂度： O(N)，每个节点都入队出队了 1 次。 空间复杂度： O(N)，使用了额外的队列空间。 class Solution { public List\u003cInteger\u003e rightSideView(TreeNode root) { if(root == null) return new ArrayList\u003cInteger\u003e(); Queue\u003cTreeNode\u003e queue = new LinkedList\u003c\u003e(); List\u003cInteger\u003e ls =new ArrayList\u003c\u003e(); queue.offer(root); while(!queue.isEmpty()){ int size = queue.size(); while(size\u003e0){ TreeNode node = queue.poll(); if(size==1) ls.add(node.val);// 层序遍历是最右边的节点最后被遍历 if(node.left!=null) queue.offer(node.left); if(node.right!=null) queue.offer(node.right); size--; } } return ls; } } DFS （时间100%） 思路： 我们按照 「根结点 -\u003e 右子树 -\u003e 左子树」 的顺序访问，就可以保证每层都是最先访问最右边的节点的。 （与先序遍历 「根结点 -\u003e 左子树 -\u003e 右子树」 正好相反，先序遍历每层最先访问的是最左边的节点） 时间复杂度： O(N)，每个节点都访问了 1 次。 空间复杂度： O(N)，因为这不是一棵平衡二叉树，二叉树的深度最少是 logN, 最坏的情况下会退化成一条链表，深度就是 N，因此递归时使用的栈空间是 O(N) 的。 class Solution { List\u003cInteger\u003e res = new ArrayList\u003c\u003e(); public List\u003cInteger\u003e rightSideView(TreeNode root) { dfs(root, 0); // 从根节点开始访问，根节点深度是0 return res; } private void dfs(TreeNode root, int depth) { if (root == null) { return; } // 先访问 当前节点，再递归地访问 右子树 和 左子树。 if (depth == res.size()) { // 如果当前节点所在深度还没有出现在res里，说明在该深度下当前节点是第一个被访问的节点，因此将当前节点加入res中。 res.add(root.val); } depth++; dfs(root.right, depth); dfs(root.left, depth); } } ","date":"2023-01-08","objectID":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/:46:0","tags":["leetcode"],"title":"Leetcode每日一题","uri":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/"},{"categories":["数据结构与算法"],"content":"day47 429. N 叉树的层序遍历 题目 给定一个 N 叉树，返回其节点值的层序遍历。（即从左到右，逐层遍历）。 树的序列化输入是用层序遍历，每组子节点都由 null 值分隔（参见示例）。 示例 1： 输入：root = [1,null,3,2,4,null,5,6] 输出：[[1],[3,2,4],[5,6]] 示例 2： 输入：root = [1,null,2,3,4,5,null,null,6,7,null,8,null,9,10,null,null,11,null,12,null,13,null,null,14] 输出：[[1],[2,3,4,5],[6,7,8,9,10],[11,12,13],[14]] 提示： 树的高度不会超过 1000 树的节点总数在 [0, 10^4] 之间 解法 方法一：广度优先搜索 思路与算法 对于「层序遍历」的题目，我们一般使用广度优先搜索。在广度优先搜索的每一轮中，我们会遍历同一层的所有节点。 具体地，我们首先把根节点 root 放入队列中，随后在广度优先搜索的每一轮中，我们首先记录下当前队列中包含的节点个数（记为 cnt），即表示上一层的节点个数。在这之后，我们从队列中依次取出节点，直到取出了上一层的全部 cnt 个节点为止。当取出节点 cur 时，我们将 cur 的值放入一个临时列表，再将 cur 的所有子节点全部放入队列中。 当这一轮遍历完成后，临时列表中就存放了当前层所有节点的值。这样一来，当整个广度优先搜索完成后，我们就可以得到层序遍历的结果。 细节 需要特殊判断树为空的情况。 代码 class Solution { public List\u003cList\u003cInteger\u003e\u003e levelOrder(Node root) { if (root == null) return new ArrayList\u003c\u003e(); List\u003cList\u003cInteger\u003e\u003e lls = new ArrayList\u003c\u003e(); lls.add(new ArrayList\u003c\u003e()); lls.get(0).add(root.val); Queue\u003cNode\u003e queue = new LinkedList\u003c\u003e(root.children); while (!queue.isEmpty()) { int size = queue.size(); List\u003cInteger\u003e ls = new ArrayList\u003c\u003e(); while (size \u003e 0) { Node node = queue.poll(); ls.add(node.val); if (node.children != null) queue.addAll(node.children); size--; } lls.add(ls); } return lls; } } 复杂度分析 时间复杂度：O(n)，其中 n 是树中包含的节点个数。在广度优先搜索的过程中，我们需要遍历每一个节点恰好一次。 空间复杂度：O(n)，即为队列需要使用的空间。在最坏的情况下，树只有两层，且最后一层有 n-1 个节点，此时就需要 O(n) 的空间。 ","date":"2023-01-08","objectID":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/:47:0","tags":["leetcode"],"title":"Leetcode每日一题","uri":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/"},{"categories":["数据结构与算法"],"content":"day48 637. 二叉树的层平均值 题目 给定一个非空二叉树的根节点 root , 以数组的形式返回每一层节点的平均值。与实际答案相差 10-5 以内的答案可以被接受。 示例 1： 输入：root = [3,9,20,null,null,15,7] 输出：[3.00000,14.50000,11.00000] 解释：第 0 层的平均值为 3,第 1 层的平均值为 14.5,第 2 层的平均值为 11 。 因此返回 [3, 14.5, 11] 。 示例 2: 输入：root = [3,9,20,15,7] 输出：[3.00000,14.50000,11.00000] 提示： 树中节点数量在 [1, 104] 范围内 -231 \u003c= Node.val \u003c= 231 - 1 解法 方法一：深度优先搜索 使用深度优先搜索计算二叉树的层平均值，需要维护两个数组，counts 用于存储二叉树的每一层的节点数，sums 用于存储二叉树的每一层的节点值之和。搜索过程中需要记录当前节点所在层，如果访问到的节点在第 i 层，则将 counts[i] 的值加 1，并将该节点的值加到 sums[i]。 遍历结束之后，第 i 层的平均值即为 sums[i]/counts[i]。 class Solution { public List\u003cDouble\u003e averageOfLevels(TreeNode root) { List\u003cInteger\u003e counts = new ArrayList\u003cInteger\u003e(); List\u003cDouble\u003e sums = new ArrayList\u003cDouble\u003e(); dfs(root, 0, counts, sums); List\u003cDouble\u003e averages = new ArrayList\u003cDouble\u003e(); int size = sums.size(); for (int i = 0; i \u003c size; i++) { averages.add(sums.get(i) / counts.get(i)); } return averages; } public void dfs(TreeNode root, int level, List\u003cInteger\u003e counts, List\u003cDouble\u003e sums) { if (root == null) { return; } if (level \u003c sums.size()) { sums.set(level, sums.get(level) + root.val); counts.set(level, counts.get(level) + 1); } else { sums.add(1.0 * root.val); counts.add(1); } dfs(root.left, level + 1, counts, sums); dfs(root.right, level + 1, counts, sums); } } 复杂度分析 时间复杂度：O(n)，其中 n 是二叉树中的节点个数。 深度优先搜索需要对每个节点访问一次，对于每个节点，维护两个数组的时间复杂度都是 O(1)，因此深度优先搜索的时间复杂度是 O(n)。 遍历结束之后计算每层的平均值的时间复杂度是 O(h)，其中 h 是二叉树的高度，任何情况下都满足 h≤n。 因此总时间复杂度是 O(n)。 空间复杂度：O(n)，其中 n 是二叉树中的节点个数。空间复杂度取决于两个数组的大小和递归调用的层数，两个数组的大小都等于二叉树的高度，递归调用的层数不会超过二叉树的高度，最坏情况下，二叉树的高度等于节点个数。 方法二：广度优先搜索 也可以使用广度优先搜索计算二叉树的层平均值。从根节点开始搜索，每一轮遍历同一层的全部节点，计算该层的节点数以及该层的节点值之和，然后计算该层的平均值。 如何确保每一轮遍历的是同一层的全部节点呢？我们可以借鉴层次遍历的做法，广度优先搜索使用队列存储待访问节点，只要确保在每一轮遍历时，队列中的节点是同一层的全部节点即可。具体做法如下： 初始时，将根节点加入队列； 每一轮遍历时，将队列中的节点全部取出，计算这些节点的数量以及它们的节点值之和，并计算这些节点的平均值，然后将这些节点的全部非空子节点加入队列，重复上述操作直到队列为空，遍历结束。 由于初始时队列中只有根节点，满足队列中的节点是同一层的全部节点，每一轮遍历时都会将队列中的当前层节点全部取出，并将下一层的全部节点加入队列，因此可以确保每一轮遍历的是同一层的全部节点。 具体实现方面，可以在每一轮遍历之前获得队列中的节点数量 size，遍历时只遍历 size 个节点，即可满足每一轮遍历的是同一层的全部节点。 class Solution { public List\u003cDouble\u003e averageOfLevels(TreeNode root) { List\u003cDouble\u003e averages = new ArrayList\u003cDouble\u003e(); Queue\u003cTreeNode\u003e queue = new LinkedList\u003cTreeNode\u003e(); queue.offer(root); while (!queue.isEmpty()) { double sum = 0; int size = queue.size(); for (int i = 0; i \u003c size; i++) { TreeNode node = queue.poll(); sum += node.val; TreeNode left = node.left, right = node.right; if (left != null) { queue.offer(left); } if (right != null) { queue.offer(right); } } averages.add(sum / size); } return averages; } } 复杂度分析 时间复杂度：O(n)，其中 n 是二叉树中的节点个数。 广度优先搜索需要对每个节点访问一次，时间复杂度是 O(n)。 需要对二叉树的每一层计算平均值，时间复杂度是 O(h)，其中 h 是二叉树的高度，任何情况下都满足 h≤n。 因此总时间复杂度是 O(n)。 空间复杂度：O(n)，其中 n 是二叉树中的节点个数。空间复杂度取决于队列开销，队列中的节点个数不会超过 n。 ","date":"2023-01-08","objectID":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/:48:0","tags":["leetcode"],"title":"Leetcode每日一题","uri":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/"},{"categories":["数据结构与算法"],"content":"day49 98. 验证二叉搜索树 题目 给你一个二叉树的根节点 root ，判断其是否是一个有效的二叉搜索树。 有效 二叉搜索树定义如下： 节点的左子树只包含 小于 当前节点的数。 节点的右子树只包含 大于 当前节点的数。 所有左子树和右子树自身必须也是二叉搜索树。 示例 1： 输入：root = [2,1,3] 输出：true 示例 2： 输入：root = [5,1,4,null,null,3,6] 输出：false 解释：根节点的值是 5 ，但是右子节点的值是 4 。 提示： 树中节点数目范围在[1, 104] 内 -2^31^ \u003c= Node.val \u003c= 2^31^ - 1 解法 方法一: 递归 思路和算法 要解决这道题首先我们要了解二叉搜索树有什么性质可以给我们利用，由题目给出的信息我们可以知道：如果该二叉树的左子树不为空，则左子树上所有节点的值均小于它的根节点的值； 若它的右子树不空，则右子树上所有节点的值均大于它的根节点的值；它的左右子树也为二叉搜索树。 这启示我们设计一个递归函数 helper(root, lower, upper) 来递归判断，函数表示考虑以 root 为根的子树，判断子树中所有节点的值是否都在 (l,r) 的范围内（注意是开区间）。如果 root 节点的值 val 不在 (l,r) 的范围内说明不满足条件直接返回，否则我们要继续递归调用检查它的左右子树是否满足，如果都满足才说明这是一棵二叉搜索树。 那么根据二叉搜索树的性质，在递归调用左子树时，我们需要把上界 upper 改为 root.val，即调用 helper(root.left, lower, root.val)，因为左子树里所有节点的值均小于它的根节点的值。同理递归调用右子树时，我们需要把下界 lower 改为 root.val，即调用 helper(root.right, root.val, upper)。 函数递归调用的入口为 helper(root, -inf, +inf)， inf 表示一个无穷大的值。 下图展示了算法如何应用在示例 2 上： class Solution { public boolean isValidBST(TreeNode root) { return isValidBST(root, Long.MIN_VALUE, Long.MAX_VALUE); } // 主要是递归过程中传递了左右子节点的值用于比较 public boolean isValidBST(TreeNode node, long lower, long upper) { if (node == null) { return true; } if (node.val \u003c= lower || node.val \u003e= upper) { return false; } return isValidBST(node.left, lower, node.val) \u0026\u0026 isValidBST(node.right, node.val, upper); } } 复杂度分析 时间复杂度：O(n)，其中 n 为二叉树的节点个数。在递归调用的时候二叉树的每个节点最多被访问一次，因此时间复杂度为 O(n)。 空间复杂度：O(n)，其中 n 为二叉树的节点个数。递归函数在递归过程中需要为每一层递归函数分配栈空间，所以这里需要额外的空间且该空间取决于递归的深度，即二叉树的高度。最坏情况下二叉树为一条链，树的高度为 n ，递归最深达到 n 层，故最坏情况下空间复杂度为 O(n) 。 方法二：中序遍历+类似于单调栈 思路和算法 基于方法一中提及的性质，我们可以进一步知道二叉搜索树「中序遍历」得到的值构成的序列一定是升序的，这启示我们在中序遍历的时候实时检查当前节点的值是否大于前一个中序遍历到的节点的值即可。如果均大于说明这个序列是升序的，整棵树是二叉搜索树，否则不是，下面的代码我们使用栈来模拟中序遍历的过程。 可能有读者不知道中序遍历是什么，我们这里简单提及。中序遍历是二叉树的一种遍历方式，它先遍历左子树，再遍历根节点，最后遍历右子树。而我们二叉搜索树保证了左子树的节点的值均小于根节点的值，根节点的值均小于右子树的值，因此中序遍历以后得到的序列一定是升序序列。 class Solution { public boolean isValidBST(TreeNode root) { Deque\u003cTreeNode\u003e stack = new LinkedList\u003cTreeNode\u003e(); double inorder = -Double.MAX_VALUE; while (!stack.isEmpty() || root != null) { while (root != null) { stack.push(root); root = root.left; } root = stack.pop(); // 如果中序遍历得到的节点的值小于等于前一个 inorder，说明不是二叉搜索树 if (root.val \u003c= inorder) { return false; } inorder = root.val; root = root.right; } return true; } } 复杂度分析 时间复杂度：O(n)，其中 n 为二叉树的节点个数。二叉树的每个节点最多被访问一次，因此时间复杂度为 O(n) 。 空间复杂度：O(n)，其中 n 为二叉树的节点个数。栈最多存储 n 个节点，因此需要额外的 O(n) 的空间。 ","date":"2023-01-08","objectID":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/:49:0","tags":["leetcode"],"title":"Leetcode每日一题","uri":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/"},{"categories":["数据结构与算法"],"content":"day50 108. 将有序数组转换为二叉搜索树 题目 给你一个整数数组 nums ，其中元素已经按 升序 排列，请你将其转换为一棵 高度平衡 二叉搜索树。 高度平衡 二叉树是一棵满足「每个节点的左右两个子树的高度差的绝对值不超过 1 」的二叉树。 示例 1： 输入：nums = [-10,-3,0,5,9] 输出：[0,-3,9,-10,null,5] 解释：[0,-10,5,null,-3,null,9] 也将被视为正确答案： 示例 2： 输入：nums = [1,3] 输出：[3,1] 解释：[1,null,3] 和 [3,1] 都是高度平衡二叉搜索树。 提示： 1 \u003c= nums.length \u003c= 104 -104 \u003c= nums[i] \u003c= 104 nums 按 严格递增 顺序排列 解法 前言 二叉搜索树的中序遍历是升序序列，题目给定的数组是按照升序排序的有序数组，因此可以确保数组是二叉搜索树的中序遍历序列。 **给定二叉搜索树的中序遍历，是否可以唯一地确定二叉搜索树？答案是否定的。**如果没有要求二叉搜索树的高度平衡，则任何一个数字都可以作为二叉搜索树的根节点，因此可能的二叉搜索树有多个。 如果增加一个限制条件，即要求二叉搜索树的高度平衡，是否可以唯一地确定二叉搜索树？答案仍然是否定的。 直观地看，我们可以选择中间数字作为二叉搜索树的根节点，这样分给左右子树的数字个数相同或只相差 11，可以使得树保持平衡。如果数组长度是奇数，则根节点的选择是唯一的，如果数组长度是偶数，则可以选择中间位置左边的数字作为根节点或者选择中间位置右边的数字作为根节点，选择不同的数字作为根节点则创建的平衡二叉搜索树也是不同的。 确定平衡二叉搜索树的根节点之后，其余的数字分别位于平衡二叉搜索树的左子树和右子树中，左子树和右子树分别也是平衡二叉搜索树，因此可以通过递归的方式创建平衡二叉搜索树。 当然，这只是我们直观的想法，为什么这么建树一定能保证是「平衡」的呢？这里可以参考「1382. 将二叉搜索树变平衡」，这两道题的构造方法完全相同，这种方法是正确的，1382 题解中给出了这个方法的正确性证明：1382 官方题解，感兴趣的同学可以戳进去参考。 递归的基准情形是平衡二叉搜索树不包含任何数字，此时平衡二叉搜索树为空。 在给定中序遍历序列数组的情况下，每一个子树中的数字在数组中一定是连续的，因此可以通过数组下标范围确定子树包含的数字，下标范围记为 [left,right]。对于整个中序遍历序列，下标范围从 left=0 到 right*=nums.length−1。当 left\u003e*right 时，平衡二叉搜索树为空。 以下三种方法中，方法一总是选择中间位置左边的数字作为根节点，方法二总是选择中间位置右边的数字作为根节点，方法三是方法一和方法二的结合，选择任意一个中间位置数字作为根节点。 方法一：中序遍历，总是选择中间位置左边的数字作为根节点 选择中间位置左边的数字作为根节点，则根节点的下标为 mid*=(*left+right)/2，此处的除法为整数除法。 class Solution { public TreeNode sortedArrayToBST(int[] nums) { return helper(nums, 0, nums.length - 1); } public TreeNode helper(int[] nums, int left, int right) { if (left \u003e right) { return null; } // 总是选择中间位置左边的数字作为根节点 int mid = (left + right) / 2; TreeNode root = new TreeNode(nums[mid]); root.left = helper(nums, left, mid - 1); root.right = helper(nums, mid + 1, right); return root; } } 复杂度分析 时间复杂度：O(n)，其中 n 是数组的长度。每个数字只访问一次。 空间复杂度：O(log n)，其中 n 是数组的长度。空间复杂度不考虑返回值，因此空间复杂度主要取决于递归栈的深度，递归栈的深度是 O(log n)。 方法二：中序遍历，总是选择中间位置右边的数字作为根节点 选择中间位置右边的数字作为根节点，则根节点的下标为 mid=(left+right+1)/2，此处的除法为整数除法。 class Solution { public TreeNode sortedArrayToBST(int[] nums) { return helper(nums, 0, nums.length - 1); } public TreeNode helper(int[] nums, int left, int right) { if (left \u003e right) { return null; } // 总是选择中间位置右边的数字作为根节点 int mid = (left + right + 1) / 2; TreeNode root = new TreeNode(nums[mid]); root.left = helper(nums, left, mid - 1); root.right = helper(nums, mid + 1, right); return root; } } 复杂度分析 时间复杂度：O(n)，其中 n 是数组的长度。每个数字只访问一次。 空间复杂度：O(log n)，其中 nn 是数组的长度。空间复杂度不考虑返回值，因此空间复杂度主要取决于递归栈的深度，递归栈的深度是 O(log n)。 方法三：中序遍历，选择任意一个中间位置数字作为根节点 选择任意一个中间位置数字作为根节点，则根节点的下标为 mid=(left+right)/2 和 mid=(left+right+1)/2 两者中随机选择一个，此处的除法为整数除法。 class Solution { Random rand = new Random(); public TreeNode sortedArrayToBST(int[] nums) { return helper(nums, 0, nums.length - 1); } public TreeNode helper(int[] nums, int left, int right) { if (left \u003e right) { return null; } // 选择任意一个中间位置数字作为根节点 int mid = (left + right + rand.nextInt(2)) / 2; TreeNode root = new TreeNode(nums[mid]); root.left = helper(nums, left, mid - 1); root.right = helper(nums, mid + 1, right); return root; } } 复杂度分析 时间复杂度：O(n)，其中 n 是数组的长度。每个数字只访问一次。 空间复杂度：O(log n)，其中 n 是数组的长度。空间复杂度不考虑返回值，因此空间复杂度主要取决于递归栈的深度，递归栈的深度是 O(log n)。 ","date":"2023-01-08","objectID":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/:50:0","tags":["leetcode"],"title":"Leetcode每日一题","uri":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/"},{"categories":["数据结构与算法"],"content":"day51 235. 二叉搜索树的最近公共祖先 题目 给定一个二叉搜索树, 找到该树中两个指定节点的最近公共祖先。 百度百科中最近公共祖先的定义为：“对于有根树 T 的两个结点 p、q，最近公共祖先表示为一个结点 x，满足 x 是 p、q 的祖先且 x 的深度尽可能大（一个节点也可以是它自己的祖先）。” 例如，给定如下二叉搜索树: root = [6,2,8,0,4,7,9,null,null,3,5] 示例 1: 输入: root = [6,2,8,0,4,7,9,null,null,3,5], p = 2, q = 8 输出: 6 解释: 节点 2 和节点 8 的最近公共祖先是 6。 示例 2: 输入: root = [6,2,8,0,4,7,9,null,null,3,5], p = 2, q = 4 输出: 2 解释: 节点 2 和节点 4 的最近公共祖先是 2, 因为根据定义最近公共祖先节点可以为节点本身。 说明: 所有节点的值都是唯一的。 p、q 为不同节点且均存在于给定的二叉搜索树中。 解法 方法一：两次遍历 思路与算法 注意到题目中给出的是一棵「二叉搜索树」，因此我们可以快速地找出树中的某个节点以及从根节点到该节点的路径，例如我们需要找到节点 p： 我们从根节点开始遍历； 如果当前节点就是 p，那么成功地找到了节点； 如果当前节点的值大于 p 的值，说明 p 应该在当前节点的左子树，因此将当前节点移动到它的左子节点； 如果当前节点的值小于 p 的值，说明 p 应该在当前节点的右子树，因此将当前节点移动到它的右子节点。 对于节点 q 同理。在寻找节点的过程中，我们可以顺便记录经过的节点，这样就得到了从根节点到被寻找节点的路径。 当我们分别得到了从根节点到 p 和 q 的路径之后，我们就可以很方便地找到它们的最近公共祖先了。显然，p 和 q 的最近公共祖先就是从根节点到它们路径上的「分岔点」，也就是最后一个相同的节点。因此，如果我们设从根节点到 p 的路径为数组 path_p，从根节点到 q 的路径为数组 path_q，那么只要找出最大的编号 i，其满足path_p[i]=path_q[i] 那么对应的节点就是「分岔点」，即 p 和 q 的最近公共祖先就是 path_p[i]（或 path_q[i]）。 代码 class Solution { public TreeNode lowestCommonAncestor(TreeNode root, TreeNode p, TreeNode q) { List\u003cTreeNode\u003e path_p = getPath(root, p); List\u003cTreeNode\u003e path_q = getPath(root, q); TreeNode ancestor = null; for (int i = 0; i \u003c path_p.size() \u0026\u0026 i \u003c path_q.size(); ++i) { if (path_p.get(i) == path_q.get(i)) { ancestor = path_p.get(i); } else { break; } } return ancestor; } public List\u003cTreeNode\u003e getPath(TreeNode root, TreeNode target) { List\u003cTreeNode\u003e path = new ArrayList\u003cTreeNode\u003e(); TreeNode node = root; while (node != target) { path.add(node); if (target.val \u003c node.val) { node = node.left; } else { node = node.right; } } path.add(node); return path; } } 复杂度分析 时间复杂度：O(n)，其中 n 是给定的二叉搜索树中的节点个数。上述代码需要的时间与节点 p 和 q 在树中的深度线性相关，而在最坏的情况下，树呈现链式结构，p 和 q 一个是树的唯一叶子结点，一个是该叶子结点的父节点，此时时间复杂度为Θ(n)。 空间复杂度：O(n)，我们需要存储根节点到 p 和 q 的路径。和上面的分析方法相同，在最坏的情况下，路径的长度为 Θ(n)，因此需要 Θ(n) 的空间。 方法二：一次遍历 思路与算法 在方法一中，我们对从根节点开始，通过遍历找出到达节点 p 和 q 的路径，一共需要两次遍历。我们也可以考虑将这两个节点放在一起遍历。 整体的遍历过程与方法一中的类似： 我们从根节点开始遍历； 如果当前节点的值大于 p 和 q 的值，说明 p 和 q 应该在当前节点的左子树，因此将当前节点移动到它的左子节点； 如果当前节点的值小于 p 和 q 的值，说明 p 和 q 应该在当前节点的右子树，因此将当前节点移动到它的右子节点； 如果当前节点的值不满足上述两条要求，那么说明当前节点就是「分岔点」。此时，p 和 q 要么在当前节点的不同的子树中，要么其中一个就是当前节点。 可以发现，如果我们将这两个节点放在一起遍历，我们就省去了存储路径需要的空间。 代码 class Solution { public TreeNode lowestCommonAncestor(TreeNode root, TreeNode p, TreeNode q) { TreeNode ancestor = root; while (true) { if (p.val \u003c ancestor.val \u0026\u0026 q.val \u003c ancestor.val) { ancestor = ancestor.left; } else if (p.val \u003e ancestor.val \u0026\u0026 q.val \u003e ancestor.val) { ancestor = ancestor.right; } else { break; } } return ancestor; } } 复杂度分析 时间复杂度：O(n)，其中 n 是给定的二叉搜索树中的节点个数。分析思路与方法一相同。 空间复杂度：O(1)。 ","date":"2023-01-08","objectID":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/:51:0","tags":["leetcode"],"title":"Leetcode每日一题","uri":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/"},{"categories":["数据结构与算法"],"content":"day51 450. 删除二叉搜索树中的节点 题目 给定一个二叉搜索树的根节点 root 和一个值 key，删除二叉搜索树中的 key 对应的节点，并保证二叉搜索树的性质不变。返回二叉搜索树（有可能被更新）的根节点的引用。 一般来说，删除节点可分为两个步骤： 首先找到需要删除的节点； 如果找到了，删除它。 示例 1: 输入：root = [5,3,6,2,4,null,7], key = 3 输出：[5,4,6,2,null,null,7] 解释：给定需要删除的节点值是 3，所以我们首先找到 3 这个节点，然后删除它。 一个正确的答案是 [5,4,6,2,null,null,7], 如下图所示。 另一个正确答案是 [5,2,6,null,4,null,7]。 示例 2: 输入: root = [5,3,6,2,4,null,7], key = 0 输出: [5,3,6,2,4,null,7] 解释: 二叉树不包含值为 0 的节点 示例 3: 输入: root = [], key = 0 输出: [] 提示: 节点数的范围 [0, 104]. -105 \u003c= Node.val \u003c= 105 节点值唯一 root 是合法的二叉搜索树 -105 \u003c= key \u003c= 105 进阶： 要求算法时间复杂度为 O(h)，h 为树的高度。 解法 方法一：递归 思路 二叉搜索树有以下性质： 左子树的所有节点（如果有）的值均小于当前节点的值； 右子树的所有节点（如果有）的值均大于当前节点的值； 左子树和右子树均为二叉搜索树。 二叉搜索树的题目往往可以用递归来解决。此题要求删除二叉树的节点，函数 deleteNode 的输入是二叉树的根节点 root 和一个整数 key，输出是删除值为 key 的节点后的二叉树，并保持二叉树的有序性。可以按照以下情况分类讨论： root 为空，代表未搜索到值为 key 的节点，返回空。 root.val*\u003e*key，表示值为 key 的节点可能存在于 root 的左子树中，需要递归地在 root.left 调用 deleteNode，并返回 root。 root.val*\u003c*key，表示值为 key 的节点可能存在于 root 的右子树中，需要递归地在 root.right 调用 deleteNode，并返回root。 root.val*=*key，root即为要删除的节点。此时要做的是删除root，并将它的子树合并成一棵子树，保持有序性，并返回根节点。根据root的子树情况分成以下情况讨论： root 为叶子节点，没有子树。此时可以直接将它删除，即返回空。 root 只有左子树，没有右子树。此时可以将它的左子树作为新的子树，返回它的左子节点。 root 只有右子树，没有左子树。此时可以将它的右子树作为新的子树，返回它的右子节点。 root 有左右子树，这时可以将 root 的后继节点（比 root 大的最小节点，即它的右子树中的最小节点，记为 successor）作为新的根节点替代 root，并将 successor 从 root 的右子树中删除，使得在保持有序性的情况下合并左右子树。 简单证明，successor 位于 root 的右子树中，因此大于 root 的所有左子节点；successor 是 root 的右子树中的最小节点，因此小于 root 的右子树中的其他节点。以上两点保持了新子树的有序性。 在代码实现上，我们可以先寻找 successor，再删除它。successor 是 root 的右子树中的最小节点，可以先找到 root 的右子节点，再不停地往左子节点寻找，直到找到一个不存在左子节点的节点，这个节点即为 successor。然后递归地在 root.right 调用 deleteNode 来删除 successor。因为 successor 没有左子节点，因此这一步递归调用不会再次步入这一种情况。然后将 successor 更新为新的 root 并返回。 代码 class Solution { public TreeNode deleteNode(TreeNode root, int key) { if (root == null) { return null; } if (root.val \u003e key) { root.left = deleteNode(root.left, key); return root; } if (root.val \u003c key) { root.right = deleteNode(root.right, key); return root; } if (root.val == key) { if (root.left == null \u0026\u0026 root.right == null) { return null; } if (root.right == null) { return root.left; } if (root.left == null) { return root.right; } TreeNode successor = root.right; while (successor.left != null) { successor = successor.left; } root.right = deleteNode(root.right, successor.val); successor.right = root.right; successor.left = root.left; return successor; } return root; } } 复杂度分析 时间复杂度：O(n)，其中 n 为 root 的节点个数。最差情况下，寻找和删除 successor 各需要遍历一次树。 空间复杂度：O(n)，其中 n 为 root 的节点个数。递归的深度最深为 O(n)。 方法二：迭代 思路 方法一的递归深度最多为 n，而大部分是由寻找值为 key 的节点贡献的，而寻找节点这一部分可以用迭代来优化。寻找并删除 successor 时，也可以用一个变量保存它的父节点，从而可以节省一步递归操作。 代码 class Solution { public TreeNode deleteNode(TreeNode root, int key) { TreeNode cur = root, curParent = null; while (cur != null \u0026\u0026 cur.val != key) { curParent = cur; if (cur.val \u003e key) { cur = cur.left; } else { cur = cur.right; } } if (cur == null) { return root; } if (cur.left == null \u0026\u0026 cur.right == null) { cur = null; } else if (cur.right == null) { cur = cur.left; } else if (cur.left == null) { cur = cur.right; } else { TreeNode successor = cur.right, successorParent = cur; while (successor.left != null) { successorParent = successor; successor = successor.left; } if (successorParent.val == cur.val) { successorParent.right = successor.right; } else { successorParent.left = successor.right; } successor.right = cur.right; successor.left = cur.left; cur = successor; } if (curParent == null) { return cur; } else { if (curParent.left != null \u0026\u0026 curParent.left.val == key) { curParent.left = cur; } else { curParent.right = cur; } return root; } } } 复杂度分析 时间复杂度：O(n)，其中 n 为 root 的节点个数。最差情况下，需要遍历一次树。 空间复杂度：O(1)。使用的空间为常数。 ","date":"2023-01-08","objectID":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/:52:0","tags":["leetcode"],"title":"Leetcode每日一题","uri":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/"},{"categories":["数据结构与算法"],"content":"day52 501. 二叉搜索树中的众数 题目 给你一个含重复值的二叉搜索树（BST）的根节点 root ，找出并返回 BST 中的所有 众数（即，出现频率最高的元素）。 如果树中有不止一个众数，可以按 任意顺序 返回。 假定 BST 满足如下定义： 结点左子树中所含节点的值 小于等于 当前节点的值 结点右子树中所含节点的值 大于等于 当前节点的值 左子树和右子树都是二叉搜索树 示例 1： 输入：root = [1,null,2,2] 输出：[2] 示例 2： 输入：root = [0] 输出：[0] 提示： 树中节点的数目在范围 [1, 104] 内 -105 \u003c= Node.val \u003c= 105 **进阶：**你可以不使用额外的空间吗？（假设由递归产生的隐式调用栈的开销不被计算在内） 解法 方法一：当作普通二叉树 先便利二叉树，然后值作为键，重复次数作为值放到一个map，之后统计这个map即可 方法二：中序遍历 思路与算法 首先我们一定能想到一个最朴素的做法：因为这棵树的中序遍历是一个有序的序列，所以我们可以先获得这棵树的中序遍历，然后从扫描这个中序遍历序列，然后用一个哈希表来统计每个数字出现的个数，这样就可以找到出现次数最多的数字。但是这样做的空间复杂度显然不是 O(1) 的，原因是哈希表和保存中序遍历序列的空间代价都是 O(n)。 首先，我们考虑在寻找出现次数最多的数时，不使用哈希表。 这个优化是基于二叉搜索树中序遍历的性质：一棵二叉搜索树的中序遍历序列是一个非递减的有序序列。例如： 1 / \\ 0 2 / \\ / -1 0 2 这样一颗二叉搜索树的中序遍历序列是 {−1,0,0,1,2,2}。我们可以发现重复出现的数字一定是一个连续出现的，例如这里的 0 和 2，它们都重复出现了，并且所有的 0 都集中在一个连续的段内，所有的 2 也集中在一个连续的段内。我们可以顺序扫描中序遍历序列，用 base 记录当前的数字，用 count 记录当前数字重复的次数，用 maxCount 来维护已经扫描过的数当中出现最多的那个数字的出现次数，用 answer 数组记录出现的众数。每次扫描到一个新的元素： 首先更新base和count: 如果该元素和 base 相等，那么 count 自增 1； 否则将 base 更新为当前数字，count 复位为 1。 然后更新maxCount： 如果 count = maxCount，那么说明当前的这个数字（base）出现的次数等于当前众数出现的次数，将 base 加入 answer 数组； 如果 count*\u003e*maxCount，那么说明当前的这个数字（base）出现的次数大于当前众数出现的次数，因此，我们需要将 maxCount 更新为 count，清空 answer 数组后将 base 加入 answer 数组。 我们可以把这个过程写成一个 update 函数。这样我们在寻找出现次数最多的数字的时候就可以省去一个哈希表带来的空间消耗。 然后，我们考虑不存储这个中序遍历序列。 如果我们在递归进行中序遍历的过程中，访问当了某个点的时候直接使用上面的 update 函数，就可以省去中序遍历序列的空间，代码如下。 代码 class Solution { List\u003cInteger\u003e answer = new ArrayList\u003cInteger\u003e(); int base, count, maxCount; public int[] findMode(TreeNode root) { dfs(root); int[] mode = new int[answer.size()]; for (int i = 0; i \u003c answer.size(); ++i) { mode[i] = answer.get(i); } return mode; } public void dfs(TreeNode o) { if (o == null) { return; } dfs(o.left); update(o.val); dfs(o.right); } public void update(int x) { if (x == base) { ++count; } else { count = 1; base = x; } if (count == maxCount) { answer.add(base); } if (count \u003e maxCount) { maxCount = count; answer.clear(); answer.add(base); } } } 复杂度分析 时间复杂度：O(n)。即遍历这棵树的复杂度。 空间复杂度：O(n)。即递归的栈空间的空间代价。 方法三：Morris 中序遍历 思路与算法 接着上面的思路，我们用 Morris 中序遍历的方法把中序遍历的空间复杂度优化到 O(1)。 我们在中序遍历的时候，一定先遍历左子树，然后遍历当前节点，最后遍历右子树。在常规方法中，我们用递归回溯或者是栈来保证遍历完左子树可以再回到当前节点，但这需要我们付出额外的空间代价。我们需要用一种巧妙地方法可以在 O(1) 的空间下，遍历完左子树可以再回到当前节点。我们希望当前的节点在遍历完当前点的前驱之后被遍历，我们可以考虑修改它的前驱节点的 right 指针。当前节点的前驱节点的 right 指针可能本来就指向当前节点（前驱是当前节点的父节点），也可能是当前节点左子树最右下的节点。如果是后者，我们希望遍历完这个前驱节点之后再回到当前节点，可以将它的 right 指针指向当前节点。 Morris 中序遍历的一个重要步骤就是寻找当前节点的前驱节点，并且 Morris 中序遍历寻找下一个点始终是通过转移到 right 指针指向的位置来完成的。 如果当前节点没有左子树，则遍历这个点，然后跳转到当前节点的右子树。 如果当前节点有左子树，那么它的前驱节点一定在左子树上，我们可以在左子树上一直向右行走，找到当前点的前驱节点。 如果前驱节点没有右子树，就将前驱节点的 right 指针指向当前节点。这一步是为了在遍历完前驱节点后能找到前驱节点的后继，也就是当前节点。 如果前驱节点的右子树为当前节点，说明前驱节点已经被遍历过并被修改了 right 指针，这个时候我们重新将前驱的右孩子设置为空，遍历当前的点，然后跳转到当前节点的右子树。 因此我们可以得到这样的代码框架： TreeNode *cur = root, *pre = nullptr; while (cur) { if (!cur-\u003eleft) { // ...遍历 cur cur = cur-\u003eright; continue; } pre = cur-\u003eleft; while (pre-\u003eright \u0026\u0026 pre-\u003eright != cur) { pre = pre-\u003eright; } if (!pre-\u003eright) { pre-\u003eright = cur; cur = cur-\u003eleft; } else { pre-\u003eright = nullptr; // ...遍历 cur cur = cur-\u003eright; } } 最后我们将 ...遍历 cur 替换成之前的 update 函数即可。 代码 class Solution { int base, count, maxCount; List\u003cInteger\u003e answer = new ArrayList\u003cInteger\u003e(); public int[] findMode(TreeNode root) { TreeNode cur = root, pre = null; while (cur != null) { if (cur.left == null) { update(cur.val); cur = cur.right; continue; } pre = cur.left; while (pre.right != null \u0026\u0026 pre.right != cur) { pre = pre.right; } if (pre.right == null) { pre.right = cur; cur = cur.left; } else { pre.right = null; update(cur.val); cur = cur.right; } } int[] mode = new int[answer.size()]; for (int i = 0; i \u003c answer.size(); ++i) { mode[i] = answer.get(i); } return mode; } public void update(int x) { if (x == base) { ++count; } else { count = 1; base = x; } if (count == maxCount) { answer.add(base); } if (count \u003e maxCount) { maxCount = count; answer.clear(); answer.add(base); } } } 复杂度分析 时间复杂度：O(n)。每个点被访问的次数不会超过两次，故这里的时间复杂度是 O(n)。 空间复杂度：O(1)。使用临时空间的大小和输入规模无关。 ","date":"2023-01-08","objectID":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/:53:0","tags":["leetcode"],"title":"Leetcode每日一题","uri":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/"},{"categories":["数据结构与算法"],"content":"day53 530. 二叉搜索树的最小绝对差 题目 给你一个二叉搜索树的根节点 root ，返回 树中任意两不同节点值之间的最小差值 。 差值是一个正数，其数值等于两值之差的绝对值。 示例 1： 输入：root = [4,2,6,1,3] 输出：1 示例 2： 输入：root = [1,0,48,null,null,12,49] 输出：1 提示： 树中节点的数目范围是 [2, 104] 0 \u003c= Node.val \u003c= 105 **注意：**本题与 783 https://leetcode-cn.com/problems/minimum-distance-between-bst-nodes/ 相同 解法 方法一：中序遍历 思路与算法 考虑对升序数组 aa 求任意两个元素之差的绝对值的最小值，答案一定为相邻两个元素之差的最小值，即 $$ \\textit{ans}=\\min_{i=0}^{n-2}\\left{a[i+1]-a[i]\\right} $$ 其中 n 为数组 a 的长度。其他任意间隔距离大于等于 2 的下标对 (i,j) 的元素之差一定大于下标对 (i,i+1) 的元素之差，故不需要再被考虑。 回到本题，本题要求二叉搜索树任意两节点差的绝对值的最小值，而我们知道二叉搜索树有个性质为二叉搜索树中序遍历得到的值序列是递增有序的，因此我们只要得到中序遍历后的值序列即能用上文提及的方法来解决。 朴素的方法是经过一次中序遍历将值保存在一个数组中再进行遍历求解，我们也可以在中序遍历的过程中用 pre 变量保存前驱节点的值，这样即能边遍历边更新答案，不再需要显式创建数组来保存，需要注意的是 pre 的初始值需要设置成任意负数标记开头，下文代码中设置为 -1−1。 二叉树的中序遍历有多种方式，包括递归、栈、Morris 遍历等，读者可选择自己最擅长的来实现。下文代码提供最普遍的递归方法来实现，其他遍历方法的介绍可以详细看「94. 二叉树的中序遍历的官方题解」，这里不再赘述。 代码 class Solution { int pre; int ans; public int getMinimumDifference(TreeNode root) { ans = Integer.MAX_VALUE; pre = -1; dfs(root); return ans; } public void dfs(TreeNode root) { if (root == null) { return; } dfs(root.left); if (pre == -1) { pre = root.val; } else { ans = Math.min(ans, root.val - pre); pre = root.val; } dfs(root.right); } } 复杂度分析 时间复杂度：O(n)，其中 n 为二叉搜索树节点的个数。每个节点在中序遍历中都会被访问一次且只会被访问一次，因此总时间复杂度为 O(n)。 空间复杂度：O(n)。递归函数的空间复杂度取决于递归的栈深度，而栈深度在二叉搜索树为一条链的情况下会达到 O(n) 级别。 ","date":"2023-01-08","objectID":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/:54:0","tags":["leetcode"],"title":"Leetcode每日一题","uri":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/"},{"categories":["数据结构与算法"],"content":"day54 538. 把二叉搜索树转换为累加树 题目 给出二叉 搜索 树的根节点，该树的节点值各不相同，请你将其转换为累加树（Greater Sum Tree），使每个节点 node 的新值等于原树中大于或等于 node.val 的值之和。 提醒一下，二叉搜索树满足下列约束条件： 节点的左子树仅包含键 小于 节点键的节点。 节点的右子树仅包含键 大于 节点键的节点。 左右子树也必须是二叉搜索树。 **注意：**本题和 1038: https://leetcode-cn.com/problems/binary-search-tree-to-greater-sum-tree/ 相同 示例 1： 输入：[4,1,6,0,2,5,7,null,null,null,3,null,null,null,8] 输出：[30,36,21,36,35,26,15,null,null,null,33,null,null,null,8] 示例 2： 输入：root = [0,null,1] 输出：[1,null,1] 示例 3： 输入：root = [1,0,2] 输出：[3,3,2] 示例 4： 输入：root = [3,2,4,1] 输出：[7,9,4,10] 提示： 树中的节点数介于 0 和 104 之间。 每个节点的值介于 -104 和 104 之间。 树中的所有值 互不相同 。 给定的树为二叉搜索树。 解法 前言 二叉搜索树是一棵空树，或者是具有下列性质的二叉树： 若它的左子树不空，则左子树上所有节点的值均小于它的根节点的值； 若它的右子树不空，则右子树上所有节点的值均大于它的根节点的值； 它的左、右子树也分别为二叉搜索树。 由这样的性质我们可以发现，二叉搜索树的中序遍历是一个单调递增的有序序列。如果我们反序地中序遍历该二叉搜索树，即可得到一个单调递减的有序序列。 方法一：反序中序遍历 思路及算法 本题中要求我们将每个节点的值修改为原来的节点值加上所有大于它的节点值之和。这样我们只需要反序中序遍历该二叉搜索树，记录过程中的节点值之和，并不断更新当前遍历到的节点的节点值，即可得到题目要求的累加树。 代码 class Solution { int sum = 0; public TreeNode convertBST(TreeNode root) { if (root != null) { convertBST(root.right); sum += root.val; root.val = sum; convertBST(root.left); } return root; } } 复杂度分析 时间复杂度：O(n)，其中 n 是二叉搜索树的节点数。每一个节点恰好被遍历一次。 空间复杂度：O(n)，为递归过程中栈的开销，平均情况下为 O(log n)，最坏情况下树呈现链状，为 O(n)。 方法二：Morris 遍历 思路及算法 有一种巧妙的方法可以在线性时间内，只占用常数空间来实现中序遍历。这种方法由 J. H. Morris 在 1979 年的论文「Traversing Binary Trees Simply and Cheaply」中首次提出，因此被称为 Morris 遍历。 Morris 遍历的核心思想是利用树的大量空闲指针，实现空间开销的极限缩减。其反序中序遍历规则总结如下： 如果当前节点的右子节点为空，处理当前节点，并遍历当前节点的左子节点； 如果当前节点的右子节点不为空，找到当前节点右子树的最左节点（该节点为当前节点中序遍历的前驱节点）； 如果最左节点的左指针为空，将最左节点的左指针指向当前节点，遍历当前节点的右子节点； 如果最左节点的左指针不为空，将最左节点的左指针重新置为空（恢复树的原状），处理当前节点，并将当前节点置为其左节点； 重复步骤 1 和步骤 2，直到遍历结束。 这样我们利用 Morris 遍历的方法，反序中序遍历该二叉搜索树，即可实现线性时间与常数空间的遍历。 代码 class Solution { public TreeNode convertBST(TreeNode root) { int sum = 0; TreeNode node = root; while (node != null) { if (node.right == null) { sum += node.val; node.val = sum; node = node.left; } else { TreeNode succ = getSuccessor(node); if (succ.left == null) { succ.left = node; node = node.right; } else { succ.left = null; sum += node.val; node.val = sum; node = node.left; } } } return root; } public TreeNode getSuccessor(TreeNode node) { TreeNode succ = node.right; while (succ.left != null \u0026\u0026 succ.left != node) { succ = succ.left; } return succ; } } 复杂度分析 时间复杂度：O(n)，其中 n 是二叉搜索树的节点数。没有左子树的节点只被访问一次，有左子树的节点被访问两次。 空间复杂度：O(1)。只操作已经存在的指针（树的空闲指针），因此只需要常数的额外空间。 ","date":"2023-01-08","objectID":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/:55:0","tags":["leetcode"],"title":"Leetcode每日一题","uri":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/"},{"categories":["数据结构与算法"],"content":"day55 669. 修剪二叉搜索树 题目 给你二叉搜索树的根节点 root ，同时给定最小边界low 和最大边界 high。通过修剪二叉搜索树，使得所有节点的值在[low, high]中。修剪树 不应该 改变保留在树中的元素的相对结构 (即，如果没有被移除，原有的父代子代关系都应当保留)。 可以证明，存在 唯一的答案 。 所以结果应当返回修剪好的二叉搜索树的新的根节点。注意，根节点可能会根据给定的边界发生改变。 示例 1： 输入：root = [1,0,2], low = 1, high = 2 输出：[1,null,2] 示例 2： 输入：root = [3,0,4,null,2,null,null,1], low = 1, high = 3 输出：[3,2,null,1] 提示： 树中节点数在范围 [1, 104] 内 0 \u003c= Node.val \u003c= 104 树中每个节点的值都是 唯一 的 题目数据保证输入是一棵有效的二叉搜索树 0 \u003c= low \u003c= high \u003c= 104 解法 方法一：递归 对根结点 root 进行深度优先遍历。对于当前访问的结点，如果结点为空结点，直接返回空结点；如果结点的值小于 low，那么说明该结点及它的左子树都不符合要求，我们返回对它的右结点进行修剪后的结果；如果结点的值大于 high，那么说明该结点及它的右子树都不符合要求，我们返回对它的左子树进行修剪后的结果；如果结点的值位于区间 [low,high]，我们将结点的左结点设为对它的左子树修剪后的结果，右结点设为对它的右子树进行修剪后的结果。 class Solution { public TreeNode trimBST(TreeNode root, int low, int high) { if (root == null) { return null; } if (root.val \u003c low) { return trimBST(root.right, low, high); } else if (root.val \u003e high) { return trimBST(root.left, low, high); } else { root.left = trimBST(root.left, low, high); root.right = trimBST(root.right, low, high); return root; } } } 复杂度分析 时间复杂度：O(n)，其中 n 为二叉树的结点数目。 空间复杂度：O(n)。递归栈最坏情况下需要 O(n) 的空间。 方法二：迭代 如果一个结点 node 符合要求，即它的值位于区间 [low, high]，那么它的左子树与右子树应该如何修剪？ 我们先讨论左子树的修剪： node 的左结点为空结点：不需要修剪 node 的左结点非空： 如果它的左结点 left 的值小于 low，那么 left 以及 left 的左子树都不符合要求，我们将 node 的左结点设为 left 的右结点，然后再重新对 node 的左子树进行修剪。 如果它的左结点 left 的值大于等于 low，又因为 node 的值已经符合要求，所以 left 的右子树一定符合要求。基于此，我们只需要对 left 的左子树进行修剪。我们令 node 等于 left ，然后再重新对 node 的左子树进行修剪。 以上过程可以迭代处理。对于右子树的修剪同理。 我们对根结点进行判断，如果根结点不符合要求，我们将根结点设为对应的左结点或右结点，直到根结点符合要求，然后将根结点作为符合要求的结点，依次修剪它的左子树与右子树。 class Solution { public TreeNode trimBST(TreeNode root, int low, int high) { while (root != null \u0026\u0026 (root.val \u003c low || root.val \u003e high)) { if (root.val \u003c low) { root = root.right; } else { root = root.left; } } if (root == null) { return null; } for (TreeNode node = root; node.left != null; ) { if (node.left.val \u003c low) { node.left = node.left.right; } else { node = node.left; } } for (TreeNode node = root; node.right != null; ) { if (node.right.val \u003e high) { node.right = node.right.left; } else { node = node.right; } } return root; } } 复杂度分析 时间复杂度：O(n)，其中 n 为二叉树的结点数目。最多访问 n 个结点。 空间复杂度：O(1)。 ","date":"2023-01-08","objectID":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/:56:0","tags":["leetcode"],"title":"Leetcode每日一题","uri":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/"},{"categories":["数据结构与算法"],"content":"day56 701. 二叉搜索树中的插入操作 题目 给定二叉搜索树（BST）的根节点 root 和要插入树中的值 value ，将值插入二叉搜索树。 返回插入后二叉搜索树的根节点。 输入数据 保证 ，新值和原始二叉搜索树中的任意节点值都不同。 注意，可能存在多种有效的插入方式，只要树在插入后仍保持为二叉搜索树即可。 你可以返回 任意有效的结果 。 示例 1： 输入：root = [4,2,7,1,3], val = 5 输出：[4,2,7,1,3,5] 解释：另一个满足题目要求可以通过的树是： 示例 2： 输入：root = [40,20,60,10,30,50,70], val = 25 输出：[40,20,60,10,30,50,70,null,null,25] 示例 3： 输入：root = [4,2,7,1,3,null,null,null,null,null,null], val = 5 输出：[4,2,7,1,3,5] 提示： 树中的节点数将在 [0, 104]的范围内。 -108 \u003c= Node.val \u003c= 108 所有值 Node.val 是 独一无二 的。 -108 \u003c= val \u003c= 108 保证 val 在原始BST中不存在。 解法 方法一：模拟 思路与算法 首先回顾二叉搜索树的性质：对于任意节点 root 而言，左子树（如果存在）上所有节点的值均小于 root.val，右子树（如果存在）上所有节点的值均大于 root.val，且它们都是二叉搜索树。 因此，当将 val 插入到以 root 为根的子树上时，根据 val 与 root.val 的大小关系，就可以确定要将 val 插入到哪个子树中。 如果该子树不为空，则问题转化成了将 val 插入到对应子树上。 否则，在此处新建一个以 val 为值的节点，并链接到其父节点 root 上。 代码 class Solution { public TreeNode insertIntoBST(TreeNode root, int val) { if (root == null) { return new TreeNode(val); } TreeNode pos = root; while (pos != null) { if (val \u003c pos.val) { if (pos.left == null) { pos.left = new TreeNode(val); break; } else { pos = pos.left; } } else { if (pos.right == null) { pos.right = new TreeNode(val); break; } else { pos = pos.right; } } } return root; } } 复杂度分析 时间复杂度：O(N)，其中 N 为树中节点的数目。最坏情况下，我们需要将值插入到树的最深的叶子结点上，而叶子节点最深为 O(N)。 空间复杂度：O(1)。我们只使用了常数大小的空间。 方法二：递归、 我们知道二叉搜索树插入新的节点时，如果还要满足BST性质，那么还有一种简单得思路：直接遍历到何时得叶子节点，插入到叶子节点末尾即可。 class Solution { TreeNode parent = null; int flag = 0; // 二叉排序树插入节点，一定可以插入到叶子节点 public TreeNode insertIntoBST(TreeNode root, int val) { if(root==null) { if(flag == -1){ return new TreeNode(val); }else{ return new TreeNode(val); } } if(val\u003eroot.val){ parent = root; flag = 1; root.right = insertIntoBST(root.right,val); } else if(val\u003croot.val){ parent = root; flag = -1; root.left = insertIntoBST(root.left,val); } return root; } } ","date":"2023-01-08","objectID":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/:57:0","tags":["leetcode"],"title":"Leetcode每日一题","uri":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/"},{"categories":["数据结构与算法"],"content":"day57 110. 平衡二叉树 题目 给定一个二叉树，判断它是否是高度平衡的二叉树。 本题中，一棵高度平衡二叉树定义为： 一个二叉树每个节点 的左右两个子树的高度差的绝对值不超过 1 。 示例 1： 输入：root = [3,9,20,null,null,15,7] 输出：true 示例 2： 输入：root = [1,2,2,3,3,null,null,4,4] 输出：false 示例 3： 输入：root = [] 输出：true 提示： 树中的节点数在范围 [0, 5000] 内 -104 \u003c= Node.val \u003c= 104 解法 前言 这道题中的平衡二叉树的定义是：二叉树的每个节点的左右子树的高度差的绝对值不超过 1，则二叉树是平衡二叉树。根据定义，一棵二叉树是平衡二叉树，当且仅当其所有子树也都是平衡二叉树，因此可以使用递归的方式判断二叉树是不是平衡二叉树，递归的顺序可以是自顶向下或者自底向上。 方法一：自顶向下的递归 定义函数 height，用于计算二叉树中的任意一个节点 p 的高度： $$ \\texttt{height}(p) = \\begin{cases} 0 \u0026 p \\text{ 是空节点}\\ \\max(\\texttt{height}(p.\\textit{left}), \\texttt{height}(p.\\textit{right}))+1 \u0026 p \\text{ 是非空节点} \\end{cases} $$ 有了计算节点高度的函数，即可判断二叉树是否平衡。具体做法类似于二叉树的前序遍历，即对于当前遍历到的节点，首先计算左右子树的高度，如果左右子树的高度差是否不超过 1，再分别递归地遍历左右子节点，并判断左子树和右子树是否平衡。这是一个自顶向下的递归的过程。 class Solution { public boolean isBalanced(TreeNode root) { if (root == null) { return true; } else { return Math.abs(height(root.left) - height(root.right)) \u003c= 1 \u0026\u0026 isBalanced(root.left) \u0026\u0026 isBalanced(root.right); } } public int height(TreeNode root) { if (root == null) { return 0; } else { return Math.max(height(root.left), height(root.right)) + 1; } } } 复杂度分析 时间复杂度：O(n^2)，其中 n 是二叉树中的节点个数。 最坏情况下，二叉树是满二叉树，需要遍历二叉树中的所有节点，时间复杂度是 O(n)。 对于节点 p，如果它的高度是 d，则 height(p) 最多会被调用 d 次（即遍历到它的每一个祖先节点时）。对于平均的情况，一棵树的高度 h 满足 O(h)=O(logn)，因为 d≤h，所以总时间复杂度为 O(nlog n)。对于最坏的情况，二叉树形成链式结构，高度为 O(n)，此时总时间复杂度为 O(n^2)。 空间复杂度：O(n)，其中 n 是二叉树中的节点个数。空间复杂度主要取决于递归调用的层数，递归调用的层数不会超过 n。 方法二：自底向上的递归 方法一由于是自顶向下递归，因此对于同一个节点，函数 height 会被重复调用，导致时间复杂度较高。如果使用自底向上的做法，则对于每个节点，函数 height 只会被调用一次。 自底向上递归的做法类似于后序遍历，对于当前遍历到的节点，先递归地判断其左右子树是否平衡，再判断以当前节点为根的子树是否平衡。如果一棵子树是平衡的，则返回其高度（高度一定是非负整数），否则返回 -1。如果存在一棵子树不平衡，则整个二叉树一定不平衡。 class Solution { public boolean isBalanced(TreeNode root) { return height(root) \u003e= 0; } public int height(TreeNode root) { if (root == null) { return 0; } int leftHeight = height(root.left); int rightHeight = height(root.right); if (leftHeight == -1 || rightHeight == -1 || Math.abs(leftHeight - rightHeight) \u003e 1) { return -1; } else { return Math.max(leftHeight, rightHeight) + 1; } } } 复杂度分析 时间复杂度：O(n)，其中 n 是二叉树中的节点个数。使用自底向上的递归，每个节点的计算高度和判断是否平衡都只需要处理一次，最坏情况下需要遍历二叉树中的所有节点，因此时间复杂度是 O(n)。 空间复杂度：O(n)，其中 n 是二叉树中的节点个数。空间复杂度主要取决于递归调用的层数，递归调用的层数不会超过 n。 ","date":"2023-01-08","objectID":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/:58:0","tags":["leetcode"],"title":"Leetcode每日一题","uri":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/"},{"categories":["数据结构与算法"],"content":"day58 1382. 将二叉搜索树变平衡 题目 给你一棵二叉搜索树，请你返回一棵 平衡后 的二叉搜索树，新生成的树应该与原来的树有着相同的节点值。如果有多种构造方法，请你返回任意一种。 如果一棵二叉搜索树中，每个节点的两棵子树高度差不超过 1 ，我们就称这棵二叉搜索树是 平衡的 。 示例 1： 输入：root = [1,null,2,null,3,null,4,null,null] 输出：[2,1,3,null,null,null,4] 解释：这不是唯一的正确答案，[3,1,4,null,2,null,null] 也是一个可行的构造方案。 示例 2： 输入: root = [2,1,3] 输出: [2,1,3] 提示： 树节点的数目在 [1, 104] 范围内。 1 \u003c= Node.val \u003c= 105 解法 贪心构造 ","date":"2023-01-08","objectID":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/:59:0","tags":["leetcode"],"title":"Leetcode每日一题","uri":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/"},{"categories":["数据结构与算法"],"content":"day59 28. 找出字符串中第一个匹配项的下标 题目 给你两个字符串 haystack 和 needle ，请你在 haystack 字符串中找出 needle 字符串的第一个匹配项的下标（下标从 0 开始）。如果 needle 不是 haystack 的一部分，则返回 -1 。 示例 1： 输入：haystack = \"sadbutsad\", needle = \"sad\" 输出：0 解释：\"sad\" 在下标 0 和 6 处匹配。 第一个匹配项的下标是 0 ，所以返回 0 。 示例 2： 输入：haystack = \"leetcode\", needle = \"leeto\" 输出：-1 解释：\"leeto\" 没有在 \"leetcode\" 中出现，所以返回 -1 。 提示： 1 \u003c= haystack.length, needle.length \u003c= 104 haystack 和 needle 仅由小写英文字符组成 解法 KMP ","date":"2023-01-08","objectID":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/:60:0","tags":["leetcode"],"title":"Leetcode每日一题","uri":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/"},{"categories":["数据结构与算法"],"content":"day60 459. 重复的子字符串 题目 给定一个非空的字符串 s ，检查是否可以通过由它的一个子串重复多次构成。 示例 1: 输入: s = \"abab\" 输出: true 解释: 可由子串 \"ab\" 重复两次构成。 示例 2: 输入: s = \"aba\" 输出: false 示例 3: 输入: s = \"abcabcabcabc\" 输出: true 解释: 可由子串 \"abc\" 重复四次构成。 (或子串 \"abcabc\" 重复两次构成。) 提示： 1 \u003c= s.length \u003c= 104 s 由小写英文字母组成 解法 https://leetcode.cn/problems/repeated-substring-pattern/solution/zhong-fu-de-zi-zi-fu-chuan-by-leetcode-solution/ ","date":"2023-01-08","objectID":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/:61:0","tags":["leetcode"],"title":"Leetcode每日一题","uri":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/"},{"categories":["数据结构与算法"],"content":"day61 50. Pow(x, n) 题目 实现 pow(x, n) ，即计算 x 的整数 n 次幂函数（即，xn ）。 示例 1： 输入：x = 2.00000, n = 10 输出：1024.00000 示例 2： 输入：x = 2.10000, n = 3 输出：9.26100 示例 3： 输入：x = 2.00000, n = -2 输出：0.25000 解释：2-2 = 1/22 = 1/4 = 0.25 提示： -100.0 \u003c x \u003c 100.0 -231 \u003c= n \u003c= 231-1 -104 \u003c= xn \u003c= 104 解法 使用暴力迭代和暴力递归均会爆栈或超时 // 递归求解爆栈 public double myPow(double x, int n) { if(n==0) return 1.0; else if(n\u003e0) return myPow(x,n-1)*x; else return myPow(x,n+1)*(1.0/x); } // 迭代超时 public double myPow(double x, int n) { if(n==0) return 1.0; else if(n\u003e0) { double r=x; for(int i=0;i\u003cn-1;i++){ r=r*x; } return r; }else { double r=(1.0/x); for(int i=0;i\u003c-n-1;i++){ r =r*(1.0/x); } return r; } } 方法一：快速幂+递归 方法二：快速幂+迭代 https://leetcode.cn/problems/powx-n/solution/powx-n-by-leetcode-solution/ ","date":"2023-01-08","objectID":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/:62:0","tags":["leetcode"],"title":"Leetcode每日一题","uri":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/"},{"categories":["数据结构与算法"],"content":"day62 33. 搜索旋转排序数组 题目 整数数组 nums 按升序排列，数组中的值 互不相同 。 在传递给函数之前，nums 在预先未知的某个下标 k（0 \u003c= k \u003c nums.length）上进行了 旋转，使数组变为 [nums[k], nums[k+1], ..., nums[n-1], nums[0], nums[1], ..., nums[k-1]]（下标 从 0 开始 计数）。例如， [0,1,2,4,5,6,7] 在下标 3 处经旋转后可能变为 [4,5,6,7,0,1,2] 。 给你 旋转后 的数组 nums 和一个整数 target ，如果 nums 中存在这个目标值 target ，则返回它的下标，否则返回 -1 。 你必须设计一个时间复杂度为 O(log n) 的算法解决此问题。 示例 1： 输入：nums = [4,5,6,7,0,1,2], target = 0 输出：4 示例 2： 输入：nums = [4,5,6,7,0,1,2], target = 3 输出：-1 示例 3： 输入：nums = [1], target = 0 输出：-1 提示： 1 \u003c= nums.length \u003c= 5000 -104 \u003c= nums[i] \u003c= 104 nums 中的每个值都 独一无二 题目数据保证 nums 在预先未知的某个下标上进行了旋转 -104 \u003c= target \u003c= 104 解法 https://leetcode.cn/problems/search-in-rotated-sorted-array/solution/sou-suo-xuan-zhuan-pai-xu-shu-zu-by-leetcode-solut/ func search(nums []int, target int) int { l:=0 r:=len(nums)-1 mid :=0 for l\u003c=r { mid = (r-l)/2+l if nums[mid] == target { return mid } if nums[mid]\u003cnums[0]{// 右边有序 if target\u003c=nums[r]\u0026\u0026target\u003enums[mid]{ l = mid + 1 }else{ r = mid - 1 } }else{// 左边有序 if target\u003e=nums[l]\u0026\u0026target\u003cnums[mid]{ r = mid - 1 }else{ l = mid + 1 } } } return -1 } ","date":"2023-01-08","objectID":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/:63:0","tags":["leetcode"],"title":"Leetcode每日一题","uri":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/"},{"categories":["数据结构与算法"],"content":"day63 34. 在排序数组中查找元素的第一个和最后一个位置 题目 给你一个按照非递减顺序排列的整数数组 nums，和一个目标值 target。请你找出给定目标值在数组中的开始位置和结束位置。 如果数组中不存在目标值 target，返回 [-1, -1]。 你必须设计并实现时间复杂度为 O(log n) 的算法解决此问题。 示例 1： 输入：nums = [5,7,7,8,8,10], target = 8 输出：[3,4] 示例 2： 输入：nums = [5,7,7,8,8,10], target = 6 输出：[-1,-1] 示例 3： 输入：nums = [], target = 0 输出：[-1,-1] 提示： 0 \u003c= nums.length \u003c= 105 -109 \u003c= nums[i] \u003c= 109 nums 是一个非递减数组 -109 \u003c= target \u003c= 109 解法 官方题解 二分+暴力 先二分找数，找到之后再两边扩展。 问题：如果查找的数组重复度很高，而且刚好是target的重复，算法逐渐退化到O(n). func searchRange(nums []int, target int) []int { ans:=[]int{-1,-1} idx:=-1 l:=0 r:=len(nums)-1 mid:=0 for l\u003c=r{ mid = (r-l)/2+l if nums[mid]==target{ idx = mid break }else if nums[mid]\u003etarget{ r = mid - 1 }else{ l = mid + 1 } } if idx!=-1{ l=idx r=idx for l\u003e=0\u0026\u0026nums[l]==target { l-- } l++ for r\u003clen(nums)\u0026\u0026nums[r]==target { r++ } r-- ans[0] = l ans[1] = r } return ans } 二分+二分 针对问题二再进行优化：二分都是找到索引最小的那个值。第一次二分找target，第二次二分找target+1的索引再-1 func searchRange(nums []int, target int) []int { leftmost := sort.SearchInts(nums, target) if leftmost == len(nums) || nums[leftmost] != target { return []int{-1, -1} } rightmost := sort.SearchInts(nums, target + 1) - 1 return []int{leftmost, rightmost} } ","date":"2023-01-08","objectID":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/:64:0","tags":["leetcode"],"title":"Leetcode每日一题","uri":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/"},{"categories":["数据结构与算法"],"content":"day64 81. 搜索旋转排序数组 II 题目 已知存在一个按非降序排列的整数数组 nums ，数组中的值不必互不相同。 在传递给函数之前，nums 在预先未知的某个下标 k（0 \u003c= k \u003c nums.length）上进行了 旋转 ，使数组变为 [nums[k], nums[k+1], ..., nums[n-1], nums[0], nums[1], ..., nums[k-1]]（下标 从 0 开始 计数）。例如， [0,1,2,4,4,4,5,6,6,7] 在下标 5 处经旋转后可能变为 [4,5,6,6,7,0,1,2,4,4] 。 给你 旋转后 的数组 nums 和一个整数 target ，请你编写一个函数来判断给定的目标值是否存在于数组中。如果 nums 中存在这个目标值 target ，则返回 true ，否则返回 false 。 你必须尽可能减少整个操作步骤。 示例 1： 输入：nums = [2,5,6,0,0,1,2], target = 0 输出：true 示例 2： 输入：nums = [2,5,6,0,0,1,2], target = 3 输出：false 提示： 1 \u003c= nums.length \u003c= 5000 -104 \u003c= nums[i] \u003c= 104 题目数据保证 nums 在预先未知的某个下标上进行了旋转 -104 \u003c= target \u003c= 104 进阶： 这是 搜索旋转排序数组 的延伸题目，本题中的 nums 可能包含重复元素。 这会影响到程序的时间复杂度吗？会有怎样的影响，为什么？ 解法 https://leetcode.cn/problems/search-in-rotated-sorted-array-ii/solution/sou-suo-xuan-zhuan-pai-xu-shu-zu-ii-by-l-0nmp/ ","date":"2023-01-08","objectID":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/:65:0","tags":["leetcode"],"title":"Leetcode每日一题","uri":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/"},{"categories":["数据结构与算法"],"content":"day65 153. 寻找旋转排序数组中的最小值 题目 已知一个长度为 n 的数组，预先按照升序排列，经由 1 到 n 次 旋转 后，得到输入数组。例如，原数组 nums = [0,1,2,4,5,6,7] 在变化后可能得到： 若旋转 4 次，则可以得到 [4,5,6,7,0,1,2] 若旋转 7 次，则可以得到 [0,1,2,4,5,6,7] 注意，数组 [a[0], a[1], a[2], ..., a[n-1]] 旋转一次 的结果为数组 [a[n-1], a[0], a[1], a[2], ..., a[n-2]] 。 给你一个元素值 互不相同 的数组 nums ，它原来是一个升序排列的数组，并按上述情形进行了多次旋转。请你找出并返回数组中的 最小元素 。 你必须设计一个时间复杂度为 O(log n) 的算法解决此问题。 示例 1： 输入：nums = [3,4,5,1,2] 输出：1 解释：原数组为 [1,2,3,4,5] ，旋转 3 次得到输入数组。 示例 2： 输入：nums = [4,5,6,7,0,1,2] 输出：0 解释：原数组为 [0,1,2,4,5,6,7] ，旋转 4 次得到输入数组。 示例 3： 输入：nums = [11,13,15,17] 输出：11 解释：原数组为 [11,13,15,17] ，旋转 4 次得到输入数组。 提示： n == nums.length 1 \u003c= n \u003c= 5000 -5000 \u003c= nums[i] \u003c= 5000 nums 中的所有整数 互不相同 nums 原来是一个升序排序的数组，并进行了 1 至 n 次旋转 解法 Leecode官方题解 ","date":"2023-01-08","objectID":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/:66:0","tags":["leetcode"],"title":"Leetcode每日一题","uri":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/"},{"categories":["数据结构与算法"],"content":"day66 11. 盛最多水的容器 题目 给定一个长度为 n 的整数数组 height 。有 n 条垂线，第 i 条线的两个端点是 (i, 0) 和 (i, height[i]) 。 找出其中的两条线，使得它们与 x 轴共同构成的容器可以容纳最多的水。 返回容器可以储存的最大水量。 **说明：**你不能倾斜容器。 示例 1： 输入：[1,8,6,2,5,4,8,3,7] 输出：49 解释：图中垂直线代表输入数组 [1,8,6,2,5,4,8,3,7]。在此情况下，容器能够容纳水（表示为蓝色部分）的最大值为 49。 示例 2： 输入：height = [1,1] 输出：1 提示： n == height.length 2 \u003c= n \u003c= 105 0 \u003c= height[i] \u003c= 104 解法 双指针解法 ","date":"2023-01-08","objectID":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/:67:0","tags":["leetcode"],"title":"Leetcode每日一题","uri":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/"},{"categories":["数据结构与算法"],"content":"day67 45. 跳跃游戏 II 题目 给你一个非负整数数组 nums ，你最初位于数组的第一个位置。 数组中的每个元素代表你在该位置可以跳跃的最大长度。 你的目标是使用最少的跳跃次数到达数组的最后一个位置。 假设你总是可以到达数组的最后一个位置。 示例 1: 输入: nums = [2,3,1,1,4] 输出: 2 解释: 跳到最后一个位置的最小跳跃数是 2。 从下标为 0 跳到下标为 1 的位置，跳 1 步，然后跳 3 步到达数组的最后一个位置。 示例 2: 输入: nums = [2,3,0,1,4] 输出: 2 提示: 1 \u003c= nums.length \u003c= 104 0 \u003c= nums[i] \u003c= 1000 解法 两种贪心策略解决：一种是反向查找出发位置；另一种是正向查找可达到的最大位置。 Leecode官方题解 ","date":"2023-01-08","objectID":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/:68:0","tags":["leetcode"],"title":"Leetcode每日一题","uri":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/"},{"categories":["数据结构与算法"],"content":"day68 53. 最大子数组和 题目 给你一个整数数组 nums ，请你找出一个具有最大和的连续子数组（子数组最少包含一个元素），返回其最大和。 子数组 是数组中的一个连续部分。 示例 1： 输入：nums = [-2,1,-3,4,-1,2,1,-5,4] 输出：6 解释：连续子数组 [4,-1,2,1] 的和最大，为 6 。 示例 2： 输入：nums = [1] 输出：1 示例 3： 输入：nums = [5,4,-1,7,8] 输出：23 提示： 1 \u003c= nums.length \u003c= 105 -104 \u003c= nums[i] \u003c= 104 **进阶：**如果你已经实现复杂度为 O(n) 的解法，尝试使用更为精妙的 分治法 求解。 解法 官方题解：贪心、动态规划、分治 贪心： public int maxSubArray(int[] nums) { int maxSum = Integer.MIN_VALUE;//最大和 int thisSum = 0;//当前和 int len = nums.length; for(int i = 0; i \u003c len; i++) { thisSum += nums[i]; if(maxSum \u003c thisSum) { maxSum = thisSum; } //如果当前和小于0则归零，因为对于后面的元素来说这些是减小的。于是归零，意即从此处算开始最大和 if(thisSum \u003c 0) { thisSum = 0; } } return maxSum; } ","date":"2023-01-08","objectID":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/:69:0","tags":["leetcode"],"title":"Leetcode每日一题","uri":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/"},{"categories":["数据结构与算法"],"content":"day69 55. 跳跃游戏 题目 给定一个非负整数数组 nums ，你最初位于数组的 第一个下标 。 数组中的每个元素代表你在该位置可以跳跃的最大长度。 判断你是否能够到达最后一个下标。 示例 1： 输入：nums = [2,3,1,1,4] 输出：true 解释：可以先跳 1 步，从下标 0 到达下标 1, 然后再从下标 1 跳 3 步到达最后一个下标。 示例 2： 输入：nums = [3,2,1,0,4] 输出：false 解释：无论怎样，总会到达下标为 3 的位置。但该下标的最大跳跃长度是 0 ， 所以永远不可能到达最后一个下标。 提示： 1 \u003c= nums.length \u003c= 3 * 104 0 \u003c= nums[i] \u003c= 105 解法 解法一：贪心 官方题解 解法一：反向遍历 不断地从最后一个位置开始往前找能到达当前位置的索引，然后再往前找。直到找到第一个位置可以到达为止 func canJump(nums []int) bool { if len(nums)\u003c=1{ return true } if nums[0] == 0 { return false } idx:=len(nums)-1 for i:=len(nums)-2;i\u003e=0;i--{ if i+nums[i]\u003e=idx{ // fmt.Println(idx,nums[idx]) idx = i } } return idx == 0 } ","date":"2023-01-08","objectID":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/:70:0","tags":["leetcode"],"title":"Leetcode每日一题","uri":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/"},{"categories":["数据结构与算法"],"content":"day70 56. 合并区间 题目 以数组 intervals 表示若干个区间的集合，其中单个区间为 intervals[i] = [starti, endi] 。请你合并所有重叠的区间，并返回 一个不重叠的区间数组，该数组需恰好覆盖输入中的所有区间 。 示例 1： 输入：intervals = [[1,3],[2,6],[8,10],[15,18]] 输出：[[1,6],[8,10],[15,18]] 解释：区间 [1,3] 和 [2,6] 重叠, 将它们合并为 [1,6]. 示例 2： 输入：intervals = [[1,4],[4,5]] 输出：[[1,5]] 解释：区间 [1,4] 和 [4,5] 可被视为重叠区间。 提示： 1 \u003c= intervals.length \u003c= 104 intervals[i].length == 2 0 \u003c= starti \u003c= endi \u003c= 104 解法 总的来说，就是找规律 官方题解 func merge(intervals [][]int) [][]int { sort.Slice(intervals,func(i,j int) bool{ return intervals[i][0]\u003cintervals[j][0] }) merged := [][]int{{intervals[0][0],intervals[0][1]}} for i:=1;i\u003clen(intervals);i++{ if intervals[i][0]\u003emerged[len(merged)-1][1]{ merged = append(merged,intervals[i]) }else if merged[len(merged)-1][1]\u003cintervals[i][1] { merged[len(merged)-1][1] = intervals[i][1] } } return merged } ","date":"2023-01-08","objectID":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/:71:0","tags":["leetcode"],"title":"Leetcode每日一题","uri":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/"},{"categories":["数据结构与算法"],"content":"day71 121. 买卖股票的最佳时机 题目 给定一个数组 prices ，它的第 i 个元素 prices[i] 表示一支给定股票第 i 天的价格。 你只能选择 某一天 买入这只股票，并选择在 未来的某一个不同的日子 卖出该股票。设计一个算法来计算你所能获取的最大利润。 返回你可以从这笔交易中获取的最大利润。如果你不能获取任何利润，返回 0 。 示例 1： 输入：[7,1,5,3,6,4] 输出：5 解释：在第 2 天（股票价格 = 1）的时候买入，在第 5 天（股票价格 = 6）的时候卖出，最大利润 = 6-1 = 5 。 注意利润不能是 7-1 = 6, 因为卖出价格需要大于买入价格；同时，你不能在买入前卖出股票。 示例 2： 输入：prices = [7,6,4,3,1] 输出：0 解释：在这种情况下, 没有交易完成, 所以最大利润为 0。 提示： 1 \u003c= prices.length \u003c= 105 0 \u003c= prices[i] \u003c= 104 解法 暴力\\动态规划(记录历史最小值) 官方题解 ","date":"2023-01-08","objectID":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/:72:0","tags":["leetcode"],"title":"Leetcode每日一题","uri":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/"},{"categories":["数据结构与算法"],"content":"day72 122. 买卖股票的最佳时机 II 题目 给你一个整数数组 prices ，其中 prices[i] 表示某支股票第 i 天的价格。 在每一天，你可以决定是否购买和/或出售股票。你在任何时候 最多 只能持有 一股 股票。你也可以先购买，然后在 同一天 出售。 返回 你能获得的 最大 利润 。 示例 1： 输入：prices = [7,1,5,3,6,4] 输出：7 解释：在第 2 天（股票价格 = 1）的时候买入，在第 3 天（股票价格 = 5）的时候卖出, 这笔交易所能获得利润 = 5 - 1 = 4 。 随后，在第 4 天（股票价格 = 3）的时候买入，在第 5 天（股票价格 = 6）的时候卖出, 这笔交易所能获得利润 = 6 - 3 = 3 。 总利润为 4 + 3 = 7 。 示例 2： 输入：prices = [1,2,3,4,5] 输出：4 解释：在第 1 天（股票价格 = 1）的时候买入，在第 5 天 （股票价格 = 5）的时候卖出, 这笔交易所能获得利润 = 5 - 1 = 4 。 总利润为 4 。 示例 3： 输入：prices = [7,6,4,3,1] 输出：0 解释：在这种情况下, 交易无法获得正利润，所以不参与交易可以获得最大利润，最大利润为 0 。 题解 贪心\\动态规划 官方题解 // dp class Solution { public int maxProfit(int[] prices) { // dp[i][0]表示第i天不持有股票的最大利润，dp[i][1]表示第i天持有股票的最大利润 int[][] dp = new int[prices.length][2]; // 初始化 dp[0][0] = 0; dp[0][1] = -prices[0]; // 状态转移 for(int i=1;i\u003cprices.length;i++){ // 不持有股票的利润，应该是前一天不持有股票和持有股票但是今天要卖出的最大利润 dp[i][0] = Math.max(dp[i-1][0],dp[i-1][1]+prices[i]); // 持有股票的利润，应该是前一天不持有股票，今天买入的和前一天持有股票的最大利润 dp[i][1] = Math.max(dp[i-1][0]-prices[i],dp[i-1][1]); } return dp[prices.length-1][0]; } } // dp 优化 class Solution { public int maxProfit(int[] prices) { // 初始化 int dp0 = 0; int dp1 = -prices[0]; // 状态转移 for(int i=1;i\u003cprices.length;i++){ // 不持有股票的利润，应该是前一天不持有股票和持有股票但是今天要卖出的最大利润 dp0 = Math.max(dp0,dp1+prices[i]); // 持有股票的利润，应该是前一天不持有股票，今天买入的和前一天持有股票的最大利润 dp1 = Math.max(dp0-prices[i],dp1); } return dp0; } } 例如：7,1,2,5,4,6,4 找局部递增序列，局部递增序列最小值买入，最大值卖出 先找最小值，再找较大值，遇到小的值就结束，找下一个最小值 由于递增序列内的数字序列：5-2+2-1 = 5-1 所以，我们可以再遇到一个正的利润就可以直接把这个利润加上即可，当然，也可以找出递增区间的最值的差值就是利润 class Solution { public int maxProfit(int[] prices) { int min = Integer.MAX_VALUE;// 最小值 int w = 0;// 利润 for(int i=0;i\u003cprices.length;i++){ if(prices[i]-min\u003e0){ w += prices[i]-min; min = Integer.MAX_VALUE; // 重置 } if(min\u003eprices[i]){ min = prices[i]; } } return w; } } ","date":"2023-01-08","objectID":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/:73:0","tags":["leetcode"],"title":"Leetcode每日一题","uri":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/"},{"categories":["数据结构与算法"],"content":"day73 134. 加油站 题目 在一条环路上有 n 个加油站，其中第 i 个加油站有汽油 gas[i] 升。 你有一辆油箱容量无限的的汽车，从第 i 个加油站开往第 i+1 个加油站需要消耗汽油 cost[i] 升。你从其中的一个加油站出发，开始时油箱为空。 给定两个整数数组 gas 和 cost ，如果你可以绕环路行驶一周，则返回出发时加油站的编号，否则返回 -1 。如果存在解，则 保证 它是 唯一 的。 示例 1: 输入: gas = [1,2,3,4,5], cost = [3,4,5,1,2] 输出: 3 解释: 从 3 号加油站(索引为 3 处)出发，可获得 4 升汽油。此时油箱有 = 0 + 4 = 4 升汽油 开往 4 号加油站，此时油箱有 4 - 1 + 5 = 8 升汽油 开往 0 号加油站，此时油箱有 8 - 2 + 1 = 7 升汽油 开往 1 号加油站，此时油箱有 7 - 3 + 2 = 6 升汽油 开往 2 号加油站，此时油箱有 6 - 4 + 3 = 5 升汽油 开往 3 号加油站，你需要消耗 5 升汽油，正好足够你返回到 3 号加油站。 因此，3 可为起始索引。 示例 2: 输入: gas = [2,3,4], cost = [3,4,3] 输出: -1 解释: 你不能从 0 号或 1 号加油站出发，因为没有足够的汽油可以让你行驶到下一个加油站。 我们从 2 号加油站出发，可以获得 4 升汽油。 此时油箱有 = 0 + 4 = 4 升汽油 开往 0 号加油站，此时油箱有 4 - 3 + 2 = 3 升汽油 开往 1 号加油站，此时油箱有 3 - 3 + 3 = 3 升汽油 你无法返回 2 号加油站，因为返程需要消耗 4 升汽油，但是你的油箱只有 3 升汽油。 因此，无论怎样，你都不可能绕环路行驶一周。 提示: gas.length == n cost.length == n 1 \u003c= n \u003c= 105 0 \u003c= gas[i], cost[i] \u003c= 104 解法 数学证明，降低时间复杂度 ","date":"2023-01-08","objectID":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/:74:0","tags":["leetcode"],"title":"Leetcode每日一题","uri":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/"},{"categories":["数据结构与算法"],"content":"day74 135. 分发糖果 题目 n 个孩子站成一排。给你一个整数数组 ratings 表示每个孩子的评分。 你需要按照以下要求，给这些孩子分发糖果： 每个孩子至少分配到 1 个糖果。 相邻两个孩子评分更高的孩子会获得更多的糖果。 请你给每个孩子分发糖果，计算并返回需要准备的 最少糖果数目 。 示例 1： 输入：ratings = [1,0,2] 输出：5 解释：你可以分别给第一个、第二个、第三个孩子分发 2、1、2 颗糖果。 示例 2： 输入：ratings = [1,2,2] 输出：4 解释：你可以分别给第一个、第二个、第三个孩子分发 1、2、1 颗糖果。 第三个孩子只得到 1 颗糖果，这满足题面中的两个条件。 提示： n == ratings.length 1 \u003c= n \u003c= 2 * 104 0 \u003c= ratings[i] \u003c= 2 * 104 解法 方法一：两次遍历 我们可以将「相邻的孩子中，评分高的孩子必须获得更多的糖果」这句话拆分为两个规则，分别处理。 左规则：当 $\\textit{ratings}[i - 1] \u003c \\textit{ratings}[i]$ 时，i 号学生的糖果数量将比 i - 1 号孩子的糖果数量多。 右规则：当 $\\textit{ratings}[i] \u003e \\textit{ratings}[i + 1]$ 时，i 号学生的糖果数量将比 i + 1 号孩子的糖果数量多。 我们遍历该数组两次，处理出每一个学生分别满足左规则或右规则时，最少需要被分得的糖果数量。每个人最终分得的糖果数量即为这两个数量的最大值。 具体地，以左规则为例：我们从左到右遍历该数组，假设当前遍历到位置 i，如果有 $\\textit{ratings}[i - 1] \u003c \\textit{ratings}[i]$ 那么 i 号学生的糖果数量将比 i - 1 号孩子的糖果数量多，我们令 $\\textit{left}[i] = \\textit{left}[i - 1] + 1$ 即可，否则我们令 $\\textit{left}[i] = 1$。 在实际代码中，我们先计算出左规则 $\\textit{left}$ 数组，在计算右规则的时候只需要用单个变量记录当前位置的右规则，同时计算答案即可。 // 形式一 class Solution { public int candy(int[] ratings) { // 每人先发一个糖果 int sum = ratings.length; // 挨个比较相邻评分的大小，分高者糖果数+1 int[] matrix = new int[ratings.length]; for(int i=1;i\u003cratings.length;i++){ if(ratings[i]\u003eratings[i-1]\u0026\u0026matrix[i]\u003c=matrix[i-1]){ matrix[i]=matrix[i-1]+1; } } // System.out.println(Arrays.toString(matrix)); for(int i=ratings.length-2;i\u003e=0;i--){ if(ratings[i]\u003eratings[i+1]\u0026\u0026matrix[i]\u003c=matrix[i+1]){ matrix[i]=matrix[i+1]+1; } } // System.out.println(Arrays.toString(matrix)); for(int i=0;i\u003cmatrix.length;i++){sum+=matrix[i];} return sum; } } // 形式二 class Solution { public int candy(int[] ratings) { int n = ratings.length; int[] left = new int[n]; for (int i = 0; i \u003c n; i++) { if (i \u003e 0 \u0026\u0026 ratings[i] \u003e ratings[i - 1]) { left[i] = left[i - 1] + 1; } else { left[i] = 1; } } int right = 0, ret = 0; for (int i = n - 1; i \u003e= 0; i--) { if (i \u003c n - 1 \u0026\u0026 ratings[i] \u003e ratings[i + 1]) { right++; } else { right = 1; } ret += Math.max(left[i], right); } return ret; } } 方法二：常数空间遍历 https://leetcode.cn/problems/candy/solution/fen-fa-tang-guo-by-leetcode-solution-f01p/ ","date":"2023-01-08","objectID":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/:75:0","tags":["leetcode"],"title":"Leetcode每日一题","uri":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/"},{"categories":["数据结构与算法"],"content":"day75 376. 摆动序列 题目 如果连续数字之间的差严格地在正数和负数之间交替，则数字序列称为 **摆动序列 。**第一个差（如果存在的话）可能是正数或负数。仅有一个元素或者含两个不等元素的序列也视作摆动序列。 例如， [1, 7, 4, 9, 2, 5] 是一个 摆动序列 ，因为差值 (6, -3, 5, -7, 3) 是正负交替出现的。 相反，[1, 4, 7, 2, 5] 和 [1, 7, 4, 5, 5] 不是摆动序列，第一个序列是因为它的前两个差值都是正数，第二个序列是因为它的最后一个差值为零。 子序列 可以通过从原始序列中删除一些（也可以不删除）元素来获得，剩下的元素保持其原始顺序。 给你一个整数数组 nums ，返回 nums 中作为 摆动序列 的 最长子序列的长度 。 示例 1： 输入：nums = [1,7,4,9,2,5] 输出：6 解释：整个序列均为摆动序列，各元素之间的差值为 (6, -3, 5, -7, 3) 。 示例 2： 输入：nums = [1,17,5,10,13,15,10,5,16,8] 输出：7 解释：这个序列包含几个长度为 7 摆动序列。 其中一个是 [1, 17, 10, 13, 10, 16, 8] ，各元素之间的差值为 (16, -7, 3, -3, 6, -8) 。 示例 3： 输入：nums = [1,2,3,4,5,6,7,8,9] 输出：2 提示： 1 \u003c= nums.length \u003c= 1000 0 \u003c= nums[i] \u003c= 1000 **进阶：**你能否用 O(n) 时间复杂度完成此题? 解法 动态规划\\贪心算法 class Solution { public int wiggleMaxLength(int[] nums) { // 顺序遍历，“峰”才算上：一旦遇到非递增数据或者非递减数据，就算做最长子序列的一个元素 if(nums.length==1){ return 1; }else if(nums.length==2\u0026\u0026nums[0]!=nums[1]){ return 2; } int ans = 1; int state = 0; //表示先前的状态：0表示前面的两个数差为0，1表示差为正，-1表示差为负 // 只需计数“峰”的数量即可 for(int i=1;i\u003cnums.length;i++){ if(nums[i]\u003cnums[i-1]\u0026\u0026state==1){ ans++; state = -1; }else if(nums[i]\u003enums[i-1]\u0026\u0026state==-1){ ans++; state = 1; }else if(nums[i]\u003enums[i-1]\u0026\u0026state==0){ ans++; state = 1; }else if(nums[i]\u003cnums[i-1]\u0026\u0026state==0){ ans++; state = -1; } } return ans; } } ","date":"2023-01-08","objectID":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/:76:0","tags":["leetcode"],"title":"Leetcode每日一题","uri":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/"},{"categories":["数据结构与算法"],"content":"day76 409. 最长回文串 题目 给定一个包含大写字母和小写字母的字符串 s ，返回 通过这些字母构造成的 最长的回文串 。 在构造过程中，请注意 区分大小写 。比如 \"Aa\" 不能当做一个回文字符串。 示例 1: 输入:s = \"abccccdd\" 输出:7 解释: 我们可以构造的最长的回文串是\"dccaccd\", 它的长度是 7。 示例 2: 输入:s = \"a\" 输入:1 示例 3： 输入:s = \"aaaaaccc\" 输入:7 提示: 1 \u003c= s.length \u003c= 2000 s 只由小写 和/或 大写英文字母组成 解法 解法一：数学规律 题目要求仅仅只是需要根据给出的字符序列构造出最长回文串序列，回文串的长度如果是偶数那么前一半字符序列是后一半字符序列的“镜像”；如果是奇数，那么中间会有一个字符作为分隔，左右两边是“镜像”关系。针对“镜像”字符序列，也就是说，一个字符重复n次，如果n是偶数，那么可以将这些字符一半放在左边，一半放在右边；如果是奇数，那么可以去掉一个字符，然后将一半放左边，一半放右边。如果给的字符序列里存在奇数个重复字符，都可以作为“镜像”序列的地中情况；如果都是偶数，那么只能构造第一个“镜像”字符序列。 class Solution { public int longestPalindrome(String s) { if(s.length()==1) return 1; // 先字典排序 char[] chs = s.toCharArray(); Arrays.sort(chs); // System.out.println(Arrays.toString(chs)); char pre = chs[0]; int count = 1; int[] counts = new int[chs.length]; int countIdx = 0; for(int i=1;i\u003cchs.length;i++){ if(chs[i]==pre){ count++; }else{ // 暂存 counts[countIdx] = count; countIdx++; count=1; pre = chs[i]; } } counts[countIdx]=count; // System.out.println(Arrays.toString(counts)); int Len = 1; boolean flag = false; for(int i=0;i\u003ccounts.length\u0026\u0026counts[i]!=0;i++){ if(counts[i]%2==0){ Len+=counts[i]; }else{ if(counts[i]\u003e1){ Len+=counts[i]-1; } flag=true; } } return flag?Len:Len-1; } } 解法二：官方解法 https://leetcode.cn/problems/longest-palindrome/solution/zui-chang-hui-wen-chuan-by-leetcode-solution/ ","date":"2023-01-08","objectID":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/:77:0","tags":["leetcode"],"title":"Leetcode每日一题","uri":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/"},{"categories":["数据结构与算法"],"content":"day77 435. 无重叠区间 题目 给定一个区间的集合 intervals ，其中 intervals[i] = [starti, endi] 。返回 需要移除区间的最小数量，使剩余区间互不重叠 。 示例 1: 输入: intervals = [[1,2],[2,3],[3,4],[1,3]] 输出: 1 解释: 移除 [1,3] 后，剩下的区间没有重叠。 示例 2: 输入: intervals = [ [1,2], [1,2], [1,2] ] 输出: 2 解释: 你需要移除两个 [1,2] 来使剩下的区间没有重叠。 示例 3: 输入: intervals = [ [1,2], [2,3] ] 输出: 0 解释: 你不需要移除任何区间，因为它们已经是无重叠的了。 提示: 1 \u003c= intervals.length \u003c= 105 intervals[i].length == 2 -5 * 104 \u003c= starti \u003c endi \u003c= 5 * 104 解法 动态规划\\贪心 ","date":"2023-01-08","objectID":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/:78:0","tags":["leetcode"],"title":"Leetcode每日一题","uri":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/"},{"categories":["数据结构与算法"],"content":"day78 452. 用最少数量的箭引爆气球 题目 有一些球形气球贴在一堵用 XY 平面表示的墙面上。墙面上的气球记录在整数数组 points ，其中points[i] = [xstart, xend] 表示水平直径在 xstart 和 xend之间的气球。你不知道气球的确切 y 坐标。 一支弓箭可以沿着 x 轴从不同点 完全垂直 地射出。在坐标 x 处射出一支箭，若有一个气球的直径的开始和结束坐标为 x``start，x``end， 且满足 xstart ≤ x ≤ x``end，则该气球会被 引爆 。可以射出的弓箭的数量 没有限制 。 弓箭一旦被射出之后，可以无限地前进。 给你一个数组 points ，返回引爆所有气球所必须射出的 最小 弓箭数 。 示例 1： 输入：points = [[10,16],[2,8],[1,6],[7,12]] 输出：2 解释：气球可以用2支箭来爆破: -在x = 6处射出箭，击破气球[2,8]和[1,6]。 -在x = 11处发射箭，击破气球[10,16]和[7,12]。 示例 2： 输入：points = [[1,2],[3,4],[5,6],[7,8]] 输出：4 解释：每个气球需要射出一支箭，总共需要4支箭。 示例 3： 输入：points = [[1,2],[2,3],[3,4],[4,5]] 输出：2 解释：气球可以用2支箭来爆破: - 在x = 2处发射箭，击破气球[1,2]和[2,3]。 - 在x = 4处射出箭，击破气球[3,4]和[4,5]。 提示: 1 \u003c= points.length \u003c= 105 points[i].length == 2 -231 \u003c= xstart \u003c xend \u003c= 231 - 1 解法 注意给出的数据有大数，所以涉及到运算，不能直接向加减乘除等。 解法一：暴力 问题可以看成是：我们删除一些重叠的区间，之后再统计这些没有删除的区间 class Solution { public int findMinArrowShots(int[][] points) { if(points.length==1) return 1; // 按照右端点排序 Arrays.sort(points, new Comparator\u003cint[]\u003e() { public int compare(int[] point1, int[] point2) { if (point1[1] \u003e point2[1]) { return 1; } else if (point1[1] \u003c point2[1]) { return -1; } else { return 0; } } }); // 记录是否删除区间 boolean[] isDel = new boolean[points.length]; int ans = 0; // 基准区间的左右值 for(int i=0;i\u003cpoints.length;i++){ if(!isDel[i]){ int pivotLeft = points[i][0]; int pivotRight = points[i][1]; for(int j=i+1;j\u003cpoints.length;j++){ // 当前区间还在 if(!isDel[j]){ if(points[j][0]\u003epivotRight){} else{ isDel[j] = true; } } } } } for(int i=0;i\u003cisDel.length;i++){ if(!isDel[i]){ ans++; } } return ans; } } 解法二：排序+贪心 ","date":"2023-01-08","objectID":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/:79:0","tags":["leetcode"],"title":"Leetcode每日一题","uri":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/"},{"categories":["数据结构与算法"],"content":"day79 455. 分发饼干 题目 假设你是一位很棒的家长，想要给你的孩子们一些小饼干。但是，每个孩子最多只能给一块饼干。 对每个孩子 i，都有一个胃口值 g[i]，这是能让孩子们满足胃口的饼干的最小尺寸；并且每块饼干 j，都有一个尺寸 s[j] 。如果 s[j] \u003e= g[i]，我们可以将这个饼干 j 分配给孩子 i ，这个孩子会得到满足。你的目标是尽可能满足越多数量的孩子，并输出这个最大数值。 示例 1: 输入: g = [1,2,3], s = [1,1] 输出: 1 解释: 你有三个孩子和两块小饼干，3个孩子的胃口值分别是：1,2,3。 虽然你有两块小饼干，由于他们的尺寸都是1，你只能让胃口值是1的孩子满足。 所以你应该输出1。 示例 2: 输入: g = [1,2], s = [1,2,3] 输出: 2 解释: 你有两个孩子和三块小饼干，2个孩子的胃口值分别是1,2。 你拥有的饼干数量和尺寸都足以让所有孩子满足。 所以你应该输出2. 提示： 1 \u003c= g.length \u003c= 3 * 104 0 \u003c= s.length \u003c= 3 * 104 1 \u003c= g[i], s[j] \u003c= 231 - 1 解法 排序+双指针+贪心 ","date":"2023-01-08","objectID":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/:80:0","tags":["leetcode"],"title":"Leetcode每日一题","uri":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/"},{"categories":["数据结构与算法"],"content":"day80 605. 种花问题 题目 假设有一个很长的花坛，一部分地块种植了花，另一部分却没有。可是，花不能种植在相邻的地块上，它们会争夺水源，两者都会死去。 给你一个整数数组 flowerbed 表示花坛，由若干 0 和 1 组成，其中 0 表示没种植花，1 表示种植了花。另有一个数 n ，能否在不打破种植规则的情况下种入 n 朵花？能则返回 true ，不能则返回 false。 示例 1： 输入：flowerbed = [1,0,0,0,1], n = 1 输出：true 示例 2： 输入：flowerbed = [1,0,0,0,1], n = 2 输出：false 提示： 1 \u003c= flowerbed.length \u003c= 2 * 104 flowerbed[i] 为 0 或 1 flowerbed 中不存在相邻的两朵花 0 \u003c= n \u003c= flowerbed.length 解法 解法一：模拟 触雷首尾位置需要特殊处理，其余位置就是比较前一个和后一个是否都没种花，如果都没种花，再看当前位置是否种花，如果没，就种花，否则就需要迭代下一个花坛位置 class Solution { public boolean canPlaceFlowers(int[] flowerbed, int n) { if(flowerbed.length==1){ if(flowerbed[0]==1) return n==0; else return n\u003c=1; } for(int i=0;i\u003cflowerbed.length;i++){ // 当碰到收尾时，需要收敛比较 if(i==0){ if(flowerbed[i]!=flowerbed[i+1]){}else { if(n==0) break; flowerbed[i]=1; n--; } }else if(i==flowerbed.length-1){ if(flowerbed[i]!=flowerbed[i-1]){}else { if(n==0) break; flowerbed[i]=1; n--; } }else{ if(flowerbed[i]!=flowerbed[i-1]||flowerbed[i]!=flowerbed[i+1]){} else { if(n==0) break; flowerbed[i]=1; n--; } } } // System.out.println(Arrays.toString(flowerbed)); return n==0; } } 解法二：贪心 https://leetcode.cn/problems/can-place-flowers/solution/chong-hua-wen-ti-by-leetcode-solution-sojr/ ","date":"2023-01-08","objectID":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/:81:0","tags":["leetcode"],"title":"Leetcode每日一题","uri":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/"},{"categories":["数据结构与算法"],"content":"day81 738. 单调递增的数字 题目 当且仅当每个相邻位数上的数字 x 和 y 满足 x \u003c= y 时，我们称这个整数是单调递增的。 给定一个整数 n ，返回 小于或等于 n 的最大数字，且数字呈 单调递增 。 示例 1: 输入: n = 10 输出: 9 示例 2: 输入: n = 1234 输出: 1234 示例 3: 输入: n = 332 输出: 299 提示: 0 \u003c= n \u003c= 109 解法 解法一：数学规律 我们只需要找到单调递增的最后一个数位上的数字，如果该数字是最后一个，那么整体都满足单调递增特性直接返回；如果不是，那么当前数位和前面的数位组成的数字减去1，再把后面的数位全部变成数字9，之后前面和后面凭借在一起返回即可 class Solution { public int monotoneIncreasingDigits(int n) { // 取位数 int Len = 0; int tmp = n; while(tmp!=0){ tmp=tmp/10; Len++; } // System.out.println(\"位数\"+Len); // 如果是一位直接返回结果 if(Len==1) return n; // 其他位数，循环处理 tmp = n; // 保证单调递增 int preMax = 0; int curMax = 0; int index = 0; while(index\u003cLen){ // 取出当前位 curMax = tmp/(int)Math.pow(10,Len-index-1)%10; // System.out.println(\"preMax=\"+preMax+\"curMax=\"+curMax); if(curMax\u003cpreMax){ // 往前回溯分割点 int i = index-1; while(i\u003e=0){ if(tmp/(int)Math.pow(10,Len-i-1)%10!=preMax){ break; } i--; } i++; // System.out.println(\"回溯次数=\"+i); // 将该位及以前的数据减去（该位数字+1），再和后面等位数的数字9拼接 // int lastCast = tmp/(int)Math.pow(10,Len-i-1); // System.out.println(\"lastCast=\"+lastCast); int front = tmp/(int)Math.pow(10,Len-i-1)-1; int back = 0; int j=Len-i-1; while(j\u003e0){ back=back*10+9; j--; } // System.out.println(\"front=\"+front+\"back=\"+back); return front*((int)Math.pow(10,Len-i-1))+back; } preMax = curMax; index++; } return n; } } 解法二：贪心 https://leetcode.cn/problems/monotone-increasing-digits/solution/dan-diao-di-zeng-de-shu-zi-by-leetcode-s-5908/ ","date":"2023-01-08","objectID":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/:82:0","tags":["leetcode"],"title":"Leetcode每日一题","uri":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/"},{"categories":["数据结构与算法"],"content":"day82 1005. K 次取反后最大化的数组和 题目 给你一个整数数组 nums 和一个整数 k ，按以下方法修改该数组： 选择某个下标 i 并将 nums[i] 替换为 -nums[i] 。 重复这个过程恰好 k 次。可以多次选择同一个下标 i 。 以这种方式修改数组后，返回数组 可能的最大和 。 示例 1： 输入：nums = [4,2,3], k = 1 输出：5 解释：选择下标 1 ，nums 变为 [4,-2,3] 。 示例 2： 输入：nums = [3,-1,0,2], k = 3 输出：6 解释：选择下标 (1, 2, 2) ，nums 变为 [3,1,0,2] 。 示例 3： 输入：nums = [2,-3,-1,5,-4], k = 2 输出：13 解释：选择下标 (1, 4) ，nums 变为 [2,3,-1,5,4] 。 提示： 1 \u003c= nums.length \u003c= 104 -100 \u003c= nums[i] \u003c= 100 1 \u003c= k \u003c= 104 解法 解法一：寻找数学规律 // 代码一 class Solution { public int largestSumAfterKNegations(int[] nums, int k) { Arrays.sort(nums); for(int i=0;i\u003cnums.length;i++){ if(k\u003c=0) break; if(nums[i]\u003c0) { nums[i]*=-1; k--; } } // 可以通过找最值，省去第二次排序懒得写了 Arrays.sort(nums); int sum = 0; // System.out.println(Arrays.toString(nums)); // 找和 for(int i=0;i\u003cnums.length;i++){ sum+=nums[i]; } return k%2==1?sum-2*nums[0]:sum; } } // 官方解法 class Solution { public int largestSumAfterKNegations(int[] nums, int k) { Map\u003cInteger, Integer\u003e freq = new HashMap\u003cInteger, Integer\u003e(); for (int num : nums) { freq.put(num, freq.getOrDefault(num, 0) + 1); } int ans = Arrays.stream(nums).sum(); for (int i = -100; i \u003c 0; ++i) { if (freq.containsKey(i)) { int ops = Math.min(k, freq.get(i)); ans += (-i) * ops * 2; freq.put(i, freq.get(i) - ops); freq.put(-i, freq.getOrDefault(-i, 0) + ops); k -= ops; if (k == 0) { break; } } } if (k \u003e 0 \u0026\u0026 k % 2 == 1 \u0026\u0026 !freq.containsKey(0)) { for (int i = 1; i \u003c= 100; ++i) { if (freq.containsKey(i)) { ans -= i * 2; break; } } } return ans; } } ","date":"2023-01-08","objectID":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/:83:0","tags":["leetcode"],"title":"Leetcode每日一题","uri":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/"},{"categories":["数据结构与算法"],"content":"day83 1013. 将数组分成和相等的三个部分 题目 给你一个整数数组 arr，只有可以将其划分为三个和相等的 非空 部分时才返回 true，否则返回 false。 形式上，如果可以找出索引 i + 1 \u003c j 且满足 (arr[0] + arr[1] + ... + arr[i] == arr[i + 1] + arr[i + 2] + ... + arr[j - 1] == arr[j] + arr[j + 1] + ... + arr[arr.length - 1]) 就可以将数组三等分。 示例 1： 输入：arr = [0,2,1,-6,6,-7,9,1,2,0,1] 输出：true 解释：0 + 2 + 1 = -6 + 6 - 7 + 9 + 1 = 2 + 0 + 1 示例 2： 输入：arr = [0,2,1,-6,6,7,9,-1,2,0,1] 输出：false 示例 3： 输入：arr = [3,3,6,5,-2,2,5,1,-9,4] 输出：true 解释：3 + 3 = 6 = 5 - 2 + 2 + 5 + 1 - 9 + 4 提示： 3 \u003c= arr.length \u003c= 5 * 104 -104 \u003c= arr[i] \u003c= 104 解法 解法一：暴力法 题目要求求出是否存在三个相等的划分。显而易见，我们可以暴力。 class Solution { public boolean canThreePartsEqualSum(int[] arr) { // 先放第一个挡板 for(int i=0;i\u003carr.length-2;i++){ // 继续放第二个挡板 for(int j=i+1;j\u003carr.length-1;j++){ int firstPart = sum(0,i,arr); // System.out.println(firstPart+\"-\"+sum(i+1,j,arr)+\"-\"+sum(j+1,arr.length-1,arr)); if(firstPart==sum(i+1,j,arr)){ if(firstPart==sum(j+1,arr.length-1,arr)){ return true; } } } } return false; } public int sum(int l,int r,int[] arr){ int sum=0; for(;l\u003c=r;l++){ sum+=arr[l]; } return sum; } } 遗憾的是，暴力算法的时间复杂度为O(n^3^)。问题出现在，每次求和都需要重新计算和。可以使用前缀和的方式优化一下算法 解法二：前缀和优化 class Solution { public boolean canThreePartsEqualSum(int[] arr) { int sum = 0; for(int i=0;i\u003carr.length;i++){ sum+=arr[i]; } int firstSum = 0; int secondSum = 0; // 先放第一个挡板 for(int i=0;i\u003carr.length-2;i++){ firstSum +=arr[i]; // 继续放第二个挡板 for(int j=i+1;j\u003carr.length-1;j++){ secondSum += arr[j]; // System.out.println(firstSum+\"-\"+secondSum+\"-\"+(sum-firstSum-secondSum)); if(firstSum==secondSum){ if(firstSum==sum-firstSum-secondSum){ return true; } } } secondSum=0; } return false; } } 解法三：贪心 发现数学规律，寻找切分点 ","date":"2023-01-08","objectID":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/:84:0","tags":["leetcode"],"title":"Leetcode每日一题","uri":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/"},{"categories":["数据结构与算法"],"content":"day84 392. 判断子序列 题目 给定字符串 s 和 t ，判断 s 是否为 t 的子序列。 字符串的一个子序列是原始字符串删除一些（也可以不删除）字符而不改变剩余字符相对位置形成的新字符串。（例如，\"ace\"是\"abcde\"的一个子序列，而\"aec\"不是）。 进阶： 如果有大量输入的 S，称作 S1, S2, … , Sk 其中 k \u003e= 10亿，你需要依次检查它们是否为 T 的子序列。在这种情况下，你会怎样改变代码？ 致谢： 特别感谢 @pbrother 添加此问题并且创建所有测试用例。 示例 1： 输入：s = \"abc\", t = \"ahbgdc\" 输出：true 示例 2： 输入：s = \"axc\", t = \"ahbgdc\" 输出：false 提示： 0 \u003c= s.length \u003c= 100 0 \u003c= t.length \u003c= 10^4 两个字符串都只由小写字符组成。 解法 解法一：双指针 https://leetcode.cn/problems/is-subsequence/solution/pan-duan-zi-xu-lie-by-leetcode-solution/ 解法二：动态规划 https://leetcode.cn/problems/is-subsequence/solution/shi-pin-jiang-jie-dong-tai-gui-hua-qiu-jie-is-subs/ 比官方题解稍微好理解一点 ","date":"2023-01-08","objectID":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/:85:0","tags":["leetcode"],"title":"Leetcode每日一题","uri":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/"},{"categories":["数据结构与算法"],"content":"day85 62. 不同路径 题目 一个机器人位于一个 m x n 网格的左上角 （起始点在下图中标记为 “Start” ）。 机器人每次只能向下或者向右移动一步。机器人试图达到网格的右下角（在下图中标记为 “Finish” ）。 问总共有多少条不同的路径？ 示例 1： 输入：m = 3, n = 7 输出：28 示例 2： 输入：m = 3, n = 2 输出：3 解释： 从左上角开始，总共有 3 条路径可以到达右下角。 1. 向右 -\u003e 向下 -\u003e 向下 2. 向下 -\u003e 向下 -\u003e 向右 3. 向下 -\u003e 向右 -\u003e 向下 示例 3： 输入：m = 7, n = 3 输出：28 示例 4： 输入：m = 3, n = 3 输出：6 提示： 1 \u003c= m, n \u003c= 100 题目数据保证答案小于等于 2 * 109 解法 动态规划/组合数学 class Solution { public int uniquePaths(int m, int n) { // 定义状态：dp[i][j]表示移动到i行j列位置有多少条路径 long[][] dp = new long[m][n]; // 初始状态：第一行和第一列都只有一条路径 for(int i=0;i\u003cm;i++){ dp[i][0] = 1; } for(int i=0;i\u003cn;i++){ dp[0][i] = 1; } // 状态转移方程：dp[i][j]显然是由dp[i-1][j]或dp[i][j-1]转移而来，那么结果应该取二者之和 for(int i=1;i\u003cm;i++){ for(int j=1;j\u003cn;j++){ dp[i][j] = dp[i-1][j]+dp[i][j-1]; } } return (int)dp[m-1][n-1]; } } func uniquePaths(m int, n int) int { dp := make([][]int,m) for i:=0;i\u003cm;i++{ dp[i] = make([]int,n) dp[i][0] = 1 } for j:=0;j\u003cn;j++{ dp[0][j] = 1 } // 状态转移 for i:=1;i\u003cm;i++{ for j:=1;j\u003cn;j++{ dp[i][j] = dp[i-1][j]+dp[i][j-1] } } return dp[m-1][n-1] } ","date":"2023-01-08","objectID":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/:86:0","tags":["leetcode"],"title":"Leetcode每日一题","uri":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/"},{"categories":["数据结构与算法"],"content":"day86 63. 不同路径 II 题目 一个机器人位于一个 m x n 网格的左上角 （起始点在下图中标记为 “Start” ）。 机器人每次只能向下或者向右移动一步。机器人试图达到网格的右下角（在下图中标记为 “Finish”）。 现在考虑网格中有障碍物。那么从左上角到右下角将会有多少条不同的路径？ 网格中的障碍物和空位置分别用 1 和 0 来表示。 示例 1： 输入：obstacleGrid = [[0,0,0],[0,1,0],[0,0,0]] 输出：2 解释：3x3 网格的正中间有一个障碍物。 从左上角到右下角一共有 2 条不同的路径： 1. 向右 -\u003e 向右 -\u003e 向下 -\u003e 向下 2. 向下 -\u003e 向下 -\u003e 向右 -\u003e 向右 示例 2： 输入：obstacleGrid = [[0,1],[0,0]] 输出：1 提示： m == obstacleGrid.length n == obstacleGrid[i].length 1 \u003c= m, n \u003c= 100 obstacleGrid[i][j] 为 0 或 1 解法 解法一：动态规划 思路和上一道题一样，都满足动态规划的几个条件 官方动态规划题解 class Solution { public int uniquePathsWithObstacles(int[][] obstacleGrid) { int rowNum = obstacleGrid.length; int spanNum = obstacleGrid[0].length; int[][] dp = new int[rowNum][spanNum]; // 初始化dp for(int i=0;i\u003crowNum;i++){ if(obstacleGrid[i][0]==1){ break; } dp[i][0] = 1; } for(int i=0;i\u003cspanNum;i++){ if(obstacleGrid[0][i]==1){ break; } dp[0][i] = 1; } // System.out.println(rowNum+\"-\"+spanNum); // 状态转移 for(int i=1;i\u003crowNum;i++){ for(int j=1;j\u003cspanNum;j++){ if(obstacleGrid[i][j]==0){ dp[i][j]=dp[i-1][j]+dp[i][j-1]; // System.out.println(dp[i][j]+\"=\"+dp[i-1][j]+\"+\"+dp[i][j-1]); } } } //最后结果 return dp[rowNum-1][spanNum-1]; } } ","date":"2023-01-08","objectID":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/:87:0","tags":["leetcode"],"title":"Leetcode每日一题","uri":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/"},{"categories":["数据结构与算法"],"content":"day87 64. 最小路径和 题目 给定一个包含非负整数的 *m* x *n* 网格 grid ，请找出一条从左上角到右下角的路径，使得路径上的数字总和为最小。 **说明：**每次只能向下或者向右移动一步。 示例 1： 输入：grid = [[1,3,1],[1,5,1],[4,2,1]] 输出：7 解释：因为路径 1→3→1→1→1 的总和最小。 示例 2： 输入：grid = [[1,2,3],[4,5,6]] 输出：12 提示： m == grid.length n == grid[i].length 1 \u003c= m, n \u003c= 200 0 \u003c= grid[i][j] \u003c= 100 解法 解法一：动态规划 https://leetcode.cn/problems/minimum-path-sum/solution/zui-xiao-lu-jing-he-by-leetcode-solution/ 这题和不同的路径的状态转移方程有点类似 func minPathSum(grid [][]int) int { m:=len(grid) n:=len(grid[0]) dp := make([][]int,m) s:=0 for i:=0;i\u003cm;i++{ dp[i] = make([]int,n) dp[i][0] = s + grid[i][0] s = dp[i][0] } s=0 for j:=0;j\u003cn;j++{ dp[0][j] =s + grid[0][j] s = dp[0][j] } for i:=1;i\u003cm;i++{ for j:=1;j\u003cn;j++{ if dp[i-1][j]\u003edp[i][j-1]{ dp[i][j] = dp[i][j-1]+grid[i][j] }else{ dp[i][j] = dp[i-1][j]+grid[i][j] } } } // fmt.Println(dp) return dp[m-1][n-1] } ","date":"2023-01-08","objectID":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/:88:0","tags":["leetcode"],"title":"Leetcode每日一题","uri":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/"},{"categories":["数据结构与算法"],"content":"day88 70. 爬楼梯 题目 假设你正在爬楼梯。需要 n 阶你才能到达楼顶。 每次你可以爬 1 或 2 个台阶。你有多少种不同的方法可以爬到楼顶呢？ 示例 1： 输入：n = 2 输出：2 解释：有两种方法可以爬到楼顶。 1. 1 阶 + 1 阶 2. 2 阶 示例 2： 输入：n = 3 输出：3 解释：有三种方法可以爬到楼顶。 1. 1 阶 + 1 阶 + 1 阶 2. 1 阶 + 2 阶 3. 2 阶 + 1 阶 提示： 1 \u003c= n \u003c= 45 解法 官方题解：动态规划/矩阵快速幂/通项公式/ dp应该是最简单的 https://leetcode.cn/problems/climbing-stairs/solution/pa-lou-ti-by-leetcode-solution/ class Solution { public int climbStairs(int n) { if(n\u003c=2) return n; // 状态表示：dp[i]表示爬第i层楼梯的方法总数 int[] dp = new int[n+1]; // 状态初始化 //dp[0] = 1;// 没有意义，为了凑数,也可以置为1；也可以当作非法状态看待 dp[1] = 1;// 当爬第一层时，只有一种方法：爬一个台阶 // 状态转移方程：dp[i]是由dp[i-1]或dp[i-2]转移过来，那么显然方法总数就是前两者之和; dp[2] = 2; for(int i=3;i\u003c=n;i++){ dp[i] = dp[i-1] + dp[i-2]; } return dp[n]; } } 还可以将这个问题看成是“物品的重量分别是1和2，背包的容量是n”的完全背包问题。 二维DP解法： class Solution { public int climbStairs(int n) { if(n\u003c=2) return n; // 状态表示：dp[i][j]表示取前i物品，装下容量j的方法数，注意此处要排列数 int[][] dp = new int[3][n+1]; int[] w = new int[]{1,2}; // 状态初始化：取前i个物品装容量为0，只有一种方法，那就是什么都不取 dp[0][0]=1; // 状态转移 // 二维dp的顺序无所谓 for(int i=1;i\u003c=2;i++){ for(int j=0;j\u003c=n;j++){ if(j\u003e=w[i-1]) { // 每次可以放的时候，都是取前0-i个数的和，这样才是排列数。 // 可以画出dp转移表验证一下。 //（其实这是画表时候找到的规律。感觉是一种数学定律） for(int k=0;k\u003ci;k++){ dp[i][j] += dp[i][j-w[k]]; } } else dp[i][j] = dp[i-1][j]; } } // System.out.println(Arrays.deepToString(dp)); return dp[2][n]; } } 一维dp解法，这里的一维dp不再是优化二维得来的了。 class Solution { public int climbStairs(int n) { if(n\u003c=2) return n; // 状态表示：dp[j]表示装下容量j的方法数，注意此处要排列数 int[] dp = new int[n+1]; int[] w = new int[]{1,2}; // 状态初始化：装容量为0，只有一种方法，那就是什么都不取 dp[0]=1; // 状态转移 for(int j=0;j\u003c=n;j++){ for(int i=1;i\u003c=2;i++){ if(j\u003e=w[i-1]) { dp[j] += dp[j-w[i-1]]; } } } // System.out.println(Arrays.toString(dp)); return dp[n]; } } ","date":"2023-01-08","objectID":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/:89:0","tags":["leetcode"],"title":"Leetcode每日一题","uri":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/"},{"categories":["数据结构与算法"],"content":"day89 72. 编辑距离 题目 给你两个单词 word1 和 word2， 请返回将 word1 转换成 word2 所使用的最少操作数 。 你可以对一个单词进行如下三种操作： 插入一个字符 删除一个字符 替换一个字符 示例 1： 输入：word1 = \"horse\", word2 = \"ros\" 输出：3 解释： horse -\u003e rorse (将 'h' 替换为 'r') rorse -\u003e rose (删除 'r') rose -\u003e ros (删除 'e') 示例 2： 输入：word1 = \"intention\", word2 = \"execution\" 输出：5 解释： intention -\u003e inention (删除 't') inention -\u003e enention (将 'i' 替换为 'e') enention -\u003e exention (将 'n' 替换为 'x') exention -\u003e exection (将 'n' 替换为 'c') exection -\u003e execution (插入 'u') 提示： 0 \u003c= word1.length, word2.length \u003c= 500 word1 和 word2 由小写英文字母组成 解法 方法一：动态规划找编辑距离 好理解的图表解法 官方题解 ","date":"2023-01-08","objectID":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/:90:0","tags":["leetcode"],"title":"Leetcode每日一题","uri":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/"},{"categories":["数据结构与算法"],"content":"day90 ","date":"2023-01-08","objectID":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/:91:0","tags":["leetcode"],"title":"Leetcode每日一题","uri":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/"},{"categories":["数据结构与算法"],"content":"分解质因数 题目 问题描述 求出区间[a,b]中所有整数的质因数分解。 输入格式 输入两个整数a，b。 输出格式 每行输出一个数的分解，形如k=a1a2a3…(a1\u003c=a2\u003c=a3…，k也是从小到大的)(具体可看样例) 样例输入 3 10 样例输出 3=3 4=2*2 5=5 6=2*3 7=7 8=2*2*2 9=3*3 10=2*5 提示 先筛出所有素数，然后再分解。 数据规模和约定 2\u003c=a\u003c=b\u003c=10000 解法 思路：这道题的暴力解法思路显而易见：对区间内的每一个数都进行质因数分解。但是这样做显然做了很多的重复计算，我们可以利用空间换时间的原理，将第一次计算出来的结果保存好，这样第二次再计算时，直接拿出来用即可，这样就避免了大量的重复计算 代码演示 import java.util.*; public class Main{ public static void main(String args[]){ Scanner sc = new Scanner(System.in); int a = sc.nextInt(); int b = sc.nextInt(); String[] dict = new String[b+1]; dict[2] = \"2*\"; dict[3] = \"3*\"; // System.out.println(\"2=2\"); for(int i=2;i\u003c=b;i++) { int tmp = i; boolean flag = true; // 算质因数 for(int n=2;n\u003ci\u0026\u0026tmp\u003e=1;n++) { // 找到的第一个最小因子n，就是质因数。再拆分剩余因子 if(tmp%n==0) { flag = false; // 将第一个因子拼凑在tmp位置上 if(dict[n]!=null) { dict[tmp]=dict[n]; }else { dict[tmp] = n+\"*\"; } // 剩余因子的记忆化搜索，降低时间复杂度 if(dict[tmp/n]!=null) { dict[tmp]=dict[tmp]+dict[tmp/n]; break; } // 迭代剩余因子的分解，需要重新开始 tmp = tmp/n; n=2; } } // 如果i为素数，那么i就没有因子，所以需要直接初始化dict if(flag) { dict[i] = i+\"*\"; } if(i\u003e=a) { if(dict[i].length()\u003e1\u0026\u0026dict[i].charAt(dict[i].length()-1)=='*') { System.out.println(i+\"=\"+dict[i].substring(0, dict[i].length()-1)); }else System.out.println(i+\"=\"+dict[i]); } } //System.out.print(Arrays.toString(dict)); sc.close(); } ","date":"2023-01-08","objectID":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/:91:1","tags":["leetcode"],"title":"Leetcode每日一题","uri":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/"},{"categories":["数据结构与算法"],"content":"day91 ","date":"2023-01-08","objectID":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/:92:0","tags":["leetcode"],"title":"Leetcode每日一题","uri":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/"},{"categories":["数据结构与算法"],"content":"最大分解 题目 问题描述 给出一个正整数n，求一个和最大的序列a0，a1，a2，……，ap，满足n=a0\u003ea1\u003ea2\u003e……\u003eap且ai+1是ai的约数，输出a1+a2+……+ap的最大值 输入格式 输入仅一行，包含一个正整数n 输出格式 一个正整数，表示最大的序列和，即a1+a2+……+ap的最大值 样例输入 10 样例输出 6 数据规模和约定 1\u003cn\u003c=10^6 样例说明 p=2 a0=10，a1=5，a2=1，6=5+1 解法 主要采用dfs+贪心优化 思路：这个题目本身就是不断得分解得到约数，显而易见应该使用递归编程求解；我们再思考题目：题目的意思是让我们找出约数序列和得最大值，可以验证这种贪心策略：每次都取最大约数，这样得到的结果总是当前最优，这样每一步的最优解的和自然是最终的最大值。 import java.util.Scanner; public class Main { public static void main(String[] args) { Scanner sc =new Scanner(System.in); System.out.print(dfs(sc.nextInt())); sc.close(); } // dfs+贪心优化 public static int dfs(int num) { // 判断当前数是否是素数，是的话就停止当前分支递归 if(num\u003c=3) return 1; boolean flag = false; for(int i=2;i\u003cnum;i++) { if(num%i==0) { flag =true; // 找到第一个约数i，也就是最小得那个约数i;就去递归最大得那个约数 // System.out.println(num+\"|\"+num/i+\"|\"+i); return dfs(num/i)+num/i; } } // 素数 if(!flag) { return 1; } return 1; } } ","date":"2023-01-08","objectID":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/:92:1","tags":["leetcode"],"title":"Leetcode每日一题","uri":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/"},{"categories":["数据结构与算法"],"content":"day92 46. 全排列 题目 给定一个不含重复数字的数组 nums ，返回其 所有可能的全排列 。你可以 按任意顺序 返回答案。 示例 1： 输入：nums = [1,2,3] 输出：[[1,2,3],[1,3,2],[2,1,3],[2,3,1],[3,1,2],[3,2,1]] 示例 2： 输入：nums = [0,1] 输出：[[0,1],[1,0]] 示例 3： 输入：nums = [1] 输出：[[1]] 提示： 1 \u003c= nums.length \u003c= 6 -10 \u003c= nums[i] \u003c= 10 nums 中的所有整数 互不相同 解法 解法一：dfs+回溯 这个问题可以看作有 n 个排列成一行的空格，我们需要从左往右依此填入题目给定的 n 个数，每个数只能使用一次。那么很直接的可以想到一种穷举的算法，即从左往右每一个位置都依此尝试填入一个数，看能不能填完这 n 个空格，在程序中我们可以用「回溯法」来模拟这个过程。 DFS+回溯 解法二：dfs+回溯+哈希去重 全排列可以从【组合数】里筛选出来，利用哈希表去重。同时，需要注意深度拷贝的问题 class Solution { HashSet\u003cInteger\u003e hm; public Solution(){ hm = new HashSet\u003c\u003e(); } public List\u003cList\u003cInteger\u003e\u003e permute(int[] nums) { List\u003cList\u003cInteger\u003e\u003e lls = new ArrayList\u003c\u003e(); dfs(lls,new ArrayList\u003c\u003e(),nums,nums.length,0); return lls; } public void dfs(List\u003cList\u003cInteger\u003e\u003e lls,List\u003cInteger\u003e ls,int[] nums,int len,int step){ if(step==len){ // 核心关键: ls是引用类型,所以此处要深拷贝出来 lls.add(new ArrayList\u003c\u003e(ls)); return; } for(int i=0;i\u003clen;i++){ if (hm.contains(nums[i])){ continue; } ls.add(nums[i]); hm.add(nums[i]); dfs(lls,ls,nums,len,step+1); hm.remove(nums[i]); ls.remove(ls.size()-1); } } } // 全排列不去重 func permute(nums []int) [][]int { ans := [][]int{} used := make([]bool,len(nums)) var dfs func(nums []int,item []int,step int) dfs = func(nums []int,item []int,step int){ if step==len(nums){ newItem:=make([]int,len(item)) copy(newItem,item) ans = append(ans,newItem) return } for i:=0;i\u003clen(nums);i++{ if used[i] { continue } item = append(item,nums[i]) used[i] = true dfs(nums,item,step+1) used[i] = false item = item[:len(item)-1] } } dfs(nums,[]int{},0) return ans } ","date":"2023-01-08","objectID":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/:93:0","tags":["leetcode"],"title":"Leetcode每日一题","uri":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/"},{"categories":["数据结构与算法"],"content":"day93 ","date":"2023-01-08","objectID":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/:94:0","tags":["leetcode"],"title":"Leetcode每日一题","uri":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/"},{"categories":["数据结构与算法"],"content":"马虎的算式 题目 小明是个急性子，上小学的时候经常把老师写在黑板上的题目抄错了。有一次，老师出的题目是：36 x 495 = ? 他却给抄成了：396 x 45 = ?但结果却很戏剧性，他的答案竟然是对的！！因为 36 * 495 = 396 * 45 = 17820 类似这样的巧合情况可能还有很多，比如：27 * 594 = 297 * 54 假设 a b c d e 代表1~9不同的5个数字（注意是各不相同的数字，且不含0）能满足形如： ab * cde = adb * ce 这样的算式一共有多少种呢？请你利用计算机的优势寻找所有的可能，并回答不同算式的种类数。满足乘法交换律的算式计为不同的种类，所以答案肯定是个偶数。 解法 解法一：暴力枚举 题目的意思就是在1~9里找到五个不同的数字组合，满足题目的要求。直接五个for循环嵌套解决 public class Main { public static void main(String[] args){ int a,b,c,d,e; int count=0;//记录满足条件的个数 for(a=1;a\u003c=9;a++) for(b=1;b\u003c=9;b++) for(c=1;c\u003c=9;c++) for(d=1;d\u003c=9;d++) for(e=1;e\u003c=9;e++){//abcde代表1-9各不同的5个数字 if(a!=b\u0026\u0026a!=c\u0026\u0026a!=d\u0026\u0026a!=e\u0026\u0026b!=c\u0026\u0026b!=d\u0026\u0026b!=e \u0026\u0026c!=d\u0026\u0026c!=e\u0026\u0026d!=e){ int sum1=(a*10+b)*(c*100+d*10+e); int sum2=(a*100+d*10+b)*(c*10+e); if(sum1==sum2) { count++; } } } System.out.println(count); } } 解法二：优雅的DFS+回溯 我们可以通过dfs搜索解决。 import java.util.Arrays; public class Main { static int[] nums = {1,2,3,4,5,6,7,8,9}; static boolean[] vs = new boolean[9]; static int count = 0; public static void main(String[] args) { dfs(new int[5],0); System.out.print(count); } // DFS枚举: 找出五个数的所有组合 public static void dfs(int[] zuhe,int step) { if(step\u003e4) { // 处理五个数 int a = zuhe[0]*10+zuhe[1]; int b = zuhe[2]*100+zuhe[3]*10+zuhe[4]; int c = zuhe[0]*100+zuhe[3]*10+zuhe[1]; int d = zuhe[2]*10+zuhe[4]; if(a*b==c*d) { // System.out.println(Arrays.toString(zuhe)); count++; } return; } for(int i=0;i\u003cnums.length;i++) { // 取出没有被拿的数，保证每位数字都不一样 if(!vs[i]) { vs[i]=true; zuhe[step]=nums[i]; dfs(zuhe,step+1); vs[i]=false;// 回溯 zuhe[step]=0; // 回溯 } } } } ","date":"2023-01-08","objectID":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/:94:1","tags":["leetcode"],"title":"Leetcode每日一题","uri":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/"},{"categories":["数据结构与算法"],"content":"day94 ","date":"2023-01-08","objectID":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/:95:0","tags":["leetcode"],"title":"Leetcode每日一题","uri":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/"},{"categories":["数据结构与算法"],"content":"振兴中华 题目 小明参加了学校的趣味运动会，其中的一个项目是：跳格子。地上画着一些格子，每个格子里写一个字，如下所示：（也可参见p1.jpg） 从我做起振 我做起振兴 做起振兴中 起振兴中华 比赛时，先站在左上角的写着“从”字的格子里，可以横向或纵向跳到相邻的格子里，但不能跳到对角的格子或其它位置。一直要跳到“华”字结束。 要求跳过的路线刚好构成“从我做起振兴中华”这句话。 请你帮助小明算一算他一共有多少种可能的跳跃路线呢？ 解法 解法一：DFS 思路：只要每次都是往右或往左走，就一定满足题目要求。所以，就是求这样走的路径有多少。显然，可以通过DFS直接模拟这个过程。 public class Main { static String[][] matrix = { {\"从\",\"我\",\"做,\",\"起\",\"振\"}, {\"我\",\"做,\",\"起\",\"振\",\"兴\"}, {\"做,\",\"起\",\"振\",\"兴\",\"中\"}, {\"起\",\"振\",\"兴\",\"中\",\"华\"} }; public static void main(String[] args) { System.out.print(dfs(0,0)); } public static int dfs(int i,int j) { if(i==3||j==4) { return 1; } // 下一步走,且回溯前面往下走的步数，换一种往右走 return dfs(i+1,j)+dfs(i,j+1); } } ","date":"2023-01-08","objectID":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/:95:1","tags":["leetcode"],"title":"Leetcode每日一题","uri":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/"},{"categories":["数据结构与算法"],"content":"day95 123. 买卖股票的最佳时机 III 题目 给定一个数组，它的第 i 个元素是一支给定的股票在第 i 天的价格。 设计一个算法来计算你所能获取的最大利润。你最多可以完成 两笔 交易。 **注意：**你不能同时参与多笔交易（你必须在再次购买前出售掉之前的股票）。 示例 1: 输入：prices = [3,3,5,0,0,3,1,4] 输出：6 解释：在第 4 天（股票价格 = 0）的时候买入，在第 6 天（股票价格 = 3）的时候卖出，这笔交易所能获得利润 = 3-0 = 3 。 随后，在第 7 天（股票价格 = 1）的时候买入，在第 8 天 （股票价格 = 4）的时候卖出，这笔交易所能获得利润 = 4-1 = 3 。 示例 2： 输入：prices = [1,2,3,4,5] 输出：4 解释：在第 1 天（股票价格 = 1）的时候买入，在第 5 天 （股票价格 = 5）的时候卖出, 这笔交易所能获得利润 = 5-1 = 4 。 注意你不能在第 1 天和第 2 天接连购买股票，之后再将它们卖出。 因为这样属于同时参与了多笔交易，你必须在再次购买前出售掉之前的股票。 示例 3： 输入：prices = [7,6,4,3,1] 输出：0 解释：在这个情况下, 没有交易完成, 所以最大利润为 0。 示例 4： 输入：prices = [1] 输出：0 提示： 1 \u003c= prices.length \u003c= 105 0 \u003c= prices[i] \u003c= 105 解法 动态规划 class Solution { public int maxProfit(int[] prices) { // 状态定义：dp[i][0]表示没有操作 // dp[i][1]表示前i天第一次持有股票的最大利润 // dp[i][2]表示前i天第一次卖出股票的最大利润 // dp[i][3]表示前i天第二次持有股票的最大利润 // dp[i][4]表示前i天第二次卖出股票的最大利润 int[][] dp = new int[prices.length+1][5]; // 初始化 dp[0][0] = 0; dp[0][2] =0; dp[0][1] = -prices[0]; dp[0][3] = -prices[0]; for(int i=1;i\u003c=prices.length;i++){ dp[i][1] = Math.max(dp[i-1][0]-prices[i-1],dp[i-1][1]); dp[i][2] = Math.max(dp[i-1][1]+prices[i-1],dp[i-1][2]); dp[i][3] = Math.max(dp[i-1][2]-prices[i-1],dp[i-1][3]); dp[i][4] = Math.max(dp[i-1][3]+prices[i-1],dp[i-1][4]); } return dp[prices.length][4]; } } ","date":"2023-01-08","objectID":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/:96:0","tags":["leetcode"],"title":"Leetcode每日一题","uri":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/"},{"categories":["数据结构与算法"],"content":"day96 152. 乘积最大子数组 题目 给你一个整数数组 nums ，请你找出数组中乘积最大的非空连续子数组（该子数组中至少包含一个数字），并返回该子数组所对应的乘积。 测试用例的答案是一个 32-位 整数。 子数组 是数组的连续子序列。 示例 1: 输入: nums = [2,3,-2,4] 输出: 6 解释: 子数组 [2,3] 有最大乘积 6。 示例 2: 输入: nums = [-2,0,-1] 输出: 0 解释: 结果不能为 2, 因为 [-2,-1] 不是子数组。 提示: 1 \u003c= nums.length \u003c= 2 * 104 -10 \u003c= nums[i] \u003c= 10 nums 的任何前缀或后缀的乘积都 保证 是一个 32-位 整数 解法 方法一：数学规律 依题意不难看出：我们可以顺序连乘，直到遇到0时，我们需要重新连乘。每次连乘都要找到最大的乘积。我们需要正向和反向两个操作才能把所有组合全部遍历一遍。 class Solution { public int maxProduct(int[] nums) { int ans = 1; int max = Integer.MIN_VALUE; if(nums.length==1) return nums[0]; // 正向求最大 for(int i=0;i\u003cnums.length;i++){ if(nums[i]==0){ ans=1; continue; } ans*=nums[i]; if(ans\u003emax){ max = ans; } } // 反向求最大 ans = 1; for(int i=nums.length-1;i\u003e=0;i--){ if(nums[i]==0){ ans=1; continue; } ans*=nums[i]; if(ans\u003emax){ max = ans; } } return max\u003e0?max:0; } } 方法二：动态规划 ","date":"2023-01-08","objectID":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/:97:0","tags":["leetcode"],"title":"Leetcode每日一题","uri":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/"},{"categories":["数据结构与算法"],"content":"day97 416. 分割等和子集 题目 给你一个 只包含正整数 的 非空 数组 nums 。请你判断是否可以将这个数组分割成两个子集，使得两个子集的元素和相等。 示例 1： 输入：nums = [1,5,11,5] 输出：true 解释：数组可以分割成 [1, 5, 5] 和 [11] 。 示例 2： 输入：nums = [1,2,3,5] 输出：false 解释：数组不能分割成两个元素和相等的子集。 提示： 1 \u003c= nums.length \u003c= 200 1 \u003c= nums[i] \u003c= 100 解法 每个元素只可以被选择1次，就是01背包问题 背包容量为sum/2，也就是在nums里挑选出刚好装满背包可能性 01背包的应用 二维DP class Solution { public boolean canPartition(int[] nums) { if(nums.length\u003c=1) return false; int sum = 0; for(int v:nums) sum+=v; if(sum%2!=0) return false; int bag = sum/2;// 背包容量 // 定义状态：dp[i][j]表示放入前i个物品时，是否可以凑出容量j boolean[][] dp = new boolean[nums.length+1][bag+1]; // 初始化 dp[0][0] = true; for(int i=1;i\u003c=nums.length;i++){ for(int j=0;j\u003c=bag;j++){ if(j\u003e=nums[i-1]) dp[i][j] = dp[i-1][j]||dp[i-1][j-nums[i-1]]; else dp[i][j] = dp[i-1][j]; } } return dp[nums.length][bag]; } } 滚动数组优化二维DP class Solution { public boolean canPartition(int[] nums) { if(nums.length\u003c=1) return false; int sum = 0; for(int v:nums) sum+=v; if(sum%2!=0) return false; int bag = sum/2;// 背包容量 // 定义状态：dp[j]表示是否可以凑出容量j boolean[] dp = new boolean[bag+1]; // 初始化 dp[0] = true; // 注意一维dp为了避免状态转移过程中状态的覆盖。需要先遍历物品再遍历背包； // 而且由于物品只放入一次，所以背包需要倒序遍历，否则就会放入多次物品，那这就不是01背包了，而是完全背包 for(int i=1;i\u003c=nums.length;i++){ for(int j=bag;j\u003e=0;j--){ if(j\u003e=nums[i-1]) dp[j] = dp[j]||dp[j-nums[i-1]]; } } return dp[bag]; } } 背包倒序遍历的解释： 需要倒序遍历背包容量。如果是顺序放入，那么根据dp转移方程： 第j个状态可能是由前面的第j-w[i]个状态转移过来。 如果顺序遍历，那么就会先算出前面的值，算出前面的值过后，后面dp转移的时候又可能会用到， 那么也就是说，一个物品被放入了多次。这就不是01背包问题了，而是完全背包 ","date":"2023-01-08","objectID":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/:98:0","tags":["leetcode"],"title":"Leetcode每日一题","uri":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/"},{"categories":["数据结构与算法"],"content":"day98 474. 一和零 题目 给你一个二进制字符串数组 strs 和两个整数 m 和 n 。 请你找出并返回 strs 的最大子集的长度，该子集中 最多 有 m 个 0 和 n 个 1 。 如果 x 的所有元素也是 y 的元素，集合 x 是集合 y 的 子集 。 示例 1： 输入：strs = [\"10\", \"0001\", \"111001\", \"1\", \"0\"], m = 5, n = 3 输出：4 解释：最多有 5 个 0 和 3 个 1 的最大子集是 {\"10\",\"0001\",\"1\",\"0\"} ，因此答案是 4 。 其他满足题意但较小的子集包括 {\"0001\",\"1\"} 和 {\"10\",\"1\",\"0\"} 。{\"111001\"} 不满足题意，因为它含 4 个 1 ，大于 n 的值 3 。 示例 2： 输入：strs = [\"10\", \"0\", \"1\"], m = 1, n = 1 输出：2 解释：最大的子集是 {\"0\", \"1\"} ，所以答案是 2 。 提示： 1 \u003c= strs.length \u003c= 600 1 \u003c= strs[i].length \u003c= 100 strs[i] 仅由 '0' 和 '1' 组成 1 \u003c= m, n \u003c= 100 解法 解法一：01背包的变种 https://leetcode.cn/problems/ones-and-zeroes/solution/yi-he-ling-by-leetcode-solution-u2z2/ 二维DP class Solution { public int findMaxForm(String[] strs, int m, int n) { // 统计1的个数 int[] ones = new int[strs.length]; int count = 0; for(int i=0;i\u003cstrs.length;i++){ for(int j=0;j\u003cstrs[i].length();j++){ if(strs[i].charAt(j)=='1') count++; } ones[i] = count; count = 0; } // 定义dp: 表示选择的字符串个数 int[][][] dp = new int[strs.length+1][m+1][n+1]; // 初始化状态：当i==0时，表示取零个串，当然dp[0][][] = 0 // 状态转移: 完全就是01背包问题，只不过背包的维度增加了。 for(int i=1;i\u003c=strs.length;i++){ for(int j=m;j\u003e=0;j--){ for(int k=n;k\u003e=0;k--){ int a = strs[i-1].length()-ones[i-1]; int b = ones[i-1]; if(j\u003e=a\u0026\u0026k\u003e=b){ dp[i][j][k] = Math.max(dp[i-1][j][k],dp[i-1][j-a][k-b]+1); }else{ dp[i][j][k] = dp[i-1][j][k]; } } } } return dp[strs.length][m][n]; } } 滚动数组优化为一维DP class Solution { public int findMaxForm(String[] strs, int m, int n) { // 统计1的个数 int[] ones = new int[strs.length]; int count = 0; for(int i=0;i\u003cstrs.length;i++){ for(int j=0;j\u003cstrs[i].length();j++){ if(strs[i].charAt(j)=='1') count++; } ones[i] = count; count = 0; } // 定义dp: 表示选择的字符串个数 int[][] dp = new int[m+1][n+1]; // 初始化状态：为0 // 状态转移 for(int i=1;i\u003c=strs.length;i++){ for(int j=m;j\u003e=0;j--){// 倒序 for(int k=n;k\u003e=0;k--){// 倒序 int a = strs[i-1].length()-ones[i-1]; int b = ones[i-1]; if(j\u003e=a\u0026\u0026k\u003e=b){ dp[j][k] = Math.max(dp[j][k],dp[j-a][k-b]+1); } } } } return dp[m][n]; } } ","date":"2023-01-08","objectID":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/:99:0","tags":["leetcode"],"title":"Leetcode每日一题","uri":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/"},{"categories":["数据结构与算法"],"content":"day99 494. 目标和 题目 给你一个整数数组 nums 和一个整数 target 。 向数组中的每个整数前添加 '+' 或 '-' ，然后串联起所有整数，可以构造一个 表达式 ： 例如，nums = [2, 1] ，可以在 2 之前添加 '+' ，在 1 之前添加 '-' ，然后串联起来得到表达式 \"+2-1\" 。 返回可以通过上述方法构造的、运算结果等于 target 的不同 表达式 的数目。 示例 1： 输入：nums = [1,1,1,1,1], target = 3 输出：5 解释：一共有 5 种方法让最终目标和为 3 。 -1 + 1 + 1 + 1 + 1 = 3 +1 - 1 + 1 + 1 + 1 = 3 +1 + 1 - 1 + 1 + 1 = 3 +1 + 1 + 1 - 1 + 1 = 3 +1 + 1 + 1 + 1 - 1 = 3 示例 2： 输入：nums = [1], target = 1 输出：1 提示： 1 \u003c= nums.length \u003c= 20 0 \u003c= nums[i] \u003c= 1000 0 \u003c= sum(nums[i]) \u003c= 1000 -1000 \u003c= target \u003c= 1000 解法 方法一：DFS 暴力DFS搜索所有解空间即可 class Solution { public int findTargetSumWays(int[] nums, int target) { return dfs(nums,target,0,0); } public int dfs(int[] nums, int target,int i,int next){ if(i==nums.length){ if(next==target) return 1; return 0; } return dfs(nums,target,i+1,next+nums[i])+dfs(nums,target,i+1,next-nums[i]); } } 方法二：DP https://leetcode.cn/problems/target-sum/solution/mu-biao-he-by-leetcode-solution-o0cp/ 将nums分成两堆数字（s1,s2），使得这两堆数字的差值等于target s1-s2=target s1+s2=sum 2*s1 = sum+target s1 = (sum+target)/2 s2 = (sum-target)/2 也就是说需要分出一堆数字为和为s1或s2的堆 将问题转化为为了背包问题 这里的话还是选择填充容量为s2的背包 因为选择s1的背包的话，如果要求的target为负数的话，数组的索引初始化会溢出 二维DP class Solution { public int findTargetSumWays(int[] nums, int target) { if(nums.length\u003c=1) return nums[0]==Math.abs(target)?1:0; int sum = 0; for(int v:nums) sum+=v; if((sum-target)%2!=0||target\u003esum) return 0; int bag = Math.abs((sum-target))/2; // 状态表示：dp[i][j]表示取第i个数后j容量背包的方法总数 int[][] dp = new int[nums.length+1][bag+1]; // 初始化 dp[0][0] = 1; // 状态转移 for(int i=1;i\u003c=nums.length;i++){ for(int j=0;j\u003c=bag;j++){ if(j\u003e=nums[i-1]){ dp[i][j] = dp[i-1][j] + dp[i-1][j-nums[i-1]]; }else if(j\u003cnums[i-1]){ // 放不下，方法数不变 dp[i][j] = dp[i-1][j]; } } } return dp[nums.length][bag]; } } 滚动数组优化为一维DP class Solution { public int findTargetSumWays(int[] nums, int target) { if(nums.length\u003c=1) return nums[0]==Math.abs(target)?1:0; int sum = 0; for(int v:nums) sum+=v; if((sum-target)%2!=0||target\u003esum) return 0; int bag = Math.abs((sum-target))/2; // 状态表示：dp[i][j]表示取第i个数后j容量背包的方法总数 int[] dp = new int[bag+1]; // 初始化 dp[0] = 1; // 状态转移 for(int i=1;i\u003c=nums.length;i++){ for(int j=bag;j\u003e=0;j--){// 倒序避免状态的覆盖，从而造成01背包，避免完全背包 if(j\u003e=nums[i-1]){ dp[j] = dp[j] + dp[j-nums[i-1]]; } } } return dp[bag]; } } ","date":"2023-01-08","objectID":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/:100:0","tags":["leetcode"],"title":"Leetcode每日一题","uri":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/"},{"categories":["数据结构与算法"],"content":"day100 1049. 最后一块石头的重量 II 题目 有一堆石头，用整数数组 stones 表示。其中 stones[i] 表示第 i 块石头的重量。 每一回合，从中选出任意两块石头，然后将它们一起粉碎。假设石头的重量分别为 x 和 y，且 x \u003c= y。那么粉碎的可能结果如下： 如果 x == y，那么两块石头都会被完全粉碎； 如果 x != y，那么重量为 x 的石头将会完全粉碎，而重量为 y 的石头新重量为 y-x。 最后，最多只会剩下一块 石头。返回此石头 最小的可能重量 。如果没有石头剩下，就返回 0。 示例 1： 输入：stones = [2,7,4,1,8,1] 输出：1 解释： 组合 2 和 4，得到 2，所以数组转化为 [2,7,1,8,1]， 组合 7 和 8，得到 1，所以数组转化为 [2,1,1,1]， 组合 2 和 1，得到 1，所以数组转化为 [1,1,1]， 组合 1 和 1，得到 0，所以数组转化为 [1]，这就是最优值。 示例 2： 输入：stones = [31,26,33,21,40] 输出：5 提示： 1 \u003c= stones.length \u003c= 30 1 \u003c= stones[i] \u003c= 100 解法 方法一：动态规划 https://leetcode.cn/problems/last-stone-weight-ii/solution/zui-hou-yi-kuai-shi-tou-de-zhong-liang-i-95p9/ 二维DP class Solution { public int lastStoneWeightII(int[] stones) { if(stones.length\u003c=1) return stones[0]; int sum = 0; for(int v:stones) sum+=v; int bag = sum/2;// 直接向下取整 // 状态表示：dp[i][j]表示选取前i个石头，容量为j时，能放入的最大重量 int[][] dp = new int[stones.length][bag+1]; // 初始化 //dp[i][0] = 0; // dp[0][j] = stones[0]; 初始化i=0时的情况，装得下就可以取第0个石头 for(int j=0;j\u003c=bag;j++){ if(j\u003e=stones[0]){ dp[0][j] = stones[0]; } } for(int i=1;i\u003cstones.length;i++){ for(int j=1;j\u003c=bag;j++){ if(j\u003e=stones[i]) dp[i][j] = Math.max(dp[i-1][j],dp[i-1][j-stones[i]]+stones[i]); else dp[i][j] = dp[i-1][j]; } } // System.out.println(Arrays.deepToString(dp)); return sum-2*dp[stones.length-1][bag]; } } // 将石头分成总重量最接近的两堆，这样碰撞后可以得到最小的堆 // 也就是要在一堆石头里找到可以最接近sum的half堆 滚动数组优化的DP class Solution { public int lastStoneWeightII(int[] stones) { if(stones.length\u003c=1) return stones[0]; int sum = 0; for(int v:stones) sum+=v; int bag = sum/2;// 直接向下取整 // 状态表示：dp[j]表示选取容量为j时，能放入的最大重量 int[] dp = new int[bag+1]; // 初始化 dp[0] = 0; // 初始化从0开始，因为第一行还没被填写 for(int i=0;i\u003cstones.length;i++){ for(int j=bag;j\u003e=stones[i];j--){// 倒序 if(j\u003e=stones[i]) dp[j] = Math.max(dp[j],dp[j-stones[i]]+stones[i]); } } // System.out.println(Arrays.deepToString(dp)); return sum-2*dp[bag]; } } ","date":"2023-01-08","objectID":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/:101:0","tags":["leetcode"],"title":"Leetcode每日一题","uri":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/"},{"categories":["数据结构与算法"],"content":"day101 139. 单词拆分 题目 给你一个字符串 s 和一个字符串列表 wordDict 作为字典。请你判断是否可以利用字典中出现的单词拼接出 s 。 **注意：**不要求字典中出现的单词全部都使用，并且字典中的单词可以重复使用。 示例 1： 输入: s = \"leetcode\", wordDict = [\"leet\", \"code\"] 输出: true 解释: 返回 true 因为 \"leetcode\" 可以由 \"leet\" 和 \"code\" 拼接成。 示例 2： 输入: s = \"applepenapple\", wordDict = [\"apple\", \"pen\"] 输出: true 解释: 返回 true 因为 \"applepenapple\" 可以由 \"apple\" \"pen\" \"apple\" 拼接成。 注意，你可以重复使用字典中的单词。 示例 3： 输入: s = \"catsandog\", wordDict = [\"cats\", \"dog\", \"sand\", \"and\", \"cat\"] 输出: false 提示： 1 \u003c= s.length \u003c= 300 1 \u003c= wordDict.length \u003c= 1000 1 \u003c= wordDict[i].length \u003c= 20 s 和 wordDict[i] 仅有小写英文字母组成 wordDict 中的所有字符串 互不相同 解法 解法一：DP https://leetcode.cn/problems/word-break/solution/dan-ci-chai-fen-by-leetcode-solution/ ","date":"2023-01-08","objectID":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/:102:0","tags":["leetcode"],"title":"Leetcode每日一题","uri":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/"},{"categories":["数据结构与算法"],"content":"day102 279. 完全平方数 题目 给你一个整数 n ，返回 和为 n 的完全平方数的最少数量 。 完全平方数 是一个整数，其值等于另一个整数的平方；换句话说，其值等于一个整数自乘的积。例如，1、4、9 和 16 都是完全平方数，而 3 和 11 不是。 示例 1： 输入：n = 12 输出：3 解释：12 = 4 + 4 + 4 示例 2： 输入：n = 13 输出：2 解释：13 = 4 + 9 提示： 1 \u003c= n \u003c= 104 解法 解法一：DP https://leetcode.cn/problems/perfect-squares/solution/wan-quan-ping-fang-shu-by-leetcode-solut-t99c/ class Solution { public int numSquares(int n) { // dp[i][j]表示选取前i个完全平方数，和为j时的所选完全平方数最少数量 int num = 1; for(;num\u003cn;num++){ if(num*num\u003e=n) break; } int[][] dp = new int[num+1][n+1]; // 初始化：毫无疑问，求得最小值，为了避免初始状态的覆盖，需要初始化为n+1，因为最多的情况就是n个1 for(int[] one:dp) Arrays.fill(one,n+1); // 开始 dp[0][0] = 0; for(int i=1;i\u003c=num;i++){ for(int j=0;j\u003c=n;j++){ if(j\u003e=i*i) dp[i][j] = Math.min(dp[i-1][j],dp[i][j-i*i]+1); else dp[i][j] = dp[i-1][j]; } } // System.out.println(Arrays.deepToString(dp)); return dp[num][n]; } } 优化DP class Solution { public int numSquares(int n) { // dp[j]表示所选完全平方数和为j时的最少数量 int num = 1; for(;num\u003cn;num++){ if(num*num\u003e=n) break; } int[] dp = new int[n+1]; // 初始化：毫无疑问，求得最小值，为了避免初始状态的覆盖，需要初始化为n+1，因为最多的情况就是n个1 Arrays.fill(dp,n+1); // 开始 dp[0] = 0; for(int i=1;i\u003c=num;i++){ for(int j=0;j\u003c=n;j++){ if(j\u003e=i*i) dp[j] = Math.min(dp[j],dp[j-i*i]+1); } } // System.out.println(Arrays.toString(dp)); return dp[n]; } } ","date":"2023-01-08","objectID":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/:103:0","tags":["leetcode"],"title":"Leetcode每日一题","uri":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/"},{"categories":["数据结构与算法"],"content":"day103 322. 零钱兑换 题目 给你一个整数数组 coins ，表示不同面额的硬币；以及一个整数 amount ，表示总金额。 计算并返回可以凑成总金额所需的 最少的硬币个数 。如果没有任何一种硬币组合能组成总金额，返回 -1 。 你可以认为每种硬币的数量是无限的。 示例 1： 输入：coins = [1, 2, 5], amount = 11 输出：3 解释：11 = 5 + 5 + 1 示例 2： 输入：coins = [2], amount = 3 输出：-1 示例 3： 输入：coins = [1], amount = 0 输出：0 提示： 1 \u003c= coins.length \u003c= 12 1 \u003c= coins[i] \u003c= 231 - 1 0 \u003c= amount \u003c= 104 解法 方法一：记忆化搜索 方法二：DP https://leetcode.cn/problems/coin-change/solution/322-ling-qian-dui-huan-by-leetcode-solution/ class Solution { public int coinChange(int[] coins, int amount) { // 状态定义：dp[i][j]表示取前i种面额的硬币，容量为j，所需的最少硬币个数 int[][] dp = new int[coins.length + 1][amount + 1]; // 初始状态：因为状态方程是求最小值，所以这里需要初始化一个较大的数即可 // 不然结果全为0.当然初始化也有讲究，不能初始化一个特别大的数如Integer.MAX_VALUE， // 太大的数在状态转移过程中会溢出。我们知道硬币的面值最小是1，那么最多的硬币数量就是面值，所以初始化为面值+1 for (int[] ints : dp) { Arrays.fill(ints,amount+1); } // 注意因为状态转移取得是最小值，所以这里状态初始化，需要初始化为大于amount的数字才行。不然小了。就直接取那个小的数了。显然不正确，因为会被覆盖了 dp[0][0]=0; // 状态转移：如果当前硬币面额可以取出，既可以取也可以不取，取两种方式的最优解 for (int i = 1; i \u003c= coins.length; i++) { for (int j = 0; j \u003c= amount; j++) { if (coins[i - 1] \u003c= j) dp[i][j] = Math.min(dp[i][j - coins[i - 1]] + 1, dp[i - 1][j]); else dp[i][j] = dp[i - 1][j]; } } // System.out.println(Arrays.deepToString(dp)); return dp[coins.length][amount]\u003eamount?-1:dp[coins.length][amount]; } } 优化DP class Solution { public int coinChange(int[] coins, int amount) { // dp[j]表示凑出金额j的最少硬币个数 int[] dp = new int[amount+1]; // 初始化 Arrays.fill(dp,amount+1); dp[0]=0; // 状态转移 for(int i=1;i\u003c=coins.length;i++){ for(int j=0;j\u003c=amount;j++){ if(j\u003e=coins[i-1]) dp[j] = Math.min(dp[j],dp[j-coins[i-1]]+1); } } // System.out.println(Arrays.deepToString(dp)); return dp[amount]\u003eamount?-1:dp[amount]; } } ","date":"2023-01-08","objectID":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/:104:0","tags":["leetcode"],"title":"Leetcode每日一题","uri":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/"},{"categories":["数据结构与算法"],"content":"day104 377. 组合总和 Ⅳ 题目 给你一个由 不同 整数组成的数组 nums ，和一个目标整数 target 。请你从 nums 中找出并返回总和为 target 的元素组合的个数。 题目数据保证答案符合 32 位整数范围。 示例 1： 输入：nums = [1,2,3], target = 4 输出：7 解释： 所有可能的组合为： (1, 1, 1, 1) (1, 1, 2) (1, 2, 1) (1, 3) (2, 1, 1) (2, 2) (3, 1) 请注意，顺序不同的序列被视作不同的组合。 示例 2： 输入：nums = [9], target = 3 输出：0 提示： 1 \u003c= nums.length \u003c= 200 1 \u003c= nums[i] \u003c= 1000 nums 中的所有元素 互不相同 1 \u003c= target \u003c= 1000 **进阶：**如果给定的数组中含有负数会发生什么？问题会产生何种变化？如果允许负数出现，需要向题目中添加哪些限制条件？ 解法 方法一：记忆化搜索 方法二：DP https://leetcode.cn/problems/combination-sum-iv/solution/zu-he-zong-he-iv-by-leetcode-solution-q8zv/ ","date":"2023-01-08","objectID":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/:105:0","tags":["leetcode"],"title":"Leetcode每日一题","uri":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/"},{"categories":["数据结构与算法"],"content":"day105 509. 斐波那契数 斐波那契数 （通常用 F(n) 表示）形成的序列称为 斐波那契数列 。该数列由 0 和 1 开始，后面的每一项数字都是前面两项数字的和。也就是： F(0) = 0，F(1) = 1 F(n) = F(n - 1) + F(n - 2)，其中 n \u003e 1 给定 n ，请计算 F(n) 。 示例 1： 输入：n = 2 输出：1 解释：F(2) = F(1) + F(0) = 1 + 0 = 1 示例 2： 输入：n = 3 输出：2 解释：F(3) = F(2) + F(1) = 1 + 1 = 2 示例 3： 输入：n = 4 输出：3 解释：F(4) = F(3) + F(2) = 2 + 1 = 3 提示： 0 \u003c= n \u003c= 30 解法 方法一：DFS暴搜 斐波那契数的定义很符合递归函数，因此比较容易写出递归函数写法 class Solution { public int fib(int n) { if(n\u003c=1) return n; return fib(n-1)+fib(n-2); } } 方法二：DFS+记忆化搜索 方法一还可以改进一下。我们很容易知道：斐波那契数是不断往两个分支递归，这样会出现一些数字的重复计算。我们可以在递归的过程中将这些数据记录下来，下次要用时，直接拿出来用即可，避免多余的递归。 class Solution { public int fib(int n) { int[] history = new int[n]; return dfs(n,history,0); } public int dfs(int n,int[] history,int i){ if(n\u003c=1) return n; // 有记录，则直接拿 if(history[i]!=0) return history[i]; // 没有记录，需要计算 else { history[i] = fib(n-1)+fib(n-2); return history[i]; } } } 方法三：DP 这道题也容易想到动态规划的解法，因为题目的定义看着就很动态规划，状态转移方程已经很显然地给出了。 class Solution { public int fib(int n) { if(n\u003c=1) return n; // 状态表示：dp[i]表示第i个斐波那契数 int[] dp = new int[n+1]; // 初始化dp dp[0] = 0; dp[1] = 1; for(int i=2;i\u003c=n;i++){ dp[i] = dp[i-1] + dp[i-2]; } return dp[n]; } } // 由于当前状态只与前两个状态有关，所以可以利用滚动数组地思想优化为常数空间复杂度 ","date":"2023-01-08","objectID":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/:106:0","tags":["leetcode"],"title":"Leetcode每日一题","uri":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/"},{"categories":["数据结构与算法"],"content":"day106 746. 使用最小花费爬楼梯 题目 给你一个整数数组 cost ，其中 cost[i] 是从楼梯第 i 个台阶向上爬需要支付的费用。一旦你支付此费用，即可选择向上爬一个或者两个台阶。 你可以选择从下标为 0 或下标为 1 的台阶开始爬楼梯。 请你计算并返回达到楼梯顶部的最低花费。 示例 1： 输入：cost = [10,15,20] 输出：15 解释：你将从下标为 1 的台阶开始。 - 支付 15 ，向上爬两个台阶，到达楼梯顶部。 总花费为 15 。 示例 2： 输入：cost = [1,100,1,1,1,100,1,1,100,1] 输出：6 解释：你将从下标为 0 的台阶开始。 - 支付 1 ，向上爬两个台阶，到达下标为 2 的台阶。 - 支付 1 ，向上爬两个台阶，到达下标为 4 的台阶。 - 支付 1 ，向上爬两个台阶，到达下标为 6 的台阶。 - 支付 1 ，向上爬一个台阶，到达下标为 7 的台阶。 - 支付 1 ，向上爬两个台阶，到达下标为 9 的台阶。 - 支付 1 ，向上爬一个台阶，到达楼梯顶部。 总花费为 6 。 提示： 2 \u003c= cost.length \u003c= 1000 0 \u003c= cost[i] \u003c= 999 解法 方法一：DP https://leetcode.cn/problems/min-cost-climbing-stairs/solution/shi-yong-zui-xiao-hua-fei-pa-lou-ti-by-l-ncf8/ class Solution { public int minCostClimbingStairs(int[] cost) { // 状态定义：dp[i]表示爬到第i阶台阶的最低花费 int[] dp = new int[cost.length+1]; // 状态初始化：题目说可以直接不花费任何费用到达1或0 dp[0] = 0; dp[1] = 0; // 状态转移方程：dp[i]状态是由dp[i-1]或dp[i-2]转移过来，选择费用最低的方案即可 for(int i=2;i\u003cdp.length;i++){ dp[i] = Math.min(dp[i-1]+cost[i-1],dp[i-2]+cost[i-2]); } // System.out.println(Arrays.toString(dp)); return dp[cost.length]; } } ","date":"2023-01-08","objectID":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/:107:0","tags":["leetcode"],"title":"Leetcode每日一题","uri":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/"},{"categories":["数据结构与算法"],"content":"day107 17. 电话号码的字母组合 题目 给定一个仅包含数字 2-9 的字符串，返回所有它能表示的字母组合。答案可以按 任意顺序 返回。 给出数字到字母的映射如下（与电话按键相同）。注意 1 不对应任何字母。 示例 1： 输入：digits = \"23\" 输出：[\"ad\",\"ae\",\"af\",\"bd\",\"be\",\"bf\",\"cd\",\"ce\",\"cf\"] 示例 2： 输入：digits = \"\" 输出：[] 示例 3： 输入：digits = \"2\" 输出：[\"a\",\"b\",\"c\"] 提示： 0 \u003c= digits.length \u003c= 4 digits[i] 是范围 ['2', '9'] 的一个数字。 解法 本题很容易想到DFS暴力搜索的方法，再加上题目给的数据集很小，所以DFS暴力搜索可以解决。依据题目输入digits字符串，找到其中每个数字字符匹配的一个字符之后再组合即是答案，于是，就可以不断地遍历digits，去搜索所有的字符情况，当然需要注意回溯。 class Solution { String[] ss; public List\u003cString\u003e letterCombinations(String digits) { ss = new String[]{\"\",\"\",\"abc\",\"def\",\"ghi\",\"jkl\",\"mno\",\"pqrs\",\"tuv\",\"wxyz\"}; List\u003cString\u003e ls = new ArrayList\u003c\u003e(); if(digits.length()==0) { return ls; } dfs(digits,digits.length(),\"\",ls); return ls; } public void dfs(String digits,int step,String ans,List\u003cString\u003e ls){ // 结束条件 if(step==0){ // System.out.println(ans); ls.add(ans); return; } // 开始放第sIdx个位置 int sIdx = Integer.parseInt(Character.toString(digits.charAt(digits.length()-step)),10); for(int j=0;j\u003css[sIdx].length();j++){ // 先取第一个 ans+= ss[sIdx].charAt(j); dfs(digits,step-1,ans,ls); // 回溯到上一层 ans = ans.substring(0,ans.length()-1); } } } func letterCombinations(digits string) []string { if len(digits) == 0 { return []string{} } table:=[]string{\"\",\"\",\"abc\",\"def\",\"ghi\",\"jkl\",\"mno\",\"pqrs\",\"tuv\",\"wxyz\"} return dfs(table,digits,0,\"\") } func dfs(table []string, digits string, step int, cur string) []string { if step == len(digits){ return []string{cur} } ans := []string{} // 依次取号 didx,_:=strconv.Atoi(string(digits[step])) // 遍历号对应所有字符 for i:=0;i\u003clen(table[didx]);i++{ cur+=string(table[didx][i]) ans = append(ans,dfs(table,digits,step+1,cur)...) cur = string(cur[:len(cur)-1]) } return ans } ","date":"2023-01-08","objectID":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/:108:0","tags":["leetcode"],"title":"Leetcode每日一题","uri":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/"},{"categories":["数据结构与算法"],"content":"day108 22. 括号生成 题目 数字 n 代表生成括号的对数，请你设计一个函数，用于能够生成所有可能的并且 有效的 括号组合。 示例 1： 输入：n = 3 输出：[\"((()))\",\"(()())\",\"(())()\",\"()(())\",\"()()()\"] 示例 2： 输入：n = 1 输出：[\"()\"] 提示： 1 \u003c= n \u003c= 8 解法 官方解法：暴力与回溯 我这里的解法是暴力回溯。其实这个问题也是属于组合问题，但是组合是有条件限制的。必须满足括号匹配原则的组合才能追加到最后的结果里。那么我们怎样判断呢？ 可以通过栈这种数据结构来实现括号匹配：当栈空的时候就往里面加括号，如果非空，就看栈顶的括号与即将加入的括号是否配对，配对就pop掉栈顶的括号，否则就加入到栈中。最后如果栈空了，就表明括号匹配，否则不能追加到结果里。 当然，其实这个过程可以优化一下：没必要等到所有括号处理完毕再判断，其实中间过程就可以提前判断出来，终止继续深度递归了，可以在一定程度优化时间复杂度。 // 先dfs放下所有组合，然后用栈来判断dfs的过程是否合理 class Solution { String[] table; public Solution(){ table = new String[]{\"(\",\")\"}; } public List\u003cString\u003e generateParenthesis(int n) { List\u003cString\u003e list = new ArrayList\u003c\u003e(); Stack\u003cString\u003e st = new Stack\u003c\u003e(); dfs(list,0,n,\"\",st); return list; } public void dfs(List\u003cString\u003e list,int step,int n,String ans,Stack\u003cString\u003e st){ if(step==2*n){ if(st.empty()) list.add(ans); return; } for(int i=0;i\u003c2;i++){ // 在放括号之前，需要保证括号配对 // 下面的变量用来记录当前深度的值，方便回溯 boolean op=false; String tmp =\"\"; if(st.empty()){ st.push(table[i]); }else{ if(table[i]==\")\"\u0026\u0026st.peek()==\"(\"){ tmp = st.pop(); op = true; }else{ st.push(table[i]); } } ans+=table[i]; dfs(list,step+1,n,ans,st); // 注意：这里借助栈来判断是否括号配对的同时也是dfs，所以需要注意回溯 ans = ans.substring(0,ans.length()-1); if(!op){ st.pop(); }else{ st.push(tmp); } } } } func generateParenthesis(n int) []string { stack :=[]byte{} return dfs(stack,\"\",0,n) } func dfs(stack []byte,cur string,step int,n int) []string{ if step==2*n { if len(stack)==0{ return []string{cur} }else{ return []string{} } } ans := []string{} for i:=0;i\u003c2;i++{ next:=byte('(') if i==1{ next = ')' } flag:=false last :=byte(' ') if len(stack)!=0\u0026\u0026next==')'\u0026\u0026stack[len(stack)-1]=='('{ last = stack[len(stack)-1] stack = stack[:len(stack)-1] }else{ stack = append(stack,next) flag = true } ans = append(ans,dfs(stack,cur+string(next),step+1,n)...) // 回溯 栈 if flag{ stack = stack[:len(stack)-1] }else{ stack = append(stack,last) } } return ans } ","date":"2023-01-08","objectID":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/:109:0","tags":["leetcode"],"title":"Leetcode每日一题","uri":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/"},{"categories":["数据结构与算法"],"content":"day109 39. 组合总和 题目 给你一个 无重复元素 的整数数组 candidates 和一个目标整数 target ，找出 candidates 中可以使数字和为目标数 target 的 所有 不同组合 ，并以列表形式返回。你可以按 任意顺序 返回这些组合。 candidates 中的 同一个 数字可以 无限制重复被选取 。如果至少一个数字的被选数量不同，则两种组合是不同的。 对于给定的输入，保证和为 target 的不同组合数少于 150 个。 示例 1： 输入：candidates = [2,3,6,7], target = 7 输出：[[2,2,3],[7]] 解释： 2 和 3 可以形成一组候选，2 + 2 + 3 = 7 。注意 2 可以使用多次。 7 也是一个候选， 7 = 7 。 仅有这两种组合。 示例 2： 输入: candidates = [2,3,5], target = 8 输出: [[2,2,2,2],[2,3,3],[3,5]] 示例 3： 输入: candidates = [2], target = 1 输出: [] 提示： 1 \u003c= candidates.length \u003c= 30 2 \u003c= candidates[i] \u003c= 40 candidates 的所有元素 互不相同 1 \u003c= target \u003c= 40 解法 题解 import java.util.ArrayDeque; import java.util.ArrayList; import java.util.Deque; import java.util.List; public class Solution { public List\u003cList\u003cInteger\u003e\u003e combinationSum(int[] candidates, int target) { int len = candidates.length; List\u003cList\u003cInteger\u003e\u003e res = new ArrayList\u003c\u003e(); if (len == 0) { return res; } Deque\u003cInteger\u003e path = new ArrayDeque\u003c\u003e(); dfs(candidates, 0, len, target, path, res); return res; } /** * @param candidates 候选数组 * @param begin 搜索起点 * @param len 冗余变量，是 candidates 里的属性，可以不传 * @param target 每减去一个元素，目标值变小 * @param path 从根结点到叶子结点的路径，是一个栈 * @param res 结果集列表 */ private void dfs(int[] candidates, int begin, int len, int target, Deque\u003cInteger\u003e path, List\u003cList\u003cInteger\u003e\u003e res) { // target 为负数和 0 的时候不再产生新的孩子结点 if (target \u003c 0) { return; } if (target == 0) { res.add(new ArrayList\u003c\u003e(path)); return; } // 重点理解这里从 begin 开始搜索的语意 for (int i = begin; i \u003c len; i++) { path.addLast(candidates[i]); // 注意：由于每一个元素可以重复使用，下一轮搜索的起点依然是i，这里非常容易弄错 dfs(candidates, i, len, target - candidates[i], path, res); // 状态重置 path.removeLast(); } } } // 剪枝优化版 import java.util.ArrayDeque; import java.util.ArrayList; import java.util.Arrays; import java.util.Deque; import java.util.List; public class Solution { public List\u003cList\u003cInteger\u003e\u003e combinationSum(int[] candidates, int target) { int len = candidates.length; List\u003cList\u003cInteger\u003e\u003e res = new ArrayList\u003c\u003e(); if (len == 0) { return res; } // 排序是剪枝的前提 Arrays.sort(candidates); Deque\u003cInteger\u003e path = new ArrayDeque\u003c\u003e(); dfs(candidates, 0, len, target, path, res); return res; } private void dfs(int[] candidates, int begin, int len, int target, Deque\u003cInteger\u003e path, List\u003cList\u003cInteger\u003e\u003e res) { // 由于进入更深层的时候，小于 0 的部分被剪枝，因此递归终止条件值只判断等于 0 的情况 if (target == 0) { res.add(new ArrayList\u003c\u003e(path)); return; } for (int i = begin; i \u003c len; i++) { // 重点理解这里剪枝，前提是候选数组已经有序 // 已经为负数了，没有必要再搜索下去了，提前结束 if (target - candidates[i] \u003c 0) { break; } path.addLast(candidates[i]); dfs(candidates, i, len, target - candidates[i], path, res); path.removeLast(); } } } func combinationSum(candidates []int, target int) [][]int { ans := [][]int{} var dfs func(candidates, subs []int, target, step int) dfs = func(candidates, subs []int, target, step int) { if target \u003c 0 { return } tmp := make([]int, len(subs)) if target == 0 { copy(tmp, subs) ans = append(ans, tmp) return } for i := step; i \u003c len(candidates); i++ { subs = append(subs, candidates[i]) dfs(candidates, subs, target-candidates[i], i) subs = subs[:len(subs)-1] } return } dfs(candidates, []int{}, target, 0) return ans } // 剪枝优化版 func combinationSum(candidates []int, target int) [][]int { ans := [][]int{} var dfs func(candidates, subs []int, target, step int) dfs = func(candidates, subs []int, target, step int) { tmp := make([]int, len(subs)) if target == 0 { copy(tmp, subs) ans = append(ans, tmp) return } for i := step; i \u003c len(candidates); i++ { if target-candidates[i]\u003c0{ break } subs = append(subs, candidates[i]) dfs(candidates, subs, target-candidates[i], i) subs = subs[:len(subs)-1] } return } sort.Ints(candidates) dfs(candidates, []int{}, target, 0) return ans } ","date":"2023-01-08","objectID":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/:110:0","tags":["leetcode"],"title":"Leetcode每日一题","uri":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/"},{"categories":["数据结构与算法"],"content":"day110 78. 子集 题目 给你一个整数数组 nums ，数组中的元素 互不相同 。返回该数组所有可能的子集（幂集）。 解集 不能 包含重复的子集。你可以按 任意顺序 返回解集。 示例 1： 输入：nums = [1,2,3] 输出：[[],[1],[2],[1,2],[3],[1,3],[2,3],[1,2,3]] 示例 2： 输入：nums = [0] 输出：[[],[0]] 提示： 1 \u003c= nums.length \u003c= 10 -10 \u003c= nums[i] \u003c= 10 nums 中的所有元素 互不相同 解法 DFS class Solution { public List\u003cList\u003cInteger\u003e\u003e subsets(int[] nums) { List\u003cList\u003cInteger\u003e\u003e lls = new ArrayList\u003c\u003e(); List\u003cInteger\u003e ls = new ArrayList\u003c\u003e(); dfs(lls,ls,nums,0); return lls; } public void dfs(List\u003cList\u003cInteger\u003e\u003e lls,List\u003cInteger\u003e ls,int[] nums,int step){ if(step==nums.length){ lls.add(new ArrayList\u003c\u003e(ls)); return; } // System.out.println(ls.toString()); ls.add(ls.size(),nums[step]); // System.out.println(\"递归前：\"+ls.toString()); // 取下一个数 dfs(lls,ls,nums,step+1); // System.out.println(\"递归后：\"+ls.toString()); // 回溯 ls.remove(ls.size()-1); // 直接取下一个数，不取当前数 dfs(lls,ls,nums,step+1); } } ","date":"2023-01-08","objectID":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/:111:0","tags":["leetcode"],"title":"Leetcode每日一题","uri":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/"},{"categories":["数据结构与算法"],"content":"day111 343. 整数拆分 题目 给定一个正整数 n ，将其拆分为 k 个 正整数 的和（ k \u003e= 2 ），并使这些整数的乘积最大化。 返回 你可以获得的最大乘积 。 示例 1: 输入: n = 2 输出: 1 解释: 2 = 1 + 1, 1 × 1 = 1。 示例 2: 输入: n = 10 输出: 36 解释: 10 = 3 + 3 + 4, 3 × 3 × 4 = 36。 提示: 2 \u003c= n \u003c= 58 解法 代码随想录–dp 官方题解 ","date":"2023-01-08","objectID":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/:112:0","tags":["leetcode"],"title":"Leetcode每日一题","uri":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/"},{"categories":["数据结构与算法"],"content":"day112 96. 不同的二叉搜索树 题目 给你一个整数 n ，求恰由 n 个节点组成且节点值从 1 到 n 互不相同的 二叉搜索树 有多少种？返回满足题意的二叉搜索树的种数。 示例 1： 输入：n = 3 输出：5 示例 2： 输入：n = 1 输出：1 提示： 1 \u003c= n \u003c= 19 解法 相比官方更容易看懂得解法 官方题解 func numTrees(n int) int { if n==0 || n==1{ return 1 } if c,ok:=m[n];ok{ return c } count:=0 for i:=1;i\u003c=n;i++{ l:=numTrees(i-1) r:=numTrees(n-i) count += l*r } m[n]=count return count } // 记忆化 var m = map[int]int{} ","date":"2023-01-08","objectID":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/:113:0","tags":["leetcode"],"title":"Leetcode每日一题","uri":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/"},{"categories":["数据结构与算法"],"content":"day113 518. 零钱兑换 II 题目 给你一个整数数组 coins 表示不同面额的硬币，另给一个整数 amount 表示总金额。 请你计算并返回可以凑成总金额的硬币组合数。如果任何硬币组合都无法凑出总金额，返回 0 。 假设每一种面额的硬币有无限个。 题目数据保证结果符合 32 位带符号整数。 示例 1： 输入：amount = 5, coins = [1, 2, 5] 输出：4 解释：有四种方式可以凑成总金额： 5=5 5=2+2+1 5=2+1+1+1 5=1+1+1+1+1 示例 2： 输入：amount = 3, coins = [2] 输出：0 解释：只用面额 2 的硬币不能凑成总金额 3 。 示例 3： 输入：amount = 10, coins = [10] 输出：1 提示： 1 \u003c= coins.length \u003c= 300 1 \u003c= coins[i] \u003c= 5000 coins 中的所有值 互不相同 0 \u003c= amount \u003c= 5000 解法 二维DP 典型的完全背包问题，需要注意：这里的背包容量需要正序遍历，因为状态转移方程是不断地去拿左上角的值，我们要确保这个值肯定要被算过（这样才符合完全背包的定义）。所以需要正序遍历背包容量。 状态转移方程也有不同 class Solution { public int change(int amount, int[] coins) { // 状态定义：dp[i][j]表示取1-i种硬币时，可以凑出面额为j的方法有多少种 int[][] dp = new int[coins.length+1][amount+1]; // 初始化：没有拿硬币时，可以凑出0的面额，方法有1种，就是什么都不做 dp[0][0] = 1; // 状态转移 for(int i=1;i\u003c=coins.length;i++){ for(int j=0;j\u003c=amount;j++){ if(j\u003e=coins[i-1]) dp[i][j] = dp[i-1][j] + dp[i][j-coins[i-1]]; else dp[i][j] = dp[i-1][j]; } } // System.out.println(Arrays.deepToString(dp)); return dp[coins.length][amount]; } } 滚动数组优化为一维DP class Solution { public int change(int amount, int[] coins) { // 状态定义：dp[i]可以凑出面额为i的方法有多少种 int[] dp = new int[amount+1]; // 初始化：没有拿硬币时，可以凑出0的面额，方法有1种，就是什么都不做 dp[0] = 1; // 状态转移 for(int i=1;i\u003c=coins.length;i++){ for(int j=coins[i-1];j\u003c=amount;j++){// 枚举背包容量，当然最小肯定就是cons[i-1]了 if(j\u003e=coins[i-1]) dp[j] = dp[j] + dp[j-coins[i-1]]; } } // System.out.println(Arrays.roString(dp)); return dp[amount]; } } ","date":"2023-01-08","objectID":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/:114:0","tags":["leetcode"],"title":"Leetcode每日一题","uri":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/"},{"categories":["数据结构与算法"],"content":"day114 多重背包 题目 有N种物品和一个容量为V 的背包。第i种物品最多有Mi件可用，每件耗费的空间是Ci ，价值是Wi 。求解将哪些物品装入背包可使这些物品的耗费的空间 总和不超过背包容量，且价值总和最大。 解法 多重背包和01背包是非常像的， 为什么和01背包像呢？ 每件物品最多有Mi件可用，把Mi件摊开，其实就是一个01背包问题了。 例如： 背包最大重量为10。 物品为： 重量 价值 数量 物品0 1 15 2 物品1 3 20 3 物品2 4 30 2 问背包能背的物品最大价值是多少？ 和如下情况有区别么？ 重量 价值 数量 物品0 1 15 1 物品0 1 15 1 物品1 3 20 1 物品1 3 20 1 物品1 3 20 1 物品2 4 30 1 物品2 4 30 1 毫无区别，这就转成了一个01背包问题了，且每个物品只用一次。 时间复杂度：O(m × n × k)，m：物品种类个数，n背包容量，k单类物品数量 也有另一种实现方式，就是把每种商品遍历的个数放在01背包里面在遍历一遍。 毫无区别，这就转成了一个01背包问题了，且每个物品只用一次。 时间复杂度：O(m × n × k)，m：物品种类个数，n背包容量，k单类物品数量 从代码里可以看出是01背包里面在加一个for循环遍历一个每种商品的数量。 和01背包还是如出一辙的。 当然还有那种二进制优化的方法，其实就是把每种物品的数量，打包成一个个独立的包。 和以上在循环遍历上有所不同，因为是分拆为各个包最后可以组成一个完整背包，具体原理我就不做过多解释了，大家了解一下就行，面试的话基本不会考完这个深度了，感兴趣可以自己深入研究一波。 public void testMultiPack1(){ // 版本一：改变物品数量为01背包格式 List\u003cInteger\u003e weight = new ArrayList\u003c\u003e(Arrays.asList(1, 3, 4)); List\u003cInteger\u003e value = new ArrayList\u003c\u003e(Arrays.asList(15, 20, 30)); List\u003cInteger\u003e nums = new ArrayList\u003c\u003e(Arrays.asList(2, 3, 2)); int bagWeight = 10; for (int i = 0; i \u003c nums.size(); i++) { while (nums.get(i) \u003e 1) { // 把物品展开为i weight.add(weight.get(i)); value.add(value.get(i)); nums.set(i, nums.get(i) - 1); } } int[] dp = new int[bagWeight + 1]; for(int i = 0; i \u003c weight.size(); i++) { // 遍历物品 for(int j = bagWeight; j \u003e= weight.get(i); j--) { // 遍历背包容量 dp[j] = Math.max(dp[j], dp[j - weight.get(i)] + value.get(i)); } System.out.println(Arrays.toString(dp)); } } public void testMultiPack2(){ // 版本二：改变遍历个数 int[] weight = new int[] {1, 3, 4}; int[] value = new int[] {15, 20, 30}; int[] nums = new int[] {2, 3, 2}; int bagWeight = 10; int[] dp = new int[bagWeight + 1]; for(int i = 0; i \u003c weight.length; i++) { // 遍历物品 for(int j = bagWeight; j \u003e= weight[i]; j--) { // 遍历背包容量 // 以上为01背包，然后加一个遍历个数 for (int k = 1; k \u003c= nums[i] \u0026\u0026 (j - k * weight[i]) \u003e= 0; k++) { // 遍历个数 dp[j] = Math.max(dp[j], dp[j - k * weight[i]] + k * value[i]); } System.out.println(Arrays.toString(dp)); } } } ","date":"2023-01-08","objectID":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/:115:0","tags":["leetcode"],"title":"Leetcode每日一题","uri":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/"},{"categories":["数据结构与算法"],"content":"day115 198. 打家劫舍 题目 你是一个专业的小偷，计划偷窃沿街的房屋。每间房内都藏有一定的现金，影响你偷窃的唯一制约因素就是相邻的房屋装有相互连通的防盗系统，如果两间相邻的房屋在同一晚上被小偷闯入，系统会自动报警。 给定一个代表每个房屋存放金额的非负整数数组，计算你 不触动警报装置的情况下 ，一夜之内能够偷窃到的最高金额。 示例 1： 输入：[1,2,3,1] 输出：4 解释：偷窃 1 号房屋 (金额 = 1) ，然后偷窃 3 号房屋 (金额 = 3)。 偷窃到的最高金额 = 1 + 3 = 4 。 示例 2： 输入：[2,7,9,3,1] 输出：12 解释：偷窃 1 号房屋 (金额 = 2), 偷窃 3 号房屋 (金额 = 9)，接着偷窃 5 号房屋 (金额 = 1)。 偷窃到的最高金额 = 2 + 9 + 1 = 12 。 提示： 1 \u003c= nums.length \u003c= 100 0 \u003c= nums[i] \u003c= 400 解法 dp class Solution { public int rob(int[] nums) { // dp[i]表示偷窃前i个房屋，得到的最高金额为多少 int[] dp = new int[nums.length+1]; // 初始化 dp[0] = 0; dp[1] = nums[0]; for(int i=2;i\u003c=nums.length;i++){ dp[i] = Math.max(dp[i-1],dp[i-2]+nums[i-1]); } //System.out.println(Arrays.toString(dp)); return dp[nums.length]; } } ","date":"2023-01-08","objectID":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/:116:0","tags":["leetcode"],"title":"Leetcode每日一题","uri":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/"},{"categories":["数据结构与算法"],"content":"day116 213. 打家劫舍 II 题目 你是一个专业的小偷，计划偷窃沿街的房屋，每间房内都藏有一定的现金。这个地方所有的房屋都 围成一圈 ，这意味着第一个房屋和最后一个房屋是紧挨着的。同时，相邻的房屋装有相互连通的防盗系统，如果两间相邻的房屋在同一晚上被小偷闯入，系统会自动报警 。 给定一个代表每个房屋存放金额的非负整数数组，计算你 在不触动警报装置的情况下 ，今晚能够偷窃到的最高金额。 示例 1： 输入：nums = [2,3,2] 输出：3 解释：你不能先偷窃 1 号房屋（金额 = 2），然后偷窃 3 号房屋（金额 = 2）, 因为他们是相邻的。 示例 2： 输入：nums = [1,2,3,1] 输出：4 解释：你可以先偷窃 1 号房屋（金额 = 1），然后偷窃 3 号房屋（金额 = 3）。 偷窃到的最高金额 = 1 + 3 = 4 。 示例 3： 输入：nums = [1,2,3] 输出：3 提示： 1 \u003c= nums.length \u003c= 100 0 \u003c= nums[i] \u003c= 1000 解法 dp 这道题目和198.打家劫舍 (opens new window)是差不多的，唯一区别就是成环了。 对于一个数组，成环的话主要有如下三种情况： 情况一：考虑不包含首尾元素 情况二：考虑包含首元素，不包含尾元素 情况三：考虑包含尾元素，不包含首元素 注意我这里用的是\"考虑\"，例如情况三，虽然是考虑包含尾元素，但不一定要选尾部元素！ 对于情况三，取nums[1] 和 nums[3]就是最大的。 而情况二 和 情况三 都包含了情况一了，所以只考虑情况二和情况三就可以了。 分析到这里，本题其实比较简单了。 剩下的和198.打家劫舍 (opens new window)就是一样的了。 ","date":"2023-01-08","objectID":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/:117:0","tags":["leetcode"],"title":"Leetcode每日一题","uri":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/"},{"categories":["数据结构与算法"],"content":"day117 337. 打家劫舍 III 题目 小偷又发现了一个新的可行窃的地区。这个地区只有一个入口，我们称之为 root 。 除了 root 之外，每栋房子有且只有一个“父“房子与之相连。一番侦察之后，聪明的小偷意识到“这个地方的所有房屋的排列类似于一棵二叉树”。 如果 两个直接相连的房子在同一天晚上被打劫 ，房屋将自动报警。 给定二叉树的 root 。返回 在不触动警报的情况下 ，小偷能够盗取的最高金额 。 示例 1: 输入: root = [3,2,3,null,3,null,1] 输出: 7 解释: 小偷一晚能够盗取的最高金额 3 + 3 + 1 = 7 示例 2: 输入: root = [3,4,5,1,3,null,1] 输出: 9 解释: 小偷一晚能够盗取的最高金额 4 + 5 = 9 提示： 树的节点数在 [1, 104] 范围内 0 \u003c= Node.val \u003c= 104 解法 树形dp ","date":"2023-01-08","objectID":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/:118:0","tags":["leetcode"],"title":"Leetcode每日一题","uri":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/"},{"categories":["数据结构与算法"],"content":"day118 188. 买卖股票的最佳时机 IV 题目 给定一个整数数组 prices ，它的第 i 个元素 prices[i] 是一支给定的股票在第 i 天的价格，和一个整型 k 。 设计一个算法来计算你所能获取的最大利润。你最多可以完成 k 笔交易。也就是说，你最多可以买 k 次，卖 k 次。 **注意：**你不能同时参与多笔交易（你必须在再次购买前出售掉之前的股票）。 示例 1： 输入：k = 2, prices = [2,4,1] 输出：2 解释：在第 1 天 (股票价格 = 2) 的时候买入，在第 2 天 (股票价格 = 4) 的时候卖出，这笔交易所能获得利润 = 4-2 = 2 。 示例 2： 输入：k = 2, prices = [3,2,6,5,0,3] 输出：7 解释：在第 2 天 (股票价格 = 2) 的时候买入，在第 3 天 (股票价格 = 6) 的时候卖出, 这笔交易所能获得利润 = 6-2 = 4 。 随后，在第 5 天 (股票价格 = 0) 的时候买入，在第 6 天 (股票价格 = 3) 的时候卖出, 这笔交易所能获得利润 = 3-0 = 3 。 提示： 0 \u003c= k \u003c= 100 0 \u003c= prices.length \u003c= 1000 0 \u003c= prices[i] \u003c= 1000 解法 dp 思路是\"买卖股票最大利润Ⅲ\"的变形思维。在原来基础上，由5个状态变为2*k+1个状态，遵循先买后卖原则，找到最大利润 class Solution { public int maxProfit(int k, int[] prices) { int[][] dp = new int[prices.length+1][2*k+1]; // 初始化 dp[0][0] = 0; dp[0][2] =0; for(int i=0;i\u003c2*k+1;i++){ if(i%2==1) dp[0][i] = -prices[0]; } for(int i=1;i\u003c=prices.length;i++){ for(int j=0;j\u003c2*k;j++){ dp[i][1+j] = Math.max(dp[i-1][j]+(int)Math.pow(-1,j+1)*prices[i-1],dp[i-1][1+j]); } } return dp[prices.length][2*k]; } } ","date":"2023-01-08","objectID":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/:119:0","tags":["leetcode"],"title":"Leetcode每日一题","uri":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/"},{"categories":["数据结构与算法"],"content":"day119 300. 最长递增子序列 题目 给你一个整数数组 nums ，找到其中最长严格递增子序列的长度。 子序列 是由数组派生而来的序列，删除（或不删除）数组中的元素而不改变其余元素的顺序。例如，[3,6,2,7] 是数组 [0,3,1,6,2,2,7] 的子序列。 示例 1： 输入：nums = [10,9,2,5,3,7,101,18] 输出：4 解释：最长递增子序列是 [2,3,7,101]，因此长度为 4 。 示例 2： 输入：nums = [0,1,0,3,2,3] 输出：4 示例 3： 输入：nums = [7,7,7,7,7,7,7] 输出：1 提示： 1 \u003c= nums.length \u003c= 2500 -104 \u003c= nums[i] \u003c= 104 进阶： 你能将算法的时间复杂度降低到 O(n log(n)) 吗? 解法 官方题解 比较容易想的应该是【贪心+二分搜索】 ","date":"2023-01-08","objectID":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/:120:0","tags":["leetcode"],"title":"Leetcode每日一题","uri":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/"},{"categories":["数据结构与算法"],"content":"day120 309. 最佳买卖股票时机含冷冻期 题目 给定一个整数数组prices，其中第 prices[i] 表示第 *i* 天的股票价格 。 设计一个算法计算出最大利润。在满足以下约束条件下，你可以尽可能地完成更多的交易（多次买卖一支股票）: 卖出股票后，你无法在第二天买入股票 (即冷冻期为 1 天)。 **注意：**你不能同时参与多笔交易（你必须在再次购买前出售掉之前的股票）。 示例 1: 输入: prices = [1,2,3,0,2] 输出: 3 解释: 对应的交易状态为: [买入, 卖出, 冷冻期, 买入, 卖出] 示例 2: 输入: prices = [1] 输出: 0 提示： 1 \u003c= prices.length \u003c= 5000 0 \u003c= prices[i] \u003c= 1000 解法 dp ","date":"2023-01-08","objectID":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/:121:0","tags":["leetcode"],"title":"Leetcode每日一题","uri":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/"},{"categories":["数据结构与算法"],"content":"day121 516. 最长回文子序列 题目 给你一个字符串 s ，找出其中最长的回文子序列，并返回该序列的长度。 子序列定义为：不改变剩余字符顺序的情况下，删除某些字符或者不删除任何字符形成的一个序列。 示例 1： 输入：s = \"bbbab\" 输出：4 解释：一个可能的最长回文子序列为 \"bbbb\" 。 示例 2： 输入：s = \"cbbd\" 输出：2 解释：一个可能的最长回文子序列为 \"bb\" 。 提示： 1 \u003c= s.length \u003c= 1000 s 仅由小写英文字母组成 解法 dp 这题的难点主要在于定义状态和找到状态转移方程。 ","date":"2023-01-08","objectID":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/:122:0","tags":["leetcode"],"title":"Leetcode每日一题","uri":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/"},{"categories":["数据结构与算法"],"content":"day122 583. 两个字符串的删除操作 题目 给定两个字符串 text1 和 text2，返回这两个字符串的最长 公共子序列 的长度。如果不存在 公共子序列 ，返回 0 。 一个字符串的 子序列 是指这样一个新的字符串：它是由原字符串在不改变字符的相对顺序的情况下删除某些字符（也可以不删除任何字符）后组成的新字符串。 例如，\"ace\" 是 \"abcde\" 的子序列，但 \"aec\" 不是 \"abcde\" 的子序列。 两个字符串的 公共子序列 是这两个字符串所共同拥有的子序列。 示例 1： 输入：text1 = \"abcde\", text2 = \"ace\" 输出：3 解释：最长公共子序列是 \"ace\" ，它的长度为 3 。 示例 2： 输入：text1 = \"abc\", text2 = \"abc\" 输出：3 解释：最长公共子序列是 \"abc\" ，它的长度为 3 。 示例 3： 输入：text1 = \"abc\", text2 = \"def\" 输出：0 解释：两个字符串没有公共子序列，返回 0 。 提示： 1 \u003c= text1.length, text2.length \u003c= 1000 text1 和 text2 仅由小写英文字符组成。 题解 可以将问题转化为求解【最长公共子序列问题】. dp class Solution { public int minDistance(String word1, String word2) { int m = word1.length(), n = word2.length(); int[][] dp = new int[m + 1][n + 1]; for (int i = 1; i \u003c= m; i++) { char c1 = word1.charAt(i - 1); for (int j = 1; j \u003c= n; j++) { char c2 = word2.charAt(j - 1); if (c1 == c2) { dp[i][j] = dp[i - 1][j - 1] + 1; } else { dp[i][j] = Math.max(dp[i - 1][j], dp[i][j - 1]); } } } return word1.length()+word2.length()-2*dp[m][n]; } } ","date":"2023-01-08","objectID":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/:123:0","tags":["leetcode"],"title":"Leetcode每日一题","uri":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/"},{"categories":["数据结构与算法"],"content":"day123 1143. 最长公共子序列 题目 给定两个单词 word1 和 word2 ，返回使得 word1 和 word2 相同所需的最小步数。 每步 可以删除任意一个字符串中的一个字符。 示例 1： 输入: word1 = \"sea\", word2 = \"eat\" 输出: 2 解释: 第一步将 \"sea\" 变为 \"ea\" ，第二步将 \"eat \"变为 \"ea\" 示例 2: 输入：word1 = \"leetcode\", word2 = \"etco\" 输出：4 提示： 1 \u003c= word1.length, word2.length \u003c= 500 word1 和 word2 只包含小写英文字母 题解 dp ","date":"2023-01-08","objectID":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/:124:0","tags":["leetcode"],"title":"Leetcode每日一题","uri":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/"},{"categories":["数据结构与算法"],"content":"day124 647. 回文子串 题目 给你一个字符串 s ，请你统计并返回这个字符串中 回文子串 的数目。 回文字符串 是正着读和倒过来读一样的字符串。 子字符串 是字符串中的由连续字符组成的一个序列。 具有不同开始位置或结束位置的子串，即使是由相同的字符组成，也会被视作不同的子串。 示例 1： 输入：s = \"abc\" 输出：3 解释：三个回文子串: \"a\", \"b\", \"c\" 示例 2： 输入：s = \"aaa\" 输出：6 解释：6个回文子串: \"a\", \"a\", \"a\", \"aa\", \"aa\", \"aaa\" 提示： 1 \u003c= s.length \u003c= 1000 s 由小写英文字母组成 解法 dp+中心拓展 ","date":"2023-01-08","objectID":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/:125:0","tags":["leetcode"],"title":"Leetcode每日一题","uri":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/"},{"categories":["数据结构与算法"],"content":"day125 674. 最长连续递增序列 问题 给定一个未经排序的整数数组，找到最长且 连续递增的子序列，并返回该序列的长度。 连续递增的子序列 可以由两个下标 l 和 r（l \u003c r）确定，如果对于每个 l \u003c= i \u003c r，都有 nums[i] \u003c nums[i + 1] ，那么子序列 [nums[l], nums[l + 1], ..., nums[r - 1], nums[r]] 就是连续递增子序列。 示例 1： 输入：nums = [1,3,5,4,7] 输出：3 解释：最长连续递增序列是 [1,3,5], 长度为3。 尽管 [1,3,5,7] 也是升序的子序列, 但它不是连续的，因为 5 和 7 在原数组里被 4 隔开。 示例 2： 输入：nums = [2,2,2,2,2] 输出：1 解释：最长连续递增序列是 [2], 长度为1。 提示： 1 \u003c= nums.length \u003c= 104 -109 \u003c= nums[i] \u003c= 109 解法 解法一：双指针 不断地寻找连续递增子序列的长度，取最长。 class Solution { public int findLengthOfLCIS(int[] nums) { if(nums.length\u003c=1) return nums.length; int ans = 1; int tmp = 1; for(int l=0,r=1;l\u003cnums.length\u0026\u0026r\u003cnums.length;){ if(nums[r]\u003enums[l]){ l++;r++; tmp++; }else{ tmp = Math.max(tmp,ans); ans = tmp; tmp = 1; l = r; r++; } } // 额外检验递增序列 return tmp\u003eans?tmp:ans; } } 解法二：贪心 下面的算法其实和双指针本质上是一个东西。 贪心 ","date":"2023-01-08","objectID":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/:126:0","tags":["leetcode"],"title":"Leetcode每日一题","uri":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/"},{"categories":["数据结构与算法"],"content":"day126 714. 买卖股票的最佳时机含手续费 题目 给定一个整数数组 prices，其中 prices[i]表示第 i 天的股票价格 ；整数 fee 代表了交易股票的手续费用。 你可以无限次地完成交易，但是你每笔交易都需要付手续费。如果你已经购买了一个股票，在卖出它之前你就不能再继续购买股票了。 返回获得利润的最大值。 **注意：**这里的一笔交易指买入持有并卖出股票的整个过程，每笔交易你只需要为支付一次手续费。 示例 1： 输入：prices = [1, 3, 2, 8, 4, 9], fee = 2 输出：8 解释：能够达到的最大利润: 在此处买入 prices[0] = 1 在此处卖出 prices[3] = 8 在此处买入 prices[4] = 4 在此处卖出 prices[5] = 9 总利润: ((8 - 1) - 2) + ((9 - 4) - 2) = 8 示例 2： 输入：prices = [1,3,7,5,10,3], fee = 3 输出：6 提示： 1 \u003c= prices.length \u003c= 5 * 104 1 \u003c= prices[i] \u003c 5 * 104 0 \u003c= fee \u003c 5 * 104 解法 解法一：dp 本题只是在【188. 买卖股票的最佳时机 IV】的基础上再附加手续费。所以，我们直接在原来的动态规划转移方程上再加上“如果是买入股票，无需处理；如果是卖出股票，需增加手续费”。 class Solution { public int maxProfit(int[] prices, int fee) { // dp[i][j]表示第i天（从1开始），第j个状态的最大利润 int[][] dp = new int[prices.length+1][2*prices.length+1]; // 初始化 for(int i=0;i\u003c2*prices.length+1;i++){ if(i%2==1) dp[0][i] = - prices[0]; } int s = 0; int fuhao = 0; for(int i=1;i\u003c=prices.length;i++){ for(int j=0;j\u003c2*prices.length;j++){ fuhao = (int)Math.pow(-1,j+1); if (fuhao==1 ){ s = fee; }else { s = 0; } dp[i][j+1] = Math.max(dp[i-1][j]+fuhao*prices[i-1] - s ,dp[i-1][j+1]); } } return dp[prices.length][2*prices.length]; } } 上面的解法中，定义的状态太多，有较高的空间复杂度。leetcode提交中，显示“内存超出限制”。 解法二：优化dp 优化dp ","date":"2023-01-08","objectID":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/:127:0","tags":["leetcode"],"title":"Leetcode每日一题","uri":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/"},{"categories":["数据结构与算法"],"content":"day127 208. 实现 Trie (前缀树) 题目 Trie（发音类似 “try”）或者说 前缀树 是一种树形数据结构，用于高效地存储和检索字符串数据集中的键。这一数据结构有相当多的应用情景，例如自动补完和拼写检查。 请你实现 Trie 类： Trie() 初始化前缀树对象。 void insert(String word) 向前缀树中插入字符串 word 。 boolean search(String word) 如果字符串 word 在前缀树中，返回 true（即，在检索之前已经插入）；否则，返回 false 。 boolean startsWith(String prefix) 如果之前已经插入的字符串 word 的前缀之一为 prefix ，返回 true ；否则，返回 false 。 示例： 输入 [\"Trie\", \"insert\", \"search\", \"search\", \"startsWith\", \"insert\", \"search\"] [[], [\"apple\"], [\"apple\"], [\"app\"], [\"app\"], [\"app\"], [\"app\"]] 输出 [null, null, true, false, true, null, true] 解释 Trie trie = new Trie(); trie.insert(\"apple\"); trie.search(\"apple\"); // 返回 True trie.search(\"app\"); // 返回 False trie.startsWith(\"app\"); // 返回 True trie.insert(\"app\"); trie.search(\"app\"); // 返回 True 提示： 1 \u003c= word.length, prefix.length \u003c= 2000 word 和 prefix 仅由小写英文字母组成 insert、search 和 startsWith 调用次数 总计 不超过 3 * 104 次 解法 解法一：模拟 就是不断地去匹配寻找Trie树的下一个节点 class Trie { boolean isWord; // 单词是否以当前字符结尾 Trie[] nextNode; public Trie() { nextNode = new Trie[26]; } public void insert(String word) { Trie root = this; for(int i=0;i\u003cword.length();i++){ int idx = word.charAt(i)-'a'; if(root.nextNode[idx]==null){ Trie n = new Trie(); root.nextNode[idx] = n; } root = root.nextNode[idx]; } root.isWord = true; return; } public boolean search(String word) { Trie root = this; for(int i=0;i\u003cword.length();i++){ int idx = word.charAt(i)-'a'; if(root.nextNode[idx]==null){ return false; } root = root.nextNode[idx]; } return root.isWord; } public boolean startsWith(String prefix) { Trie root = this; for(int i=0;i\u003cprefix.length();i++){ int idx = prefix.charAt(i)-'a'; if(root.nextNode[idx]==null){ return false; } root = root.nextNode[idx]; } return true; } } /** * Your Trie object will be instantiated and called as such: * Trie obj = new Trie(); * obj.insert(word); * boolean param_2 = obj.search(word); * boolean param_3 = obj.startsWith(prefix); */ ","date":"2023-01-08","objectID":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/:128:0","tags":["leetcode"],"title":"Leetcode每日一题","uri":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/"},{"categories":["数据结构与算法"],"content":"day128 ","date":"2023-01-08","objectID":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/:129:0","tags":["leetcode"],"title":"Leetcode每日一题","uri":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/"},{"categories":["数据结构与算法"],"content":"718. 最长重复子数组 题目 给两个整数数组 nums1 和 nums2 ，返回 两个数组中 公共的 、长度最长的子数组的长度 。 示例 1： 输入：nums1 = [1,2,3,2,1], nums2 = [3,2,1,4,7] 输出：3 解释：长度最长的公共子数组是 [3,2,1] 。 示例 2： 输入：nums1 = [0,0,0,0,0], nums2 = [0,0,0,0,0] 输出：5 提示： 1 \u003c= nums1.length, nums2.length \u003c= 1000 0 \u003c= nums1[i], nums2[i] \u003c= 100 代码 解法一：暴力算法 class Solution { public int findLength(int[] nums1, int[] nums2) { int maxLen = 0; for(int i=0;i\u003cnums1.length;i++){ for(int j=0;j\u003cnums2.length;j++){ int k=0; while(i+k\u003cnums1.length\u0026\u0026j+k\u003cnums2.length\u0026\u0026nums1[i+k]==nums2[j+k]){ k+=1; } if(maxLen\u003ck) maxLen = k; } } return maxLen; } } 解法二：动态规划 思路及算法 暴力解法的过程中，我们发现最坏情况下对于任意 i 与 j ，A[i] 与 B[j] 比较了 min⁡(i+1,j+1) 次。这也是导致了该暴力解法时间复杂度过高的根本原因。 不妨设 A 数组为 [1, 2, 3]，B 两数组为为 [1, 2, 4] ，那么在暴力解法中 A[2] 与 B[2] 被比较了三次。这三次比较分别是我们计算 A[0:] 与 B[0:] 最长公共前缀、 A[1:] 与 B[1:] 最长公共前缀以及 A[2:] 与 B[2:] 最长公共前缀时产生的。 我们希望优化这一过程，使得任意一对 A[i] 和 B[j] 都只被比较一次。这样我们自然而然想到利用这一次的比较结果。如果 A[i] == B[j]，那么我们知道 A[i:] 与 B[j:] 的最长公共前缀为 A[i + 1:] 与 B[j + 1:] 的最长公共前缀的长度加一，否则我们知道 A[i:] 与 B[j:] 的最长公共前缀为零。 这样我们就可以提出动态规划的解法：令 dp[i][j] 表示 A[i:] 和 B[j:] 的最长公共前缀，那么答案即为所有 dp[i][j] 中的最大值。如果 A[i] == B[j]，那么 dp[i][j] = dp[i + 1][j + 1] + 1，否则 dp[i][j] = 0。 这里借用了 Python 表示数组的方法，A[i:] 表示数组 A 中索引 i 到数组末尾的范围对应的子数组。 考虑到这里 dp[i][j] 的值从 dp[i + 1][j + 1] 转移得到，所以我们需要倒过来，首先计算 dp[len(A) - 1][len(B) - 1]，最后计算 dp[0][0]。 class Solution { public int findLength(int[] nums1, int[] nums2) { // dp[i][j]表示nums1[i:]和nums2[j:]的最长公共前缀长度 int[][] dp = new int[nums1.length+1][nums2.length+1]; int max =0; for(int i=nums1.length-1;i\u003e=0;i--){ for(int j= nums2.length-1;j\u003e=0;j--){ if(nums1[i]==nums2[j]){ dp[i][j] = dp[i+1][j+1]+1; } max = Math.max(dp[i][j],max); } } return max; } } ","date":"2023-01-08","objectID":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/:129:1","tags":["leetcode"],"title":"Leetcode每日一题","uri":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/"},{"categories":["数据结构与算法"],"content":"day129 ","date":"2023-01-08","objectID":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/:130:0","tags":["leetcode"],"title":"Leetcode每日一题","uri":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/"},{"categories":["数据结构与算法"],"content":"1035. 不相交的线 题目 在两条独立的水平线上按给定的顺序写下 nums1 和 nums2 中的整数。 现在，可以绘制一些连接两个数字 nums1[i] 和 nums2[j] 的直线，这些直线需要同时满足满足： nums1[i] == nums2[j] 且绘制的直线不与任何其他连线（非水平线）相交。 请注意，连线即使在端点也不能相交：每个数字只能属于一条连线。 以这种方法绘制线条，并返回可以绘制的最大连线数。 示例 1： 输入：nums1 = [1,4,2], nums2 = [1,2,4] 输出：2 解释：可以画出两条不交叉的线，如上图所示。 但无法画出第三条不相交的直线，因为从 nums1[1]=4 到 nums2[2]=4 的直线将与从 nums1[2]=2 到 nums2[1]=2 的直线相交。 示例 2： 输入：nums1 = [2,5,1,2,5], nums2 = [10,5,2,1,5,2] 输出：3 示例 3： 输入：nums1 = [1,3,7,1,7,5], nums2 = [1,9,2,5,1] 输出：2 提示： 1 \u003c= nums1.length, nums2.length \u003c= 500 1 \u003c= nums1[i], nums2[j] \u003c= 2000 解法 解法一：动态规划 其实这个问题本质上就是求解最长公共子序列问题 class Solution { public int maxUncrossedLines(int[] nums1, int[] nums2) { // dp[i][j]表示nums1[:i]与nums2[:j]中最长公共子序列的长度 int[][] dp = new int[nums1.length+1][nums2.length+1]; // 初始状态 dp[0][0] = 0; for(int i=1;i\u003c=nums1.length;i++){ for(int j=1;j\u003c=nums2.length;j++){ if(nums1[i-1]==nums2[j-1]){ dp[i][j] = dp[i-1][j-1]+1; }else{ dp[i][j] = Math.max(dp[i-1][j],dp[i][j-1]); } } } return dp[nums1.length][nums2.length]; } } ","date":"2023-01-08","objectID":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/:130:1","tags":["leetcode"],"title":"Leetcode每日一题","uri":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/"},{"categories":["数据结构与算法"],"content":"day130 ","date":"2023-01-08","objectID":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/:131:0","tags":["leetcode"],"title":"Leetcode每日一题","uri":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/"},{"categories":["数据结构与算法"],"content":"1646. 获取生成数组中的最大值 题目 给你一个整数 n 。按下述规则生成一个长度为 n + 1 的数组 nums ： nums[0] = 0 nums[1] = 1 当 2 \u003c= 2 * i \u003c= n 时，nums[2 * i] = nums[i] 当 2 \u003c= 2 * i + 1 \u003c= n 时，nums[2 * i + 1] = nums[i] + nums[i + 1] 返回生成数组 nums 中的 最大 值。 示例 1： 输入：n = 7 输出：3 解释：根据规则： nums[0] = 0 nums[1] = 1 nums[(1 * 2) = 2] = nums[1] = 1 nums[(1 * 2) + 1 = 3] = nums[1] + nums[2] = 1 + 1 = 2 nums[(2 * 2) = 4] = nums[2] = 1 nums[(2 * 2) + 1 = 5] = nums[2] + nums[3] = 1 + 2 = 3 nums[(3 * 2) = 6] = nums[3] = 2 nums[(3 * 2) + 1 = 7] = nums[3] + nums[4] = 2 + 1 = 3 因此，nums = [0,1,1,2,1,3,2,3]，最大值 3 示例 2： 输入：n = 2 输出：1 解释：根据规则，nums[0]、nums[1] 和 nums[2] 之中的最大值是 1 示例 3： 输入：n = 3 输出：2 解释：根据规则，nums[0]、nums[1]、nums[2] 和 nums[3] 之中的最大值是 2 提示： 0 \u003c= n \u003c= 100 解法 解法一：模拟 class Solution { public int getMaximumGenerated(int n) { if(n\u003c=0) return 0; int max = 1; int[] nums = new int[n+1]; nums[1] = 1; for(int i=2;i\u003c=n;i++){ if(i%2==0){ nums[i] = nums[i/2]; }else{ nums[i] = nums[(i-1)/2] + nums[(i+1)/2]; } max = Math.max(max,nums[i]); } // System.out.println(Arrays.toString(nums)); return max; } } ","date":"2023-01-08","objectID":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/:131:1","tags":["leetcode"],"title":"Leetcode每日一题","uri":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/"},{"categories":["数据结构与算法"],"content":"day131 ","date":"2023-01-08","objectID":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/:132:0","tags":["leetcode"],"title":"Leetcode每日一题","uri":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/"},{"categories":["数据结构与算法"],"content":"79. 单词搜索 题目 解法 解法一：二进制法和dfs法 官方解法 解法二：暴力dfs 思路比较简单，对每一个位置进行四个方向的dfs，找到即可结束dfs了。 结束条件：当前i处字符相等，且是word最后一个字符，可以return true了 class Solution { public boolean exist(char[][] board, String word) { boolean ok =false; boolean[][] v = new boolean[board.length][board[0].length]; for(int i=0;i\u003cboard.length;i++){ for(int j=0;j\u003cboard[0].length;j++){ ok = dfs(board,word,i,j,0,v); if(ok) return true; // System.out.println(ok); } } return false; } public boolean dfs(char[][] board,String word,int nextI,int nextJ,int i,boolean[][] v){ if(word.charAt(i)!=board[nextI][nextJ]){ return false; }else if (i == word.length() - 1) { return true; } // System.out.println(board[nextI][nextJ]); v[nextI][nextJ] = true; boolean ok = false; // 四个方向dfs if(ok_(board,nextI+1,nextJ)\u0026\u0026!v[nextI+1][nextJ]){// 右 ok = ok||dfs(board,word,nextI+1,nextJ,i+1,v); } if(ok_(board,nextI-1,nextJ)\u0026\u0026!v[nextI-1][nextJ]){// 左 ok = ok||dfs(board,word,nextI-1,nextJ,i+1,v); } if(ok_(board,nextI,nextJ+1)\u0026\u0026!v[nextI][nextJ+1]){// 下 ok = ok||dfs(board,word,nextI,nextJ+1,i+1,v); } if(ok_(board,nextI,nextJ-1)\u0026\u0026!v[nextI][nextJ-1]){// 上 ok = ok||dfs(board,word,nextI,nextJ-1,i+1,v); } v[nextI][nextJ] = false; return ok; } public boolean ok_(char[][] board,int newi,int newj){ return newi \u003e= 0 \u0026\u0026 newi \u003c board.length \u0026\u0026 newj \u003e= 0 \u0026\u0026 newj \u003c board[0].length; } } func exist(board [][]byte, word string) bool { var dfs func(step,row,col int) v := make([][]bool,len(board)) m:=len(board) n:=len(board[0]) for i:=0;i\u003clen(board);i++{ v[i]=make([]bool,len(board[0])) } check:=func(i,j int)bool{ return i\u003e=0\u0026\u0026i\u003cm\u0026\u0026j\u003e=0\u0026\u0026j\u003cn } ans:=false dfs = func(step,row,col int){ if board[row][col]!=word[step]{ return } if step==len(word)-1{ ans = true return } // fmt.Println(string(board[row][col]),string(word[step])) v[row][col]=true dir:=[][]int{{-1,0},{1,0},{0,1},{0,-1}} for i:=0;i\u003c4;i++{ newrow:=row+dir[i][0] newcol:=col+dir[i][1] if check(newrow,newcol)\u0026\u0026!v[newrow][newcol]{ dfs(step+1,newrow,newcol) } } v[row][col]=false } for i:=0;i\u003cm;i++{ for j:=0;j\u003cn;j++{ if word[0]==board[i][j] { // 开始dfs dfs(0,i,j) if ans { return ans } } } } return false } ","date":"2023-01-08","objectID":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/:132:1","tags":["leetcode"],"title":"Leetcode每日一题","uri":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/"},{"categories":["数据结构与算法"],"content":"day132 ","date":"2023-01-08","objectID":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/:133:0","tags":["leetcode"],"title":"Leetcode每日一题","uri":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/"},{"categories":["数据结构与算法"],"content":"36. 有效的数独 题目 请你判断一个 9 x 9 的数独是否有效。只需要 根据以下规则 ，验证已经填入的数字是否有效即可。 数字 1-9 在每一行只能出现一次。 数字 1-9 在每一列只能出现一次。 数字 1-9 在每一个以粗实线分隔的 3x3 宫内只能出现一次。（请参考示例图） 注意： 一个有效的数独（部分已被填充）不一定是可解的。 只需要根据以上规则，验证已经填入的数字是否有效即可。 空白格用 '.' 表示。 示例 1： 输入：board = [[\"5\",\"3\",\".\",\".\",\"7\",\".\",\".\",\".\",\".\"] ,[\"6\",\".\",\".\",\"1\",\"9\",\"5\",\".\",\".\",\".\"] ,[\".\",\"9\",\"8\",\".\",\".\",\".\",\".\",\"6\",\".\"] ,[\"8\",\".\",\".\",\".\",\"6\",\".\",\".\",\".\",\"3\"] ,[\"4\",\".\",\".\",\"8\",\".\",\"3\",\".\",\".\",\"1\"] ,[\"7\",\".\",\".\",\".\",\"2\",\".\",\".\",\".\",\"6\"] ,[\".\",\"6\",\".\",\".\",\".\",\".\",\"2\",\"8\",\".\"] ,[\".\",\".\",\".\",\"4\",\"1\",\"9\",\".\",\".\",\"5\"] ,[\".\",\".\",\".\",\".\",\"8\",\".\",\".\",\"7\",\"9\"]] 输出：true 示例 2： 输入：board = [[\"8\",\"3\",\".\",\".\",\"7\",\".\",\".\",\".\",\".\"] ,[\"6\",\".\",\".\",\"1\",\"9\",\"5\",\".\",\".\",\".\"] ,[\".\",\"9\",\"8\",\".\",\".\",\".\",\".\",\"6\",\".\"] ,[\"8\",\".\",\".\",\".\",\"6\",\".\",\".\",\".\",\"3\"] ,[\"4\",\".\",\".\",\"8\",\".\",\"3\",\".\",\".\",\"1\"] ,[\"7\",\".\",\".\",\".\",\"2\",\".\",\".\",\".\",\"6\"] ,[\".\",\"6\",\".\",\".\",\".\",\".\",\"2\",\"8\",\".\"] ,[\".\",\".\",\".\",\"4\",\"1\",\"9\",\".\",\".\",\"5\"] ,[\".\",\".\",\".\",\".\",\"8\",\".\",\".\",\"7\",\"9\"]] 输出：false 解释：除了第一行的第一个数字从 5 改为 8 以外，空格内其他数字均与 示例1 相同。 但由于位于左上角的 3x3 宫内有两个 8 存在, 因此这个数独是无效的。 提示： board.length == 9 board[i].length == 9 board[i][j] 是一位数字（1-9）或者 '.' 解法 数组哈希法 class Solution { public boolean isValidSudoku(char[][] board) { int[][] rows = new int[9][9]; int[][] columns = new int[9][9]; int[][][] subboxes = new int[3][3][9]; for (int i = 0; i \u003c 9; i++) { for (int j = 0; j \u003c 9; j++) { char c = board[i][j]; if (c != '.') { int index = c - '0' - 1; rows[i][index]++; columns[j][index]++; subboxes[i / 3][j / 3][index]++; if (rows[i][index] \u003e 1 || columns[j][index] \u003e 1 || subboxes[i / 3][j / 3][index] \u003e 1) { return false; } } } } return true; } } ","date":"2023-01-08","objectID":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/:133:1","tags":["leetcode"],"title":"Leetcode每日一题","uri":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/"},{"categories":["数据结构与算法"],"content":"day133 ","date":"2023-01-08","objectID":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/:134:0","tags":["leetcode"],"title":"Leetcode每日一题","uri":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/"},{"categories":["数据结构与算法"],"content":"37. 解数独 题目 编写一个程序，通过填充空格来解决数独问题。 数独的解法需 遵循如下规则： 数字 1-9 在每一行只能出现一次。 数字 1-9 在每一列只能出现一次。 数字 1-9 在每一个以粗实线分隔的 3x3 宫内只能出现一次。（请参考示例图） 数独部分空格内已填入了数字，空白格用 '.' 表示。 示例 1： 输入：board = [[\"5\",\"3\",\".\",\".\",\"7\",\".\",\".\",\".\",\".\"],[\"6\",\".\",\".\",\"1\",\"9\",\"5\",\".\",\".\",\".\"],[\".\",\"9\",\"8\",\".\",\".\",\".\",\".\",\"6\",\".\"],[\"8\",\".\",\".\",\".\",\"6\",\".\",\".\",\".\",\"3\"],[\"4\",\".\",\".\",\"8\",\".\",\"3\",\".\",\".\",\"1\"],[\"7\",\".\",\".\",\".\",\"2\",\".\",\".\",\".\",\"6\"],[\".\",\"6\",\".\",\".\",\".\",\".\",\"2\",\"8\",\".\"],[\".\",\".\",\".\",\"4\",\"1\",\"9\",\".\",\".\",\"5\"],[\".\",\".\",\".\",\".\",\"8\",\".\",\".\",\"7\",\"9\"]] 输出：[[\"5\",\"3\",\"4\",\"6\",\"7\",\"8\",\"9\",\"1\",\"2\"],[\"6\",\"7\",\"2\",\"1\",\"9\",\"5\",\"3\",\"4\",\"8\"],[\"1\",\"9\",\"8\",\"3\",\"4\",\"2\",\"5\",\"6\",\"7\"],[\"8\",\"5\",\"9\",\"7\",\"6\",\"1\",\"4\",\"2\",\"3\"],[\"4\",\"2\",\"6\",\"8\",\"5\",\"3\",\"7\",\"9\",\"1\"],[\"7\",\"1\",\"3\",\"9\",\"2\",\"4\",\"8\",\"5\",\"6\"],[\"9\",\"6\",\"1\",\"5\",\"3\",\"7\",\"2\",\"8\",\"4\"],[\"2\",\"8\",\"7\",\"4\",\"1\",\"9\",\"6\",\"3\",\"5\"],[\"3\",\"4\",\"5\",\"2\",\"8\",\"6\",\"1\",\"7\",\"9\"]] 解释：输入的数独如上图所示，唯一有效的解决方案如下所示： 提示： board.length == 9 board[i].length == 9 board[i][j] 是一位数字或者 '.' 题目数据 保证 输入数独仅有一个解 解法 解法一：dfs+回溯 其实题目的解法很明显，dfs暴搜，主要是回溯的过程和以往的题目不太一样：填数独并不是挨着格子填的，所以回溯的时候也不是挨着格子回溯，先记录这些空格子的位置，回溯的时候按照记录来回溯即可 class Solution { private boolean[][] line = new boolean[9][9]; private boolean[][] column = new boolean[9][9]; private boolean[][][] block = new boolean[3][3][9]; private boolean valid = false; private List\u003cint[]\u003e spaces = new ArrayList\u003cint[]\u003e(); public void solveSudoku(char[][] board) { for (int i = 0; i \u003c 9; ++i) { for (int j = 0; j \u003c 9; ++j) { if (board[i][j] == '.') { spaces.add(new int[]{i, j}); } else { int digit = board[i][j] - '0' - 1; line[i][digit] = column[j][digit] = block[i / 3][j / 3][digit] = true; } } } dfs(board, 0); } public void dfs(char[][] board, int pos) { if (pos == spaces.size()) { valid = true; return; } int[] space = spaces.get(pos); int i = space[0], j = space[1]; for (int digit = 0; digit \u003c 9 \u0026\u0026 !valid; ++digit) { if (!line[i][digit] \u0026\u0026 !column[j][digit] \u0026\u0026 !block[i / 3][j / 3][digit]) { line[i][digit] = column[j][digit] = block[i / 3][j / 3][digit] = true; board[i][j] = (char) (digit + '0' + 1); dfs(board, pos + 1); line[i][digit] = column[j][digit] = block[i / 3][j / 3][digit] = false; } } } } 其他解法 https://leetcode.cn/problems/sudoku-solver/solutions/414120/jie-shu-du-by-leetcode-solution/ ","date":"2023-01-08","objectID":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/:134:1","tags":["leetcode"],"title":"Leetcode每日一题","uri":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/"},{"categories":["数据结构与算法"],"content":"day134 ","date":"2023-01-08","objectID":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/:135:0","tags":["leetcode"],"title":"Leetcode每日一题","uri":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/"},{"categories":["数据结构与算法"],"content":"51. N 皇后 题目 按照国际象棋的规则，皇后可以攻击与之处在同一行或同一列或同一斜线上的棋子。 n 皇后问题 研究的是如何将 n 个皇后放置在 n×n 的棋盘上，并且使皇后彼此之间不能相互攻击。 给你一个整数 n ，返回所有不同的 n 皇后问题 的解决方案。 每一种解法包含一个不同的 n 皇后问题 的棋子放置方案，该方案中 'Q' 和 '.' 分别代表了皇后和空位。 示例 1： 输入：n = 4 输出：[[\".Q..\",\"...Q\",\"Q...\",\"..Q.\"],[\"..Q.\",\"Q...\",\"...Q\",\".Q..\"]] 解释：如上图所示，4 皇后问题存在两个不同的解法。 示例 2： 输入：n = 1 输出：[[\"Q\"]] 提示： 1 \u003c= n \u003c= 9 解法 解法一：dfs+回溯 https://leetcode.cn/problems/n-queens/solutions/398929/nhuang-hou-by-leetcode-solution/ ","date":"2023-01-08","objectID":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/:135:1","tags":["leetcode"],"title":"Leetcode每日一题","uri":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/"},{"categories":["数据结构与算法"],"content":"day135 ","date":"2023-01-08","objectID":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/:136:0","tags":["leetcode"],"title":"Leetcode每日一题","uri":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/"},{"categories":["数据结构与算法"],"content":"52. N 皇后 II 题目 n 皇后问题 研究的是如何将 n 个皇后放置在 n × n 的棋盘上，并且使皇后彼此之间不能相互攻击。 给你一个整数 n ，返回 n 皇后问题 不同的解决方案的数量。 示例 1： 输入：n = 4 输出：2 解释：如上图所示，4 皇后问题存在两个不同的解法。 示例 2： 输入：n = 1 输出：1 提示： 1 \u003c= n \u003c= 9 解法 解法一：dfs+回溯 https://leetcode.cn/problems/n-queens-ii/solutions/449388/nhuang-hou-ii-by-leetcode-solution/ class Solution { int total = 0; public int totalNQueens(int n) { int[] queens = new int[n]; Arrays.fill(queens,-1); Set\u003cInteger\u003e column = new HashSet\u003c\u003e(); Set\u003cInteger\u003e l = new HashSet\u003c\u003e(); Set\u003cInteger\u003e r = new HashSet\u003c\u003e(); dfs(n,0,column,l,r); return total; } public void dfs(int n,int k,Set\u003cInteger\u003e column,Set\u003cInteger\u003e l,Set\u003cInteger\u003e r){ if(k==n){ total++; return; } for(int i=0;i\u003cn;i++){ if(column.contains(i)) continue; if(l.contains(k-i)) continue; if(r.contains(k+i)) continue; column.add(i); l.add(k-i); r.add(k+i); dfs(n,k+1,column,l,r); column.remove(i); l.remove(k-i); r.remove(k+i); } } } ","date":"2023-01-08","objectID":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/:136:1","tags":["leetcode"],"title":"Leetcode每日一题","uri":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/"},{"categories":["数据结构与算法"],"content":"day136 ","date":"2023-01-08","objectID":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/:137:0","tags":["leetcode"],"title":"Leetcode每日一题","uri":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/"},{"categories":["数据结构与算法"],"content":"191. 位1的个数 题目 编写一个函数，输入是一个无符号整数（以二进制串的形式），返回其二进制表达式中数字位数为 ‘1’ 的个数（也被称为汉明重量）。 提示： 请注意，在某些语言（如 Java）中，没有无符号整数类型。在这种情况下，输入和输出都将被指定为有符号整数类型，并且不应影响您的实现，因为无论整数是有符号的还是无符号的，其内部的二进制表示形式都是相同的。 在 Java 中，编译器使用二进制补码记法来表示有符号整数。因此，在 示例 3 中，输入表示有符号整数 -3。 示例 1： 输入：n = 00000000000000000000000000001011 输出：3 解释：输入的二进制串 00000000000000000000000000001011 中，共有三位为 '1'。 示例 2： 输入：n = 00000000000000000000000010000000 输出：1 解释：输入的二进制串 00000000000000000000000010000000 中，共有一位为 '1'。 示例 3： 输入：n = 11111111111111111111111111111101 输出：31 解释：输入的二进制串 11111111111111111111111111111101 中，共有 31 位为 '1'。 提示： 输入必须是长度为 32 的 二进制串 。 进阶： 如果多次调用这个函数，你将如何优化你的算法？ 解法 解法一：位运算 public class Solution { // you need to treat n as an unsigned value public int hammingWeight(int n) { int cnt = 0; while(n!=0){ if(n\u003c0) { // 取最高位 if(((n\u003e\u003e31)\u00261)==1) cnt++; n\u003c\u003c=1; }else { // 取最低位 if((n\u00261)==1) cnt++; n\u003e\u003e=1; } } return cnt; } } ","date":"2023-01-08","objectID":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/:137:1","tags":["leetcode"],"title":"Leetcode每日一题","uri":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/"},{"categories":["数据结构与算法"],"content":"day137 ","date":"2023-01-08","objectID":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/:138:0","tags":["leetcode"],"title":"Leetcode每日一题","uri":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/"},{"categories":["数据结构与算法"],"content":"231. 2 的幂 题目 给你一个整数 n，请你判断该整数是否是 2 的幂次方。如果是，返回 true ；否则，返回 false 。 如果存在一个整数 x 使得 n == 2x ，则认为 n 是 2 的幂次方。 示例 1： 输入：n = 1 输出：true 解释：20 = 1 示例 2： 输入：n = 16 输出：true 解释：24 = 16 示例 3： 输入：n = 3 输出：false 示例 4： 输入：n = 4 输出：true 示例 5： 输入：n = 5 输出：false 提示： -231 \u003c= n \u003c= 231 - 1 **进阶：**你能够不使用循环/递归解决此问题吗？ 解法 解法一：位运算 32个bit位，最多只有一个bit位为1才是2的幂，否则不是。 class Solution { public boolean isPowerOfTwo(int n) { boolean ok =false; if(n\u003c=0) return false; int cnt =0; for(int i=0;i\u003c32;i++){ if((n\u0026(1\u003c\u003ci))!=0) { cnt++; if(cnt\u003e1) return false; } } return true; } } 进阶：O(1)实现 https://leetcode.cn/problems/power-of-two/solutions/796201/2de-mi-by-leetcode-solution-rny3/ // n \u0026 (n - 1)移除最低位的1，这样还剩1的话，说明不满足 class Solution { public boolean isPowerOfTwo(int n) { return n \u003e 0 \u0026\u0026 (n \u0026 (n - 1)) == 0; } } // n \u0026 -n 直接获取最低位1. 并且n\u003e0，n \u0026 -n = n。所以n为2的幂 class Solution { public boolean isPowerOfTwo(int n) { return n \u003e 0 \u0026\u0026 (n \u0026 -n) == n; } } ","date":"2023-01-08","objectID":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/:138:1","tags":["leetcode"],"title":"Leetcode每日一题","uri":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/"},{"categories":["数据结构与算法"],"content":"day138 ","date":"2023-01-08","objectID":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/:139:0","tags":["leetcode"],"title":"Leetcode每日一题","uri":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/"},{"categories":["数据结构与算法"],"content":"200. 岛屿数量 题目 解法 解法一：并查集 class Solution { public int numIslands(char[][] grid) { int m = grid.length,n = grid[0].length; int x = 0; for(int i=0;i\u003cm;i++){ for(int j=0;j\u003cn;j++){ if(grid[i][j]=='0') x++; } } Uf uf = new Uf(n*m); for(int i=0;i\u003cm;i++){ for(int j=0;j\u003cn;j++){ if(grid[i][j]=='1') { needUf(grid,i,j,m,n,uf); } } } return uf.count - x; } // 判断当前网格是否需要union. 是岛屿且与前面的连通 public void needUf(char[][] grid,int i,int j,int row,int col,Uf uf){ int[][] dirs = new int[][]{{0,-1},{0,1},{1,0},{-1,0}}; for(int[] dir:dirs){ int newi = i+dir[0],newj = j+dir[1]; if(newi\u003c0||newi\u003e=row||newj\u003c0||newj\u003e=col) continue; if(grid[newi][newj] == '1') { if(!uf.connected(newi*col+newj,i*col+j)){ uf.union(newi*col+newj,i*col+j); } } } return; } } class Uf{ int count; // 连通分量 int[] parents; // 存储每个节点的父节点 public Uf(int n){ count = n; parents = new int[n]; for(int i=0;i\u003cn;i++){ parents[i] = i; } } // 将两个点连通 public void union(int p,int q){ int rootp = find(p); int rootq = find(q); if(rootp==rootq) return; parents[rootp] = rootq; count--; } public boolean connected(int p,int q){ return find(p)==find(q); } // 查找x的根节点 public int find(int x){ if(x!=parents[x]){ x = find(parents[x]); } return parents[x]; } } ","date":"2023-01-08","objectID":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/:139:1","tags":["leetcode"],"title":"Leetcode每日一题","uri":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/"},{"categories":["数据结构与算法"],"content":"day139 ","date":"2023-01-08","objectID":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/:140:0","tags":["leetcode"],"title":"Leetcode每日一题","uri":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/"},{"categories":["数据结构与算法"],"content":"547. 省份数量 题目 有 n 个城市，其中一些彼此相连，另一些没有相连。如果城市 a 与城市 b 直接相连，且城市 b 与城市 c 直接相连，那么城市 a 与城市 c 间接相连。 省份 是一组直接或间接相连的城市，组内不含其他没有相连的城市。 给你一个 n x n 的矩阵 isConnected ，其中 isConnected[i][j] = 1 表示第 i 个城市和第 j 个城市直接相连，而 isConnected[i][j] = 0 表示二者不直接相连。 返回矩阵中 省份 的数量。 示例 1： 输入：isConnected = [[1,1,0],[1,1,0],[0,0,1]] 输出：2 示例 2： 输入：isConnected = [[1,0,0],[0,1,0],[0,0,1]] 输出：3 提示： 1 \u003c= n \u003c= 200 n == isConnected.length n == isConnected[i].length isConnected[i][j] 为 1 或 0 isConnected[i][i] == 1 isConnected[i][j] == isConnected[j][i] 解法 解法一：并查集 class Solution { public int findCircleNum(int[][] isConnected) { int n=isConnected.length; UF uf = new UF(n); for(int i=0;i\u003cn;i++){ for(int j=i+1;j\u003cn;j++){ // 剪枝：对称矩阵只需要对角线一半即可 if(isConnected[i][j]==1/*\u0026\u0026!uf.connected(i,j)*/){ uf.union(i,j); } } } return uf.cnt; } } class UF{ int cnt; // 连通分量个数 int[] parents; // 存储父节点 public UF(int n){ cnt = n; parents = new int[n]; for(int i=0;i\u003cn;i++) parents[i] = i; } public void union(int a,int b){ int ra = find(a); int rb = find(b); if(ra==rb) return; // 已经连通 parents[ra] = rb; // 没连通，就将其中一个根节点拼上去 // parents[rb] = ra; cnt--; } // 判断a和b是否连通（是否含有相同根节点） public boolean connected(int a, int b){ return find(a)==find(b); } // 寻找根节点 public int find(int x){ if(x!=parents[x]){ parents[x] = find(parents[x]); } return parents[x]; } } ","date":"2023-01-08","objectID":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/:140:1","tags":["leetcode"],"title":"Leetcode每日一题","uri":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/"},{"categories":["数据结构与算法"],"content":"day140 ","date":"2023-01-08","objectID":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/:141:0","tags":["leetcode"],"title":"Leetcode每日一题","uri":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/"},{"categories":["数据结构与算法"],"content":"2. 两数相加 题目 给你两个 非空 的链表，表示两个非负的整数。它们每位数字都是按照 逆序 的方式存储的，并且每个节点只能存储 一位 数字。 请你将两个数相加，并以相同形式返回一个表示和的链表。 你可以假设除了数字 0 之外，这两个数都不会以 0 开头。 示例 1： 输入：l1 = [2,4,3], l2 = [5,6,4] 输出：[7,0,8] 解释：342 + 465 = 807. 示例 2： 输入：l1 = [0], l2 = [0] 输出：[0] 示例 3： 输入：l1 = [9,9,9,9,9,9,9], l2 = [9,9,9,9] 输出：[8,9,9,9,0,0,0,1] 提示： 每个链表中的节点数在范围 [1, 100] 内 0 \u003c= Node.val \u003c= 9 题目数据保证列表表示的数字不含前导零 解法 解法一：模拟 模拟数学上两数相加法则：低位相加大于10，减10得进位1，模10得本位。 /** * Definition for singly-linked list. * type ListNode struct { * Val int * Next *ListNode * } */ func addTwoNumbers(l1 *ListNode, l2 *ListNode) *ListNode { jw := 0 l3 := \u0026ListNode{} tmp := l3 rs := 0 for l1!=nil\u0026\u0026l2!=nil { rs = l1.Val+l2.Val+jw; tmp.Val = rs%10 jw = rs/10 if(l1.Next!=nil||l2.Next!=nil){ tmp.Next = new(ListNode) tmp = tmp.Next } l1 = l1.Next l2 = l2.Next } for(l1!=nil){ rs = l1.Val+jw tmp.Val = rs%10 jw = rs/10 if(l1.Next!=nil){ tmp.Next = new(ListNode) tmp = tmp.Next } l1=l1.Next } for(l2!=nil){ rs = l2.Val+jw tmp.Val = rs%10 jw = rs/10 if(l2.Next!=nil) { tmp.Next = new(ListNode) tmp = tmp.Next } l2=l2.Next } if(jw!=0){ tmp.Next = new(ListNode) tmp.Next.Val = jw } return l3 } ","date":"2023-01-08","objectID":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/:141:1","tags":["leetcode"],"title":"Leetcode每日一题","uri":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/"},{"categories":["数据结构与算法"],"content":"day141 ","date":"2023-01-08","objectID":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/:142:0","tags":["leetcode"],"title":"Leetcode每日一题","uri":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/"},{"categories":["数据结构与算法"],"content":"3. 无重复字符的最长子串 题目 给定一个字符串 s ，请你找出其中不含有重复字符的 最长子串 的长度。 示例 1: 输入: s = \"abcabcbb\" 输出: 3 解释: 因为无重复字符的最长子串是 \"abc\"，所以其长度为 3。 示例 2: 输入: s = \"bbbbb\" 输出: 1 解释: 因为无重复字符的最长子串是 \"b\"，所以其长度为 1。 示例 3: 输入: s = \"pwwkew\" 输出: 3 解释: 因为无重复字符的最长子串是 \"wke\"，所以其长度为 3。 请注意，你的答案必须是 子串 的长度，\"pwke\" 是一个子序列，不是子串。 提示： 0 \u003c= s.length \u003c= 5 * 104 s 由英文字母、数字、符号和空格组成 解法 解法一：滑动窗口 https://leetcode.cn/problems/longest-substring-without-repeating-characters/solutions/227999/wu-zhong-fu-zi-fu-de-zui-chang-zi-chuan-by-leetc-2/ ","date":"2023-01-08","objectID":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/:142:1","tags":["leetcode"],"title":"Leetcode每日一题","uri":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/"},{"categories":["数据结构与算法"],"content":"day142 ","date":"2023-01-08","objectID":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/:143:0","tags":["leetcode"],"title":"Leetcode每日一题","uri":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/"},{"categories":["数据结构与算法"],"content":"4. 寻找两个正序数组的中位数 题目 给定两个大小分别为 m 和 n 的正序（从小到大）数组 nums1 和 nums2。请你找出并返回这两个正序数组的 中位数 。 算法的时间复杂度应该为 O(log (m+n)) 。 示例 1： 输入：nums1 = [1,3], nums2 = [2] 输出：2.00000 解释：合并数组 = [1,2,3] ，中位数 2 示例 2： 输入：nums1 = [1,2], nums2 = [3,4] 输出：2.50000 解释：合并数组 = [1,2,3,4] ，中位数 (2 + 3) / 2 = 2.5 提示： nums1.length == m nums2.length == n 0 \u003c= m \u003c= 1000 0 \u003c= n \u003c= 1000 1 \u003c= m + n \u003c= 2000 -106 \u003c= nums1[i], nums2[i] \u003c= 106 解法 解法一：双指针 这个问题最核心的就是合并两个有序数组。我们使用双指针算法：一个指针指向数组nums1首部，另一个指向数组nums2首部，然后比较大小，最小的放到新数组最前面，然后移动那个最小指针再和其他数组指针比较。如果某个数组元素提前结束，将这个数组的后面元素附加过来 func findMedianSortedArrays(nums1 []int, nums2 []int) float64 { // 合并 nums := make([]int,len(nums1)+len(nums2)) i1:=0 i2:=0 for i:=0;i\u003clen(nums);i++ { if i1\u003clen(nums1)\u0026\u0026i2\u003clen(nums2) { if nums1[i1]\u003cnums2[i2]{ nums[i] = nums1[i1] i1++ }else{ nums[i] = nums2[i2] i2++ } }else if i1\u003e=len(nums1) \u0026\u0026 i2 \u003clen(nums2) { nums[i] = nums2[i2] i2++ }else if i1\u003clen(nums1) \u0026\u0026 i2 \u003e=len(nums2){ nums[i] = nums1[i1] i1++ } } if len(nums)%2==1{ return float64(nums[len(nums)/2]) } return (0.0 + float64(nums[len(nums)/2]+nums[len(nums)/2-1])) / 2 } ","date":"2023-01-08","objectID":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/:143:1","tags":["leetcode"],"title":"Leetcode每日一题","uri":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/"},{"categories":["数据结构与算法"],"content":"day143 ","date":"2023-01-08","objectID":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/:144:0","tags":["leetcode"],"title":"Leetcode每日一题","uri":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/"},{"categories":["数据结构与算法"],"content":"5. 最长回文子串 题目 给你一个字符串 s，找到 s 中最长的回文子串。 如果字符串的反序与原始字符串相同，则该字符串称为回文字符串。 示例 1： 输入：s = \"babad\" 输出：\"bab\" 解释：\"aba\" 同样是符合题意的答案。 示例 2： 输入：s = \"cbbd\" 输出：\"bb\" 提示： 1 \u003c= s.length \u003c= 1000 s 仅由数字和英文字母组成 解法 解法一：动态规划 func longestPalindrome(s string) string { len:=len(s) if len\u003c2 { return s } maxlen:=1 begin:=0 dp := make([][]bool, len) // 初始化：长度为1的是回文串；长度为2的子串，但是两个字符相等的也是子串 for i := 0; i \u003c len; i++ { dp[i] = make([]bool, len) dp[i][i] = true if i\u003clen-1\u0026\u0026s[i]==s[i+1]{ dp[i][i+1] = true } } // 状态转移：P(i,j)=P(i+1,j−1)∧(Si==Sj) for L:=2;L\u003c=len;L++{ for i:=0;i\u003clen;i++{ j:=i+L-1 if j\u003e=len { break } if s[i] == s[j] \u0026\u0026 j-i+1\u003e2 { dp[i][j] = dp[i+1][j-1] } if dp[i][j] \u0026\u0026 maxlen\u003cj-i+1{ maxlen = j-i+1 begin = i } } } return s[begin:begin+maxlen] } ","date":"2023-01-08","objectID":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/:144:1","tags":["leetcode"],"title":"Leetcode每日一题","uri":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/"},{"categories":["数据结构与算法"],"content":"day144 ","date":"2023-01-08","objectID":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/:145:0","tags":["leetcode"],"title":"Leetcode每日一题","uri":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/"},{"categories":["数据结构与算法"],"content":"10. 正则表达式匹配 题目 给你一个字符串 s 和一个字符规律 p，请你来实现一个支持 '.' 和 '*' 的正则表达式匹配。 '.' 匹配任意单个字符 '*' 匹配零个或多个前面的那一个元素 所谓匹配，是要涵盖 整个 字符串 s的，而不是部分字符串。 示例 1： 输入：s = \"aa\", p = \"a\" 输出：false 解释：\"a\" 无法匹配 \"aa\" 整个字符串。 示例 2: 输入：s = \"aa\", p = \"a*\" 输出：true 解释：因为 '*' 代表可以匹配零个或多个前面的那一个元素, 在这里前面的元素就是 'a'。因此，字符串 \"aa\" 可被视为 'a' 重复了一次。 示例 3： 输入：s = \"ab\", p = \".*\" 输出：true 解释：\".*\" 表示可匹配零个或多个（'*'）任意字符（'.'）。 提示： 1 \u003c= s.length \u003c= 20 1 \u003c= p.length \u003c= 20 s 只包含从 a-z 的小写字母。 p 只包含从 a-z 的小写字母，以及字符 . 和 *。 保证每次出现字符 * 时，前面都匹配到有效的字符 解法 解法一：动态规划 https://leetcode.cn/problems/regular-expression-matching/solutions/296114/shou-hui-tu-jie-wo-tai-nan-liao-by-hyj8/ func isMatch(s string, p string) bool { m, n := len(s), len(p) // if m==0||n==0 {return false} // dp[i][j]表示s[:i]是否与p[:j]匹配 dp := make([][]bool, m+1) for i:=0;i\u003c=m;i++{ dp[i] = make([]bool,n+1) } matched := func(i,j int) bool { return s[i]==p[j]||p[j]=='.' } dp[0][0] = true for j:=1;j\u003c=n;j++{ if p[j-1]=='*' { dp[0][j] = dp[0][j-2] } } for i:=1;i\u003cm+1;i++{ for j:=1;j\u003cn+1;j++{ if matched(i-1,j-1){ dp[i][j] = dp[i-1][j-1] }else{ if p[j-1]=='*'{ if matched(i-1,j-2){// 三种情况 dp[i][j] = dp[i-1][j-1]||dp[i][j-2]||dp[i-1][j] }else{ dp[i][j] = dp[i][j-2] } } } } } return dp[m][n] } ","date":"2023-01-08","objectID":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/:145:1","tags":["leetcode"],"title":"Leetcode每日一题","uri":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/"},{"categories":["数据结构与算法"],"content":"day145 ","date":"2023-01-08","objectID":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/:146:0","tags":["leetcode"],"title":"Leetcode每日一题","uri":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/"},{"categories":["数据结构与算法"],"content":"21. 合并两个有序链表 题目 将两个升序链表合并为一个新的 升序 链表并返回。新链表是通过拼接给定的两个链表的所有节点组成的。 示例 1： 输入：l1 = [1,2,4], l2 = [1,3,4] 输出：[1,1,2,3,4,4] 示例 2： 输入：l1 = [], l2 = [] 输出：[] 示例 3： 输入：l1 = [], l2 = [0] 输出：[0] 提示： 两个链表的节点数目范围是 [0, 50] -100 \u003c= Node.val \u003c= 100 l1 和 l2 均按 非递减顺序 排列 解法 解法一：双指针 两个指针分别指向两个链表首部，依次比较首部大小，将小的从所在链表移除并移入到新的链表中，如果某个链表为空，则将另一个非空链表直接拿过来 /** * Definition for singly-linked list. * type ListNode struct { * Val int * Next *ListNode * } */ func mergeTwoLists(list1 *ListNode, list2 *ListNode) *ListNode { dummpy:=\u0026ListNode{} tmp:=dummpy for list1!=nil||list2!=nil { if list1!=nil\u0026\u0026list2!=nil{ if list1.Val\u003elist2.Val { tmp.Next = list2 tmp = tmp.Next list2 = list2.Next }else{ tmp.Next = list1 tmp = tmp.Next list1 = list1.Next } }else if list1==nil\u0026\u0026list2!=nil { tmp.Next = list2 list2 = nil }else if list1!=nil\u0026\u0026list2==nil{ tmp.Next = list1 list1 = nil } } return dummpy.Next } ","date":"2023-01-08","objectID":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/:146:1","tags":["leetcode"],"title":"Leetcode每日一题","uri":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/"},{"categories":["数据结构与算法"],"content":"day146 ","date":"2023-01-08","objectID":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/:147:0","tags":["leetcode"],"title":"Leetcode每日一题","uri":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/"},{"categories":["数据结构与算法"],"content":"23. 合并 K 个升序链表 题目 给你一个链表数组，每个链表都已经按升序排列。 请你将所有链表合并到一个升序链表中，返回合并后的链表。 示例 1： 输入：lists = [[1,4,5],[1,3,4],[2,6]] 输出：[1,1,2,3,4,4,5,6] 解释：链表数组如下： [ 1-\u003e4-\u003e5, 1-\u003e3-\u003e4, 2-\u003e6 ] 将它们合并到一个有序链表中得到。 1-\u003e1-\u003e2-\u003e3-\u003e4-\u003e4-\u003e5-\u003e6 示例 2： 输入：lists = [] 输出：[] 示例 3： 输入：lists = [[]] 输出：[] 提示： k == lists.length 0 \u003c= k \u003c= 10^4 0 \u003c= lists[i].length \u003c= 500 -10^4 \u003c= lists[i][j] \u003c= 10^4 lists[i] 按 升序 排列 lists[i].length 的总和不超过 10^4 解法 解法一：顺序地两两合并 k个链表合并可以抽象出两个链表进行合并后再和第三个链表进行两两合并，如此往复 /** * Definition for singly-linked list. * type ListNode struct { * Val int * Next *ListNode * } */ func mergeKLists(lists []*ListNode) *ListNode { if len(lists)==0{ return nil }else if len(lists)==1{ return lists[0] } var tmp *ListNode for i:=0;i\u003clen(lists);i++ { tmp = mergeTwoList(tmp,lists[i]) } return tmp } func mergeTwoList(l1,l2 *ListNode) *ListNode{ l:=\u0026ListNode{} head :=l for l1!=nil\u0026\u0026l2!=nil{ if l1.Val\u003cl2.Val{ l.Next = l1 l1 = l1.Next }else{ l.Next = l2 l2 = l2.Next } l = l.Next } for l1!=nil { l.Next = l1 l1 = nil } for l2!=nil{ l.Next = l2 l2 = nil } return head.Next } 解法二：分治法 不断将多个链表分成单个，然后两两配对合并 /** * Definition for singly-linked list. * type ListNode struct { * Val int * Next *ListNode * } */ func mergeKLists(lists []*ListNode) *ListNode { return split(lists,0,len(lists)-1) } func split(lists []*ListNode,l,r int) *ListNode { if l==r{ return lists[l] } if l\u003er{ return nil } mid :=(l+r)/2 l1:= split(lists,l,mid) l2:= split(lists,mid+1,r) return mergeTwoList(l1,l2) } func mergeTwoList(l1,l2 *ListNode) *ListNode{ l:=\u0026ListNode{} head :=l for l1!=nil\u0026\u0026l2!=nil{ if l1.Val\u003cl2.Val{ l.Next = l1 l1 = l1.Next }else{ l.Next = l2 l2 = l2.Next } l = l.Next } for l1!=nil { l.Next = l1 l1 = nil } for l2!=nil{ l.Next = l2 l2 = nil } return head.Next } ","date":"2023-01-08","objectID":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/:147:1","tags":["leetcode"],"title":"Leetcode每日一题","uri":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/"},{"categories":["数据结构与算法"],"content":"day147 ","date":"2023-01-08","objectID":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/:148:0","tags":["leetcode"],"title":"Leetcode每日一题","uri":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/"},{"categories":["数据结构与算法"],"content":"31. 下一个排列 题目 整数数组的一个 排列 就是将其所有成员以序列或线性顺序排列。 例如，arr = [1,2,3] ，以下这些都可以视作 arr 的排列：[1,2,3]、[1,3,2]、[3,1,2]、[2,3,1] 。 整数数组的 下一个排列 是指其整数的下一个字典序更大的排列。更正式地，如果数组的所有排列根据其字典顺序从小到大排列在一个容器中，那么数组的 下一个排列 就是在这个有序容器中排在它后面的那个排列。如果不存在下一个更大的排列，那么这个数组必须重排为字典序最小的排列（即，其元素按升序排列）。 例如，arr = [1,2,3] 的下一个排列是 [1,3,2] 。 类似地，arr = [2,3,1] 的下一个排列是 [3,1,2] 。 而 arr = [3,2,1] 的下一个排列是 [1,2,3] ，因为 [3,2,1] 不存在一个字典序更大的排列。 给你一个整数数组 nums ，找出 nums 的下一个排列。 必须** 原地 **修改，只允许使用额外常数空间。 示例 1： 输入：nums = [1,2,3] 输出：[1,3,2] 示例 2： 输入：nums = [3,2,1] 输出：[1,2,3] 示例 3： 输入：nums = [1,1,5] 输出：[1,5,1] 提示： 1 \u003c= nums.length \u003c= 100 0 \u003c= nums[i] \u003c= 100 解法 解法一：找数学规律 推导过程： 1,2,3-\u003e1,3,2 1,3,2-\u003e2,1,3 1,4,3,2-\u003e2,1,3,4 2,6,5,4,3,1-\u003e3,6,5,4,2,1-\u003e3,1,2,4,5,6-\u003e 2,6,7,5,4,1-\u003e2,7,1,4,5,6 3,7,5,4,1-\u003e4,7,5,3,1-\u003e4,1,3,7,5 6,5,4,1-\u003e1,4,5,6 1,3,2,4,5,6,9,10,11,15,14,19,16-\u003e1,3,2,4,5,6,9,10,11,15,16,14,19 从右往左看，碰到降序序列(两个就够了)，说明不需要修改降序序列前的顺序，只需要调整降序序列后，需要从后面的序列里挑出第一个大于当前降序序列首部的数字或没有也可以，然后再把后面的升序列反转。 func nextPermutation(nums []int) { if len(nums) \u003c= 1 { return } i := len(nums) - 2 j := len(nums) - 1 k := len(nums) - 1 // 找降序序列（从右往左） for i \u003e= 0 \u0026\u0026 j \u003e= 0 { if nums[i] \u003c nums[j] { break } i-- j-- } // i及其以后的序列是升序，i和j是降序（从右往左看） if i \u003e= 0 { // 不是最后一个排列 // find: A[i]\u003cA[k] for nums[i] \u003e= nums[k] { k-- } // swap A[i], A[k] nums[i], nums[k] = nums[k], nums[i] } // 反转 for i,j:=j,len(nums)-1;i\u003cj;i,j=i+1,j-1{ nums[i], nums[j] = nums[j], nums[i] } return } ","date":"2023-01-08","objectID":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/:148:1","tags":["leetcode"],"title":"Leetcode每日一题","uri":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/"},{"categories":["数据结构与算法"],"content":"day148 ","date":"2023-01-08","objectID":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/:149:0","tags":["leetcode"],"title":"Leetcode每日一题","uri":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/"},{"categories":["数据结构与算法"],"content":"32. 最长有效括号 题目 给你一个只包含 '(' 和 ')' 的字符串，找出最长有效（格式正确且连续）括号子串的长度。 示例 1： 输入：s = \"(()\" 输出：2 解释：最长有效括号子串是 \"()\" 示例 2： 输入：s = \")()())\" 输出：4 解释：最长有效括号子串是 \"()()\" 示例 3： 输入：s = \"\" 输出：0 提示： 0 \u003c= s.length \u003c= 3 * 104 s[i] 为 '(' 或 ')' 解法 方法一：栈 这个与普通括号匹配有点不一样，求解子串长度，因此我们只需要在栈中存放下标即可。为了分开不同连续子串括号，我们需要预留)作为分界线。 https://leetcode.cn/problems/longest-valid-parentheses/solutions/314827/shou-hua-tu-jie-zhan-de-xiang-xi-si-lu-by-hyj8/ func longestValidParentheses(s string) int { stack := []int{-1} max:=0 for i:=0;i\u003clen(s);i++{ if s[i]=='( stack = append(stack,i) }else{ stack = stack[:len(stack)-1] if len(stack)==0 { stack = append(stack,i) }else { l := i-stack[len(stack)-1] if l\u003emax{ max = l } } } } return max } 方法二：动态规划 https://leetcode.cn/problems/longest-valid-parentheses/solutions/314683/zui-chang-you-xiao-gua-hao-by-leetcode-solution/ ","date":"2023-01-08","objectID":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/:149:1","tags":["leetcode"],"title":"Leetcode每日一题","uri":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/"},{"categories":["数据结构与算法"],"content":"day149 ","date":"2023-01-08","objectID":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/:150:0","tags":["leetcode"],"title":"Leetcode每日一题","uri":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/"},{"categories":["数据结构与算法"],"content":"42. 接雨水 题目 给定 n 个非负整数表示每个宽度为 1 的柱子的高度图，计算按此排列的柱子，下雨之后能接多少雨水。 示例 1： 输入：height = [0,1,0,2,1,0,1,3,2,1,2,1] 输出：6 解释：上面是由数组 [0,1,0,2,1,0,1,3,2,1,2,1] 表示的高度图，在这种情况下，可以接 6 个单位的雨水（蓝色部分表示雨水）。 示例 2： 输入：height = [4,2,0,3,2,5] 输出：9 提示： n == height.length 1 \u003c= n \u003c= 2 * 104 0 \u003c= height[i] \u003c= 105 解法 官方三解法 https://leetcode.cn/problems/trapping-rain-water/solutions/692342/jie-yu-shui-by-leetcode-solution-tuvc/ func trap(height []int) int { n:=len(height) ans:=0 if n\u003c=1 { return ans } leftdp := make([]int,n) rightdp := make([]int,n) max := func(a,b int) int{ if a\u003eb{ return a } return b } min := func(a,b int) int{ if a\u003cb{ return a } return b } // 初始化 leftdp[0] = height[0] rightdp[n-1] = height[n-1] // 记录状态 for i:=1;i\u003cn;i++{ leftdp[i] = max(leftdp[i-1],height[i]) } for i:=n-2;i\u003e=0;i--{ rightdp[i] = max(rightdp[i+1],height[i]) } // 状态转移 for i:=0;i\u003cn;i++{ ans += min(leftdp[i],rightdp[i]) - height[i] } return ans } ","date":"2023-01-08","objectID":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/:150:1","tags":["leetcode"],"title":"Leetcode每日一题","uri":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/"},{"categories":["数据结构与算法"],"content":"day150 ","date":"2023-01-08","objectID":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/:151:0","tags":["leetcode"],"title":"Leetcode每日一题","uri":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/"},{"categories":["数据结构与算法"],"content":"48. 旋转图像 题目 解法 解法一：找数学规律 func rotate(matrix [][]int) { reverseColFunc := func(matrix [][]int,j int){ n:=len(matrix) for i:=0;i\u003cn/2;i++{ matrix[i][j],matrix[n-i-1][j] = matrix[n-i-1][j],matrix[i][j] } } for j:=0;j\u003clen(matrix[0]);j++{ reverseColFunc(matrix,j) } n:=len(matrix) m:=len(matrix[0]) for i:=0;i\u003cn;i++{ for j:=i+1;j\u003cm;j++{ matrix[i][j],matrix[j][i] = matrix[j][i],matrix[i][j] } } return } // 先按照列，对每一个列都进行反转 // 再进行矩阵对称变换 反转还可以优化 反转只需要O(n) func rotate(matrix [][]int) { n := len(matrix) // 水平翻转 for i := 0; i \u003c n/2; i++ { matrix[i], matrix[n-1-i] = matrix[n-1-i], matrix[i] } // 主对角线翻转 for i := 0; i \u003c n; i++ { for j := 0; j \u003c i; j++ { matrix[i][j], matrix[j][i] = matrix[j][i], matrix[i][j] } } } 其他解法 https://leetcode.cn/problems/rotate-image/solutions/526980/xuan-zhuan-tu-xiang-by-leetcode-solution-vu3m/ ","date":"2023-01-08","objectID":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/:151:1","tags":["leetcode"],"title":"Leetcode每日一题","uri":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/"},{"categories":["数据结构与算法"],"content":"day151 ","date":"2023-01-08","objectID":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/:152:0","tags":["leetcode"],"title":"Leetcode每日一题","uri":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/"},{"categories":["数据结构与算法"],"content":"49. 字母异位词分组 题目 给你一个字符串数组，请你将 字母异位词 组合在一起。可以按任意顺序返回结果列表。 字母异位词 是由重新排列源单词的所有字母得到的一个新单词。 示例 1: 输入: strs = [\"eat\", \"tea\", \"tan\", \"ate\", \"nat\", \"bat\"] 输出: [[\"bat\"],[\"nat\",\"tan\"],[\"ate\",\"eat\",\"tea\"]] 示例 2: 输入: strs = [\"\"] 输出: [[\"\"]] 示例 3: 输入: strs = [\"a\"] 输出: [[\"a\"]] 提示： 1 \u003c= strs.length \u003c= 104 0 \u003c= strs[i].length \u003c= 100 strs[i] 仅包含小写字母 解法 解法一：排序 题目的核心在于如何判断两个字母是“字母异位词”？其实“字母异位词”排序后是相同字母，因此，我们可以借用哈希表来存储并去重：将排序后具有相同字符串的序列合并在一起即可 func groupAnagrams(strs []string) [][]string { ans := [][]string{} m := make(map[string][]string,0) for i:=0;i\u003clen(strs);i++{ b:=[]byte(strs[i]) sort.Slice(b, func(i, j int) bool { return b[i] \u003c b[j] }) key:=string(b) m[key] = append(m[key],strs[i]) } for _,v:=range m{ ans = append(ans,v) } return ans } 解法二：计数 https://leetcode.cn/problems/group-anagrams/solutions/520469/zi-mu-yi-wei-ci-fen-zu-by-leetcode-solut-gyoc/ ","date":"2023-01-08","objectID":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/:152:1","tags":["leetcode"],"title":"Leetcode每日一题","uri":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/"},{"categories":["数据结构与算法"],"content":"day152 ","date":"2023-01-08","objectID":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/:153:0","tags":["leetcode"],"title":"Leetcode每日一题","uri":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/"},{"categories":["数据结构与算法"],"content":"53. 最大子数组和 题目 给你一个整数数组 nums ，请你找出一个具有最大和的连续子数组（子数组最少包含一个元素），返回其最大和。 子数组 是数组中的一个连续部分。 示例 1： 输入：nums = [-2,1,-3,4,-1,2,1,-5,4] 输出：6 解释：连续子数组 [4,-1,2,1] 的和最大，为 6 。 示例 2： 输入：nums = [1] 输出：1 示例 3： 输入：nums = [5,4,-1,7,8] 输出：23 提示： 1 \u003c= nums.length \u003c= 105 -104 \u003c= nums[i] \u003c= 104 **进阶：**如果你已经实现复杂度为 O(n) 的解法，尝试使用更为精妙的 分治法 求解。 解法 官方解法 func maxSubArray(nums []int) int { dp := make([]int,len(nums)) max := func(a,b int) int{ if a\u003eb { return a } return b } dp[0] = nums[0] ans := nums[0] for i:=1;i\u003clen(nums);i++{ dp[i] = max(dp[i-1]+nums[i],nums[i]) ans = max(dp[i],ans) } return ans } // dp[i]表示第i个数结尾的连续子数组的最大和 // dp[i] = max(nums[i],dp[i-1]+nums[i]) // 第i个数可以加入连续子序列，也可以不加入连续子序列，新开始一个新的子序列，主要依据是看谁最大 // 然后，找出所有连续子数组的最大和 ","date":"2023-01-08","objectID":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/:153:1","tags":["leetcode"],"title":"Leetcode每日一题","uri":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/"},{"categories":["数据结构与算法"],"content":"day153 ","date":"2023-01-08","objectID":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/:154:0","tags":["leetcode"],"title":"Leetcode每日一题","uri":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/"},{"categories":["数据结构与算法"],"content":"75. 颜色分类 题目 解法 解法一：排序 func sortColors(nums []int){ // 快速排序 // 分区 partition:=func(nums []int,start,end int)int{ pivot:=nums[end] left:=start right:=end for left\u003cright{ for left\u003cright\u0026\u0026nums[left]\u003c=pivot{ left++ } for left\u003cright\u0026\u0026nums[right]\u003e=pivot{ right-- } // 交换 nums[left],nums[right] = nums[right],nums[left] } nums[left],nums[end] = nums[end],nums[left] return left } var qsort func(nums []int,left ,right int) qsort=func(nums []int,left,right int){ if left\u003e=right{ return } p:=partition(nums,left,right) qsort(nums,left,p-1) qsort(nums,p+1,right) } qsort(nums,0,len(nums)-1) } 解法二：单指针 func swapColors(colors []int, target int) (countTarget int) { for i, c := range colors { if c == target { colors[i], colors[countTarget] = colors[countTarget], colors[i] countTarget++ } } return } func sortColors(nums []int) { count0 := swapColors(nums, 0) // 把 0 排到前面 swapColors(nums[count0:], 1) // nums[:count0] 全部是 0 了，对剩下的 nums[count0:] 把 1 排到前面 } 解法三：双指针 func sortColors(nums []int){ // 双指针 n0:=0 n1:=0 num:=0 for i:=0;i\u003clen(nums);i++{ num=nums[i] nums[i]=2 if num\u003c2{ nums[n1]=1 n1++ } if num\u003c1{ nums[n0]=0 n0++ } } } ","date":"2023-01-08","objectID":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/:154:1","tags":["leetcode"],"title":"Leetcode每日一题","uri":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/"},{"categories":["数据结构与算法"],"content":"day154 76. 最小覆盖子串 题目 给你一个字符串 s 、一个字符串 t 。返回 s 中涵盖 t 所有字符的最小子串。如果 s 中不存在涵盖 t 所有字符的子串，则返回空字符串 \"\" 。 注意： 对于 t 中重复字符，我们寻找的子字符串中该字符数量必须不少于 t 中该字符数量。 如果 s 中存在这样的子串，我们保证它是唯一的答案。 示例 1： 输入：s = \"ADOBECODEBANC\", t = \"ABC\" 输出：\"BANC\" 解释：最小覆盖子串 \"BANC\" 包含来自字符串 t 的 'A'、'B' 和 'C'。 示例 2： 输入：s = \"a\", t = \"a\" 输出：\"a\" 解释：整个字符串 s 是最小覆盖子串。 示例 3: 输入: s = \"a\", t = \"aa\" 输出: \"\" 解释: t 中两个字符 'a' 均应包含在 s 的子串中， 因此没有符合条件的子字符串，返回空字符串。 提示： m == s.length n == t.length 1 \u003c= m, n \u003c= 105 s 和 t 由英文字母组成 **进阶：**你能设计一个在 o(m+n) 时间内解决此问题的算法吗？ 解法 解法一：滑动窗口+哈希表 func minWindow(s string, t string) string { ori, cnt := map[byte]int{}, map[byte]int{} for i := 0; i \u003c len(t); i++ { ori[t[i]]++ } sLen := len(s) len := math.MaxInt32 ansL, ansR := -1, -1 check := func() bool { for k, v := range ori { if cnt[k] \u003c v { return false } } return true } for l, r := 0, 0; r \u003c sLen; r++ { if r \u003c sLen \u0026\u0026 ori[s[r]] \u003e 0 { cnt[s[r]]++ } // 查看窗口是否覆盖t，若覆盖再移动左指针缩小窗口，否则继续移动右指针扩大窗口 for check() \u0026\u0026 l \u003c= r { if (r - l + 1 \u003c len) { len = r - l + 1 ansL, ansR = l, l + len } if _, ok := ori[s[l]]; ok { cnt[s[l]] -= 1 } l++ } } if ansL == -1 { return \"\" } return s[ansL:ansR] } ","date":"2023-01-08","objectID":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/:155:0","tags":["leetcode"],"title":"Leetcode每日一题","uri":"/leecode%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/"},{"categories":["redis"],"content":"1.初识Redis Redis是一种键值型的NoSql数据库，这里有两个关键字： 键值型 NoSql 其中键值型，是指Redis中存储的数据都是以key、value对的形式存储，而value的形式多种多样，可以是字符串、数值、甚至json： 而NoSql则是相对于传统关系型数据库而言，有很大差异的一种数据库。 ","date":"2023-01-08","objectID":"/redis%E5%9F%BA%E7%A1%80/:0:0","tags":[],"title":"Redis基础","uri":"/redis%E5%9F%BA%E7%A1%80/"},{"categories":["redis"],"content":"1.1.认识NoSQL NoSql可以翻译做Not Only Sql（不仅仅是SQL），或者是No Sql（非Sql的）数据库。是相对于传统关系型数据库而言，有很大差异的一种特殊的数据库，因此也称之为非关系型数据库。 ","date":"2023-01-08","objectID":"/redis%E5%9F%BA%E7%A1%80/:1:0","tags":[],"title":"Redis基础","uri":"/redis%E5%9F%BA%E7%A1%80/"},{"categories":["redis"],"content":"1.1.1.结构化与非结构化 传统关系型数据库是结构化数据，每一张表都有严格的约束信息：字段名、字段数据类型、字段约束等等信息，插入的数据必须遵守这些约束： 而NoSql则对数据库格式没有严格约束，往往形式松散，自由。 可以是键值型： 也可以是文档型： 甚至可以是图格式： ","date":"2023-01-08","objectID":"/redis%E5%9F%BA%E7%A1%80/:1:1","tags":[],"title":"Redis基础","uri":"/redis%E5%9F%BA%E7%A1%80/"},{"categories":["redis"],"content":"1.1.2.关联和非关联 传统数据库的表与表之间往往存在关联，例如外键： 而非关系型数据库不存在关联关系，要维护关系要么靠代码中的业务逻辑，要么靠数据之间的耦合： { id: 1, name: \"张三\", orders: [ { id: 1, item: { id: 10, title: \"荣耀6\", price: 4999 } }, { id: 2, item: { id: 20, title: \"小米11\", price: 3999 } } ] } 此处要维护“张三”的订单与商品“荣耀”和“小米11”的关系，不得不冗余的将这两个商品保存在张三的订单文档中，不够优雅。还是建议用业务来维护关联关系。 ","date":"2023-01-08","objectID":"/redis%E5%9F%BA%E7%A1%80/:1:2","tags":[],"title":"Redis基础","uri":"/redis%E5%9F%BA%E7%A1%80/"},{"categories":["redis"],"content":"1.1.3.查询方式 传统关系型数据库会基于Sql语句做查询，语法有统一标准； 而不同的非关系数据库查询语法差异极大，五花八门各种各样。 ","date":"2023-01-08","objectID":"/redis%E5%9F%BA%E7%A1%80/:1:3","tags":[],"title":"Redis基础","uri":"/redis%E5%9F%BA%E7%A1%80/"},{"categories":["redis"],"content":"1.1.4.事务 传统关系型数据库能满足事务ACID的原则。 而非关系型数据库往往不支持事务，或者不能严格保证ACID的特性，只能实现基本的一致性。 ","date":"2023-01-08","objectID":"/redis%E5%9F%BA%E7%A1%80/:1:4","tags":[],"title":"Redis基础","uri":"/redis%E5%9F%BA%E7%A1%80/"},{"categories":["redis"],"content":"1.1.5.总结 除了上述四点以外，在存储方式、扩展性、查询性能上关系型与非关系型也都有着显著差异，总结如下： 存储方式 关系型数据库基于磁盘进行存储，会有大量的磁盘IO，对性能有一定影响 非关系型数据库，他们的操作更多的是依赖于内存来操作，内存的读写速度会非常快，性能自然会好一些 扩展性 关系型数据库集群模式一般是主从，主从数据一致，起到数据备份的作用，称为垂直扩展。 非关系型数据库可以将数据拆分，存储在不同机器上，可以保存海量数据，解决内存大小有限的问题。称为水平扩展。 关系型数据库因为表之间存在关联关系，如果做水平扩展会给数据查询带来很多麻烦 ","date":"2023-01-08","objectID":"/redis%E5%9F%BA%E7%A1%80/:1:5","tags":[],"title":"Redis基础","uri":"/redis%E5%9F%BA%E7%A1%80/"},{"categories":["redis"],"content":"1.2.认识Redis Redis诞生于2009年全称是Remote Dictionary Server 远程词典服务器，是一个基于内存的键值型NoSQL数据库。 特征： 键值（key-value）型，value支持多种不同数据结构，功能丰富 单线程，每个命令具备原子性 低延迟，速度快（基于内存、IO多路复用、良好的编码）。 支持数据持久化 支持主从集群、分片集群 支持多语言客户端 作者：Antirez Redis的官方网站地址：https://redis.io/ ","date":"2023-01-08","objectID":"/redis%E5%9F%BA%E7%A1%80/:2:0","tags":[],"title":"Redis基础","uri":"/redis%E5%9F%BA%E7%A1%80/"},{"categories":["redis"],"content":"1.3.安装Redis 大多数企业都是基于Linux服务器来部署项目，而且Redis官方也没有提供Windows版本的安装包。因此课程中我们会基于Linux系统来安装Redis. 此处选择的Linux版本为CentOS 7. ","date":"2023-01-08","objectID":"/redis%E5%9F%BA%E7%A1%80/:3:0","tags":[],"title":"Redis基础","uri":"/redis%E5%9F%BA%E7%A1%80/"},{"categories":["redis"],"content":"1.3.1.依赖库 Redis是基于C语言编写的，因此首先需要安装Redis所需要的gcc依赖： yum install -y gcc tcl ","date":"2023-01-08","objectID":"/redis%E5%9F%BA%E7%A1%80/:3:1","tags":[],"title":"Redis基础","uri":"/redis%E5%9F%BA%E7%A1%80/"},{"categories":["redis"],"content":"1.3.2.上传安装包并解压 然后将课前资料提供的Redis安装包上传到虚拟机的任意目录： 例如，我放到了/usr/local/src 目录： 解压缩： tar -xzf redis-6.2.6.tar.gz 解压后： 进入redis目录： cd redis-6.2.6 运行编译命令： make \u0026\u0026 make install 如果没有出错，应该就安装成功了。 默认的安装路径是在 /usr/local/bin目录下： 该目录已经默认配置到环境变量，因此可以在任意目录下运行这些命令。其中： redis-cli：是redis提供的命令行客户端 redis-server：是redis的服务端启动脚本 redis-sentinel：是redis的哨兵启动脚本 ","date":"2023-01-08","objectID":"/redis%E5%9F%BA%E7%A1%80/:3:2","tags":[],"title":"Redis基础","uri":"/redis%E5%9F%BA%E7%A1%80/"},{"categories":["redis"],"content":"1.3.3.启动 redis的启动方式有很多种，例如： 默认启动 指定配置启动 开机自启 ","date":"2023-01-08","objectID":"/redis%E5%9F%BA%E7%A1%80/:3:3","tags":[],"title":"Redis基础","uri":"/redis%E5%9F%BA%E7%A1%80/"},{"categories":["redis"],"content":"1.3.4.默认启动 安装完成后，在任意目录输入redis-server命令即可启动Redis： redis-server 如图： 这种启动属于前台启动，会阻塞整个会话窗口，窗口关闭或者按下CTRL + C则Redis停止。不推荐使用。 ","date":"2023-01-08","objectID":"/redis%E5%9F%BA%E7%A1%80/:3:4","tags":[],"title":"Redis基础","uri":"/redis%E5%9F%BA%E7%A1%80/"},{"categories":["redis"],"content":"1.3.5.指定配置启动 如果要让Redis以后台方式启动，则必须修改Redis配置文件，就在我们之前解压的redis安装包下（/usr/local/src/redis-6.2.6），名字叫redis.conf： 我们先将这个配置文件备份一份： cp redis.conf redis.conf.bck 然后修改redis.conf文件中的一些配置： # 允许访问的地址，默认是127.0.0.1，会导致只能在本地访问。修改为0.0.0.0则可以在任意IP访问，生产环境不要设置为0.0.0.0 bind 0.0.0.0 # 守护进程，修改为yes后即可后台运行 daemonize yes # 密码，设置后访问Redis必须输入密码 requirepass 123321 Redis的其它常见配置： # 监听的端口 port 6379 # 工作目录，默认是当前目录，也就是运行redis-server时的命令，日志、持久化等文件会保存在这个目录 dir . # 数据库数量，设置为1，代表只使用1个库，默认有16个库，编号0~15 databases 1 # 设置redis能够使用的最大内存 maxmemory 512mb # 日志文件，默认为空，不记录日志，可以指定日志文件名 logfile \"redis.log\" 启动Redis： # 进入redis安装目录 cd /usr/local/src/redis-6.2.6 # 启动 redis-server redis.conf 停止服务： # 利用redis-cli来执行 shutdown 命令，即可停止 Redis 服务， # 因为之前配置了密码，因此需要通过 -u 来指定密码 redis-cli -u 123321 shutdown ","date":"2023-01-08","objectID":"/redis%E5%9F%BA%E7%A1%80/:3:5","tags":[],"title":"Redis基础","uri":"/redis%E5%9F%BA%E7%A1%80/"},{"categories":["redis"],"content":"1.3.6.开机自启 我们也可以通过配置来实现开机自启。 首先，新建一个系统服务文件： vi /etc/systemd/system/redis.service 内容如下： [Unit] Description=redis-server After=network.target [Service] Type=forking ExecStart=/usr/local/bin/redis-server /usr/local/src/redis-6.2.6/redis.conf PrivateTmp=true [Install] WantedBy=multi-user.target 然后重载系统服务： systemctl daemon-reload 现在，我们可以用下面这组命令来操作redis了： # 启动 systemctl start redis # 停止 systemctl stop redis # 重启 systemctl restart redis # 查看状态 systemctl status redis 执行下面的命令，可以让redis开机自启： systemctl enable redis ","date":"2023-01-08","objectID":"/redis%E5%9F%BA%E7%A1%80/:3:6","tags":[],"title":"Redis基础","uri":"/redis%E5%9F%BA%E7%A1%80/"},{"categories":["redis"],"content":"1.4.Redis桌面客户端 安装完成Redis，我们就可以操作Redis，实现数据的CRUD了。这需要用到Redis客户端，包括： 命令行客户端 图形化桌面客户端 编程客户端 ","date":"2023-01-08","objectID":"/redis%E5%9F%BA%E7%A1%80/:4:0","tags":[],"title":"Redis基础","uri":"/redis%E5%9F%BA%E7%A1%80/"},{"categories":["redis"],"content":"1.4.1.Redis命令行客户端 Redis安装完成后就自带了命令行客户端：redis-cli，使用方式如下： redis-cli [options] [commonds] 其中常见的options有： -h 127.0.0.1：指定要连接的redis节点的IP地址，默认是127.0.0.1 -p 6379：指定要连接的redis节点的端口，默认是6379 -a 123321：指定redis的访问密码 其中的commonds就是Redis的操作命令，例如： ping：与redis服务端做心跳测试，服务端正常会返回pong 不指定commond时，会进入redis-cli的交互控制台： ","date":"2023-01-08","objectID":"/redis%E5%9F%BA%E7%A1%80/:4:1","tags":[],"title":"Redis基础","uri":"/redis%E5%9F%BA%E7%A1%80/"},{"categories":["redis"],"content":"1.4.2.图形化桌面客户端 GitHub上的大神编写了Redis的图形化桌面客户端，地址：https://github.com/uglide/RedisDesktopManager 不过该仓库提供的是RedisDesktopManager的源码，并未提供windows安装包。 在下面这个仓库可以找到安装包：https://github.com/lework/RedisDesktopManager-Windows/releases ","date":"2023-01-08","objectID":"/redis%E5%9F%BA%E7%A1%80/:4:2","tags":[],"title":"Redis基础","uri":"/redis%E5%9F%BA%E7%A1%80/"},{"categories":["redis"],"content":"1.4.3.安装 在课前资料中可以找到Redis的图形化桌面客户端： 解压缩后，运行安装程序即可安装： 安装完成后，在安装目录下找到rdm.exe文件： 双击即可运行： ","date":"2023-01-08","objectID":"/redis%E5%9F%BA%E7%A1%80/:4:3","tags":[],"title":"Redis基础","uri":"/redis%E5%9F%BA%E7%A1%80/"},{"categories":["redis"],"content":"1.4.4.建立连接 点击左上角的连接到Redis服务器按钮： 在弹出的窗口中填写Redis服务信息： 点击确定后，在左侧菜单会出现这个链接： 点击即可建立连接了。 Redis默认有16个仓库，编号从0至15. 通过配置文件可以设置仓库数量，但是不超过16，并且不能自定义仓库名称。 如果是基于redis-cli连接Redis服务，可以通过select命令来选择数据库： # 选择 0号库 select 0 2.Redis常见命令与数据类型 Redis是典型的key-value数据库，key一般是字符串，而value包含很多不同的数据类型： Redis为了方便我们学习，将操作不同数据类型的命令也做了分组，在官网（ https://redis.io/commands ）可以查看到不同的命令： 不同类型的命令称为一个group，我们也可以通过help命令来查看各种不同group的命令： 接下来，我们就学习常见的五种基本数据类型的相关命令。 ","date":"2023-01-08","objectID":"/redis%E5%9F%BA%E7%A1%80/:4:4","tags":[],"title":"Redis基础","uri":"/redis%E5%9F%BA%E7%A1%80/"},{"categories":["redis"],"content":"2.1.Redis通用命令 通用指令是部分数据类型的，都可以使用的指令，常见的有： KEYS：查看符合模板的所有key SCAN cursor [MATCH pattern] [COUNT count] [TYPE type]: 增量迭代遍历，相较于KEYS命令更友好，因为KEYS会返回所有键数据，如果数据量太大会造成比较大的延时 SORT key [BY pattern] [LIMIT offset count] [GET pattern [GET pattern …]] [ASC|DESC] [ALPHA] [STORE destination]： 返回或存储 list, set 或sorted set 中的元素。默认是按照数值排序的，并且按照两个元素的双精度浮点数类型值进行比较。 DEL：删除一个指定的key EXISTS：判断key是否存在 EXPIRE：给一个key设置有效期，有效期到期时该key会被自动删除 TTL：查看一个KEY的剩余有效期 通过help [command] 可以查看一个命令的具体用法，例如： # 查看keys命令的帮助信息： 127.0.0.1:6379\u003e help keys KEYS pattern summary: Find all keys matching the given pattern since: 1.0.0 group: generic ","date":"2023-01-08","objectID":"/redis%E5%9F%BA%E7%A1%80/:5:0","tags":[],"title":"Redis基础","uri":"/redis%E5%9F%BA%E7%A1%80/"},{"categories":["redis"],"content":"2.1.1.Key结构 Redis没有类似MySQL中的Table的概念，我们该如何区分不同类型的key呢？ 例如，需要存储用户、商品信息到redis，有一个用户id是1，有一个商品id恰好也是1，此时如果使用id作为key，那就会冲突了，该怎么办？ 我们可以通过给key添加前缀加以区分，不过这个前缀不是随便加的，有一定的规范： Redis的key允许有多个单词形成层级结构，多个单词之间用’:‘隔开，格式如下： 项目名:业务名:类型:id 这个格式并非固定，也可以根据自己的需求来删除或添加词条。这样以来，我们就可以把不同类型的数据区分开了。从而避免了key的冲突问题。 例如我们的项目名称叫 heima，有user和product两种不同类型的数据，我们可以这样定义key： user相关的key：heima:user:1 product相关的key：heima:product:1 如果Value是一个Java对象，例如一个User对象，则可以将对象序列化为JSON字符串后存储： KEY VALUE heima:user:1 {“id”:1, “name”: “Jack”, “age”: 21} heima:product:1 {“id”:1, “name”: “小米11”, “price”: 4999} 并且，在Redis的桌面客户端中，还会以相同前缀作为层级结构，让数据看起来层次分明，关系清晰： ","date":"2023-01-08","objectID":"/redis%E5%9F%BA%E7%A1%80/:5:1","tags":[],"title":"Redis基础","uri":"/redis%E5%9F%BA%E7%A1%80/"},{"categories":["redis"],"content":"2.2.String类型 String类型，也就是字符串类型，是Redis中最简单的存储类型。 其value是字符串，不过根据字符串的格式不同，又可以分为3类： string：普通字符串 int：整数类型，可以做自增、自减操作 float：浮点类型，可以做自增、自减操作 不管是哪种格式，底层都是字节数组形式存储，只不过是编码方式不同。字符串类型的最大空间不能超过512m. ","date":"2023-01-08","objectID":"/redis%E5%9F%BA%E7%A1%80/:6:0","tags":[],"title":"Redis基础","uri":"/redis%E5%9F%BA%E7%A1%80/"},{"categories":["redis"],"content":"2.2.1.String的常见命令 String的常见命令有： SET：添加或者修改已经存在的一个String类型的键值对 GET：根据key获取String类型的value MSET：批量添加多个String类型的键值对 MGET：根据多个key获取多个String类型的value INCR：让一个整型的key自增1 INCRBY:让一个整型的key自增并指定步长，例如：incrby num 2 让num值自增2 INCRBYFLOAT：让一个浮点类型的数字自增并指定步长 SETNX：添加一个String类型的键值对，前提是这个key不存在，否则不执行 SETEX：添加一个String类型的键值对，并且指定有效期 ","date":"2023-01-08","objectID":"/redis%E5%9F%BA%E7%A1%80/:6:1","tags":[],"title":"Redis基础","uri":"/redis%E5%9F%BA%E7%A1%80/"},{"categories":["redis"],"content":"2.3.Hash类型 Hash类型，也叫散列，其value是一个无序字典，类似于Java中的HashMap结构。 String结构是将对象序列化为JSON字符串后存储，当需要修改对象某个字段时很不方便： Hash结构可以将对象中的每个字段独立存储，可以针对单个字段做CRUD： Hash的常见命令有： HSET key field value：添加或者修改hash类型key的field的值 HGET key field：获取一个hash类型key的field的值 HDEL：删除一个或多个哈希表字段 HEXISTS：查看哈希表 key 中，指定的字段是否存在 HMSET：批量添加多个hash类型key的field的值 HMGET：批量获取多个hash类型key的field的值 HGETALL：获取一个hash类型的key中的所有的field和value HSCAN：迭代哈希表中的键值对（当然，一般hash不会存有巨量键值对，所以，HSCAN的意义不是很大） HKEYS：获取一个hash类型的key中的所有的field HLEN：获取哈希表中字段的数量 HSTRLEN：返回哈希表 key 中， 与给定域 field 相关联的值的字符串长度 HVALS：获取哈希表中所有值 HINCRBY:让一个hash类型key的字段值自增并指定步长 HINCRBYFLOAT：为哈希表 key 中的指定字段的浮点数值加上增量 increment HSETNX：添加一个hash类型的key的field值，前提是这个field不存在，否则不执行 ","date":"2023-01-08","objectID":"/redis%E5%9F%BA%E7%A1%80/:7:0","tags":[],"title":"Redis基础","uri":"/redis%E5%9F%BA%E7%A1%80/"},{"categories":["redis"],"content":"2.4.List类型 Redis中的List类型与Java中的LinkedList类似，可以看做是一个双向链表结构。既可以支持正向检索和也可以支持反向检索。 特征也与LinkedList类似： 有序 元素可以重复 插入和删除快 查询速度一般 常用来存储一个有序数据，例如：朋友圈点赞列表，评论列表等。 List的常见命令有： LINDEX: 返回列表 key 里索引 index 位置存储的元素。（支持负数倒序索引，-1表示倒数第一个，-2表示倒数第二个） LPUSH key element … ：向列表左侧插入一个或多个元素 LPOP key：移除并返回列表左侧的第一个元素，没有则返回nil RPUSH key element … ：向列表右侧插入一个或多个元素 RPOP key：移除并返回列表右侧的第一个元素 LRANGE key star end：返回一段角标范围内的所有元素 BLPOP和BRPOP：与LPOP和RPOP类似，只不过在没有元素时等待指定时间，而不是直接返回nil. 阻塞版本操作 ","date":"2023-01-08","objectID":"/redis%E5%9F%BA%E7%A1%80/:8:0","tags":[],"title":"Redis基础","uri":"/redis%E5%9F%BA%E7%A1%80/"},{"categories":["redis"],"content":"2.5.Set类型 Redis的Set结构与Java中的HashSet类似，可以看做是一个value为null的HashMap。因为也是一个hash表，因此具备与HashSet类似的特征： 无序 元素不可重复 查找快 支持交集、并集、差集等功能。这些功能很契合一些业务。 Set的常见命令有： SADD key member … ：向set中添加一个或多个元素 SREM key member … : 移除set中的指定元素 SCARD key： 返回set中元素的个数 SISMEMBER key member：判断一个元素是否存在于set中 SMEMBERS：获取set中的所有元素 SINTER key1 key2 … ：求key1与key2的交集 SDIFF：返回第一个集合和其他集合的差集 SDIFFSTORE：SDIFF过后再存储到destination SINTER：返回所有给定集合的成员交集。后缀-store就类似上面 SUINON：返回所有给定集合的并集。后缀-store就类似上面 SSCAN：迭代集合中的元素。当需要取出集合数据量巨大时，也可以考虑使用SSCAN迭代读取 例如两个集合：s1和s2: 求交集：SINTER s1 s2 求s1与s2的不同：SDIFF s1 s2 练习： 将下列数据用Redis的Set集合来存储： 张三的好友有：李四、王五、赵六 李四的好友有：王五、麻子、二狗 利用Set的命令实现下列功能： 计算张三的好友有几人 计算张三和李四有哪些共同好友 查询哪些人是张三的好友却不是李四的好友 查询张三和李四的好友总共有哪些人 判断李四是否是张三的好友 判断张三是否是李四的好友 将李四从张三的好友列表中移除 ","date":"2023-01-08","objectID":"/redis%E5%9F%BA%E7%A1%80/:9:0","tags":[],"title":"Redis基础","uri":"/redis%E5%9F%BA%E7%A1%80/"},{"categories":["redis"],"content":"2.6.SortedSet类型 Redis的SortedSet是一个可排序的set集合，与Java中的TreeSet有些类似，但底层数据结构却差别很大。SortedSet中的每一个元素都带有一个score属性，可以基于score属性对元素排序，底层的实现是一个跳表（SkipList）加 hash表。 SortedSet具备下列特性： 可排序 元素不重复 查询速度快 因为SortedSet的可排序特性，经常被用来实现排行榜这样的功能。 SortedSet的常见命令有： ZADD key [NX|XX] [GT|LT] [CH] [INCR] score member [score member …]：添加一个或多个元素到sorted set ，如果已经存在则更新其score值 ZADD 支持参数，参数位于 key 名字和第一个 score 参数之间: XX: 仅更新存在的成员，不添加新成员。 NX: 不更新存在的成员。只添加新成员。 LT: 更新新的分值比当前分值小的成员，不存在则新增。 GT: 更新新的分值比当前分值大的成员，不存在则新增。 CH: 返回变更成员的数量。变更的成员是指 新增成员 和 score值更新的成员，命令指明的和之前score值相同的成员不计在内。 注意: 在通常情况下，ZADD返回值只计算新添加成员的数量。 INCR: ZADD 使用该参数与 ZINCRBY 功能一样。一次只能操作一个score-element对。 注意: GT, LT 和 NX 三者互斥不能同时使用。 ZREM key member：删除sorted set中的一个指定元素 ZSCORE key member : 获取sorted set中的指定元素的score值 ZRANK key member：获取sorted set 中的指定元素的排名 ZCARD key：获取sorted set中的元素个数 ZCOUNT key min max：统计score值在给定范围内的所有元素的个数。注意时间复杂度达到了O(logN) ZINCRBY key increment member：让sorted set中的指定元素自增，步长为指定的increment值 ZRANGE key min max：按照score排序后，获取指定排名范围内的元素 ZRANGEBYSCORE key min max：按照score排序后，获取指定score范围内的元素 ZDIFF、ZINTER、ZUNION：求差集、交集、并集 注意：所有的排名默认都是升序，如果要降序则在命令的Z后面添加REV即可，例如： 升序获取sorted set 中的指定元素的排名：ZRANK key member 降序获取sorted set 中的指定元素的排名：ZREVRANK key memeber 练习题： 将班级的下列学生得分存入Redis的SortedSet中： Jack 85, Lucy 89, Rose 82, Tom 95, Jerry 78, Amy 92, Miles 76 并实现下列功能： 删除Tom同学 获取Amy同学的分数 获取Rose同学的排名 查询80分以下有几个学生 给Amy同学加2分 查出成绩前3名的同学 查出成绩80分以下的所有同学 ","date":"2023-01-08","objectID":"/redis%E5%9F%BA%E7%A1%80/:10:0","tags":[],"title":"Redis基础","uri":"/redis%E5%9F%BA%E7%A1%80/"},{"categories":["redis"],"content":"2.7.HyperLogLog类型 Redis HyperLogLog 是用来做基数统计的算法，HyperLogLog 的优点是，在输入元素的数量或者体积非常非常大时，计算基数所需的空间总是固定 的、并且是很小的。 在 Redis 里面，每个 HyperLogLog 键只需要花费 12 KB 内存，就可以计算接近 264 个不同元素的基 数。这和计算基数时，元素越多耗费内存就越多的集合形成鲜明对比。 但是，因为 HyperLogLog 只会根据输入元素来计算基数，而不会储存输入元素本身，所以 HyperLogLog 不能像集合那样，返回输入的各个元素。 什么是基数? 比如数据集 {1, 3, 5, 7, 5, 7, 8}， 那么这个数据集的基数集为 {1, 3, 5 ,7, 8}, 基数(不重复元素)为5。 基数估计就是在误差可接受的范围内，快速计算基数。 常用命令： PFADD key element [element …]：添加指定元素到 HyperLogLog 中。 PFCOUNT key [key …]：返回给定 HyperLogLog 的基数估算值。 PFMERGE destkey sourcekey [sourcekey …]：将多个 HyperLogLog 合并为一个 HyperLogLog（并集） ","date":"2023-01-08","objectID":"/redis%E5%9F%BA%E7%A1%80/:11:0","tags":[],"title":"Redis基础","uri":"/redis%E5%9F%BA%E7%A1%80/"},{"categories":["redis"],"content":"2.8.GEO类型 Redis GEO 主要用于存储地理位置信息，并对存储的信息进行操作，该功能在 Redis 3.2 版本新增。 Redis GEO 操作方法有： GEOADD key longitude latitude member [longitude latitude member …]：添加地理位置的坐标。 GEOPOS key member [member …]：获取地理位置的坐标。 GEODIST key member1 member2 [m|km|ft|mi]：计算两个位置之间的距离。 最后一个距离单位参数说明： m ：米，默认单位。 km ：千米。 mi ：英里。 ft ：英尺。 GEORADIUS key longitude latitude radius m|km|ft|mi [WITHCOORD] [WITHDIST] [WITHHASH] [COUNT count] [ASC|DESC] [STORE key] [STOREDIST key]：根据用户给定的经纬度坐标来获取指定范围内的地理位置集合。 参数说明： m ：米，默认单位。 km ：千米。 mi ：英里。 ft ：英尺 WITHDIST: 在返回位置元素的同时， 将位置元素与中心之间的距离也一并返回。 WITHCOORD: 将位置元素的经度和维度也一并返回。 WITHHASH: 以 52 位有符号整数的形式， 返回位置元素经过原始 geohash 编码的有序集合分值。 这个选项主要用于底层应用或者调试， 实际中的作用并不大。 COUNT 限定返回的记录数。 ASC: 查找结果根据距离从近到远排序。 DESC: 查找结果根据从远到近排序。 GEORADIUSBYMEMBER key member radius m|km|ft|mi [WITHCOORD] [WITHDIST] [WITHHASH] [COUNT count] [ASC|DESC] [STORE key] [STOREDIST key]：根据储存在位置集合里面的某个地点获取指定范围内的地理位置集合。 参数说明： m ：米，默认单位。 km ：千米。 mi ：英里。 ft ：英尺。 WITHDIST: 在返回位置元素的同时， 将位置元素与中心之间的距离也一并返回。 WITHCOORD: 将位置元素的经度和维度也一并返回。 WITHHASH: 以 52 位有符号整数的形式， 返回位置元素经过原始 geohash 编码的有序集合分值。 这个选项主要用于底层应用或者调试， 实际中的作用并不大。 COUNT 限定返回的记录数。 ASC: 查找结果根据距离从近到远排序。 DESC: 查找结果根据从远到近排序。 GEOHASH key member [member …]：返回一个或多个位置对象的 geohash 值。 ","date":"2023-01-08","objectID":"/redis%E5%9F%BA%E7%A1%80/:12:0","tags":[],"title":"Redis基础","uri":"/redis%E5%9F%BA%E7%A1%80/"},{"categories":["redis"],"content":"2.9.Stream类型 Redis Stream 是 Redis 5.0 版本新增加的数据结构。 Stream 实际上是一个具有消息发布/订阅功能的组件，也就常说的消息队列。其实这种类似于 broker/consumer(生产者/消费者)的数据结构很常见，比如 RabbitMQ 消息中间件、Celery 消息中间件，以及 Kafka 分布式消息系统等，而 Redis Stream 正是借鉴了 Kafaka 系统。 1) 优点 Strean 除了拥有很高的性能和内存利用率外, 它最大的特点就是提供了消息的持久化存储，以及主从复制功能，从而解决了网络断开、Redis 宕机情况下，消息丢失的问题，即便是重启 Redis，存储的内容也会存在。 2) 流程 Stream 消息队列主要由四部分组成，分别是：消息本身、生产者、消费者和消费组，对于前述三者很好理解，下面了解什么是消费组。 一个 Stream 队列可以拥有多个消费组，每个消费组中又包含了多个消费者，组内消费者之间存在竞争关系。当某个消费者消费了一条消息时，同组消费者，都不会再次消费这条消息。被消费的消息 ID 会被放入等待处理的 Pending_ids 中。每消费完一条信息，消费组的游标就会向前移动一位，组内消费者就继续去争抢下消息。 Redis Stream 的结构如下所示，它有一个消息链表，将所有加入的消息都串起来，每个消息都有一个唯一的 ID 和对应的内容： 每个 Stream 都有唯一的名称，它就是 Redis 的 key，在我们首次使用 xadd 指令追加消息时自动创建。 上图解析： Consumer Group ：消费组，使用 XGROUP CREATE 命令创建，一个消费组有多个消费者(Consumer)。 last_delivered_id ：游标，每个消费组会有个游标 last_delivered_id ，任意一个消费者读取了消息都会使游标 last_delivered_id 往前移动。 pending_ids ：消费者(Consumer)的状态变量，作用是维护消费者的未确认的 id。 pending_ids 记录了当前已经被客户端读取的消息，但是还没有 ack (Acknowledge character：确认字符）。 消息队列相关命令： XADD - 添加消息到末尾 XTRIM - 对流进行修剪，限制长度 XDEL - 删除消息 XLEN - 获取流包含的元素数量，即消息长度 XRANGE - 获取消息列表，会自动过滤已经删除的消息 XREVRANGE - 反向获取消息列表，ID 从大到小 XREAD - 以阻塞或非阻塞方式获取消息列表 消费者组相关命令： XGROUP CREATE - 创建消费者组 XREADGROUP GROUP - 读取消费者组中的消息 XACK - 将消息标记为\"已处理\" XGROUP SETID - 为消费者组设置新的最后递送消息ID XGROUP DELCONSUMER - 删除消费者 XGROUP DESTROY - 删除消费者组 XPENDING - 显示待处理消息的相关信息 XCLAIM - 转移消息的归属权 XINFO - 查看流和消费者组的相关信息； XINFO GROUPS - 打印消费者组的信息； XINFO STREAM - 打印流信息 ","date":"2023-01-08","objectID":"/redis%E5%9F%BA%E7%A1%80/:13:0","tags":[],"title":"Redis基础","uri":"/redis%E5%9F%BA%E7%A1%80/"},{"categories":["redis"],"content":"2.10.bitmap位图操作 在平时开发过程中，经常会有一些 bool 类型数据需要存取。比如记录用户一年内签到的次数，签了是 1，没签是 0。如果使用 key-value 来存储，那么每个用户都要记录 365 次，当用户成百上亿时，需要的存储空间将非常巨大。为了解决这个问题，Redis 提供了位图结构。 位图（bitmap）同样属于 string 数据类型。Redis 中一个字符串类型的值最多能存储 512 MB 的内容，每个字符串由多个字节组成，每个字节又由 8 个 Bit 位组成。位图结构正是使用“位”来实现存储的，它通过将比特位设置为 0 或 1来达到数据存取的目的，这大大增加了 value 存储数量，它存储上限为2^32 。 位图本质上就是一个普通的字节串，也就是 bytes 数组。您可以使用getbit/setbit命令来处理这个位数组，位图的结构如下所示： 位图适用于一些特定的应用场景，比如用户签到次数、或者登录次数等。上图是表示一位用户 10 天内来网站的签到次数，1 代表签到，0 代表未签到，这样可以很轻松地统计出用户的活跃程度。相比于直接使用字符串而言，位图中的每一条记录仅占用一个 bit 位，从而大大降低了内存空间使用率。 Redis 官方也做了一个实验，他们模拟了一个拥有 1 亿 2 千 8 百万用户的系统，然后使用 Redis 的位图来统计“日均用户数量”，最终所用时间的约为 50ms，且仅仅占用 16 MB内存。 ","date":"2023-01-08","objectID":"/redis%E5%9F%BA%E7%A1%80/:14:0","tags":[],"title":"Redis基础","uri":"/redis%E5%9F%BA%E7%A1%80/"},{"categories":["redis"],"content":"2.10.1.位图应用原理 某网站要统计一个用户一年的签到记录，若用 sring 类型存储，则需要 365 个键值对。若使用位图存储，用户签到就存 1，否则存 0。最后会生成 11010101… 这样的存储结果，其中每天的记录只占一位，一年就是 365 位，约为 46 个字节。如果只想统计用户签到的天数，那么统计 1 的个数即可。 位图操作的优势，相比于字符串而言，它不仅效率高，而且还非常的节省空间。 Redis 的位数组是自动扩展的，如果设置了某个偏移位置超出了现有的内容范围，位数组就会自动扩充。 下面设置一个名为 a 的 key，我们对这个 key 进行位图操作，使得 a 的对应的 value 变为“he”。 首先我们分别获取字符“h”和字符“e”的八位二进制码，如下所示： \u003e\u003e\u003e bin(ord(\"h\")) '0b1101000' \u003e\u003e\u003e bin(ord(\"e\")) '0b1100101' 接下来，只要对需值为 1 的位进行操作即可。如下图所示： 把 h 和 e 的二进制码连接在一起，第一位的下标是 0，依次递增至 15，然后将数字为 1 的位置标记出来，得到 1/2/4/9/10/13/15，我们把这组数字称为位的“偏置数”，最后按照上述偏置数对字符 a 进行如下位图操作。注意，key 的初始二进制位全部为 0。 C:\\Users\\Administrator\u003eredis-cli 127.0.0.1:6379\u003e SETBIT a 1 1 (integer) 0 127.0.0.1:6379\u003e SETBIT a 2 1 (integer) 0 127.0.0.1:6379\u003e SETBIT a 4 1 (integer) 0 127.0.0.1:6379\u003e get hello \"h\" 127.0.0.1:6379\u003e SETBIT a 9 1 (integer) 0 127.0.0.1:6379\u003e SETBIT a 10 1 (integer) 0 127.0.0.1:6379\u003e SETBIT a 13 1 (integer) 0 127.0.0.1:6379\u003e SETBIT a 15 1 (integer) 0 127.0.0.1:6379\u003e get hello \"he\" 从上述示例可以得出，位图操作会自动对 key 进行扩容。 如果对应位的字节是不可以被打印的，那么 Redis 会以该字符的十六进制数来表示它，如下所示： 127.0.0.1:6379\u003e SETBIT b 0 1 (integer) 0 127.0.0.1:6379\u003e SETBIT b 1 1 (integer) 0 127.0.0.1:6379\u003e get b \"\\xc0\" 常用命令： SETBIT key offset value：用来设置或者清除某一位上的值，其返回值是原来位上存储的值。key 在初始状态下所有的位都为 0 GETBIT key offset：用来获取某一位上的值。当偏移量 offset 比字符串的长度大，或者当 key 不存在时，返回 0。 BITCOUNT key [start end]：统计指定位区间上，值为 1 的个数。通过指定的 start 和 end 参数，可以让计数只在特定的字节上进行。start 和 end 参数和 GETRANGE 命令的参数类似，都可以使用负数，比如 -1 表示倒数第一个位， -2 表示倒数第二个位。 3.Redis的Java客户端 在Redis官网中提供了各种语言的客户端，地址：https://redis.io/docs/clients/ 其中Java客户端也包含很多： 标记为*的就是推荐使用的java客户端，包括： Jedis和Lettuce：这两个主要是提供了Redis命令对应的API，方便我们操作Redis，而SpringDataRedis又对这两种做了抽象和封装，因此我们后期会直接以SpringDataRedis来学习。 Redisson：是在Redis基础上实现了分布式的可伸缩的java数据结构，例如Map、Queue等，而且支持跨进程的同步机制：Lock、Semaphore等待，比较适合用来实现特殊的功能需求。 ","date":"2023-01-08","objectID":"/redis%E5%9F%BA%E7%A1%80/:14:1","tags":[],"title":"Redis基础","uri":"/redis%E5%9F%BA%E7%A1%80/"},{"categories":["redis"],"content":"3.1.Jedis客户端 Jedis的官网地址： https://github.com/redis/jedis ","date":"2023-01-08","objectID":"/redis%E5%9F%BA%E7%A1%80/:15:0","tags":[],"title":"Redis基础","uri":"/redis%E5%9F%BA%E7%A1%80/"},{"categories":["redis"],"content":"3.1.1.快速入门 我们先来个快速入门： 1）引入依赖： \u003c!--jedis--\u003e \u003cdependency\u003e \u003cgroupId\u003eredis.clients\u003c/groupId\u003e \u003cartifactId\u003ejedis\u003c/artifactId\u003e \u003cversion\u003e3.7.0\u003c/version\u003e \u003c/dependency\u003e \u003c!--单元测试--\u003e \u003cdependency\u003e \u003cgroupId\u003eorg.junit.jupiter\u003c/groupId\u003e \u003cartifactId\u003ejunit-jupiter\u003c/artifactId\u003e \u003cversion\u003e5.7.0\u003c/version\u003e \u003cscope\u003etest\u003c/scope\u003e \u003c/dependency\u003e 2）建立连接 新建一个单元测试类，内容如下： private Jedis jedis; @BeforeEach void setUp() { // 1.建立连接 // jedis = new Jedis(\"192.168.150.101\", 6379); jedis = JedisConnectionFactory.getJedis(); // 2.设置密码 jedis.auth(\"123321\"); // 3.选择库 jedis.select(0); } 3）测试： @Test void testString() { // 存入数据 String result = jedis.set(\"name\", \"虎哥\"); System.out.println(\"result = \" + result); // 获取数据 String name = jedis.get(\"name\"); System.out.println(\"name = \" + name); } @Test void testHash() { // 插入hash数据 jedis.hset(\"user:1\", \"name\", \"Jack\"); jedis.hset(\"user:1\", \"age\", \"21\"); // 获取 Map\u003cString, String\u003e map = jedis.hgetAll(\"user:1\"); System.out.println(map); } 4）释放资源 @AfterEach void tearDown() { if (jedis != null) { jedis.close(); } } ","date":"2023-01-08","objectID":"/redis%E5%9F%BA%E7%A1%80/:15:1","tags":[],"title":"Redis基础","uri":"/redis%E5%9F%BA%E7%A1%80/"},{"categories":["redis"],"content":"3.1.2.连接池 Jedis本身是线程不安全的，并且频繁的创建和销毁连接会有性能损耗，因此我们推荐大家使用Jedis连接池代替Jedis的直连方式。 package com.heima.jedis.util; import redis.clients.jedis.*; public class JedisConnectionFactory { private static JedisPool jedisPool; static { // 配置连接池 JedisPoolConfig poolConfig = new JedisPoolConfig(); poolConfig.setMaxTotal(8); poolConfig.setMaxIdle(8); poolConfig.setMinIdle(0); poolConfig.setMaxWaitMillis(1000); // 创建连接池对象，参数：连接池配置、服务端ip、服务端端口、超时时间、密码 jedisPool = new JedisPool(poolConfig, \"192.168.150.101\", 6379, 1000, \"123321\"); } public static Jedis getJedis(){ return jedisPool.getResource(); } } ","date":"2023-01-08","objectID":"/redis%E5%9F%BA%E7%A1%80/:15:2","tags":[],"title":"Redis基础","uri":"/redis%E5%9F%BA%E7%A1%80/"},{"categories":["redis"],"content":"3.2.SpringDataRedis客户端 SpringData是Spring中数据操作的模块，包含对各种数据库的集成，其中对Redis的集成模块就叫做SpringDataRedis，官网地址：https://spring.io/projects/spring-data-redis 提供了对不同Redis客户端的整合（Lettuce和Jedis） 提供了RedisTemplate统一API来操作Redis 支持Redis的发布订阅模型 支持Redis哨兵和Redis集群 支持基于Lettuce的响应式编程 支持基于JDK、JSON、字符串、Spring对象的数据序列化及反序列化 支持基于Redis的JDKCollection实现 SpringDataRedis中提供了RedisTemplate工具类，其中封装了各种对Redis的操作。并且将不同数据类型的操作API封装到了不同的类型中： ","date":"2023-01-08","objectID":"/redis%E5%9F%BA%E7%A1%80/:16:0","tags":[],"title":"Redis基础","uri":"/redis%E5%9F%BA%E7%A1%80/"},{"categories":["redis"],"content":"3.2.1.快速入门 SpringBoot已经提供了对SpringDataRedis的支持，使用非常简单。 首先，新建一个maven项目，然后按照下面步骤执行： 1）引入依赖 \u003c?xml version=\"1.0\" encoding=\"UTF-8\"?\u003e \u003cproject xmlns=\"http://maven.apache.org/POM/4.0.0\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://maven.apache.org/POM/4.0.0 https://maven.apache.org/xsd/maven-4.0.0.xsd\"\u003e \u003cmodelVersion\u003e4.0.0\u003c/modelVersion\u003e \u003cparent\u003e \u003cgroupId\u003eorg.springframework.boot\u003c/groupId\u003e \u003cartifactId\u003espring-boot-starter-parent\u003c/artifactId\u003e \u003cversion\u003e2.5.7\u003c/version\u003e \u003crelativePath/\u003e \u003c!-- lookup parent from repository --\u003e \u003c/parent\u003e \u003cgroupId\u003ecom.heima\u003c/groupId\u003e \u003cartifactId\u003eredis-demo\u003c/artifactId\u003e \u003cversion\u003e0.0.1-SNAPSHOT\u003c/version\u003e \u003cname\u003eredis-demo\u003c/name\u003e \u003cdescription\u003eDemo project for Spring Boot\u003c/description\u003e \u003cproperties\u003e \u003cjava.version\u003e1.8\u003c/java.version\u003e \u003c/properties\u003e \u003cdependencies\u003e \u003c!--redis依赖--\u003e \u003cdependency\u003e \u003cgroupId\u003eorg.springframework.boot\u003c/groupId\u003e \u003cartifactId\u003espring-boot-starter-data-redis\u003c/artifactId\u003e \u003c/dependency\u003e \u003c!--common-pool--\u003e \u003cdependency\u003e \u003cgroupId\u003eorg.apache.commons\u003c/groupId\u003e \u003cartifactId\u003ecommons-pool2\u003c/artifactId\u003e \u003c/dependency\u003e \u003c!--Jackson依赖--\u003e \u003cdependency\u003e \u003cgroupId\u003ecom.fasterxml.jackson.core\u003c/groupId\u003e \u003cartifactId\u003ejackson-databind\u003c/artifactId\u003e \u003c/dependency\u003e \u003cdependency\u003e \u003cgroupId\u003eorg.projectlombok\u003c/groupId\u003e \u003cartifactId\u003elombok\u003c/artifactId\u003e \u003coptional\u003etrue\u003c/optional\u003e \u003c/dependency\u003e \u003cdependency\u003e \u003cgroupId\u003eorg.springframework.boot\u003c/groupId\u003e \u003cartifactId\u003espring-boot-starter-test\u003c/artifactId\u003e \u003cscope\u003etest\u003c/scope\u003e \u003c/dependency\u003e \u003c/dependencies\u003e \u003cbuild\u003e \u003cplugins\u003e \u003cplugin\u003e \u003cgroupId\u003eorg.springframework.boot\u003c/groupId\u003e \u003cartifactId\u003espring-boot-maven-plugin\u003c/artifactId\u003e \u003cconfiguration\u003e \u003cexcludes\u003e \u003cexclude\u003e \u003cgroupId\u003eorg.projectlombok\u003c/groupId\u003e \u003cartifactId\u003elombok\u003c/artifactId\u003e \u003c/exclude\u003e \u003c/excludes\u003e \u003c/configuration\u003e \u003c/plugin\u003e \u003c/plugins\u003e \u003c/build\u003e \u003c/project\u003e 2）配置Redis spring: redis: host: 192.168.150.101 port: 6379 password: 123321 lettuce: pool: max-active: 8 max-idle: 8 min-idle: 0 max-wait: 100ms 3）注入RedisTemplate 因为有了SpringBoot的自动装配，我们可以拿来就用： @SpringBootTest class RedisStringTests { @Autowired private RedisTemplate redisTemplate; } 4）编写测试 @SpringBootTest class RedisStringTests { @Autowired private RedisTemplate edisTemplate; @Test void testString() { // 写入一条String数据 redisTemplate.opsForValue().set(\"name\", \"虎哥\"); // 获取string数据 Object name = stringRedisTemplate.opsForValue().get(\"name\"); System.out.println(\"name = \" + name); } } ","date":"2023-01-08","objectID":"/redis%E5%9F%BA%E7%A1%80/:16:1","tags":[],"title":"Redis基础","uri":"/redis%E5%9F%BA%E7%A1%80/"},{"categories":["redis"],"content":"3.2.2.自定义序列化 RedisTemplate可以接收任意Object作为值写入Redis： 只不过写入前会把Object序列化为字节形式，默认是采用JDK序列化，得到的结果是这样的： 缺点： 可读性差 内存占用较大 我们可以自定义RedisTemplate的序列化方式，代码如下： @Configuration public class RedisConfig { @Bean public RedisTemplate\u003cString, Object\u003e redisTemplate(RedisConnectionFactory connectionFactory){ // 创建RedisTemplate对象 RedisTemplate\u003cString, Object\u003e template = new RedisTemplate\u003c\u003e(); // 设置连接工厂 template.setConnectionFactory(connectionFactory); // 创建JSON序列化工具 GenericJackson2JsonRedisSerializer jsonRedisSerializer = new GenericJackson2JsonRedisSerializer(); // 设置Key的序列化 template.setKeySerializer(RedisSerializer.string()); template.setHashKeySerializer(RedisSerializer.string()); // 设置Value的序列化 template.setValueSerializer(jsonRedisSerializer); template.setHashValueSerializer(jsonRedisSerializer); // 返回 return template; } } 这里采用了JSON序列化来代替默认的JDK序列化方式。最终结果如图： 整体可读性有了很大提升，并且能将Java对象自动的序列化为JSON字符串，并且查询时能自动把JSON反序列化为Java对象。不过，其中记录了序列化时对应的class名称，目的是为了查询时实现自动反序列化。这会带来额外的内存开销。 ","date":"2023-01-08","objectID":"/redis%E5%9F%BA%E7%A1%80/:16:2","tags":[],"title":"Redis基础","uri":"/redis%E5%9F%BA%E7%A1%80/"},{"categories":["redis"],"content":"3.2.3.StringRedisTemplate 为了节省内存空间，我们可以不使用JSON序列化器来处理value，而是统一使用String序列化器，要求只能存储String类型的key和value。当需要存储Java对象时，手动完成对象的序列化和反序列化。 因为存入和读取时的序列化及反序列化都是我们自己实现的，SpringDataRedis就不会将class信息写入Redis了。 这种用法比较普遍，因此SpringDataRedis就提供了RedisTemplate的子类：StringRedisTemplate，它的key和value的序列化方式默认就是String方式。 省去了我们自定义RedisTemplate的序列化方式的步骤，而是直接使用： @Autowired private StringRedisTemplate stringRedisTemplate; // JSON序列化工具 private static final ObjectMapper mapper = new ObjectMapper(); @Test void testSaveUser() throws JsonProcessingException { // 创建对象 User user = new User(\"虎哥\", 21); // 手动序列化 String json = mapper.writeValueAsString(user); // 写入数据 stringRedisTemplate.opsForValue().set(\"user:200\", json); // 获取数据 String jsonUser = stringRedisTemplate.opsForValue().get(\"user:200\"); // 手动反序列化 User user1 = mapper.readValue(jsonUser, User.class); System.out.println(\"user1 = \" + user1); } ","date":"2023-01-08","objectID":"/redis%E5%9F%BA%E7%A1%80/:16:3","tags":[],"title":"Redis基础","uri":"/redis%E5%9F%BA%E7%A1%80/"},{"categories":["汇编"],"content":"[toc] ","date":"2023-01-07","objectID":"/%E6%B1%87%E7%BC%96%E4%B9%8B%E5%8C%85%E5%90%AB%E5%A4%9A%E4%B8%AA%E6%AE%B5%E7%9A%84%E7%A8%8B%E5%BA%8F/:0:0","tags":[],"title":"包含多个段的程序","uri":"/%E6%B1%87%E7%BC%96%E4%B9%8B%E5%8C%85%E5%90%AB%E5%A4%9A%E4%B8%AA%E6%AE%B5%E7%9A%84%E7%A8%8B%E5%BA%8F/"},{"categories":["汇编"],"content":"多个段程序 程序中对段名的引用，将被编译器处理为一个表示段地址的数值。 mov ax, data mov ds, ax mov bx, ds:[6] ","date":"2023-01-07","objectID":"/%E6%B1%87%E7%BC%96%E4%B9%8B%E5%8C%85%E5%90%AB%E5%A4%9A%E4%B8%AA%E6%AE%B5%E7%9A%84%E7%A8%8B%E5%BA%8F/:1:0","tags":[],"title":"包含多个段的程序","uri":"/%E6%B1%87%E7%BC%96%E4%B9%8B%E5%8C%85%E5%90%AB%E5%A4%9A%E4%B8%AA%E6%AE%B5%E7%9A%84%E7%A8%8B%E5%BA%8F/"},{"categories":["汇编"],"content":"在代码段中使用数据 ;计算 8 个数据的和存到 ax 寄存器 assume cs:code code segment dw 0123h,0456h,0789h,0abch,0defh,0fedh,0cbah,0987h ;define word 定义8个字形数据 start: mov bx, 0 ;标号start mov ax, 0 mov cx, 8 s: add ax, cs:[bx] add bx, 2 loop s mov ax, 4c00h int 21h code ends end start ;end除了通知编译器程序结束外，还可以通知编译器程序的入口在什么地方 ;用end指令指明了程序的入口在标号start处，也就是说，“mov bx，0”是程序的第一条指令。 ","date":"2023-01-07","objectID":"/%E6%B1%87%E7%BC%96%E4%B9%8B%E5%8C%85%E5%90%AB%E5%A4%9A%E4%B8%AA%E6%AE%B5%E7%9A%84%E7%A8%8B%E5%BA%8F/:1:1","tags":[],"title":"包含多个段的程序","uri":"/%E6%B1%87%E7%BC%96%E4%B9%8B%E5%8C%85%E5%90%AB%E5%A4%9A%E4%B8%AA%E6%AE%B5%E7%9A%84%E7%A8%8B%E5%BA%8F/"},{"categories":["汇编"],"content":"在代码段中使用栈 ;利用栈，将程序中定义的数据逆序存放。 assume cs:codesg codesg segment dw 0123h，0456h，0789h，0abch，0defh，0fedh，0cbah，0987h ; 0-15单元 dw 0，0，0，0，0，0，0，0，0，0，0，0，0，0，0，0 ; 16-47单元作为栈使用 start: mov ax, cs mov ss, ax mov sp, 30h ;将设置栈顶ss:sp指向栈底cs:30。 30h = 48d mov bx, 0 mov cx, 8 s: push cs:[bx] add bx, 2 loop s ;以上将代码段0~15单元中的8个字型数据依次入栈 mov bx, 0 mov cx, 8 s0: pop cs:[bx] add bx，2 loop s0 ;以上依次出栈8个字型数据到代码段0~15单元中 mov ax，4c00h int 21h codesg ends end start ;指明程序的入口在start处 ","date":"2023-01-07","objectID":"/%E6%B1%87%E7%BC%96%E4%B9%8B%E5%8C%85%E5%90%AB%E5%A4%9A%E4%B8%AA%E6%AE%B5%E7%9A%84%E7%A8%8B%E5%BA%8F/:1:2","tags":[],"title":"包含多个段的程序","uri":"/%E6%B1%87%E7%BC%96%E4%B9%8B%E5%8C%85%E5%90%AB%E5%A4%9A%E4%B8%AA%E6%AE%B5%E7%9A%84%E7%A8%8B%E5%BA%8F/"},{"categories":["汇编"],"content":"将数据、代码、栈放入不同的段 assume cs:code,ds:data,ss:stack data segment dw 0123h,0456h,0789h,0abch,0defh,0fedh,0cbah,0987h ;0-15单元 data ends stack segment dw 0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0 ;0-31单元 stack ends code segment start: mov ax, stack;将名称为“stack”的段的段地址送入ax mov ss, ax mov sp, 20h ;设置栈顶ss:sp指向stack:20。 20h = 32d mov ax, data ;将名称为“data”的段的段地址送入ax mov ds, ax ;ds指向data段 mov bx, 0 ;ds:bx指向data段中的第一个单元 mov cx, 8 s: push [bx] add bx, 2 loop s ;以上将data段中的0~15单元中的8个字型数据依次入栈 mov bx, 0 mov cx, 8 s0: pop [bx] add bx, 2 loop s0 ;以上依次出栈8个字型数据到data段的0~15单元中 mov ax, 4c00h int 21h code ends end start ;“end start”说明了程序的入口，这个入口将被写入可执行文件的描述信息， ;可执行文件中的程序被加载入内存后，CPU的CS:IP被设置指向这个入口，从而开始执行程序中的第一条指令 ","date":"2023-01-07","objectID":"/%E6%B1%87%E7%BC%96%E4%B9%8B%E5%8C%85%E5%90%AB%E5%A4%9A%E4%B8%AA%E6%AE%B5%E7%9A%84%E7%A8%8B%E5%BA%8F/:1:3","tags":[],"title":"包含多个段的程序","uri":"/%E6%B1%87%E7%BC%96%E4%B9%8B%E5%8C%85%E5%90%AB%E5%A4%9A%E4%B8%AA%E6%AE%B5%E7%9A%84%E7%A8%8B%E5%BA%8F/"},{"categories":["汇编"],"content":"[bx] 1、[bx]是什么呢？ 和[0]类似，[0]表示内存单元，它的偏移地址是0； 2、内存单元的描述 我们要完整地描述一个内存单元，需要两种信息： （1）内存单元的地址； （2）内存单元的长度（类型）； 用[0]表示一个内存单元时，0表示单元的偏移地址，段地址默认在 ds 中，单元的长度（类型）可以由具体指令中的其他操作对象（比如说寄存器）指出； [bx]同样也表示一个内存单元，它的偏移地址在bx中，比如下面的指令： mov ax,[bx] mov al,[bx] 3、 描述性符号“()” 为了描述上的简洁，以后将使用一个描述性的符号 “() ”来表示一个寄存器或一个内存单元中的内容。比如： （1）ax 中的内容为0010H，我们可以这样来描述：(ax)=0010H； （2）2000:1000 处的内容为0010H，我们可以这样来描述：(21000H)=0010H； （3）对于 mov ax,[2] 的功能，我们可以这样来描述：(ax)=((ds)*16+2)； （4）对于 mov [2],ax 的功能，我们可以这样来描述：((ds)*16+2)=(ax)； （5）对于 add ax,2 的功能，我们可以这样来描述：(ax)=(ax)+2； （6）对于 add ax,bx 的功能，我们可以这样来描述：(ax)=(ax)+(bx)； （7）对于 push ax 的功能，我们可以这样来描述：(sp) = (sp)-2 ((ss)*16＋(sp))=(ax) （8）对于 pop ax 的功能，我们可以这样来描述：(ax)=((ss)*16+(sp)) (sp)=(sp)+2 4、约定符号 idata 表示常量 我们在 Debug 中写过类似的指令：mov ax,[0]，表示将 ds:0 处的数据送入 ax 中。指令中，在“[…]”里用一个常量0表示内存单元的偏移地址。以后，我们用 idata 表示常量。 例如： mov ax,[idata] 就代表 mov ax,[1]、mov ax,[2]、mov ax,[3] 等； mov bx,idata就代表 mov bx,1、mov bx,2、mov bx,3 等； mov ds,idata就代表 mov ds,1、mov ds,2 等； 我们之前的用法都是非法指令； 5、[bx] mov [bx],ax 功能：bx 中存放的数据作为一个偏移地址 EA，段地址 SA 默认在 ds 中，将 ax 中的数据送入内存 SA:EA 处，即：(ds*16 +(bx)) = (ax)； 问题：程序和内存中的情况如下图所示，写出程序执行后，21000H~21007H 单元中的内容。 （1）先看一下程序的前三条指令： mov ax,2000H mov ds,ax mov bx,1000H 这三条指令执行后，ds=2000H,bx=1000H； （2）再看第四条指令：mov ax,[bx]， 指令执行前： ds=2000H，bx=1000H，则 mov ax,[bx] 将把内存2000:1000处的字型数据送入 ax 中； 指令执行后: ax=00beH； （3）再看第五六两条指令： inc bx inc bx 指令执行前： bx=1000H； 指令执行后: bx=0002H； （4）再看第七条指令：mov [bx],ax， 指令执行前： ds=2000H，bx=1002H，则 mov [bx],ax 将把 ax 中的数据送入内存2000:1002处； 指令执行后: 2000:1002单元的内容为BE，2000:1003单元的内容为00； 之后的操作类似，最终结果： ","date":"2022-12-09","objectID":"/%E6%B1%87%E7%BC%96%E4%B9%8Bbx%E5%92%8Cloop%E6%8C%87%E4%BB%A4/:1:0","tags":[],"title":"汇编之BX和loop指令","uri":"/%E6%B1%87%E7%BC%96%E4%B9%8Bbx%E5%92%8Cloop%E6%8C%87%E4%BB%A4/"},{"categories":["汇编"],"content":"Loop指令 这个指令和循环有关； 1、指令的格式是：loop 标号，CPU 执行 loop 指令的时候，要进行两步操作： (cx)=(cx)-1; 判断 cx 中的值，若不为零，则转至标号处执行；程序若为零，则向下执行。 2、通常，loop 指令实现循环，cx 中存放循环的次数； 问题：编程计算212； assume cs:code code segment mov ax,2 mov cx,11 s: add ax,ax loop s mov ax,4c00h int 21h code ends end 分析： （1）标号：在汇编语言中，标号代表一个地址，此程序中有一个标号 s 。它实际上标识了一个地址，这个地址处有一条指令：add ax,ax； （2）loop s：CPU 执行 loop s 的时候，要进行两步操作： (cx)=(cx)-1； 判断 cx 中的值，不为0则转至标号 s 所标识的地址处执行(这里的指令是 add ax,ax)，如果为0则执行下一条指令(下一条指令是 mov ax,4c00h)； 总结： （1）在 cx 中存放循环次数； （2）loop 指令中的标号所标识地址要在前面； （3）要循环执行的程序段，要写在标号和 loop 指令的中间； 用 cx 和 loop 指令相配合实现循环功能的程序框架如下： mov cx,循环次数 s: 循环执行的程序段 loop s 3、在 Debug 中跟踪供 loop 指令实现的循环程序 注意：在汇编程序中，数据不能以字母开头，如果要输入像 FFFFH 这样的数，则要在前面添加一个0； 在 debug 程序中引入 G 命令和 P 命令： G命令 G命令如果后面不带参数，则一直执行程序，直到程序结束； G命令后面如果带参数，则执行到 ip 为那个参数地址停止； P命令 T命令相当于单步进入（step into）； P命令相当于单步通过（step over）； ","date":"2022-12-09","objectID":"/%E6%B1%87%E7%BC%96%E4%B9%8Bbx%E5%92%8Cloop%E6%8C%87%E4%BB%A4/:2:0","tags":[],"title":"汇编之BX和loop指令","uri":"/%E6%B1%87%E7%BC%96%E4%B9%8Bbx%E5%92%8Cloop%E6%8C%87%E4%BB%A4/"},{"categories":["汇编"],"content":"Debug 和汇编编译器 Masm 对指令的不同处理 1、在 debug 中，可以直接用指令 mov ax,[0] 将偏移地址为0号单元的内容赋值给 ax； 2、但通过 masm 编译器，mov ax,[0] 会被编译成 mov ax,0； 要写成这样才能实现：mov ax,ds:[0]； 也可以写成这样： mov bx,0 mov ax,[bx] 或者 mov ax,ds:[bx] ","date":"2022-12-09","objectID":"/%E6%B1%87%E7%BC%96%E4%B9%8Bbx%E5%92%8Cloop%E6%8C%87%E4%BB%A4/:3:0","tags":[],"title":"汇编之BX和loop指令","uri":"/%E6%B1%87%E7%BC%96%E4%B9%8Bbx%E5%92%8Cloop%E6%8C%87%E4%BB%A4/"},{"categories":["汇编"],"content":"loop 和 [bx] 的联合应用 计算 ffff:0~ffff:b 单元中的数据的和，结果存储在 dx 中： 1、注意两个问题： 12个8位数据加载一起，最后的结果可能会超出8位（越界），故要用16位寄存器存放结果； 将一个8位的数据加入到16位寄存器中，类型不匹配，8位的数据不能与16位相加； 2、【解决办法】 把原来8位的数据，先通过通用寄存器 ax，将它们转化成16位的； 3、代码如下： assume cs:codesg codesg segment start: ;指定数据段 mov ax,0ffffh mov ds,ax ;初始化 mov ax,0 mov dx,0 mov bx,0 ;指定循环次数 12次 mov cx,0ch circ: ;把8位数据存入al中,即ax中存放的是[bx]转化之后的16位数据，前8位都是0 mov al,[bx] ;进行累加 add dx,ax ;bx自增，变化内存的偏移地址 inc bx loop circ ;程序返回 mov ax,4c00h int 21H codesg ends end start ","date":"2022-12-09","objectID":"/%E6%B1%87%E7%BC%96%E4%B9%8Bbx%E5%92%8Cloop%E6%8C%87%E4%BB%A4/:4:0","tags":[],"title":"汇编之BX和loop指令","uri":"/%E6%B1%87%E7%BC%96%E4%B9%8Bbx%E5%92%8Cloop%E6%8C%87%E4%BB%A4/"},{"categories":["汇编"],"content":"段前缀 指令 mov ax,[bx] 中，内存单元的偏移地址由 bx 给出，而段地址默认在 ds 中； 我们可以在访问内存单元的指令中，显式地给出内存单元的段地址所在的段寄存器，比如 mov ax,ds:[0]，mov ax,ds:[bx] 这里的 ds 就叫做【段前缀】； ","date":"2022-12-09","objectID":"/%E6%B1%87%E7%BC%96%E4%B9%8Bbx%E5%92%8Cloop%E6%8C%87%E4%BB%A4/:5:0","tags":[],"title":"汇编之BX和loop指令","uri":"/%E6%B1%87%E7%BC%96%E4%B9%8Bbx%E5%92%8Cloop%E6%8C%87%E4%BB%A4/"},{"categories":["汇编"],"content":"一段安全的空间 8086模式中，随意向一段内存空间写入内容是很危险的，因为这段空间中可能存放着【重要的系统数据或代码】； 在一般的 PC 机中，DOS 方式下，DOS 和其他合法的程序一般都不会使用【0:200~0:2FF】的256个字节的空间。所以，我们使用这段空间是安全的； ","date":"2022-12-09","objectID":"/%E6%B1%87%E7%BC%96%E4%B9%8Bbx%E5%92%8Cloop%E6%8C%87%E4%BB%A4/:6:0","tags":[],"title":"汇编之BX和loop指令","uri":"/%E6%B1%87%E7%BC%96%E4%B9%8Bbx%E5%92%8Cloop%E6%8C%87%E4%BB%A4/"},{"categories":["汇编"],"content":"段前缀的使用 将内存 ffff:0~ffff:b 段元中的数据拷贝到 0:200~0:20b 单元中； assume cs:code code segment mov bx,0 ; (bx)=0，偏移地址从0开始 mov cx,12 ; (cx)=12，循环12次 s: mov ax,offffh mov ds,ax ; (ds)=0ffffh mov dl,[bx] ; (dl)= ((ds)*16+(bx))，将ffff:bx中的数据送入dl mov ax,0020h mov ds,ax ; (ds)=0020h mov [bx],dl ; ((ds)*16+(bx))=(dl)，将中dl的数据送入0020:bx inc bx ; (bx)=(bx)+1 loop s mov ax,4c00h int 21h code ends end 由于上述效率不高，因此进行如下优化： assume cs:code code segment mov ax,offffh mov ds,ax ; (ds)=0ffffh mov ax,0020h mov es,ax ; (es)=0020h mov bx,0 ; (bx)=0，此时ds:bx指向ffff:0，es:bx指向0020:0 mov cx,12 ; (cx)=12，循环12次 s: mov dl,[bx] ; (dl)= ((ds)*16+(bx))，将ffff:bx中的数据送入dl mov es:[bx],dl ; ((es)*16+(bx))=(dl)，将中dl的数据送入0020:bx inc bx ; (bx)=(bx)+1 loop s mov ax,4c00h int 21h code ends end ","date":"2022-12-09","objectID":"/%E6%B1%87%E7%BC%96%E4%B9%8Bbx%E5%92%8Cloop%E6%8C%87%E4%BB%A4/:7:0","tags":[],"title":"汇编之BX和loop指令","uri":"/%E6%B1%87%E7%BC%96%E4%B9%8Bbx%E5%92%8Cloop%E6%8C%87%E4%BB%A4/"},{"categories":["汇编"],"content":"汇编之第一个程序 ","date":"2022-12-06","objectID":"/%E6%B1%87%E7%BC%96%E4%B9%8B%E7%AC%AC%E4%B8%80%E4%B8%AA%E7%A8%8B%E5%BA%8F/:0:0","tags":[],"title":"汇编之第一个程序","uri":"/%E6%B1%87%E7%BC%96%E4%B9%8B%E7%AC%AC%E4%B8%80%E4%B8%AA%E7%A8%8B%E5%BA%8F/"},{"categories":["汇编"],"content":"一个源程序从写出到执行的过程 一个汇编语言程序从写出到最终执行的简要过程： 编写 -\u003e 编译连接 -\u003e 执行 对源程序进行编译连接： 使用汇编语言编译程序（MASM.EXE）对源程序文件中的源程序进行编译，产生目标文件【.obj文件】 再用连接程序（LINK.EXE）对目标文件进行连接，生成可在操作系统中直接运行的可执行文件【.EXE文件】 可执行文件包含两部分内容： 程序（从源程序的汇编指令翻译过来的机器码）和数据（源程序中定义的数据）； 相关的描述信息（比如：程序有多大、要占多少内存空间等）； 执行可执行文件中的程序： 在操作系统（如：MSDOS）中，执行可执行文件中的程序； 操作系统依照可执行文件中的描述信息，将可执行文件中的机器码和数据加载入内存，并进行相关的初始化（比如：设置 CS:IP 指向第一条要执行的指令），然后由 CPU 执行程序； ","date":"2022-12-06","objectID":"/%E6%B1%87%E7%BC%96%E4%B9%8B%E7%AC%AC%E4%B8%80%E4%B8%AA%E7%A8%8B%E5%BA%8F/:1:0","tags":[],"title":"汇编之第一个程序","uri":"/%E6%B1%87%E7%BC%96%E4%B9%8B%E7%AC%AC%E4%B8%80%E4%B8%AA%E7%A8%8B%E5%BA%8F/"},{"categories":["汇编"],"content":"源程序的主要结构 源程序由“ 汇编指令+伪指令+宏指令 ”组成： 伪指令：编译器处理； 汇编指令：编译为机器码； 伪指令： 没有对应的机器码的指令，不能由 CPU 直接执行； 伪指令是由编译器来执行的指令，编译器根据伪指令来进行相关的编译工作； segment 和 ends【定义一个段】 segment 和 ends 是一对成对使用的伪指令； 编写汇编程序【必须】使用到的指令； segment 和 ends 的功能是定义一个段： segment：说明一个段开始； ends：说明一个段结束； 一个段必须有一个名称来标识，使用格式为 段名 segment 段名 ends 一个汇编程序由多个段组成： 这些段用来存放【代码，数据或当作栈空间】来使用，一个有意义的汇编程序至少要有一个段，这个段用来存放代码。 end【真正的没了】 end 是一个汇编程序的结束标记； 编译器在编译汇编程序的过程中，如果碰到了伪指令 end，就结束对源程序的编译； 如果程序写完了，要在结尾处加上伪指令 end，否则，编译器无法知道程序在何处结束； 【切记】不要把 end 和 ends 搞混了！ end：汇编程序的结束标记； ends：与 segment 成对出现，表示一个段结束； assume【寄存器和段的关联假设】 它假设某一段寄存器和程序中的某一个用 segment…ends 定义的段相关联； 通过 assume 说明这种关联，在需要的情况下，编译程序可以将段寄存器和某一具体的段相联系； 程序和源程序 我们将源程序文件中的所有内容称为【源程序】 将源程序中最终由计算机执行处理的指令或数据称为【程序】 程序最先以汇编指令的形式，存储在源程序中，然后经过编译、连接后转变为机器码，存储在可执行文件中； 标号，标号与段名称有所区别： 一个标号指代了一个地址，即是段名称，类似指针。 段名称（codesg）放在 segment 的前面，作为一个段的名称，这个段的名称最终将被汇编、连接程序处理为一个段的段地址； 任务：编程运算 23； assume cs:abc abc segment mov ax,2 add ax,ax add ax,ax abc ends end DOS 中的程序运行： DOS 是一个单任务操作系统： 1） 一个程序 P2 在可执行文件中，则必须有一个正在运行的程序 P1，将 P2 从可执行文件中加载入内存后，将 CPU 的控制权交给 P2，P2 才能得以运行。P2 开始运行后，P1 暂停运行。 2） 而当 P2 运行完毕后，应该将 CPU 的控制权交还给使它得以运行的程序 P1，此后，P1 继续运行。 一个程序结束后，将 CPU 的控制权交还给是他得以运行的程序，称这个过程为：程序返回； 程序返回 应该在程序的末尾添加返回的程序段。 mov ax，4c00H int 21H 【中断机制】是 DOS 最伟大的机制，Windows 系统上是【消息机制】，这两条指令所实现的功能就是程序返回； 几个和结束相关的内容： 段结束：伪指令 通知编译器一个段的结束【ends】 程序结束：伪指令 通知编译器程序的结束【end】 程序返回：汇编指令 mov ax,4c00H int 21H 语法错误和逻辑错误： 语法错误 程序在编译时被编译器发现的错误； 容易发现； 逻辑错误 在编写时不会表现出来的错误、在运行时会发生的错误； 不容易发现； ","date":"2022-12-06","objectID":"/%E6%B1%87%E7%BC%96%E4%B9%8B%E7%AC%AC%E4%B8%80%E4%B8%AA%E7%A8%8B%E5%BA%8F/:2:0","tags":[],"title":"汇编之第一个程序","uri":"/%E6%B1%87%E7%BC%96%E4%B9%8B%E7%AC%AC%E4%B8%80%E4%B8%AA%E7%A8%8B%E5%BA%8F/"},{"categories":["汇编"],"content":"以简化的方式进行汇编和连接 汇编使用的程序：masm.exe 连接使用的程序：link.exe 简化方式进行汇编和连接的程序：ml.exe MASM下载链接，提取码：gd2c； 跟之前 汇编（三）：DEBUG 中提到的操作一样，修改配置文件，自动挂载 MASM 目录，可以输入 dir 进行验证； 编写一个 Hello World 程序： .model small .data strs DB 'hello world',13,10,'$' .code start: mov ax,@data mov ds,ax mov dx,offset strs mov ah,09h int 21h mov ah,4ch int 21h end start 先复制到 txt 文本中，然后将后缀改成 asm，使用 masm 1.asm 命令进行汇编； 然后通过 link 1.obj 进行链接； 最后执行所生成的 exe 文件； ","date":"2022-12-06","objectID":"/%E6%B1%87%E7%BC%96%E4%B9%8B%E7%AC%AC%E4%B8%80%E4%B8%AA%E7%A8%8B%E5%BA%8F/:3:0","tags":[],"title":"汇编之第一个程序","uri":"/%E6%B1%87%E7%BC%96%E4%B9%8B%E7%AC%AC%E4%B8%80%E4%B8%AA%E7%A8%8B%E5%BA%8F/"},{"categories":["汇编"],"content":"汇编和连接的作用 连接的作用： 当源程序很大时，可以将他们分成多个源程序文件夹编译，每个源程序编译成为目标文件后，再用连接程序将它们连接在一起，生成一个可执行文件； 程序中调用了某个库文件中的子程序，需要将这个库文件和该程序生成的目标文件连接到一起，生成一个可执行文件； 一个源程序编译后，得到了存有机器码的目标文件，目标文件中的有些内容还不能直接用来生成可执行文件，连接程序将这些内容处理为最终的可执行信息。所以在只有一个源程序文件，而又不需要调用某个库中的子程序的情况下，也必须用连接程序对目标文件进行处理，生成可执行文件； ","date":"2022-12-06","objectID":"/%E6%B1%87%E7%BC%96%E4%B9%8B%E7%AC%AC%E4%B8%80%E4%B8%AA%E7%A8%8B%E5%BA%8F/:4:0","tags":[],"title":"汇编之第一个程序","uri":"/%E6%B1%87%E7%BC%96%E4%B9%8B%E7%AC%AC%E4%B8%80%E4%B8%AA%E7%A8%8B%E5%BA%8F/"},{"categories":["汇编"],"content":"可执行文件中的程序装入内存并运行的原理 在 DOS 中，可执行文件中的程序 P1 若要运行，必须有一个正在运行的程序 P2，将 P1 从可执行文件中加载入内存，将 CP U的控制权交给P1，P1 才能得以运行； 当 P1 运行完毕后，应该将 CPU 的控制权交还给使他得以运行的程序； 操作系统的外壳： 操作系统是由多个功能模块组成的庞大、复杂的软件系统，任何通用的操作系统，都需要提供一个称为 shell（外壳）的程序，用户（操作人员）使用这个程序来操作计算机系统工作； DOS 中有一个程序 command.com，这个程序在 DOS 中称为命令解释器，也就是 DOS 系统的 shell； 执行可执行文件 1.exe 时， （1）什么程序将 CPU 的控制权交给了 1.exe？ （2）将程序 1.exe 加载入内存后，如何使程序得以运行？ （3）1.exe 程序运行结束后，返回到了哪里？ 在 DOS 中直接执行 1.exe 时，是正在运行的 cmd.exe 将 1.exe 中的程序加载入内存； cmd.exe 设置 CPU 的 CS:IP 指向程序的第一条指令（即，程序的入口），从而使程序得以运行； 程序运行结束后，返回 cmd.exe 中，CPU 继续运行 cmd.exe； 汇编程序从写出到执行的过程： ","date":"2022-12-06","objectID":"/%E6%B1%87%E7%BC%96%E4%B9%8B%E7%AC%AC%E4%B8%80%E4%B8%AA%E7%A8%8B%E5%BA%8F/:5:0","tags":[],"title":"汇编之第一个程序","uri":"/%E6%B1%87%E7%BC%96%E4%B9%8B%E7%AC%AC%E4%B8%80%E4%B8%AA%E7%A8%8B%E5%BA%8F/"},{"categories":["汇编"],"content":"EXE文件中的程序的加载过程 程序加载后，ds 中存放着程序所在内存区的段地址，这个内存区的偏移地址为 0 ，则程序所在的内存区的地址为：ds:0； 这个内存区的前256个字节中存放的是 PSP，dos 用来和程序进行通信。 从 256字节处向后的空间存放的是程序。 所以，我们从 ds 中可以得到 PSP 的段地址 SA，PSP 的偏移地址为 0，则物理地址为 SA×16+0。 因为 PSP 占256（100H）字节，所以程序的物理地址是：SA×16+0+256= SA×16+16×16=（SA+16）×16+0，可用段地址和偏移地址表示为：SA+10:0。 ","date":"2022-12-06","objectID":"/%E6%B1%87%E7%BC%96%E4%B9%8B%E7%AC%AC%E4%B8%80%E4%B8%AA%E7%A8%8B%E5%BA%8F/:6:0","tags":[],"title":"汇编之第一个程序","uri":"/%E6%B1%87%E7%BC%96%E4%B9%8B%E7%AC%AC%E4%B8%80%E4%B8%AA%E7%A8%8B%E5%BA%8F/"},{"categories":["汇编"],"content":"寄存器 ","date":"2022-11-30","objectID":"/%E6%B1%87%E7%BC%96%E4%B9%8B%E5%AF%84%E5%AD%98%E5%99%A8/:1:0","tags":[],"title":"汇编之寄存器","uri":"/%E6%B1%87%E7%BC%96%E4%B9%8B%E5%AF%84%E5%AD%98%E5%99%A8/"},{"categories":["汇编"],"content":"概述 一个典型的CPU主要由运算器，控制器，寄存器等器件构成，它们靠内部总线相连（内部总线实现CPU内部各机器件间的联系，外部总线实现CPU和主板上其他器件之间的联系）。 本篇博文叙述CPU中的寄存器，程序员可以通过指令读写寄存器，从而实现对CPU的控制。不同的CPU其寄存器的个数与结构也不相同，以8086CPU为例，其有14各寄存器，每个寄存器有一个名称，且所有寄存器都是16位的，可以存放两个字节，分别为：AX、BX、CX、DX、SI、DI、SP、BP、IP、CS、SS、DS、ES、PSW，后面将一一进行介绍。 ","date":"2022-11-30","objectID":"/%E6%B1%87%E7%BC%96%E4%B9%8B%E5%AF%84%E5%AD%98%E5%99%A8/:1:1","tags":[],"title":"汇编之寄存器","uri":"/%E6%B1%87%E7%BC%96%E4%B9%8B%E5%AF%84%E5%AD%98%E5%99%A8/"},{"categories":["汇编"],"content":"通用寄存器 AX，BX，CX，DX为四个通用寄存器，用于存放一般性数据（所谓一般性数据指的是那些并非用于CPU控制的数据）。如前所述，8086CPU使用的是16位寄存器，为了兼容之前针对8位CPU编写的程序，每个通用寄存器都可以分为两个8位寄存器来独立使用，即AX可分为AH和AL，BX可分为BH和BL，CX可分为CH和CL,DX可分为DH和DL。如下图所示： 通过汇编语言使用通用寄存器：（汇编语言中不能直接相加两个数字，而是要通过寄存器来做加法） // 18 + 2 MOV AX 18 // 将18送入寄存器AX add ax 8 // 将AX中的数值加上8 // 或如下写法 MOV AX 18 MOV BX 2 add ax bx // 将AX和BX中的数值相加，结果保存在AX中 **【注】：**在写一条汇编指令或一个寄存器名称时不区分大小写。 ","date":"2022-11-30","objectID":"/%E6%B1%87%E7%BC%96%E4%B9%8B%E5%AF%84%E5%AD%98%E5%99%A8/:1:2","tags":[],"title":"汇编之寄存器","uri":"/%E6%B1%87%E7%BC%96%E4%B9%8B%E5%AF%84%E5%AD%98%E5%99%A8/"},{"categories":["汇编"],"content":"代码段寄存器与指令指针寄存器 要说明这两个寄存器，首先我们要理解一下几个概念。 物理地址 每个内存单元都有一个编号，而这就是内存单元唯一的地址，也就是物理地址。 16位结构的CPU 一个16位结构的CPU意味着该CPU具有以下特征： 运算器一次最多可以处理16位的数据 寄存器的最大宽度位16位 寄存器与运算器之间的通路为16位 8086CPU给出的物理地址方法（分段结构） 8086CPU的物理总线的宽度为20，而CPU内部只能处理和传输16位地址，那么如果将地址从内部简单的发出，那么只能送出16位的地址，表现出的寻址能力只有64KB，因此，8080CPU添加了了一个地址加法器，将两个16位地址合并为一个20位的物理地址。其中两个16位地址分别位段地址和偏移地址，地址加法器的计算方法为：物理地址 = 段地址 * 16 + 偏移地址， 即物理地址 = 段地址 \u003c\u003c (20 - 16) + 偏移地址。如下图所示： 代码段寄存器CS，指令指针寄存器IP 8086CPU含有4个段寄存器(存放段地址)：CS，DS，SS，ES。本节只说明CS。CS为代码段寄存器，IP为指令指针寄存器，8086CPU从CS*16 + IP单元开始读取一条指令并执行，即通过CS寄存器找到段地址，IP则记录的是偏移地址，通过二者可以找到要执行的指令代码。 **其过程如下：**将段地址和偏移地址送至地址加法器，之后通过地址总线送至内存芯片，随后内存芯片将对应地址空间中的内容（指令）通过数据总线送达CPU，此时CPU将增加IP寄存器的值（增加值为所读取指令的长度），以便读入下一条指令。 修改CS、IP的指令 mov指令可以用于修改大部分寄存器的值，但不可以用于修改CS、IP的内容。若想修改，可以使用如下语法：*jmp 段地址 ： 偏移地址*，例如：jmp 2AE3:3 执行后 cs = 2AE3H, IP = 0003H。若仅想修改IP的内容，可用形如：*jmp 某一合法寄存器* 来完成，比如 jmp ax或 jmp bx ，那么该语句执行后便会用寄存器中的值修改IP寄存器。 ","date":"2022-11-30","objectID":"/%E6%B1%87%E7%BC%96%E4%B9%8B%E5%AF%84%E5%AD%98%E5%99%A8/:1:3","tags":[],"title":"汇编之寄存器","uri":"/%E6%B1%87%E7%BC%96%E4%B9%8B%E5%AF%84%E5%AD%98%E5%99%A8/"},{"categories":["汇编"],"content":"数据段寄存器DS和偏移地址[address] 8086CPU中有一个DS寄存器，通常用于存放要访问数据的段地址，在配合偏移地址[address]来使用，则可以读取指定内存单元中的数据，如： // 读取10000H单元的内容 mov bx, 1000H mov ds, bx mov al, [0] // [adderss]表示偏移地址 mov [0] cx; 上述代码描述了mov指令可以完成的四种操作：1）將数据直接写入寄存器；2）将一个寄存器中的内容送至另一个寄存器；3）将一个内存单元中的内容送至寄存器；4）将寄存器中的数据送入内存单元。 **【注】：**8086CPU不支持将数据直接送入段寄存器，因此修改DS寄存器需要从另一个寄存器移入。 ","date":"2022-11-30","objectID":"/%E6%B1%87%E7%BC%96%E4%B9%8B%E5%AF%84%E5%AD%98%E5%99%A8/:1:4","tags":[],"title":"汇编之寄存器","uri":"/%E6%B1%87%E7%BC%96%E4%B9%8B%E5%AF%84%E5%AD%98%E5%99%A8/"},{"categories":["汇编"],"content":"栈寄存器 出栈入栈指令 汇编语言的出栈入栈指令为PUSH和POP，其使用方式如下： push/pop 寄存器； push/pop 段寄存器 push/pop 内存单元 栈段寄存器SS与偏移寄存器SP 栈顶通过段寄存器ss与偏移寄存器SP指出。在任意时刻SS:SP总是指向栈顶元素，即最后入栈的数据。每次入栈时，push指令都将先移动栈顶指针再写入数据。当栈为空时，栈顶指针SS:SP指向比栈底地址高一个元素大小的位置（栈由高地址向低地址增长）。 **【注】：**将一段地址当作栈段仅仅是编程时的一种安排，而CPU并不会区别的对待这段内存地址空间。 ","date":"2022-11-30","objectID":"/%E6%B1%87%E7%BC%96%E4%B9%8B%E5%AF%84%E5%AD%98%E5%99%A8/:1:5","tags":[],"title":"汇编之寄存器","uri":"/%E6%B1%87%E7%BC%96%E4%B9%8B%E5%AF%84%E5%AD%98%E5%99%A8/"},{"categories":["汇编"],"content":"Debug工具 Debug是供程序员使用的程序调试工具，可以用它检查内存中任何地方的字节以及修改任何地方的字节。 -r命令 r命令用于查看和改变CPU寄存器的内容，使用如下图所示 -d命令 可以通过\"d 段地址：偏移地址\"的格式查看从指定地址起的内存块中的内容。 -e命令 改写内存单元的内容 -a命令 以汇编指令的形式改写指定的内存单元。如下所示： -t命令 执行-t命令后，CPU将执行CS:IP指向的指令。如下图所示： ","date":"2022-11-30","objectID":"/%E6%B1%87%E7%BC%96%E4%B9%8B%E5%AF%84%E5%AD%98%E5%99%A8/:1:6","tags":[],"title":"汇编之寄存器","uri":"/%E6%B1%87%E7%BC%96%E4%B9%8B%E5%AF%84%E5%AD%98%E5%99%A8/"},{"categories":["汇编"],"content":"小结 CPU访问内存时，必须向内存提供内存单元的物理地址。8086CPU在内部用段地址和偏移地址移位相加的方法形成最终的物理地址 结论：CPU可以使用不同的段地址和偏移地址形成同一个物理地址 偏移地址16位，变化范围为0~FFFFH，仅用偏移地址来寻址最多可寻64kb个内存单元 在8086PC机，存储单元的地址用两个元素来表述————段地址和偏移地址 可以根据需要，将地址连续，起始地址为16的倍数的一组内存单元定义为一个段 段地址在8086CPU的段寄存器中存放。当8086CPU要访问内存时，由段寄存器提供内存单元的段地址。8086CPU有四个段寄存器，其中CS用来存放指令的段地址 CS存放指令的段地址，IP存放指令的偏移地址 8086机中，任意时刻，CPU将CS:IP指向的内容当作指令来执行 8086CPU的工作执行过程： 从CS:IP指向的内存单元读取指令，读取的指令进入指令缓冲器 IP指向下一条指令 执行指令（转到步骤1，重复这个过程） 8086CPU提供转移指令修改CS:IP的值（jmp指令） 字在内存里存储时，要用两个连续的地址单元来存储，字的低位字节存放在低地址单元中，高位字节存放在地址单元中 用mov指令访问内存单元，可以在mov指令中只给出内存单元的偏移地址，此时，段地址默认在DS寄存器中 [address]表示一个偏移地址为address的内存单元 在内存和寄存器之间传送字型数据时，高地址单元和高8位寄存器，低地址单元和低8位寄存器相对应 mov、add、sub是具有两个操作对象的指令。jmp是具有一个操作对象的指令 在8086CPU提供了栈操作机制：SS段寄存器存放栈顶的段地址，在SP寄存器里存放栈顶的偏移地址。提供入栈和出栈的指令，它们根据SS:IP指示的地址，按照栈的方式访问内存单元 push：①SP=SP-2;②向SS:IP指向的字单元中送入数据 pop：①从SS:IP指向的字单元中读取数据;②SP=SP+2 任意时刻，SS:IP指向的栈顶元素 8086CPU只记录栈顶，栈顶空间大小，是否栈溢出等需要我们自己管理 可以用栈来保存需要恢复寄存器的内容 我们可以将一段内存定义为一个段，用一个段地址指示段，用偏移地址访问段内的内存单元。 我们可以用一个段存放数据，将他定义为“数据段” 我们可以用一个段存放代码，将他定义为“代码段” 我们可以用一个段当作栈，将他定义为“栈段” 不管我们如何安排，CPU将内存中的某些内容当作代码，是因为CS:IP指向了那里；CPU将内存中的某些内容当作栈，是因为CS:IP指向了那里 所以，一段内存既可以是代码的存储空间，又可以是数据的存储空间，还可以是栈空间，也可以是栈空间，也可以什么都不是。关键在于CPU寄存器的设置，即CS、IP、SS、SP、DS的指向。 ","date":"2022-11-30","objectID":"/%E6%B1%87%E7%BC%96%E4%B9%8B%E5%AF%84%E5%AD%98%E5%99%A8/:1:7","tags":[],"title":"汇编之寄存器","uri":"/%E6%B1%87%E7%BC%96%E4%B9%8B%E5%AF%84%E5%AD%98%E5%99%A8/"},{"categories":["汇编"],"content":"汇编基础术语 什么是汇编语言？汇编语言就是直接工作硬件之上的编程语言。 ","date":"2022-11-29","objectID":"/%E6%B1%87%E7%BC%96%E4%B9%8B%E6%A6%82%E8%BF%B0/:1:0","tags":[],"title":"汇编之概述","uri":"/%E6%B1%87%E7%BC%96%E4%B9%8B%E6%A6%82%E8%BF%B0/"},{"categories":["汇编"],"content":"机器语言 机器语言就是机器指令的集合。电子计算机的机器指令是一串二进制数字，而且不同的微处理器有不同的机器指令集，也就是机器语言。 早期的程序都是编写一大段01的二进制代码，也就是机器语言，显然不利于人工的记忆和操作。为了解决机器语言的晦涩难记的问题，人们想出了汇编语言。 ","date":"2022-11-29","objectID":"/%E6%B1%87%E7%BC%96%E4%B9%8B%E6%A6%82%E8%BF%B0/:1:1","tags":[],"title":"汇编之概述","uri":"/%E6%B1%87%E7%BC%96%E4%B9%8B%E6%A6%82%E8%BF%B0/"},{"categories":["汇编"],"content":"汇编语言 汇编语言的主体是汇编指令。汇编指令和机器指令的差别在于指令的表示方法上。当然，汇编语言是人来设计的，机器是看不懂的，需要经过汇编语言的编译器，将汇编代码翻译成机器指令集，然后机器才可以执行代码。 汇编语言的组成： 汇编指令：机器码的助记符，有对应的机器码（核心） 伪指令：没有对应的机器码，由编译器执行，计算机并不执行 其他符号：如+、-、*、/等，有编译器识别，没有对应的机器码 ","date":"2022-11-29","objectID":"/%E6%B1%87%E7%BC%96%E4%B9%8B%E6%A6%82%E8%BF%B0/:1:2","tags":[],"title":"汇编之概述","uri":"/%E6%B1%87%E7%BC%96%E4%B9%8B%E6%A6%82%E8%BF%B0/"},{"categories":["汇编"],"content":"存储器 CPU是计算机的核心部件，它控制整个计算机的运作并进行计算。要想让一个CPU工作，就必须向它提供指令和数据。指令和数据就放在存储器里，当然也指内存。CPU是与内存打交道，磁盘不能直接与CPU进行数据交换，中间需要存取速度更快的内存。 ","date":"2022-11-29","objectID":"/%E6%B1%87%E7%BC%96%E4%B9%8B%E6%A6%82%E8%BF%B0/:1:3","tags":[],"title":"汇编之概述","uri":"/%E6%B1%87%E7%BC%96%E4%B9%8B%E6%A6%82%E8%BF%B0/"},{"categories":["汇编"],"content":"指令和数据 在内存或磁盘上，指令和数据没有任何区别，都是二进制信息。CPU在工作的时候将有些信息看成指令，有些看成数据。 ","date":"2022-11-29","objectID":"/%E6%B1%87%E7%BC%96%E4%B9%8B%E6%A6%82%E8%BF%B0/:1:4","tags":[],"title":"汇编之概述","uri":"/%E6%B1%87%E7%BC%96%E4%B9%8B%E6%A6%82%E8%BF%B0/"},{"categories":["汇编"],"content":"存储单元 存储器被划分为若干个存储单元，每个存储单元从0开始顺序编号，例如一个存储器有128个存储单元，编号从0~127. 一般一个存储单元就是一个字节 ","date":"2022-11-29","objectID":"/%E6%B1%87%E7%BC%96%E4%B9%8B%E6%A6%82%E8%BF%B0/:1:5","tags":[],"title":"汇编之概述","uri":"/%E6%B1%87%E7%BC%96%E4%B9%8B%E6%A6%82%E8%BF%B0/"},{"categories":["汇编"],"content":"CPU对存储器的读写 存储器被划分为多个存储单元，存储单元从零开始顺序编号，这些编号可以看作是存储单元在存储器的地址。 CPU要从内存中读写数据，首先要指定存储单元的地址，然后需要明确进行何种操作。 也就是说，CPU对数据进行读写时，必须和外部器件进行3类信息的交互 存储单元的地址信息（地址信息） 器件的选择，读或写的命令（控制信息） 读或写的数据（数据信息） 那么，CPU通过什么将地址、数据和控制信息传到存储器的呢？——总线。 总线从物理上来讲，就是一根根导线的集合。根据传送信息的不同，总线从逻辑上又分为3类：地址总线、控制总线、数据总线。 CPU读数据： CPU通过地址线将地址信息3发出 CPU通过控制线发出内存读命令，选中存储器芯片并通知它，将要从中读取数据 存储器将3号单元里的数据8通过数据线送入CPU CPU写数据： CPU通过地址线将地址信息3发出 CPU通过控制线发出内存写命令，选中存储器芯片并通知它，将要从中写入数据 CPU通过数据线将数据26送入内存的3号单元中 ","date":"2022-11-29","objectID":"/%E6%B1%87%E7%BC%96%E4%B9%8B%E6%A6%82%E8%BF%B0/:1:6","tags":[],"title":"汇编之概述","uri":"/%E6%B1%87%E7%BC%96%E4%B9%8B%E6%A6%82%E8%BF%B0/"},{"categories":["汇编"],"content":"地址总线 CPU通过地址总线来指定存储单元，由此可推知，地址总线上能传递多少种不同的信息，CPU就可以对多少个存储单元进行寻址，假设一个CPU有10根地址总线，那么其寻址空间为0 ~1023，即2^10个存储单元。即地址总线的宽度决定了CPU的寻址范围。 ","date":"2022-11-29","objectID":"/%E6%B1%87%E7%BC%96%E4%B9%8B%E6%A6%82%E8%BF%B0/:1:7","tags":[],"title":"汇编之概述","uri":"/%E6%B1%87%E7%BC%96%E4%B9%8B%E6%A6%82%E8%BF%B0/"},{"categories":["汇编"],"content":"数据总线 CPU和其它器件之间的数据传递是通过数据总线进行传递的，数据总线的宽度决定了CPU和其它器件间数据传送速度。8根数据总线一次可以传送一个8位二进制数据，因为每一根总线只能传递一个高电平或低电平来代表0或1，即1根总线一次只能传递一位。 ","date":"2022-11-29","objectID":"/%E6%B1%87%E7%BC%96%E4%B9%8B%E6%A6%82%E8%BF%B0/:1:8","tags":[],"title":"汇编之概述","uri":"/%E6%B1%87%E7%BC%96%E4%B9%8B%E6%A6%82%E8%BF%B0/"},{"categories":["汇编"],"content":"控制总线 CPU对外部器件的控制是通过控制总线完成的，有多少根控制总线就意味着CPU对外部器件有多少种控制。即控制总线的宽度对外部器件的控制能力。 ","date":"2022-11-29","objectID":"/%E6%B1%87%E7%BC%96%E4%B9%8B%E6%A6%82%E8%BF%B0/:1:9","tags":[],"title":"汇编之概述","uri":"/%E6%B1%87%E7%BC%96%E4%B9%8B%E6%A6%82%E8%BF%B0/"},{"categories":["汇编"],"content":"主板 每台PC都有一个主板，每个主板上都存在一些核心器件：CPU，存储器，外围芯片组，扩展卡槽等。扩展卡槽上一般有RAM内存条和各类接口卡。这些器件通过总线相连。 ","date":"2022-11-29","objectID":"/%E6%B1%87%E7%BC%96%E4%B9%8B%E6%A6%82%E8%BF%B0/:1:10","tags":[],"title":"汇编之概述","uri":"/%E6%B1%87%E7%BC%96%E4%B9%8B%E6%A6%82%E8%BF%B0/"},{"categories":["汇编"],"content":"接口卡 CPU对于诸如：显示器，音箱，打印机这样的外设不能直接控制，直接控制这些设备工作的是插在扩展插槽上的接口卡，由于扩展插槽通过总线和CPU相连，所以接口卡也通过总线和CPU相连。之后CPU通过总线向接口卡发送命令，接口卡在根据CPU的命令控制外设进行工作。（例：比如一个音箱通过USB插槽连接到了电脑，而该插槽通过总线和CPU相连，此时当要播放声音时会将数据指令等通过数据总线发送到卡槽，即音箱内的接口卡也会收到，之后控制音箱工作） ","date":"2022-11-29","objectID":"/%E6%B1%87%E7%BC%96%E4%B9%8B%E6%A6%82%E8%BF%B0/:1:11","tags":[],"title":"汇编之概述","uri":"/%E6%B1%87%E7%BC%96%E4%B9%8B%E6%A6%82%E8%BF%B0/"},{"categories":["汇编"],"content":"各类存储器芯片 存储器从读写属性上可以分为随机存储器（RAM）和只读存储器(ROM)，其中随机存储器（RAM）可读可写但必须带电存储，关机后存储内容会丢失。只读存储器只能读取，不能写入，关机后其内容不会丢失。 而从功能和连接方式看又可以分为以下3类： 随机存储器（内存）：用于存放供CPU使用的绝大部分程序和数据 装有BIOS的ROM：BIOS（基本输入/输出系统）是由主板和各类接口卡厂商提供的软件系统，可以通过BIOS使用硬件设备进行最基本的输入输出。在主板和某写接口卡上插有存储相应BIOS的ROM，如：主板上的ROM中存储着主板的BIOS（通常即系统BIOS），显卡上的ROM存储着显卡的BIOS，网卡上装有网卡的BIOS。 接口卡上的RAM：有的接口卡需要对大批量的输入输出数据进行进行暂存，因此需要RAM。最典型的就是显卡上的RAM,一般称为显存。显卡随时将显存中的数据输出到显示器上。 ","date":"2022-11-29","objectID":"/%E6%B1%87%E7%BC%96%E4%B9%8B%E6%A6%82%E8%BF%B0/:1:12","tags":[],"title":"汇编之概述","uri":"/%E6%B1%87%E7%BC%96%E4%B9%8B%E6%A6%82%E8%BF%B0/"},{"categories":["汇编"],"content":"CPU的内存地址空间 假设一个CPU的地址总线宽度为10，那么其寻址范围为1024个内存单元，而这1024个内存单元就构成了这个CPU的内存地址空间。 各类物理上独立的器件都是通过总线与CPU相连的，CPU操作它们时也是将它们当作内存来看待的，每个物理存储器占用CPU的内存地址空间中的一部分，当CPU在某个物理器件对应的地址空间中读写数据，实际上就是在对应的物理存储器中读写数据。 ","date":"2022-11-29","objectID":"/%E6%B1%87%E7%BC%96%E4%B9%8B%E6%A6%82%E8%BF%B0/:1:13","tags":[],"title":"汇编之概述","uri":"/%E6%B1%87%E7%BC%96%E4%B9%8B%E6%A6%82%E8%BF%B0/"},{"categories":["golang"],"content":"本文介绍了go语言的原子操作，并对字节跳动开源的协程池进行代码分析","date":"2022-11-09","objectID":"/%E5%B9%B6%E5%8F%91%E5%AE%89%E5%85%A8%E4%B9%8B%E5%8E%9F%E5%AD%90%E6%93%8D%E4%BD%9C/","tags":["go编程技巧"],"title":"并发安全之原子操作","uri":"/%E5%B9%B6%E5%8F%91%E5%AE%89%E5%85%A8%E4%B9%8B%E5%8E%9F%E5%AD%90%E6%93%8D%E4%BD%9C/"},{"categories":["golang"],"content":"[toc] ","date":"2022-11-09","objectID":"/%E5%B9%B6%E5%8F%91%E5%AE%89%E5%85%A8%E4%B9%8B%E5%8E%9F%E5%AD%90%E6%93%8D%E4%BD%9C/:0:0","tags":["go编程技巧"],"title":"并发安全之原子操作","uri":"/%E5%B9%B6%E5%8F%91%E5%AE%89%E5%85%A8%E4%B9%8B%E5%8E%9F%E5%AD%90%E6%93%8D%E4%BD%9C/"},{"categories":["golang"],"content":"原子操作 并发是业务开发中经常要面对的问题，很多时候我们会直接用一把 sync.Mutex 互斥锁来线性化处理，保证每一时刻进入临界区的 goroutine 只有一个。这样避免了并发，但性能也随着降低。 所以，我们进而又有了 RWMutex 读写锁，保障了多个读请求的并发处理，对共享资源的写操作和读操作则区别看待，并消除了读操作之间的互斥。 Mutex 和 RWMutex 锁的实现本身还是基于 atomic 包提供的原子操作，辅之以自旋等处理。很多时候其实我们不太需要锁住资源这个语意，而是一个原子操作就ok。这篇文章我们来看一下 atomic 包提供的能力。 ","date":"2022-11-09","objectID":"/%E5%B9%B6%E5%8F%91%E5%AE%89%E5%85%A8%E4%B9%8B%E5%8E%9F%E5%AD%90%E6%93%8D%E4%BD%9C/:1:0","tags":["go编程技巧"],"title":"并发安全之原子操作","uri":"/%E5%B9%B6%E5%8F%91%E5%AE%89%E5%85%A8%E4%B9%8B%E5%8E%9F%E5%AD%90%E6%93%8D%E4%BD%9C/"},{"categories":["golang"],"content":"定义 不会被线程调度机制打断的操作。 “原子操作(atomic operation)是不需要synchronized”，这是多线程编程的老生常谈了。所谓原子操作是指不会被线程调度机制打断的操作；这种操作一旦开始，就一直运行到结束，中间不会有任何 context switch（切换到另一个线程）。使用原子操作，就可以保证你的操作一定是原子的，也就是不会被同一时刻其他并发线程打断，一定会执行。 ","date":"2022-11-09","objectID":"/%E5%B9%B6%E5%8F%91%E5%AE%89%E5%85%A8%E4%B9%8B%E5%8E%9F%E5%AD%90%E6%93%8D%E4%BD%9C/:1:1","tags":["go编程技巧"],"title":"并发安全之原子操作","uri":"/%E5%B9%B6%E5%8F%91%E5%AE%89%E5%85%A8%E4%B9%8B%E5%8E%9F%E5%AD%90%E6%93%8D%E4%BD%9C/"},{"categories":["golang"],"content":"Golang的原子操作 Go语言通过内置包sync/atomic提供了对原子操作的支持，其提供的原子操作有以下几大类： 增减，操作的方法名方式为AddXXXType，保证对操作数进行原子的增减，支持的类型为int32、int64、uint32、uint64、uintptr，使用时以实际类型替换前面我说的XXXType就是对应的操作方法。 载入，保证了读取到操作数前没有其他任务对它进行变更，操作方法的命名方式为LoadXXXType，支持的类型除了基础类型外还支持Pointer，也就是支持载入任何类型的指针。 存储，有载入了就必然有存储操作，这类操作的方法名以Store开头，支持的类型跟载入操作支持的那些一样。 比较并交换，也就是CAS（Compare And Swap），像Go的很多并发原语实现就是依赖的CAS操作，同样是支持上面列的那些类型。 交换，这个简单粗暴一些，不比较直接交换，这个操作很少会用。 ","date":"2022-11-09","objectID":"/%E5%B9%B6%E5%8F%91%E5%AE%89%E5%85%A8%E4%B9%8B%E5%8E%9F%E5%AD%90%E6%93%8D%E4%BD%9C/:1:2","tags":["go编程技巧"],"title":"并发安全之原子操作","uri":"/%E5%B9%B6%E5%8F%91%E5%AE%89%E5%85%A8%E4%B9%8B%E5%8E%9F%E5%AD%90%E6%93%8D%E4%BD%9C/"},{"categories":["golang"],"content":"互斥锁跟原子操作的区别 平日里，在并发编程里，Go语言sync包里的同步原语Mutex是我们经常用来保证并发安全的，那么他跟atomic包里的这些操作有啥区别呢？在我看来他们在使用目的和底层实现上都不一样： 使用目的：互斥锁是用来保护一段逻辑，原子操作用于对一个变量的更新保护。 底层实现：Mutex由操作系统的调度器实现（上下文切换开销较大），而atomic包中的原子操作则由底层硬件指令直接提供支持，这些指令在执行的过程中是不允许中断的，因此原子操作可以在lock-free的情况下保证并发安全，并且它的性能也能做到随CPU个数的增多而线性扩展。 对于一个变量更新的保护，原子操作通常会更有效率，并且更能利用计算机多核的优势。 所以，如果在业务里只是针对单个变量并发运算的安全时，不应该考虑加锁而是考虑使用原子操作保证其原子性。貌似原子操作是乐观锁的实现，如果同一时刻有太多变量对同一个变量进行写操作，使用CAS机制时，最多同一时刻只能有一个操作成功，其余操作全部失败终止或者进入原地地“自旋”。 使用互斥锁的并发计数器程序的例子： func mutexAdd() { var a int32 = 0 var wg sync.WaitGroup var mu sync.Mutex // 互斥锁 start := time.Now() for i := 0; i \u003c 100000000; i++ { wg.Add(1) go func() { defer wg.Done() mu.Lock() a += 1 mu.Unlock() }() } wg.Wait() timeSpends := time.Now().Sub(start).Nanoseconds() fmt.Printf(\"use mutex a is %d, spend time: %v\\n\", a, timeSpends) } 把Mutex改成用方法atomic.AddInt32(\u0026a, 1)调用，在不加锁的情况下仍然能确保对变量递增的并发安全。 func AtomicAdd() { var a int32 = 0 var wg sync.WaitGroup start := time.Now() for i := 0; i \u003c 1000000; i++ { wg.Add(1) go func() { defer wg.Done() atomic.AddInt32(\u0026a, 1) }() } wg.Wait() timeSpends := time.Now().Sub(start).Nanoseconds() fmt.Printf(\"use atomic a is %d, spend time: %v\\n\", atomic.LoadInt32(\u0026a), timeSpends) } 可以在本地运行以上这两段代码，可以观察到计数器的结果都最后都是1000000，都是线程安全的。 上面两种方式：第一种互斥锁实现的并发计数例子里，每一次开了大量携程来对a加1，为了保证并发安全加了互斥锁，虽然有效但是性能开销太大了，期间发生了很多次上下文切换；第二种使用原子操作来对a变量实现加1操作，这个过程是原子的，其他协程无法打断，只能等待指令执行成功之后，其他协程才能拿到这个a的地址又进行原子操作，这样循环直到所有协程执行完毕后程序结束。相比第一种加互斥锁保证并发安全的方式，第二种原子操作的方式是通过CPU指令集实现的，没有上下文切换的开销，只是执行一个CPU指令而已。 ","date":"2022-11-09","objectID":"/%E5%B9%B6%E5%8F%91%E5%AE%89%E5%85%A8%E4%B9%8B%E5%8E%9F%E5%AD%90%E6%93%8D%E4%BD%9C/:1:3","tags":["go编程技巧"],"title":"并发安全之原子操作","uri":"/%E5%B9%B6%E5%8F%91%E5%AE%89%E5%85%A8%E4%B9%8B%E5%8E%9F%E5%AD%90%E6%93%8D%E4%BD%9C/"},{"categories":["golang"],"content":"比较并交换 该操作简称CAS (Compare And Swap)。这类操作的前缀为 CompareAndSwap : func CompareAndSwapInt32(addr *int32, old, new int32) (swapped bool) func CompareAndSwapPointer(addr *unsafe.Pointer, old, new unsafe.Pointer) (swapped bool) 该操作在进行交换前首先确保被操作数的值未被更改，即仍然保存着参数 old 所记录的值，满足此前提条件下才进行交换操作。CAS的做法类似操作数据库时常见的乐观锁机制。 需要注意的是：当有大量的goroutine 对变量进行读写操作时，可能导致CAS操作无法成功，这时可以利用for循环多次尝试。(乐观锁不适合写多读少的场景) 上面我只列出了比较典型的int32和unsafe.Pointer类型的CAS方法，主要是想说除了读数值类型进行比较交换，还支持对指针进行比较交换。 unsafe.Pointer提供了绕过Go语言指针类型限制的方法，unsafe指的并不是说不安全，而是说官方并不保证向后兼容。 // 定义一个struct类型P type P struct{ x, y, z int } // 执行类型P的指针 var pP *P func main() { // 定义一个执行unsafe.Pointer值的指针变量 var unsafe1 = (*unsafe.Pointer)(unsafe.Pointer(\u0026pP)) // Old pointer var sy P // 为了演示效果先将unsafe1设置成Old Pointer px := atomic.SwapPointer( unsafe1, unsafe.Pointer(\u0026sy)) // 执行CAS操作，交换成功，结果返回true y := atomic.CompareAndSwapPointer( unsafe1, unsafe.Pointer(\u0026sy), px) fmt.Println(y) } 上面的示例并不是在并发环境下进行的CAS，只是为了演示效果，先把被操作数设置成了Old Pointer。 其实Mutex的底层实现也是依赖原子操作中的CAS实现的，原子操作的atomic包相当于是sync包里的那些同步原语的实现依赖。 比如互斥锁Mutex的结构里有一个state字段，其是表示锁状态的状态位。 type Mutex struct { state int32 sema uint32 } 为了方便理解，我们在这里将它的状态定义为0和1，0代表目前该锁空闲，1代表已被加锁，以下是sync.Mutex中Lock方法的部分实现代码。 func (m *Mutex) Lock() { // Fast path: grab unlocked mutex. if atomic.CompareAndSwapInt32(\u0026m.state, 0, mutexLocked) { if race.Enabled { race.Acquire(unsafe.Pointer(m)) } return } // Slow path (outlined so that the fast path can be inlined) m.lockSlow() } 在atomic.CompareAndSwapInt32(\u0026m.state, 0, mutexLocked)中，m.state代表锁的状态，通过CAS方法，判断锁此时的状态是否空闲（m.state==0），是，则对其加锁（mutexLocked常量的值为1）。 ","date":"2022-11-09","objectID":"/%E5%B9%B6%E5%8F%91%E5%AE%89%E5%85%A8%E4%B9%8B%E5%8E%9F%E5%AD%90%E6%93%8D%E4%BD%9C/:1:4","tags":["go编程技巧"],"title":"并发安全之原子操作","uri":"/%E5%B9%B6%E5%8F%91%E5%AE%89%E5%85%A8%E4%B9%8B%E5%8E%9F%E5%AD%90%E6%93%8D%E4%BD%9C/"},{"categories":["golang"],"content":"atomic.Value保证任意值的读写安全 atomic包里提供了一套Store开头的方法，用来保证各种类型变量的并发写安全，避免其他操作读到了修改变量过程中的脏数据。 func StoreInt32(addr *int32, val int32) func StoreInt64(addr *int64, val int64) func StorePointer(addr *unsafe.Pointer, val unsafe.Pointer) ... 这些操作方法的定义与上面介绍的那些操作的方法类似，我就不再演示怎么使用这些方法了。 值得一提的是如果你想要并发安全的设置一个结构体的多个字段，除了把结构体转换为指针，通过StorePointer设置外，还可以使用atomic包后来引入的atomic.Value，它在底层为我们完成了从具体指针类型到unsafe.Pointer之间的转换。 有了atomic.Value后，它使得我们可以不依赖于不保证兼容性的unsafe.Pointer类型，同时又能将任意数据类型的读写操作封装成原子性操作（中间状态对外不可见）。 atomic.Value类型对外暴露了两个方法： v.Store(c) - 写操作，将原始的变量c存放到一个atomic.Value类型的v里。 c := v.Load() - 读操作，从线程安全的v中读取上一步存放的内容。 1.17 版本我看还增加了Swap和CompareAndSwap方法。 简洁的接口使得它的使用也很简单，只需将需要做并发保护的变量读取和赋值操作用Load()和Store()代替就行了。 由于Load()返回的是一个interface{}类型，所以在使用前我们记得要先转换成具体类型的值，再使用。下面是一个简单的例子演示atomic.Value的用法。 type Demo struct { A int B int } var rect atomic.Value func main() { wg := sync.WaitGroup{} var container *Demo = \u0026Demo{} // 10 个协程并发更新 for i := 0; i \u003c 100; i++ { wg.Add(1) go func() { defer wg.Done() container.A = i container.B = i + 1 rect.Store(container) }() } wg.Wait() _r := rect.Load().(*Demo) fmt.Printf(\"rect.width=%d\\nrect.length=%d\\n\", _r.A, _r.B) } 你也可以试试，不用atomic.Value，看看在并发条件下，两个字段的值是不是能跟预期的一样变成10和15。 ","date":"2022-11-09","objectID":"/%E5%B9%B6%E5%8F%91%E5%AE%89%E5%85%A8%E4%B9%8B%E5%8E%9F%E5%AD%90%E6%93%8D%E4%BD%9C/:1:5","tags":["go编程技巧"],"title":"并发安全之原子操作","uri":"/%E5%B9%B6%E5%8F%91%E5%AE%89%E5%85%A8%E4%B9%8B%E5%8E%9F%E5%AD%90%E6%93%8D%E4%BD%9C/"},{"categories":["golang"],"content":"总结 原子操作由底层硬件支持，而锁则由操作系统的调度器实现。 锁应当用来保护一段逻辑 对于一个变量更新的保护，原子操作通常会更有效率，并且更能利用计算机多核的优势 如果要更新的是一个复合对象，则应当使用atomic.Value封装好的实现 ","date":"2022-11-09","objectID":"/%E5%B9%B6%E5%8F%91%E5%AE%89%E5%85%A8%E4%B9%8B%E5%8E%9F%E5%AD%90%E6%93%8D%E4%BD%9C/:1:6","tags":["go编程技巧"],"title":"并发安全之原子操作","uri":"/%E5%B9%B6%E5%8F%91%E5%AE%89%E5%85%A8%E4%B9%8B%E5%8E%9F%E5%AD%90%E6%93%8D%E4%BD%9C/"},{"categories":["golang"],"content":"项目案例分析 在net\\http官方包里，http.ListenAndServe()里面的逻辑就是：来一个请求就开一个协程去处理它。在高并发的场景下，虽然go可以轻松开启数百万协程，每个协程初始被分配2kb的内存大小。来一个请求就开一个协程，这样的方式太过粗暴，耗费资源。我们可以对goroutine做池化处理：维护cap容量的goroutine工作，当执行的任务小于cap时，这时资源是充足的，可以让每个goroutine对应一个执行任务；当执行任务大于cap时，这时认为资源是不足的，我们需要复用goroutine，也就是会存在一个goroutine执行完当前任务时，就再执行下一个任务，达到复用的目的。 ","date":"2022-11-09","objectID":"/%E5%B9%B6%E5%8F%91%E5%AE%89%E5%85%A8%E4%B9%8B%E5%8E%9F%E5%AD%90%E6%93%8D%E4%BD%9C/:2:0","tags":["go编程技巧"],"title":"并发安全之原子操作","uri":"/%E5%B9%B6%E5%8F%91%E5%AE%89%E5%85%A8%E4%B9%8B%E5%8E%9F%E5%AD%90%E6%93%8D%E4%BD%9C/"},{"categories":["golang"],"content":"Pool Pool是对外暴露的方法集合的接口。内部是pool结构体来实现的 type Pool interface { // Name returns the corresponding pool name. Name() string // SetCap sets the goroutine capacity of the pool. SetCap(cap int32) // Go executes f. Go(f func()) // CtxGo executes f and accepts the context. CtxGo(ctx context.Context, f func()) // SetPanicHandler sets the panic handler. SetPanicHandler(f func(context.Context, interface{})) // WorkerCount returns the number of running workers WorkerCount() int32 } 简单看看这几个方法 Name() string 方法是指定当前池的名字 SetCap(cap int32)用来动态扩容，可以看到源码里是并发安全的（原子的将新值加载到pool.cap的） Go(f func())将函数放到任务链表里，并判断是否需要新开worker（即协程）来分担f所在的任务链表执行 CtxGo(ctx context.Context, f func())同Go(f func() SetPanicHandler(f func(context.Context, interface{}))是自定义panic处理逻辑，如果不定义将会默认调用github.com/bytedance/gopkg/util/logger来处理 WorkerCount() int32动态获取当前运行的worker，也就是正在跑的goroutine数量 ","date":"2022-11-09","objectID":"/%E5%B9%B6%E5%8F%91%E5%AE%89%E5%85%A8%E4%B9%8B%E5%8E%9F%E5%AD%90%E6%93%8D%E4%BD%9C/:2:1","tags":["go编程技巧"],"title":"并发安全之原子操作","uri":"/%E5%B9%B6%E5%8F%91%E5%AE%89%E5%85%A8%E4%B9%8B%E5%8E%9F%E5%AD%90%E6%93%8D%E4%BD%9C/"},{"categories":["golang"],"content":"pool pool是内部包自己实现Pool接口的结构体。 type pool struct { // The name of the pool name string // capacity of the pool, the maximum number of goroutines that are actually working cap int32 // 也就是限制 worker 的数量 // Configuration information config *Config // gopool提供默认配置，当任务数大于1时，就新开一个goroutine // 真正放函数的地方，所有的函数都放在这里 // linked list of tasks taskHead *task // 头节点 taskTail *task // 指向最后一个节点，实现一个O(1)的复杂度添加 taskLock sync.Mutex // 保证并发安全 taskCount int32 // 任务数量 // Record the number of running workers workerCount int32 // 正在跑的worker // This method will be called when the worker panic panicHandler func(context.Context, interface{}) } name是当前池的名字。 cap是当前池的容量，即允许在跑的最大协程数 config标识当前每个worker应当被分配的任务函数个数，包里默认是1 taskXXXX是任务链表，taskLock是为了保证代码片段的并发安全，taskCount动态标识当前还未执行的任务函数 workerCount标识当前正在跑的协程数 panicHandler自定义panic处理 ","date":"2022-11-09","objectID":"/%E5%B9%B6%E5%8F%91%E5%AE%89%E5%85%A8%E4%B9%8B%E5%8E%9F%E5%AD%90%E6%93%8D%E4%BD%9C/:2:2","tags":["go编程技巧"],"title":"并发安全之原子操作","uri":"/%E5%B9%B6%E5%8F%91%E5%AE%89%E5%85%A8%E4%B9%8B%E5%8E%9F%E5%AD%90%E6%93%8D%E4%BD%9C/"},{"categories":["golang"],"content":"pool的方法 func (p *pool) Name() string { return p.name } // SetCap 通过CPU指令集保证是原子的方式存到指定地址，保证并发安全 func (p *pool) SetCap(cap int32) { // 将值原子地设置到p里 atomic.StoreInt32(\u0026p.cap, cap) } func (p *pool) Go(f func()) { p.CtxGo(context.Background(), f) } func (p *pool) CtxGo(ctx context.Context, f func()) { // 池里取空的对象 t := taskPool.Get().(*task) // 置新 t.ctx = ctx t.f = f // 下面代码逻辑上锁 p.taskLock.Lock() // 将 task 放到任务链表里，这个过程由于是多个Goroutine操作并发操作，都在往任务链表里塞 task ， // 因此，需要将task入链表的代码锁住，防止并发问题 if p.taskHead == nil { // 链表为空 p.taskHead = t p.taskTail = t } else { // p.taskTail.next = t p.taskTail = t } p.taskLock.Unlock() // 然后原子更改任务状态，不需要加锁（避免上下文切换），这样可以获得更好的性能 atomic.AddInt32(\u0026p.taskCount, 1) // The following two conditions are met: // 1. the number of tasks is greater than the threshold. // 2. The current number of workers is less than the upper limit p.cap. // or there are currently no workers. // 根据默认的配置：worker数大于等于 p.cap时就会，就不会新建 worker了，此时就会复用已有的worker来执行； // 在小于p.cap时就会每个task新建一个goroutine if (atomic.LoadInt32(\u0026p.taskCount) \u003e= p.config.ScaleThreshold \u0026\u0026 p.WorkerCount() \u003c atomic.LoadInt32(\u0026p.cap)) || p.WorkerCount() == 0 { // 新开一个 worker p.incWorkerCount() // 池里拿 w := workerPool.Get().(*worker) // p放到worker的pool里 w.pool = p // 再开一个协程跑，取这个worker的任务并执行 w.run() } } // SetPanicHandler the func here will be called after the panic has been recovered. func (p *pool) SetPanicHandler(f func(context.Context, interface{})) { p.panicHandler = f } func (p *pool) WorkerCount() int32 { return atomic.LoadInt32(\u0026p.workerCount) } func (p *pool) incWorkerCount() { atomic.AddInt32(\u0026p.workerCount, 1) } func (p *pool) decWorkerCount() { atomic.AddInt32(\u0026p.workerCount, -1) } ","date":"2022-11-09","objectID":"/%E5%B9%B6%E5%8F%91%E5%AE%89%E5%85%A8%E4%B9%8B%E5%8E%9F%E5%AD%90%E6%93%8D%E4%BD%9C/:2:3","tags":["go编程技巧"],"title":"并发安全之原子操作","uri":"/%E5%B9%B6%E5%8F%91%E5%AE%89%E5%85%A8%E4%B9%8B%E5%8E%9F%E5%AD%90%E6%93%8D%E4%BD%9C/"},{"categories":["golang"],"content":"worker worker结构体里有一个pool指针，在gopool的逻辑实现里，多个worker的pool指向同一个pool。意思是多个worker针对同一个pool里的任务链表处理 type worker struct { pool *pool // 多个worker都指向这个pool } ","date":"2022-11-09","objectID":"/%E5%B9%B6%E5%8F%91%E5%AE%89%E5%85%A8%E4%B9%8B%E5%8E%9F%E5%AD%90%E6%93%8D%E4%BD%9C/:2:4","tags":["go编程技巧"],"title":"并发安全之原子操作","uri":"/%E5%B9%B6%E5%8F%91%E5%AE%89%E5%85%A8%E4%B9%8B%E5%8E%9F%E5%AD%90%E6%93%8D%E4%BD%9C/"},{"categories":["golang"],"content":"worker的方法 run 新开一个携程去轮询任务链表，并作panic的处理。值得注意的是：取任务链表里的任务时，必须上锁。因为此时有多个协程并发地取任务链表执行。panic处理也挺有意思，这里是将panic和任务函数调用放到一个匿名函数调用里，这样就可以使得panic地捕获一定是任务函数地panic，而不是捕获run方法里的panic func (w *worker) run() { go func() { for { var t *task // 必须上锁，因为可能有多个 worker（也就是多个协程） 同时再对这个pool里的task链表做取出操作，并执行这个task w.pool.taskLock.Lock() // 不断拿出 task if w.pool.taskHead != nil { t = w.pool.taskHead w.pool.taskHead = w.pool.taskHead.next // 原子的减少任务数 atomic.AddInt32(\u0026w.pool.taskCount, -1) } // 任务取完了 if t == nil { // if there's no task to do, exit w.close() w.pool.taskLock.Unlock() // 回收 w.Recycle() return } // 释放时机到了 w.pool.taskLock.Unlock() // 匿名函数调用，退一个栈从而形成独立环境，不让当前的defer去捕获外面函数里的异常，只是捕获这个执行函数的panic // 确保这个defer一定是捕获的task的panic func() { // 处理panic defer func() { if r := recover(); r != nil { if w.pool.panicHandler != nil { w.pool.panicHandler(t.ctx, r) } else { msg := fmt.Sprintf(\"GOPOOL: panic in pool: %s: %v: %s\", w.pool.name, r, debug.Stack()) logger.CtxErrorf(t.ctx, msg) } } }() // 真正地执行函数 t.f() }() // 回收task t.Recycle() } }() } 其他方法 // 协程数减一 func (w *worker) close() { w.pool.decWorkerCount() } func (w *worker) zero() { w.pool = nil } // Recycle 回收worker func (w *worker) Recycle() { w.zero() // 放回池里 workerPool.Put(w) } task type task struct { ctx context.Context f func() next *task } func (t *task) zero() { t.ctx = nil t.f = nil t.next = nil } // Recycle 回收到 sync.Pool里 func (t *task) Recycle() { // task 置空 t.zero() // 放到池里 taskPool.Put(t) } ","date":"2022-11-09","objectID":"/%E5%B9%B6%E5%8F%91%E5%AE%89%E5%85%A8%E4%B9%8B%E5%8E%9F%E5%AD%90%E6%93%8D%E4%BD%9C/:2:5","tags":["go编程技巧"],"title":"并发安全之原子操作","uri":"/%E5%B9%B6%E5%8F%91%E5%AE%89%E5%85%A8%E4%B9%8B%E5%8E%9F%E5%AD%90%E6%93%8D%E4%BD%9C/"},{"categories":["mysql"],"content":"[toc] 第17章_其他数据库日志 千万不要小看日志。很多看似奇怪的问题，答案往往就藏在日志里。很多情况下，只有通过查看日志才能发现问题的原因，真正解决问题。所以，一定要学会查看日志，养成检查日志的习惯，对提升你的数据库应用开发能力至关重要。 MySQL8.0 官网日志地址：“ https://dev.mysql.com/doc/refman/8.0/en/server-logs.html ” ","date":"2022-10-22","objectID":"/mysql%E6%97%A5%E5%BF%97%E4%B8%8E%E5%A4%87%E4%BB%BD%E7%AF%87/:0:0","tags":[],"title":"MySQL日志与备份篇","uri":"/mysql%E6%97%A5%E5%BF%97%E4%B8%8E%E5%A4%87%E4%BB%BD%E7%AF%87/"},{"categories":["mysql"],"content":"1. MySQL支持的日志 ","date":"2022-10-22","objectID":"/mysql%E6%97%A5%E5%BF%97%E4%B8%8E%E5%A4%87%E4%BB%BD%E7%AF%87/:1:0","tags":[],"title":"MySQL日志与备份篇","uri":"/mysql%E6%97%A5%E5%BF%97%E4%B8%8E%E5%A4%87%E4%BB%BD%E7%AF%87/"},{"categories":["mysql"],"content":"1.1 日志类型 MySQL有不同类型的日志文件，用来存储不同类型的日志，分为 二进制日志 、 错误日志 、 通用查询日志 和 慢查询日志 ，这也是常用的4种。MySQL8又新增两种支持的日志：中继日志和数据定义语句日志。使用这些日志文件，可以查看MySQL内部发生的事情。 这6类日志分别为： 慢查询日志：记录所有执行时间超过long_query_time的所有查询，方便我们对查询进行优化。 通用查询日志：记录所有连接的起始时间和终止时间，以及连接发送给数据库服务器的所有指令， 对我们复原操作的实际场景、发现问题，甚至是对数据库操作的审计都有很大的帮助。 错误日志：记录MySQL服务的启动、运行或停止MySQL服务时出现的问题，方便我们了解服务器的状态，从而对服务器进行维护。 二进制日志：记录所有更改数据的语句，可以用于主从服务器之间的数据同步，以及服务器遇到故障时数据的无损失恢复。 中继日志：用于主从服务器架构中，从服务器用来存放主服务器二进制日志内容的一个中间文件。从服务器通过读取中继日志的内容，来同步主服务器上的操作。 数据定义语句日志：记录数据定义语句执行的元数据操作。 除二进制日志外，其他日志都是 文本文件 。默认情况下，所有日志创建于 MySQL数据目录 中。 ","date":"2022-10-22","objectID":"/mysql%E6%97%A5%E5%BF%97%E4%B8%8E%E5%A4%87%E4%BB%BD%E7%AF%87/:1:1","tags":[],"title":"MySQL日志与备份篇","uri":"/mysql%E6%97%A5%E5%BF%97%E4%B8%8E%E5%A4%87%E4%BB%BD%E7%AF%87/"},{"categories":["mysql"],"content":"1.2 日志的弊端 日志功能会 降低MySQL数据库的性能 。例如，在查询非常频繁的MySQL数据库系统中，如果开启了通用查询日志和慢查询日志，MySQL数据库会花费很多时间记录日志。 日志会 占用大量的磁盘空间 。对于用户量非常大，操作非常频繁的数据库，日志文件需要的存储空间设置比数据库文件需要的存储空间还要大。 ","date":"2022-10-22","objectID":"/mysql%E6%97%A5%E5%BF%97%E4%B8%8E%E5%A4%87%E4%BB%BD%E7%AF%87/:1:2","tags":[],"title":"MySQL日志与备份篇","uri":"/mysql%E6%97%A5%E5%BF%97%E4%B8%8E%E5%A4%87%E4%BB%BD%E7%AF%87/"},{"categories":["mysql"],"content":"2. 慢查询日志(slow query log) 前面章节《第09章_性能分析工具的使用》已经详细讲述。 ","date":"2022-10-22","objectID":"/mysql%E6%97%A5%E5%BF%97%E4%B8%8E%E5%A4%87%E4%BB%BD%E7%AF%87/:2:0","tags":[],"title":"MySQL日志与备份篇","uri":"/mysql%E6%97%A5%E5%BF%97%E4%B8%8E%E5%A4%87%E4%BB%BD%E7%AF%87/"},{"categories":["mysql"],"content":"3. 通用查询日志(general query log) 通用查询日志用来 记录用户的所有操作 ，包括启动和关闭MySQL服务、所有用户的连接开始时间和截止 时间、发给 MySQL 数据库服务器的所有 SQL 指令等。当我们的数据发生异常时，查看通用查询日志， 还原操作时的具体场景，可以帮助我们准确定位问题。 ","date":"2022-10-22","objectID":"/mysql%E6%97%A5%E5%BF%97%E4%B8%8E%E5%A4%87%E4%BB%BD%E7%AF%87/:3:0","tags":[],"title":"MySQL日志与备份篇","uri":"/mysql%E6%97%A5%E5%BF%97%E4%B8%8E%E5%A4%87%E4%BB%BD%E7%AF%87/"},{"categories":["mysql"],"content":"3.1 问题场景 ","date":"2022-10-22","objectID":"/mysql%E6%97%A5%E5%BF%97%E4%B8%8E%E5%A4%87%E4%BB%BD%E7%AF%87/:3:1","tags":[],"title":"MySQL日志与备份篇","uri":"/mysql%E6%97%A5%E5%BF%97%E4%B8%8E%E5%A4%87%E4%BB%BD%E7%AF%87/"},{"categories":["mysql"],"content":"3.2 查看当前状态 mysql\u003e SHOW VARIABLES LIKE '%general%'; +------------------+------------------------------+ | Variable_name | Value | +------------------+------------------------------+ | general_log | OFF | #通用查询日志处于关闭状态 | general_log_file | /var/lib/mysql/atguigu01.log | #通用查询日志文件的名称是atguigu01.log +------------------+------------------------------+ 2 rows in set (0.03 sec) ","date":"2022-10-22","objectID":"/mysql%E6%97%A5%E5%BF%97%E4%B8%8E%E5%A4%87%E4%BB%BD%E7%AF%87/:3:2","tags":[],"title":"MySQL日志与备份篇","uri":"/mysql%E6%97%A5%E5%BF%97%E4%B8%8E%E5%A4%87%E4%BB%BD%E7%AF%87/"},{"categories":["mysql"],"content":"3.3 启动日志 方式1：永久性方式 修改my.cnf或者my.ini配置文件来设置。在[mysqld]组下加入log选项，并重启MySQL服务。格式如下： [mysqld] general_log=ON general_log_file=[path[filename]] #日志文件所在目录路径，filename为日志文件 如果不指定目录和文件名，通用查询日志将默认存储在MySQL数据目录中的hostname.log文件中， hostname表示主机名。 方式2：临时性方式 SET GLOBAL general_log=on; # 开启通用查询日志 SET GLOBAL general_log_file=’path/filename’; # 设置日志文件保存位置 对应的，关闭操作SQL命令如下： SET GLOBAL general_log=off; # 关闭通用查询日志 查看设置后情况： SHOW VARIABLES LIKE 'general_log%'; ","date":"2022-10-22","objectID":"/mysql%E6%97%A5%E5%BF%97%E4%B8%8E%E5%A4%87%E4%BB%BD%E7%AF%87/:3:3","tags":[],"title":"MySQL日志与备份篇","uri":"/mysql%E6%97%A5%E5%BF%97%E4%B8%8E%E5%A4%87%E4%BB%BD%E7%AF%87/"},{"categories":["mysql"],"content":"3.4 查看日志 通用查询日志是以 文本文件 的形式存储在文件系统中的，可以使用 文本编辑器 直接打开日志文件。每台 MySQL服务器的通用查询日志内容是不同的。 在Windows操作系统中，使用文本文件查看器； 在Linux系统中，可以使用vi工具或者gedit工具查看； 在Mac OSX系统中，可以使用文本文件查看器或者vi等工具查看。 从 SHOW VARIABLES LIKE 'general_log%'; 结果中可以看到通用查询日志的位置。 /usr/sbin/mysqld, Version: 8.0.26 (MySQL Community Server - GPL). started with: Tcp port: 3306 Unix socket: /var/lib/mysql/mysql.sock Time Id Command Argument 2022-01-04T07:44:58.052890Z 10 Query SHOW VARIABLES LIKE '%general%' 2022-01-04T07:45:15.666672Z 10 Query SHOW VARIABLES LIKE 'general_log%' 2022-01-04T07:45:28.970765Z 10 Query select * from student 2022-01-04T07:47:38.706804Z 11 Connect root@localhost on using Socket 2022-01-04T07:47:38.707435Z 11 Query select @@version_comment limit 1 2022-01-04T07:48:21.384886Z 12 Connect root@172.16.210.1 on using TCP/IP 2022-01-04T07:48:21.385253Z 12 Query SET NAMES utf8 2022-01-04T07:48:21.385640Z 12 Query USE `atguigu12` 2022-01-04T07:48:21.386179Z 12 Query SHOW FULL TABLES WHERE Table_Type != 'VIEW' 2022-01-04T07:48:23.901778Z 13 Connect root@172.16.210.1 on using TCP/IP 2022-01-04T07:48:23.902128Z 13 Query SET NAMES utf8 2022-01-04T07:48:23.905179Z 13 Query USE `atguigu` 2022-01-04T07:48:23.905825Z 13 Query SHOW FULL TABLES WHERE Table_Type != 'VIEW' 2022-01-04T07:48:32.163833Z 14 Connect root@172.16.210.1 on using TCP/IP 2022-01-04T07:48:32.164451Z 14 Query SET NAMES utf8 2022-01-04T07:48:32.164840Z 14 Query USE `atguigu` 2022-01-04T07:48:40.006687Z 14 Query select * from account 在通用查询日志里面，我们可以清楚地看到，什么时候开启了新的客户端登陆数据库，登录之后做了什么 SQL 操作，针对的是哪个数据表等信息。 ","date":"2022-10-22","objectID":"/mysql%E6%97%A5%E5%BF%97%E4%B8%8E%E5%A4%87%E4%BB%BD%E7%AF%87/:3:4","tags":[],"title":"MySQL日志与备份篇","uri":"/mysql%E6%97%A5%E5%BF%97%E4%B8%8E%E5%A4%87%E4%BB%BD%E7%AF%87/"},{"categories":["mysql"],"content":"3.5 停止日志 方式1：永久性方式 修改 my.cnf 或者 my.ini 文件，把[mysqld]组下的 general_log 值设置为 OFF 或者把general_log一项 注释掉。修改保存后，再重启MySQL服务 ，即可生效。 举例1： [mysqld] general_log=OFF 举例2： [mysqld] #general_log=ON 方式2：临时性方式 使用SET语句停止MySQL通用查询日志功能： SET GLOBAL general_log=off; 查询通用日志功能： SHOW VARIABLES LIKE 'general_log%'; ","date":"2022-10-22","objectID":"/mysql%E6%97%A5%E5%BF%97%E4%B8%8E%E5%A4%87%E4%BB%BD%E7%AF%87/:3:5","tags":[],"title":"MySQL日志与备份篇","uri":"/mysql%E6%97%A5%E5%BF%97%E4%B8%8E%E5%A4%87%E4%BB%BD%E7%AF%87/"},{"categories":["mysql"],"content":"3.6 删除\\刷新日志 如果数据的使用非常频繁，那么通用查询日志会占用服务器非常大的磁盘空间。数据管理员可以删除很长时间之前的查询日志，以保证MySQL服务器上的硬盘空间。 手动删除文件 SHOW VARIABLES LIKE 'general_log%'; 可以看出，通用查询日志的目录默认为MySQL数据目录。在该目录下手动删除通用查询日志 atguigu01.log 使用如下命令重新生成查询日志文件，具体命令如下。刷新MySQL数据目录，发现创建了新的日志文 件。前提一定要开启通用日志。 mysqladmin -uroot -p flush-logs 如果希望备份旧的通用查询日志，就必须先将旧的日志文件复制出来或者改名，然后执行上面的mysqladmin命令。正确流程如下： cd mysql-data-directory # 输入自己的通用日志文件所在目录 mv mysql.general.log mysql.general.log.old # 指定旧的文件名 以及 新的文件名 mysqladmin -uroot -p flush-logs ","date":"2022-10-22","objectID":"/mysql%E6%97%A5%E5%BF%97%E4%B8%8E%E5%A4%87%E4%BB%BD%E7%AF%87/:3:6","tags":[],"title":"MySQL日志与备份篇","uri":"/mysql%E6%97%A5%E5%BF%97%E4%B8%8E%E5%A4%87%E4%BB%BD%E7%AF%87/"},{"categories":["mysql"],"content":"4. 错误日志(error log) ","date":"2022-10-22","objectID":"/mysql%E6%97%A5%E5%BF%97%E4%B8%8E%E5%A4%87%E4%BB%BD%E7%AF%87/:4:0","tags":[],"title":"MySQL日志与备份篇","uri":"/mysql%E6%97%A5%E5%BF%97%E4%B8%8E%E5%A4%87%E4%BB%BD%E7%AF%87/"},{"categories":["mysql"],"content":"4.1 启动日志 在MySQL数据库中，错误日志功能是 默认开启 的。而且，错误日志 无法被禁止 。 默认情况下，错误日志存储在MySQL数据库的数据文件夹下，名称默认为 mysqld.log （Linux系统）或 hostname.err （mac系统）。如果需要制定文件名，则需要在my.cnf或者my.ini中做如下配置： [mysqld] log-error=[path/[filename]] #path为日志文件所在的目录路径，filename为日志文件名 修改配置项后，需要重启MySQL服务以生效。 ","date":"2022-10-22","objectID":"/mysql%E6%97%A5%E5%BF%97%E4%B8%8E%E5%A4%87%E4%BB%BD%E7%AF%87/:4:1","tags":[],"title":"MySQL日志与备份篇","uri":"/mysql%E6%97%A5%E5%BF%97%E4%B8%8E%E5%A4%87%E4%BB%BD%E7%AF%87/"},{"categories":["mysql"],"content":"4.2 查看日志 MySQL错误日志是以文本文件形式存储的，可以使用文本编辑器直接查看。 查询错误日志的存储路径： mysql\u003e SHOW VARIABLES LIKE 'log_err%'; +----------------------------+----------------------------------------+ | Variable_name | Value | +----------------------------+----------------------------------------+ | log_error | /var/log/mysqld.log | | log_error_services | log_filter_internal; log_sink_internal | | log_error_suppression_list | | | log_error_verbosity | 2 | +----------------------------+----------------------------------------+ 4 rows in set (0.01 sec) 执行结果中可以看到错误日志文件是mysqld.log，位于MySQL默认的数据目录下。 ","date":"2022-10-22","objectID":"/mysql%E6%97%A5%E5%BF%97%E4%B8%8E%E5%A4%87%E4%BB%BD%E7%AF%87/:4:2","tags":[],"title":"MySQL日志与备份篇","uri":"/mysql%E6%97%A5%E5%BF%97%E4%B8%8E%E5%A4%87%E4%BB%BD%E7%AF%87/"},{"categories":["mysql"],"content":"4.3 删除\\刷新日志 对于很久以前的错误日志，数据库管理员查看这些错误日志的可能性不大，可以将这些错误日志删除， 以保证MySQL服务器上的 硬盘空间 。MySQL的错误日志是以文本文件的形式存储在文件系统中的，可以 直接删除 。 第一步（方式1）：删除操作 rm -f /var/lib/mysql/mysqld.log 在运行状态下删除错误日志文件后，MySQL并不会自动创建日志文件。 第一步（方式2）：重命名文件 mv /var/log/mysqld.log /var/log/mysqld.log.old 第二步：重建日志 mysqladmin -uroot -p flush-logs 可能会报错 [root@atguigu01 log]# mysqladmin -uroot -p flush-logs Enter password: mysqladmin: refresh failed; error: 'Could not open file '/var/log/mysqld.log' for error logging.' 官网提示： 补充操作： install -omysql -gmysql -m0644 /dev/null /var/log/mysqld.log ","date":"2022-10-22","objectID":"/mysql%E6%97%A5%E5%BF%97%E4%B8%8E%E5%A4%87%E4%BB%BD%E7%AF%87/:4:3","tags":[],"title":"MySQL日志与备份篇","uri":"/mysql%E6%97%A5%E5%BF%97%E4%B8%8E%E5%A4%87%E4%BB%BD%E7%AF%87/"},{"categories":["mysql"],"content":"4.4 MySQL 8.0 新特性 小结： 通常情况下，管理员不需要查看错误日志。但是，MySQL服务器发生异常时，管理员可以从错误日志中找到发生异常的时间、原因，然后根据这些信息来解决异常。 ","date":"2022-10-22","objectID":"/mysql%E6%97%A5%E5%BF%97%E4%B8%8E%E5%A4%87%E4%BB%BD%E7%AF%87/:4:4","tags":[],"title":"MySQL日志与备份篇","uri":"/mysql%E6%97%A5%E5%BF%97%E4%B8%8E%E5%A4%87%E4%BB%BD%E7%AF%87/"},{"categories":["mysql"],"content":"5. 二进制日志(bin log) binlog可以说是MySQL中比较 重要 的日志了，在日常开发及运维过程中，经常会遇到。 binlog即binary log，二进制日志文件，也叫作变更日志（update log）。它记录了数据库所有执行的 DDL 和 DML 等数据库更新事件的语句，但是不包含没有修改任何数据的语句（如数据查询语句select、 show等）。 它以事件形式记录并保存在二进制文件中。通过这些信息，我们可以再现数据更新操作的全过程。 如果想要记录所有语句（例如，为了识别有问题的查询），需要使用通用查询日志。 binlog主要应用场景： ","date":"2022-10-22","objectID":"/mysql%E6%97%A5%E5%BF%97%E4%B8%8E%E5%A4%87%E4%BB%BD%E7%AF%87/:5:0","tags":[],"title":"MySQL日志与备份篇","uri":"/mysql%E6%97%A5%E5%BF%97%E4%B8%8E%E5%A4%87%E4%BB%BD%E7%AF%87/"},{"categories":["mysql"],"content":"5.1 查看默认情况 查看记录二进制日志是否开启：在MySQL8中默认情况下，二进制文件是开启的。 mysql\u003e show variables like '%log_bin%'; +---------------------------------+----------------------------------+ | Variable_name | Value | +---------------------------------+----------------------------------+ | log_bin | ON | | log_bin_basename | /var/lib/mysql/binlog | | log_bin_index | /var/lib/mysql/binlog.index | | log_bin_trust_function_creators | OFF | | log_bin_use_v1_row_events | OFF | | sql_log_bin | ON | +---------------------------------+----------------------------------+ 6 rows in set (0.00 sec) ","date":"2022-10-22","objectID":"/mysql%E6%97%A5%E5%BF%97%E4%B8%8E%E5%A4%87%E4%BB%BD%E7%AF%87/:5:1","tags":[],"title":"MySQL日志与备份篇","uri":"/mysql%E6%97%A5%E5%BF%97%E4%B8%8E%E5%A4%87%E4%BB%BD%E7%AF%87/"},{"categories":["mysql"],"content":"5.2 日志参数设置 方式1：永久性方式 修改MySQL的 my.cnf 或 my.ini 文件可以设置二进制日志的相关参数： [mysqld] #启用二进制日志 log-bin=atguigu-bin binlog_expire_logs_seconds=600 max_binlog_size=100M 重新启动MySQL服务，查询二进制日志的信息，执行结果： mysql\u003e show variables like '%log_bin%'; +---------------------------------+----------------------------------+ | Variable_name | Value | +---------------------------------+----------------------------------+ | log_bin | ON | | log_bin_basename | /var/lib/mysql/atguigu-bin | | log_bin_index | /var/lib/mysql/atguigu-bin.index | | log_bin_trust_function_creators | OFF | | log_bin_use_v1_row_events | OFF | | sql_log_bin | ON | +---------------------------------+----------------------------------+ 6 rows in set (0.00 sec) 设置带文件夹的bin-log日志存放目录 如果想改变日志文件的目录和名称，可以对my.cnf或my.ini中的log_bin参数修改如下： [mysqld] log-bin=\"/var/lib/mysql/binlog/atguigu-bin\" 注意：新建的文件夹需要使用mysql用户，使用下面的命令即可。 chown -R -v mysql:mysql binlog 方式2：临时性方式 如果不希望通过修改配置文件并重启的方式设置二进制日志的话，还可以使用如下指令，需要注意的是 在mysql8中只有 会话级别 的设置，没有了global级别的设置。 # global 级别 mysql\u003e set global sql_log_bin=0; ERROR 1228 (HY000): Variable 'sql_log_bin' is a SESSION variable and can`t be used with SET GLOBAL # session级别 mysql\u003e SET sql_log_bin=0; Query OK, 0 rows affected (0.01 秒) ","date":"2022-10-22","objectID":"/mysql%E6%97%A5%E5%BF%97%E4%B8%8E%E5%A4%87%E4%BB%BD%E7%AF%87/:5:2","tags":[],"title":"MySQL日志与备份篇","uri":"/mysql%E6%97%A5%E5%BF%97%E4%B8%8E%E5%A4%87%E4%BB%BD%E7%AF%87/"},{"categories":["mysql"],"content":"5.3 查看日志 当MySQL创建二进制日志文件时，先创建一个以“filename”为名称、以“.index”为后缀的文件，再创建一个以“filename”为名称、以“.000001”为后缀的文件。 MySQL服务 重新启动一次 ，以“.000001”为后缀的文件就会增加一个，并且后缀名按1递增。即日志文件的个数与MySQL服务启动的次数相同；如果日志长度超过了 max_binlog_size 的上限（默认是1GB），就会创建一个新的日志文件。 查看当前的二进制日志文件列表及大小。指令如下： mysql\u003e SHOW BINARY LOGS; +--------------------+-----------+-----------+ | Log_name | File_size | Encrypted | +--------------------+-----------+-----------+ | atguigu-bin.000001 | 156 | No | +--------------------+-----------+-----------+ 1 行于数据集 (0.02 秒) 所有对数据库的修改都会记录在binlog中。但binlog是二进制文件，无法直接查看，想要更直观的观测它就要借助mysqlbinlog命令工具了。指令如下：在查看执行，先执行一条SQL语句，如下 update student set name='张三_back' where id=1; 开始查看binlog mysqlbinlog -v \"/var/lib/mysql/binlog/atguigu-bin.000002\" #220105 9:16:37 server id 1 end_log_pos 324 CRC32 0x6b31978b Query thread_id=10 exec_time=0 error_code=0 SET TIMESTAMP=1641345397/*!*/; SET @@session.pseudo_thread_id=10/*!*/; SET @@session.foreign_key_checks=1, @@session.sql_auto_is_null=0, @@session.unique_checks=1, @@session.autocommit=1/*!*/; SET @@session.sql_mode=1168113696/*!*/; SET @@session.auto_increment_increment=1, @@session.auto_increment_offset=1/*!*/; /*!\\C utf8mb3 *//*!*/; SET @@session.character_set_client=33,@@session.collation_connection=33,@@session.collatio n_server=255/*!*/; SET @@session.lc_time_names=0/*!*/; SET @@session.collation_database=DEFAULT/*!*/; /*!80011 SET @@session.default_collation_for_utf8mb4=255*//*!*/; BEGIN /*!*/; # at 324 #220105 9:16:37 server id 1 end_log_pos 391 CRC32 0x74f89890 Table_map: `atguigu14`.`student` mapped to number 85 # at 391 #220105 9:16:37 server id 1 end_log_pos 470 CRC32 0xc9920491 Update_rows: table id 85 flags: STMT_END_F BINLOG ' dfHUYRMBAAAAQwAAAIcBAAAAAFUAAAAAAAEACWF0Z3VpZ3UxNAAHc3R1ZGVudAADAw8PBDwAHgAG AQEAAgEhkJj4dA== dfHUYR8BAAAATwAAANYBAAAAAFUAAAAAAAEAAgAD//8AAQAAAAblvKDkuIkG5LiA54+tAAEAAAAL 5byg5LiJX2JhY2sG5LiA54+tkQSSyQ== '/*!*/; ### UPDATE `atguigu`.`student` ### WHERE ### @1=1 ### @2='张三' ### @3='一班' ### SET ### @1=1 ### @2='张三_back' ### @3='一班' # at 470 #220105 9:16:37 server id 1 end_log_pos 501 CRC32 0xca01d30f Xid = 15 COMMIT/*!*/; 前面的命令同时显示binlog格式的语句，使用如下命令不显示它 mysqlbinlog -v --base64-output=DECODE-ROWS \"/var/lib/mysql/binlog/atguigu-bin.000002\" #220105 9:16:37 server id 1 end_log_pos 324 CRC32 0x6b31978b Query thread_id=10 exec_time=0 error_code=0 SET TIMESTAMP=1641345397/*!*/; SET @@session.pseudo_thread_id=10/*!*/; SET @@session.foreign_key_checks=1, @@session.sql_auto_is_null=0, @@session.unique_checks=1, @@session.autocommit=1/*!*/; SET @@session.sql_mode=1168113696/*!*/; SET @@session.auto_increment_increment=1, @@session.auto_increment_offset=1/*!*/; /*!\\C utf8mb3 *//*!*/; SET @@session.character_set_client=33,@@session.collation_connection=33,@@session.collatio n_server=255/*!*/; SET @@session.lc_time_names=0/*!*/; SET @@session.collation_database=DEFAULT/*!*/; /*!80011 SET @@session.default_collation_for_utf8mb4=255*//*!*/; BEGIN /*!*/; # at 324 #220105 9:16:37 server id 1 end_log_pos 391 CRC32 0x74f89890 Table_map: `atguigu14`.`student` mapped to number 85 # at 391 #220105 9:16:37 server id 1 end_log_pos 470 CRC32 0xc9920491 Update_rows: table id 85 flags: STMT_END_F ### UPDATE `atguigu14`.`student` ### WHERE ### @1=1 ### @2='张三' ### @3='一班' ### SET ### @1=1 ### @2='张三_back' ### @3='一班' # at 470 #220105 9:16:37 server id 1 end_log_pos 501 CRC32 0xca01d30f Xid = 15 关于mysqlbinlog工具的使用技巧还有很多，例如只解析对某个库的操作或者某个时间段内的操作等。简单分享几个常用的语句，更多操作可以参考官方文档。 # 可查看参数帮助 mysqlbinlog --no-defaults --help # 查看最后100行 mysqlbinlog --no-defaults --base64-output=decode-rows -vv atguigu-bin.000002 |tail -100 # 根据position查找 mysqlbinlog --no-defaults --base64-output=decode-rows -vv atguigu-bin.000002 |grep -A 20 '4939002' 上面这种办法读取出binlog日志的全文内容比较多，不容易分辨查看到pos点信息，下面介绍一种更为方便的查询命令： mysql\u003e show binlog events [IN 'log_name'] [FROM pos] [LIMIT [offset,] row_count]; IN 'log_name' ：指定要查询的binlog文件名（不指定就是第一个binlog文件）　FROM pos ：指定从哪个pos起始点开始查起（不指定就是从整个文件首个pos点开始算） LIMIT [offset] ：偏移量(不指定就是0) row_count :查询总条数（不指定就是所有行） mysql\u003e sh","date":"2022-10-22","objectID":"/mysql%E6%97%A5%E5%BF%97%E4%B8%8E%E5%A4%87%E4%BB%BD%E7%AF%87/:5:3","tags":[],"title":"MySQL日志与备份篇","uri":"/mysql%E6%97%A5%E5%BF%97%E4%B8%8E%E5%A4%87%E4%BB%BD%E7%AF%87/"},{"categories":["mysql"],"content":"5.4 使用日志恢复数据 如果MySQL服务器启用了二进制日志，在数据库出现意外丢失数据时，可以使用MySQLbinlog工具从指定的时间点开始（例如，最后一次备份）直到现在或另一个指定的时间点的日志中回复数据。 mysqlbinlog恢复数据的语法如下： mysqlbinlog [option] filename|mysql –uuser -ppass; 这个命令可以这样理解：使用mysqlbinlog命令来读取filename中的内容，然后使用mysql命令将这些内容恢复到数据库中。 filename ：是日志文件名。 option ：可选项，比较重要的两对option参数是–start-date、–stop-date 和 –start-position、– stop-position。 --start-date 和 --stop-date ：可以指定恢复数据库的起始时间点和结束时间点。 --start-position和--stop-position ：可以指定恢复数据的开始位置和结束位置。 注意：使用mysqlbinlog命令进行恢复操作时，必须是编号小的先恢复，例如atguigu-bin.000001必须在atguigu-bin.000002之前恢复。 详见p189，由于翻页过快，这部分没办法记录。 ","date":"2022-10-22","objectID":"/mysql%E6%97%A5%E5%BF%97%E4%B8%8E%E5%A4%87%E4%BB%BD%E7%AF%87/:5:4","tags":[],"title":"MySQL日志与备份篇","uri":"/mysql%E6%97%A5%E5%BF%97%E4%B8%8E%E5%A4%87%E4%BB%BD%E7%AF%87/"},{"categories":["mysql"],"content":"5.5 删除二进制日志 MySQL的二进制文件可以配置自动删除，同时MySQL也提供了安全的手动删除二进制文件的方法。 PURGE MASTER LOGS 只删除指定部分的二进制日志文件， RESET MASTER 删除所有的二进制日志文 件。具体如下： 1. PURGE MASTER LOGS：删除指定日志文件 PURGE MASTER LOGS语法如下： PURGE {MASTER | BINARY} LOGS TO ‘指定日志文件名’ PURGE {MASTER | BINARY} LOGS BEFORE ‘指定日期’ 2. RESET MASTER: 删除所有二进制日志文件 ","date":"2022-10-22","objectID":"/mysql%E6%97%A5%E5%BF%97%E4%B8%8E%E5%A4%87%E4%BB%BD%E7%AF%87/:5:5","tags":[],"title":"MySQL日志与备份篇","uri":"/mysql%E6%97%A5%E5%BF%97%E4%B8%8E%E5%A4%87%E4%BB%BD%E7%AF%87/"},{"categories":["mysql"],"content":"5.6 其它场景 二进制日志可以通过数据库的 全量备份 和二进制日志中保存的 增量信息 ，完成数据库的 无损失恢复 。 但是，如果遇到数据量大、数据库和数据表很多（比如分库分表的应用）的场景，用二进制日志进行数据恢复，是很有挑战性的，因为起止位置不容易管理。 在这种情况下，一个有效的解决办法是配置主从数据库服务器，甚至是一主多从的架构，把二进制日志文件的内容通过中继日志，同步到从数据库服务器中，这样就可以有效避免数据库故障导致的数据异常等问题。 ","date":"2022-10-22","objectID":"/mysql%E6%97%A5%E5%BF%97%E4%B8%8E%E5%A4%87%E4%BB%BD%E7%AF%87/:5:6","tags":[],"title":"MySQL日志与备份篇","uri":"/mysql%E6%97%A5%E5%BF%97%E4%B8%8E%E5%A4%87%E4%BB%BD%E7%AF%87/"},{"categories":["mysql"],"content":"6. 再谈二进制日志(binlog) ","date":"2022-10-22","objectID":"/mysql%E6%97%A5%E5%BF%97%E4%B8%8E%E5%A4%87%E4%BB%BD%E7%AF%87/:6:0","tags":[],"title":"MySQL日志与备份篇","uri":"/mysql%E6%97%A5%E5%BF%97%E4%B8%8E%E5%A4%87%E4%BB%BD%E7%AF%87/"},{"categories":["mysql"],"content":"6.1 写入机制 binlog的写入时机也非常简单，事务执行过程中，先把日志写到 binlog cache ，事务提交的时候，再把binlog cache写到binlog文件中。因为一个事务的binlog不能被拆开，无论这个事务多大，也要确保一次性写入，所以系统会给每个线程分配一个块内存作为binlog cache。(保证了事务的原子性) 我们可以通过binlog_cache_size参数控制单个线程 binlog cache 大小，如果存储内容超过了这个参数，就要暂存到磁盘（Swap）。binlog日志刷盘流程如下： 上图的write，是指把日志写入到文件系统的page cache，并没有把数据持久化到磁盘，所以速度比较快 上图的fsync，才是将数据持久化到磁盘的操作 write和fsync的时机，可以由参数 sync_binlog 控制，默认是 0 。 为0的时候，表示每次提交事务都只 write，由系统自行判断什么时候执行fsync。虽然性能得到提升，但是机器宕机，page cache里面的 binglog 会丢失。如下图： 为了安全起见，可以设置为 1 ，表示每次提交事务都会执行fsync，就如同redo log 刷盘流程一样。 最后还有一种折中方式，可以设置为N(N\u003e1)，表示每次提交事务都write，但累积N个事务后才fsync。 在出现IO瓶颈的场景里，将sync_binlog设置成一个比较大的值，可以提升性能。同样的，如果机器宕机，会丢失最近N个事务的binlog日志。 ","date":"2022-10-22","objectID":"/mysql%E6%97%A5%E5%BF%97%E4%B8%8E%E5%A4%87%E4%BB%BD%E7%AF%87/:6:1","tags":[],"title":"MySQL日志与备份篇","uri":"/mysql%E6%97%A5%E5%BF%97%E4%B8%8E%E5%A4%87%E4%BB%BD%E7%AF%87/"},{"categories":["mysql"],"content":"6.2 binlog与redo log对比 redo log 它是 物理日志 ，记录内容是“在某个数据页上做了什么修改”，属于 InnoDB 存储引擎层产生的。 而 binlog 是 逻辑日志 ，记录内容是语句的原始逻辑，类似于“给 ID=2 这一行的 c 字段加 1”，属于 MySQL Server 层。 虽然它们都属于持久化的保证，但是侧重点不同。 redo log让InnoDB存储引擎拥有了崩溃恢复能力。 binlog保证了MySQL集群架构的数据一致性。 ","date":"2022-10-22","objectID":"/mysql%E6%97%A5%E5%BF%97%E4%B8%8E%E5%A4%87%E4%BB%BD%E7%AF%87/:6:2","tags":[],"title":"MySQL日志与备份篇","uri":"/mysql%E6%97%A5%E5%BF%97%E4%B8%8E%E5%A4%87%E4%BB%BD%E7%AF%87/"},{"categories":["mysql"],"content":"6.3 两阶段提交 在执行更新语句过程，会记录redo log与binlog两块日志，以基本的事务为单位，redo log在事务执行过程中可以不断写入，而binlog只有在提交事务时才会从binlog cache写入，所以redo log与binlog的 写入时机 不一样。 redo log与binlog两份日志之间的逻辑不一致，会出现什么问题？ 以update语句为例，假设id=2的记录，字段c值是0，把字段c值更新为1，SQL语句为update T set c = 1 where id = 2。 假设执行过程中写完redo log日志后，binlog日志写期间发生了异常，会出现什么情况呢？ 由于binlog没写完就异常，这时候binlog里面没有对应的修改记录。因此，之后用binlog日志恢复数据或主从数据同步时，就会少这一次更新，恢复出来的这一行c值为0，而原库因为redo log日志恢复，这一行c的值是1，最终数据不一致。 为了解决两份日志之间的逻辑一致问题，InnoDB存储引擎使用两阶段提交方案。原理很简单，将redo log的写入拆成了两个步骤prepare和commit，这就是两阶段提交。 使用两阶段提交后，写入binlog时发生异常也不会有影响，因为MySQL根据redo log日志恢复数据时，发现redo log还处于prepare阶段，并且没有对应binlog日志，就会回滚该事务。 另一个场景，redo log设置commit阶段发生异常，那会不会回滚事务呢？ 并不会回滚事务，它会执行上图框住的逻辑，虽然redo log是处于prepare阶段，但是能通过事务id找到对应的binlog日志，所以MySQL认为是完整的，就会提交事务恢复数据。 ","date":"2022-10-22","objectID":"/mysql%E6%97%A5%E5%BF%97%E4%B8%8E%E5%A4%87%E4%BB%BD%E7%AF%87/:6:3","tags":[],"title":"MySQL日志与备份篇","uri":"/mysql%E6%97%A5%E5%BF%97%E4%B8%8E%E5%A4%87%E4%BB%BD%E7%AF%87/"},{"categories":["mysql"],"content":"7. 中继日志(relay log) ","date":"2022-10-22","objectID":"/mysql%E6%97%A5%E5%BF%97%E4%B8%8E%E5%A4%87%E4%BB%BD%E7%AF%87/:7:0","tags":[],"title":"MySQL日志与备份篇","uri":"/mysql%E6%97%A5%E5%BF%97%E4%B8%8E%E5%A4%87%E4%BB%BD%E7%AF%87/"},{"categories":["mysql"],"content":"7.1 介绍 中继日志只在主从服务器架构的从服务器上存在。从服务器为了与主服务器保持一致，要从主服务器读取二进制日志的内容，并且把读取到的信息写入 本地的日志文件 中，这个从服务器本地的日志文件就叫 中继日志 。然后，从服务器读取中继日志，并根据中继日志的内容对从服务器的数据进行更新，完成主从服务器的数据同步 。 搭建好主从服务器之后，中继日志默认会保存在从服务器的数据目录下。 文件名的格式是： 从服务器名 -relay-bin.序号 。中继日志还有一个索引文件：从服务器名 -relaybin.index ，用来定位当前正在使用的中继日志。 ","date":"2022-10-22","objectID":"/mysql%E6%97%A5%E5%BF%97%E4%B8%8E%E5%A4%87%E4%BB%BD%E7%AF%87/:7:1","tags":[],"title":"MySQL日志与备份篇","uri":"/mysql%E6%97%A5%E5%BF%97%E4%B8%8E%E5%A4%87%E4%BB%BD%E7%AF%87/"},{"categories":["mysql"],"content":"7.2 查看中继日志 中继日志与二进制日志的格式相同，可以用 mysqlbinlog 工具进行查看。下面是中继日志的一个片段： SET TIMESTAMP=1618558728/*!*/; BEGIN /*!*/; # at 950 #210416 15:38:48 server id 1 end_log_pos 832 CRC32 0xcc16d651 Table_map: `atguigu`.`test` mapped to number 91 # at 1000 #210416 15:38:48 server id 1 end_log_pos 872 CRC32 0x07e4047c Delete_rows: table id 91 flags: STMT_END_F -- server id 1 是主服务器，意思是主服务器删了一行数据 BINLOG ' CD95YBMBAAAAMgAAAEADAAAAAFsAAAAAAAEABGRlbW8ABHRlc3QAAQMAAQEBAFHWFsw= CD95YCABAAAAKAAAAGgDAAAAAFsAAAAAAAEAAgAB/wABAAAAfATkBw== '/*!*/; # at 1040 这一段的意思是，主服务器（“server id 1”）对表 atguigu.test 进行了 2 步操作： 定位到表 atguigu.test 编号是 91 的记录，日志位置是 832； 删除编号是 91 的记录，日志位置是 872 ","date":"2022-10-22","objectID":"/mysql%E6%97%A5%E5%BF%97%E4%B8%8E%E5%A4%87%E4%BB%BD%E7%AF%87/:7:2","tags":[],"title":"MySQL日志与备份篇","uri":"/mysql%E6%97%A5%E5%BF%97%E4%B8%8E%E5%A4%87%E4%BB%BD%E7%AF%87/"},{"categories":["mysql"],"content":"7.3 恢复的典型错误 如果从服务器宕机，有的时候为了系统恢复，要重装操作系统，这样就可能会导致你的 服务器名称 与之前 不同 。而中继日志里是 包含从服务器名 的。在这种情况下，就可能导致你恢复从服务器的时候，无法从宕机前的中继日志里读取数据，以为是日志文件损坏了，其实是名称不对了。 解决的方法也很简单，把从服务器的名称改回之前的名称。 第18章_主从复制 ","date":"2022-10-22","objectID":"/mysql%E6%97%A5%E5%BF%97%E4%B8%8E%E5%A4%87%E4%BB%BD%E7%AF%87/:7:3","tags":[],"title":"MySQL日志与备份篇","uri":"/mysql%E6%97%A5%E5%BF%97%E4%B8%8E%E5%A4%87%E4%BB%BD%E7%AF%87/"},{"categories":["mysql"],"content":"1. 主从复制概述 ","date":"2022-10-22","objectID":"/mysql%E6%97%A5%E5%BF%97%E4%B8%8E%E5%A4%87%E4%BB%BD%E7%AF%87/:8:0","tags":[],"title":"MySQL日志与备份篇","uri":"/mysql%E6%97%A5%E5%BF%97%E4%B8%8E%E5%A4%87%E4%BB%BD%E7%AF%87/"},{"categories":["mysql"],"content":"1.1 如何提升数据库并发能力 在实际工作中，我们常常将Redis作为缓存与MySQL配合来使用，当有请求的时候，首先会从缓存中进行查找，如果存在就直接取出。如果不存在再访问数据库，这样就提升了读取的效率，也减少了对后端数据库的访问压力。Redis的缓存架构是高并发架构中非常重要的一环。 此外，一般应用对数据库而言都是“读多写少”，也就说对数据库读取数据的压力比较大，有一个思路就是采用数据库集群的方案，做 主从架构 、进行 读写分离 ，这样同样可以提升数据库的并发处理能力。但并不是所有的应用都需要对数据库进行主从架构的设置，毕竟设置架构本身是有成本的。 如果我们的目的在于提升数据库高并发访问的效率，那么首先考虑的是如何 优化SQL和索引 ，这种方式简单有效；其次才是采用 缓存的策略 ，比如使用 Redis将热点数据保存在内存数据库中，提升读取的效率；最后才是对数据库采用 主从架构 ，进行读写分离。 按照上面的方式进行优化，使用和维护的成本是由低到高的。 ","date":"2022-10-22","objectID":"/mysql%E6%97%A5%E5%BF%97%E4%B8%8E%E5%A4%87%E4%BB%BD%E7%AF%87/:8:1","tags":[],"title":"MySQL日志与备份篇","uri":"/mysql%E6%97%A5%E5%BF%97%E4%B8%8E%E5%A4%87%E4%BB%BD%E7%AF%87/"},{"categories":["mysql"],"content":"1.2 主从复制的作用 主从同步设计不仅可以提高数据库的吞吐量，还有以下 3 个方面的作用。 **第1个作用：读写分离。**我们可以通过主从复制的方式来同步数据，然后通过读写分离提高数据库并发处理能力。 其中一个是Master主库，负责写入数据，我们称之为：写库。 其他都是Slave从库，负责读取数据，我们称之为：读库。 当主库进行更新的时候，会自动将数据复制到从库中，而我们在客户端读取数据的时候，会从从库进行读取。 面对“读多写少”的需求，采用读写分离的方式，可以实现更高的并发访问。同时，我们还能对从服务器进行负载均衡，让不同的读请求按照策略均匀地分发到不同的从服务器上，让读取更加顺畅。读取顺畅的另一个原因，就是减少了锁表的影响，比如我们让主库负责写，当主库出现写锁的时候，不会影响到从库进行SELECT的读取。 **第2个作用就是数据备份。**我们通过主从复制将主库上的数据复制到从库上，相当于一种热备份机制，也就是在主库正常运行的情况下进行的备份，不会影响到服务。 **第3个作用是具有高可用性。**数据备份实际上是一种冗余的机制，通过这种冗余的方式可以换取数据库的高可用性，也就是当服务器出现故障或宕机的情况下，可以切换到从服务器上，保证服务的正常运行。 ","date":"2022-10-22","objectID":"/mysql%E6%97%A5%E5%BF%97%E4%B8%8E%E5%A4%87%E4%BB%BD%E7%AF%87/:8:2","tags":[],"title":"MySQL日志与备份篇","uri":"/mysql%E6%97%A5%E5%BF%97%E4%B8%8E%E5%A4%87%E4%BB%BD%E7%AF%87/"},{"categories":["mysql"],"content":"2. 主从复制的原理 Slave 会从 Master 读取 binlog 来进行数据同步。 ","date":"2022-10-22","objectID":"/mysql%E6%97%A5%E5%BF%97%E4%B8%8E%E5%A4%87%E4%BB%BD%E7%AF%87/:9:0","tags":[],"title":"MySQL日志与备份篇","uri":"/mysql%E6%97%A5%E5%BF%97%E4%B8%8E%E5%A4%87%E4%BB%BD%E7%AF%87/"},{"categories":["mysql"],"content":"2.1 原理剖析 三个线程 实际上主从同步的原理就是基于 binlog 进行数据同步的。在主从复制过程中，会基于 3 个线程 来操 作，一个主库线程，两个从库线程。 二进制日志转储线程 （Binlog dump thread）是一个主库线程。当从库线程连接的时候， 主库可以将二进制日志发送给从库，当主库读取事件（Event）的时候，会在Binlog上加锁 ，读取完成之后，再将锁释放掉。 从库 I/O 线程 会连接到主库，向主库发送请求更新 Binlog。这时从库的 I/O 线程就可以读取到主库的二进制日志转储线程发送的 Binlog 更新部分，并且拷贝到本地的中继日志 （Relay log）。 从库 SQL 线程 会读取从库中的中继日志，并且执行日志中的事件，将从库中的数据与主库保持同步。 注意： 不是所有版本的MySQL都默认开启服务器的二进制日志。在进行主从同步的时候，我们需要先检查服务器是否已经开启了二进制日志。 除非特殊指定，默认情况下从服务器会执行所有主服务器中保存的事件。也可以通过配置，使从服务器执行特定的事件。 复制三步骤 步骤1： Master 将写操作记录到二进制日志（ binlog ）。 步骤2： Slave 将 Master 的binary log events拷贝到它的中继日志（ relay log ）； 步骤3： Slave 重做中继日志中的事件，将改变应用到自己的数据库中。 MySQL复制是异步的且串行化的，而且重启后从 接入点 开始复制。 复制的问题 复制的最大问题： 延时 ","date":"2022-10-22","objectID":"/mysql%E6%97%A5%E5%BF%97%E4%B8%8E%E5%A4%87%E4%BB%BD%E7%AF%87/:9:1","tags":[],"title":"MySQL日志与备份篇","uri":"/mysql%E6%97%A5%E5%BF%97%E4%B8%8E%E5%A4%87%E4%BB%BD%E7%AF%87/"},{"categories":["mysql"],"content":"2.2 复制的基本原则 每个 Slave 只有一个 Master 每个 Slave 只能有一个唯一的服务器ID 每个 Master 可以有多个 Slave ","date":"2022-10-22","objectID":"/mysql%E6%97%A5%E5%BF%97%E4%B8%8E%E5%A4%87%E4%BB%BD%E7%AF%87/:9:2","tags":[],"title":"MySQL日志与备份篇","uri":"/mysql%E6%97%A5%E5%BF%97%E4%B8%8E%E5%A4%87%E4%BB%BD%E7%AF%87/"},{"categories":["mysql"],"content":"3. 一主一从架构搭建 一台 主机 用于处理所有 写请求 ，一台 从机 负责所有 读请求 ，架构图如下: ","date":"2022-10-22","objectID":"/mysql%E6%97%A5%E5%BF%97%E4%B8%8E%E5%A4%87%E4%BB%BD%E7%AF%87/:10:0","tags":[],"title":"MySQL日志与备份篇","uri":"/mysql%E6%97%A5%E5%BF%97%E4%B8%8E%E5%A4%87%E4%BB%BD%E7%AF%87/"},{"categories":["mysql"],"content":"3.1 准备工作 1、准备 2台 CentOS 虚拟机 （具体设置内容在P192） 2、每台虚拟机上需要安装好MySQL (可以是MySQL8.0 ) 说明：前面我们讲过如何克隆一台CentOS。大家可以在一台CentOS上安装好MySQL，进而通过克隆的方式复制出1台包含MySQL的虚拟机。 注意：克隆的方式需要修改新克隆出来主机的：① MAC地址 ② hostname ③ IP 地址 ④ UUID 。 此外，克隆的方式生成的虚拟机（包含MySQL Server），则克隆的虚拟机MySQL Server的UUID相同，必须修改，否则在有些场景会报错。比如： show slave status\\G ，报如下的错误： Last_IO_Error: Fatal error: The slave I/O thread stops because master and slave have equal MySQL server UUIDs; these UUIDs must be different for replication to work. 修改MySQL Server 的UUID方式： vim /var/lib/mysql/auto.cnf systemctl restart mysqld ","date":"2022-10-22","objectID":"/mysql%E6%97%A5%E5%BF%97%E4%B8%8E%E5%A4%87%E4%BB%BD%E7%AF%87/:10:1","tags":[],"title":"MySQL日志与备份篇","uri":"/mysql%E6%97%A5%E5%BF%97%E4%B8%8E%E5%A4%87%E4%BB%BD%E7%AF%87/"},{"categories":["mysql"],"content":"3.2 主机配置文件 建议mysql版本一致且后台以服务运行，主从所有配置项都配置在 [mysqld] 节点下，且都是小写字母。 具体参数配置如下： 必选 #[必须]主服务器唯一ID server-id=1 #[必须]启用二进制日志,指名路径。比如：自己本地的路径/log/mysqlbin log-bin=atguigu-bin 可选 #[可选] 0（默认）表示读写（主机），1表示只读（从机） read-only=0 #设置日志文件保留的时长，单位是秒 binlog_expire_logs_seconds=6000 #控制单个二进制日志大小。此参数的最大和默认值是1GB max_binlog_size=200M #[可选]设置不要复制的数据库 binlog-ignore-db=test #[可选]设置需要复制的数据库,默认全部记录。比如：binlog-do-db=atguigu_master_slave binlog-do-db=需要复制的主数据库名字 #[可选]设置binlog格式 binlog_format=STATEMENT 重启后台mysql服务，使配置生效。 注意： 先搭建完主从复制，再创建数据库。 MySQL主从复制起始时，从机不继承主机数据。 ① binlog格式设置： 格式1： STATEMENT模式 （基于SQL语句的复制(statement-based replication, SBR)） binlog_format=STATEMENT 每一条会修改数据的sql语句会记录到binlog中。这是默认的binlog格式。 SBR 的优点： 历史悠久，技术成熟 不需要记录每一行的变化，减少了binlog日志量，文件较小 binlog中包含了所有数据库更改信息，可以据此来审核数据库的安全等情况 binlog可以用于实时的还原，而不仅仅用于复制 主从版本可以不一样，从服务器版本可以比主服务器版本高 SBR 的缺点： 不是所有的UPDATE语句都能被复制，尤其是包含不确定操作的时候 使用以下函数的语句也无法被复制：LOAD_FILE()、UUID()、USER()、FOUND_ROWS()、SYSDATE() (除非启动时启用了 –sysdate-is-now 选项) INSERT … SELECT 会产生比 RBR 更多的行级锁 复制需要进行全表扫描(WHERE 语句中没有使用到索引)的 UPDATE 时，需要比 RBR 请求更多的行级锁 对于有 AUTO_INCREMENT 字段的 InnoDB表而言，INSERT 语句会阻塞其他 INSERT 语句 对于一些复杂的语句，在从服务器上的耗资源情况会更严重，而 RBR 模式下，只会对那个发 生变化的记录产生影响 执行复杂语句如果出错的话，会消耗更多资源 数据表必须几乎和主服务器保持一致才行，否则可能会导致复制出错 ② ROW模式（基于行的复制(row-based replication, RBR)） binlog_format=ROW 5.1.5版本的MySQL才开始支持，不记录每条sql语句的上下文信息，仅记录哪条数据被修改了，修改成什么样了。 RBR 的优点： 任何情况都可以被复制，这对复制来说是最 安全可靠 的。（比如：不会出现某些特定情况下 的存储过程、function、trigger的调用和触发无法被正确复制的问题） 多数情况下，从服务器上的表如果有主键的话，复制就会快了很多 复制以下几种语句时的行锁更少：INSERT … SELECT、包含 AUTO_INCREMENT 字段的 INSERT、 没有附带条件或者并没有修改很多记录的 UPDATE 或 DELETE 语句 执行 INSERT，UPDATE，DELETE 语句时锁更少 从服务器上采用 多线程 来执行复制成为可能 RBR 的缺点： binlog 大了很多 复杂的回滚时 binlog 中会包含大量的数据 主服务器上执行 UPDATE 语句时，所有发生变化的记录都会写到 binlog 中，而 SBR 只会写一次，这会导致频繁发生 binlog 的并发写问题 无法从 binlog 中看到都复制了些什么语句 ③ MIXED模式（混合模式复制(mixed-based replication, MBR)） binlog_format=MIXED 从5.1.8版本开始，MySQL提供了Mixed格式，实际上就是Statement与Row的结合。 在Mixed模式下，一般的语句修改使用statment格式保存binlog。如一些函数，statement无法完成主从复制的操作，则采用row格式保存binlog。 MySQL会根据执行的每一条具体的sql语句来区分对待记录的日志形式，也就是在Statement和Row之间选择一种。 ","date":"2022-10-22","objectID":"/mysql%E6%97%A5%E5%BF%97%E4%B8%8E%E5%A4%87%E4%BB%BD%E7%AF%87/:10:2","tags":[],"title":"MySQL日志与备份篇","uri":"/mysql%E6%97%A5%E5%BF%97%E4%B8%8E%E5%A4%87%E4%BB%BD%E7%AF%87/"},{"categories":["mysql"],"content":"3.3 从机配置文件 要求主从所有配置项都配置在 my.cnf 的 [mysqld] 栏位下，且都是小写字母。 必选 #[必须]从服务器唯一ID server-id=2 可选 #[可选]启用中继日志 relay-log=mysql-relay 重启后台mysql服务，使配置生效。 注意：主从机都关闭防火墙 service iptables stop #CentOS 6 systemctl stop firewalld.service #CentOS 7 ","date":"2022-10-22","objectID":"/mysql%E6%97%A5%E5%BF%97%E4%B8%8E%E5%A4%87%E4%BB%BD%E7%AF%87/:10:3","tags":[],"title":"MySQL日志与备份篇","uri":"/mysql%E6%97%A5%E5%BF%97%E4%B8%8E%E5%A4%87%E4%BB%BD%E7%AF%87/"},{"categories":["mysql"],"content":"3.4 主机：建立账户并授权 #在主机MySQL里执行授权主从复制的命令 GRANT REPLICATION SLAVE ON *.* TO 'slave1'@'从机器数据库IP' IDENTIFIED BY 'abc123'; #5.5,5.7 注意：如果使用的是MySQL8，需要如下的方式建立账户，并授权slave: CREATE USER 'slave1'@'%' IDENTIFIED BY '123456'; GRANT REPLICATION SLAVE ON *.* TO 'slave1'@'%'; #此语句必须执行。否则见下面。 ALTER USER 'slave1'@'%' IDENTIFIED WITH mysql_native_password BY '123456'; flush privileges; 注意：在从机执行show slave status\\G时报错： Last_IO_Error: error connecting to master ‘slave1@192.168.1.150:3306’ - retry-time: 60 retries: 1 message: Authentication plugin ‘caching_sha2_password’ reported error: Authentication requires secure connection. 查询Master的状态，并记录下File和Position的值。 show master status; 记录下File和Position的值 注意：执行完此步骤后不要再操作主服务器MySQL，防止主服务器状态值变化。 ","date":"2022-10-22","objectID":"/mysql%E6%97%A5%E5%BF%97%E4%B8%8E%E5%A4%87%E4%BB%BD%E7%AF%87/:10:4","tags":[],"title":"MySQL日志与备份篇","uri":"/mysql%E6%97%A5%E5%BF%97%E4%B8%8E%E5%A4%87%E4%BB%BD%E7%AF%87/"},{"categories":["mysql"],"content":"3.5 从机：配置需要复制的主机 **步骤1：**从机上复制主机的命令 CHANGE MASTER TO MASTER_HOST='主机的IP地址', MASTER_USER='主机用户名', MASTER_PASSWORD='主机用户名的密码', MASTER_LOG_FILE='mysql-bin.具体数字', MASTER_LOG_POS=具体值; 举例： CHANGE MASTER TO MASTER_HOST='192.168.1.150',MASTER_USER='slave1',MASTER_PASSWORD='123456',MASTER_LOG_F ILE='atguigu-bin.000007',MASTER_LOG_POS=154; 步骤2： #启动slave同步 START SLAVE; 如果报错： 可以执行如下操作，删除之前的relay_log信息。然后重新执行 CHANGE MASTER TO …语句即可。 mysql\u003e reset slave; #删除SLAVE数据库的relaylog日志文件，并重新启用新的relaylog文件 接着，查看同步状态： SHOW SLAVE STATUS\\G; 上面两个参数都是Yes，则说明主从配置成功！ 显式如下的情况，就是不正确的。可能错误的原因有： 1. 网络不通 2. 账户密码错误 3. 防火墙 4. mysql配置文件问题 5. 连接服务器时语法 6. 主服务器mysql权限 ","date":"2022-10-22","objectID":"/mysql%E6%97%A5%E5%BF%97%E4%B8%8E%E5%A4%87%E4%BB%BD%E7%AF%87/:10:5","tags":[],"title":"MySQL日志与备份篇","uri":"/mysql%E6%97%A5%E5%BF%97%E4%B8%8E%E5%A4%87%E4%BB%BD%E7%AF%87/"},{"categories":["mysql"],"content":"3.6 测试 主机新建库、新建表、insert记录，从机复制： CREATE DATABASE atguigu_master_slave; CREATE TABLE mytbl(id INT,NAME VARCHAR(16)); INSERT INTO mytbl VALUES(1, 'zhang3'); INSERT INTO mytbl VALUES(2,@@hostname); ","date":"2022-10-22","objectID":"/mysql%E6%97%A5%E5%BF%97%E4%B8%8E%E5%A4%87%E4%BB%BD%E7%AF%87/:10:6","tags":[],"title":"MySQL日志与备份篇","uri":"/mysql%E6%97%A5%E5%BF%97%E4%B8%8E%E5%A4%87%E4%BB%BD%E7%AF%87/"},{"categories":["mysql"],"content":"3.7 停止主从同步 停止主从同步命令： stop slave; 如何重新配置主从 如果停止从服务器复制功能，再使用需要重新配置主从。否则会报错如下： 重新配置主从，需要在从机上执行： stop slave; reset master; #删除Master中所有的binglog文件，并将日志索引文件清空，重新开始所有新的日志文件(慎用) ","date":"2022-10-22","objectID":"/mysql%E6%97%A5%E5%BF%97%E4%B8%8E%E5%A4%87%E4%BB%BD%E7%AF%87/:10:7","tags":[],"title":"MySQL日志与备份篇","uri":"/mysql%E6%97%A5%E5%BF%97%E4%B8%8E%E5%A4%87%E4%BB%BD%E7%AF%87/"},{"categories":["mysql"],"content":"3.8 后续 搭建主从复制：双主双从 一个主机m1用于处理所有写请求，它的从机s1和另一台主机m2还有它的从机s2负责所有读请求。当m1主机宕机后，m2主机负责写请求，m1、m2互为备机。结构图如下： ","date":"2022-10-22","objectID":"/mysql%E6%97%A5%E5%BF%97%E4%B8%8E%E5%A4%87%E4%BB%BD%E7%AF%87/:10:8","tags":[],"title":"MySQL日志与备份篇","uri":"/mysql%E6%97%A5%E5%BF%97%E4%B8%8E%E5%A4%87%E4%BB%BD%E7%AF%87/"},{"categories":["mysql"],"content":"4. 同步数据一致性问题 主从同步的要求： 读库和写库的数据一致(最终一致)； 写数据必须写到写库； 读数据必须到读库(不一定)； ","date":"2022-10-22","objectID":"/mysql%E6%97%A5%E5%BF%97%E4%B8%8E%E5%A4%87%E4%BB%BD%E7%AF%87/:11:0","tags":[],"title":"MySQL日志与备份篇","uri":"/mysql%E6%97%A5%E5%BF%97%E4%B8%8E%E5%A4%87%E4%BB%BD%E7%AF%87/"},{"categories":["mysql"],"content":"4.1 理解主从延迟问题 进行主从同步的内容是二进制日志，它是一个文件，在进行 网络传输 的过程中就一定会 存在主从延迟 （比如 500ms），这样就可能造成用户在从库上读取的数据不是最新的数据，也就是主从同步中的 数据不一致性 问题。 ","date":"2022-10-22","objectID":"/mysql%E6%97%A5%E5%BF%97%E4%B8%8E%E5%A4%87%E4%BB%BD%E7%AF%87/:11:1","tags":[],"title":"MySQL日志与备份篇","uri":"/mysql%E6%97%A5%E5%BF%97%E4%B8%8E%E5%A4%87%E4%BB%BD%E7%AF%87/"},{"categories":["mysql"],"content":"4.2 主从延迟问题原因 在网络正常的时候，日志从主库传给从库所需的时间是很短的，即T2-T1的值是非常小的。即，网络正常情况下，主备延迟的主要来源是备库接收完binlog和执行完这个事务之间的时间差。 **主备延迟最直接的表现是，从库消费中继日志（relay log）的速度，比主库生产binlog的速度要慢。**造成原因： 1、从库的机器性能比主库要差 2、从库的压力大 3、大事务的执行 **举例1：**一次性用delete语句删除太多数据 结论：后续再删除数据的时候，要控制每个事务删除的数据量，分成多次删除。 **举例2：**一次性用insert…select插入太多数据 **举例3：**大表DDL 比如在主库对一张500W的表添加一个字段耗费了10分钟，那么从节点上也会耗费10分钟。 ","date":"2022-10-22","objectID":"/mysql%E6%97%A5%E5%BF%97%E4%B8%8E%E5%A4%87%E4%BB%BD%E7%AF%87/:11:2","tags":[],"title":"MySQL日志与备份篇","uri":"/mysql%E6%97%A5%E5%BF%97%E4%B8%8E%E5%A4%87%E4%BB%BD%E7%AF%87/"},{"categories":["mysql"],"content":"4.3 如何减少主从延迟 若想要减少主从延迟的时间，可以采取下面的办法： 降低多线程大事务并发的概率，优化业务逻辑 优化SQL，避免慢SQL， 减少批量操作 ，建议写脚本以update-sleep这样的形式完成。 提高从库机器的配置 ，减少主库写binlog和从库读binlog的效率差。 尽量采用 短的链路 ，也就是主库和从库服务器的距离尽量要短，提升端口带宽，减少binlog传输的网络延时。 实时性要求的业务读强制走主库，从库只做灾备，备份。 ","date":"2022-10-22","objectID":"/mysql%E6%97%A5%E5%BF%97%E4%B8%8E%E5%A4%87%E4%BB%BD%E7%AF%87/:11:3","tags":[],"title":"MySQL日志与备份篇","uri":"/mysql%E6%97%A5%E5%BF%97%E4%B8%8E%E5%A4%87%E4%BB%BD%E7%AF%87/"},{"categories":["mysql"],"content":"4.4 如何解决一致性问题 如果操作的数据存储在同一个数据库中，那么对数据进行更新的时候，可以对记录加写锁，这样在读取的时候就不会发生数据不一致的情况。但这时从库的作用就是 备份 ，并没有起到 读写分离 ，分担主库 读压力 的作用。 读写分离情况下，解决主从同步中数据不一致的问题， 就是解决主从之间 数据复制方式 的问题，如果按照数据一致性 从弱到强 来进行划分，有以下 3 种复制方式。 方法 1：异步复制 异步模式就是客户端提交 COMMIT 之后不需要等从库返回任何结果，而是直接将结果返回给客户端，这样做的好处是不会影响主库写的效率，但可能会存在主库宕机，而Binlog还没有同步到从库的情况，也就是此时的主库和从库数据不一致。这时候从从库中选择一个作为新主，那么新主则可能缺少原来主服务器中已提交的事务。所以，这种复制模式下的数据一致性是最弱的。 方法 2：半同步复制 方法 3：组复制 异步复制和半同步复制都无法最终保证数据的一致性问题，半同步复制是通过判断从库响应的个数来决定是否返回给客户端，虽然数据一致性相比于异步复制有提升，但仍然无法满足对数据一致性要求高的场景，比如金融领域。MGR 很好地弥补了这两种复制模式的不足。 组复制技术，简称 MGR（MySQL Group Replication）。是 MySQL 在 5.7.17 版本中推出的一种新的数据复制技术，这种复制技术是基于 Paxos 协议的状态机复制。 MGR 是如何工作的 首先我们将多个节点共同组成一个复制组，在 执行读写（RW）事务 的时候，需要通过一致性协议层 （Consensus 层）的同意，也就是读写事务想要进行提交，必须要经过组里“大多数人”（对应 Node 节 点）的同意，大多数指的是同意的节点数量需要大于 （N/2+1），这样才可以进行提交，而不是原发起方一个说了算。而针对 只读（RO）事务 则不需要经过组内同意，直接 COMMIT 即可。 在一个复制组内有多个节点组成，它们各自维护了自己的数据副本，并且在一致性协议层实现了原子消 息和全局有序消息，从而保证组内数据的一致性。 MGR 将 MySQL 带入了数据强一致性的时代，是一个划时代的创新，其中一个重要的原因就是MGR 是基 于 Paxos 协议的。Paxos 算法是由 2013 年的图灵奖获得者 Leslie Lamport 于 1990 年提出的，有关这个算法的决策机制可以搜一下。事实上，Paxos 算法提出来之后就作为 分布式一致性算法 被广泛应用，比如 Apache 的 ZooKeeper 也是基于 Paxos 实现的。 ","date":"2022-10-22","objectID":"/mysql%E6%97%A5%E5%BF%97%E4%B8%8E%E5%A4%87%E4%BB%BD%E7%AF%87/:11:4","tags":[],"title":"MySQL日志与备份篇","uri":"/mysql%E6%97%A5%E5%BF%97%E4%B8%8E%E5%A4%87%E4%BB%BD%E7%AF%87/"},{"categories":["mysql"],"content":"5. 知识延伸 在主从架构的配置中，如果想要采取读写分离的策略，我们可以 自己编写程序 ，也可以通过 第三方的中间件 来实现。 自己编写程序的好处就在于比较自主，我们可以自己判断哪些查询在从库上来执行，针对实时性要 求高的需求，我们还可以考虑哪些查询可以在主库上执行。同时，程序直接连接数据库，减少了中间件层，相当于减少了性能损耗。 采用中间件的方法有很明显的优势， 功能强大 ， 使用简单 。但因为在客户端和数据库之间增加了 中间件层会有一些 性能损耗 ，同时商业中间件也是有使用成本的。我们也可以考虑采取一些优秀的开源工具。 ① Cobar 属于阿里B2B事业群，始于2008年，在阿里服役3年多，接管3000+个MySQL数据库的 schema,集群日处理在线SQL请求50亿次以上。由于Cobar发起人的离职，Cobar停止维护。 ② Mycat 是开源社区在阿里cobar基础上进行二次开发，解决了cobar存在的问题，并且加入了许多新的功能在其中。青出于蓝而胜于蓝。 ③ OneProxy 基于MySQL官方的proxy思想利用c语言进行开发的，OneProxy是一款商业 收费 的中 间件。舍弃了一些功能，专注在 性能和稳定性上 。 ④ kingshard 由小团队用go语言开发，还需要发展，需要不断完善。 ⑤ Vitess 是Youtube生产在使用，架构很复杂。不支持MySQL原生协议，使用 需要大量改造成 本 。 ⑥ Atlas 是360团队基于mysql proxy改写，功能还需完善，高并发下不稳定。 ⑦ MaxScale 是mariadb（MySQL原作者维护的一个版本） 研发的中间件 ⑧ MySQLRoute 是MySQL官方Oracle公司发布的中间件 主备切换： 主动切换 被动切换 如何判断主库出问题了？如何解决过程中的数据不一致性问题 ? 第19章_数据库备份与恢复 ","date":"2022-10-22","objectID":"/mysql%E6%97%A5%E5%BF%97%E4%B8%8E%E5%A4%87%E4%BB%BD%E7%AF%87/:12:0","tags":[],"title":"MySQL日志与备份篇","uri":"/mysql%E6%97%A5%E5%BF%97%E4%B8%8E%E5%A4%87%E4%BB%BD%E7%AF%87/"},{"categories":["mysql"],"content":"1. 物理备份与逻辑备份 **物理备份：**备份数据文件，转储数据库物理文件到某一目录。物理备份恢复速度比较快，但占用空间比较大，MySQL中可以用 xtrabackup 工具来进行物理备份。 **逻辑备份：**对数据库对象利用工具进行导出工作，汇总入备份文件内。逻辑备份恢复速度慢，但占用空间小，更灵活。MySQL 中常用的逻辑备份工具为 mysqldump 。逻辑备份就是 备份sql语句 ，在恢复的 时候执行备份的sql语句实现数据库数据的重现。 ","date":"2022-10-22","objectID":"/mysql%E6%97%A5%E5%BF%97%E4%B8%8E%E5%A4%87%E4%BB%BD%E7%AF%87/:13:0","tags":[],"title":"MySQL日志与备份篇","uri":"/mysql%E6%97%A5%E5%BF%97%E4%B8%8E%E5%A4%87%E4%BB%BD%E7%AF%87/"},{"categories":["mysql"],"content":"2. mysqldump实现逻辑备份 mysqldump是MySQL提供的一个非常有用的数据库备份工具。 ","date":"2022-10-22","objectID":"/mysql%E6%97%A5%E5%BF%97%E4%B8%8E%E5%A4%87%E4%BB%BD%E7%AF%87/:14:0","tags":[],"title":"MySQL日志与备份篇","uri":"/mysql%E6%97%A5%E5%BF%97%E4%B8%8E%E5%A4%87%E4%BB%BD%E7%AF%87/"},{"categories":["mysql"],"content":"2.1 备份一个数据库 mysqldump命令执行时，可以将数据库备份成一个文本文件，该文件中实际上包含多个CREATE和INSERT语句，使用这些语句可以重新创建表和插入数据。 查出需要备份的表的结构，在文本文件中生成一个CREATE语句 将表中的所有记录转换为一条INSERT语句。 基本语法： mysqldump –u 用户名称 –h 主机名称 –p密码 待备份的数据库名称[tbname, [tbname...]]\u003e 备份文件名称.sql 说明： 备份的文件并非一定要求后缀名为.sql，例如后缀名为.txt的文件也是可以的。 举例：使用root用户备份atguigu数据库： mysqldump -uroot -p atguigu\u003eatguigu.sql #备份文件存储在当前目录下 mysqldump -uroot -p atguigudb1 \u003e /var/lib/mysql/atguigu.sql 备份文件剖析： -- MySQL dump 10.13 Distrib 8.0.26, for Linux (x86_64) -- -- Host: localhost Database: atguigu -- ------------------------------------------------------ -- Server version 8.0.26 /*!40101 SET @OLD_CHARACTER_SET_CLIENT=@@CHARACTER_SET_CLIENT */; /*!40101 SET @OLD_CHARACTER_SET_RESULTS=@@CHARACTER_SET_RESULTS */; /*!40101 SET @OLD_COLLATION_CONNECTION=@@COLLATION_CONNECTION */; /*!50503 SET NAMES utf8mb4 */; /*!40103 SET @OLD_TIME_ZONE=@@TIME_ZONE */; /*!40103 SET TIME_ZONE='+00:00' */; /*!40014 SET @OLD_UNIQUE_CHECKS=@@UNIQUE_CHECKS, UNIQUE_CHECKS=0 */; /*!40014 SET @OLD_FOREIGN_KEY_CHECKS=@@FOREIGN_KEY_CHECKS, FOREIGN_KEY_CHECKS=0 */; /*!40101 SET @OLD_SQL_MODE=@@SQL_MODE, SQL_MODE='NO_AUTO_VALUE_ON_ZERO' */; /*!40111 SET @OLD_SQL_NOTES=@@SQL_NOTES, SQL_NOTES=0 */; -- -- Current Database: `atguigu` -- CREATE DATABASE /*!32312 IF NOT EXISTS*/ `atguigu` /*!40100 DEFAULT CHARACTER SET utf8mb4 COLLATE utf8mb4_0900_ai_ci */ /*!80016 DEFAULT ENCRYPTION='N' */; USE `atguigu`; -- -- Table structure for table `student` -- DROP TABLE IF EXISTS `student`; /*!40101 SET @saved_cs_client = @@character_set_client */; /*!50503 SET character_set_client = utf8mb4 */; CREATE TABLE `student` ( `studentno` int NOT NULL, `name` varchar(20) DEFAULT NULL, `class` varchar(20) DEFAULT NULL, PRIMARY KEY (`studentno`) ) ENGINE=InnoDB DEFAULT CHARSET=utf8mb3; /*!40101 SET character_set_client = @saved_cs_client */; INSERT INTO `student` VALUES (1,'张三_back','一班'),(3,'李四','一班'),(8,'王五','二班'), (15,'赵六','二班'),(20,'钱七','\u003e三班'),(22,'zhang3_update','1ban'),(24,'wang5','2ban'); /*!40000 ALTER TABLE `student` ENABLE KEYS */; UNLOCK TABLES; . . . . /*!40101 SET SQL_MODE=@OLD_SQL_MODE */; /*!40014 SET FOREIGN_KEY_CHECKS=@OLD_FOREIGN_KEY_CHECKS */; /*!40014 SET UNIQUE_CHECKS=@OLD_UNIQUE_CHECKS */; /*!40101 SET CHARACTER_SET_CLIENT=@OLD_CHARACTER_SET_CLIENT */; /*!40101 SET CHARACTER_SET_RESULTS=@OLD_CHARACTER_SET_RESULTS */; /*!40101 SET COLLATION_CONNECTION=@OLD_COLLATION_CONNECTION */; /*!40111 SET SQL_NOTES=@OLD_SQL_NOTES */; -- Dump completed on 2022-01-07 9:58:23 ","date":"2022-10-22","objectID":"/mysql%E6%97%A5%E5%BF%97%E4%B8%8E%E5%A4%87%E4%BB%BD%E7%AF%87/:14:1","tags":[],"title":"MySQL日志与备份篇","uri":"/mysql%E6%97%A5%E5%BF%97%E4%B8%8E%E5%A4%87%E4%BB%BD%E7%AF%87/"},{"categories":["mysql"],"content":"2.2 备份全部数据库 若想用mysqldump备份整个实例，可以使用 –all-databases 或 -A 参数： mysqldump -uroot -pxxxxxx --all-databases \u003e all_database.sql mysqldump -uroot -pxxxxxx -A \u003e all_database.sql ","date":"2022-10-22","objectID":"/mysql%E6%97%A5%E5%BF%97%E4%B8%8E%E5%A4%87%E4%BB%BD%E7%AF%87/:14:2","tags":[],"title":"MySQL日志与备份篇","uri":"/mysql%E6%97%A5%E5%BF%97%E4%B8%8E%E5%A4%87%E4%BB%BD%E7%AF%87/"},{"categories":["mysql"],"content":"2.3 备份部分数据库 使用 --databases 或 -B 参数了，该参数后面跟数据库名称，多个数据库间用空格隔开。如果指定 databases参数，备份文件中会存在创建数据库的语句，如果不指定参数，则不存在。语法如下： mysqldump –u user –h host –p --databases [数据库的名称1 [数据库的名称2...]] \u003e 备份文件名称.sql 举例： mysqldump -uroot -p --databases atguigu atguigu12 \u003etwo_database.sql 或 mysqldump -uroot -p -B atguigu atguigu12 \u003e two_database.sql ","date":"2022-10-22","objectID":"/mysql%E6%97%A5%E5%BF%97%E4%B8%8E%E5%A4%87%E4%BB%BD%E7%AF%87/:14:3","tags":[],"title":"MySQL日志与备份篇","uri":"/mysql%E6%97%A5%E5%BF%97%E4%B8%8E%E5%A4%87%E4%BB%BD%E7%AF%87/"},{"categories":["mysql"],"content":"2.4 备份部分表 比如，在表变更前做个备份。语法如下： mysqldump –u user –h host –p 数据库的名称 [表名1 [表名2...]] \u003e 备份文件名称.sql 举例：备份atguigu数据库下的book表 mysqldump -uroot -p atguigu book\u003e book.sql book.sql文件内容如下 mysqldump -uroot -p atguigu book\u003e book.sql^C [root@node1 ~]# ls kk kubekey kubekey-v1.1.1-linux-amd64.tar.gz README.md test1.sql two_database.sql [root@node1 ~]# mysqldump -uroot -p atguigu book\u003e book.sql Enter password: [root@node1 ~]# ls book.sql kk kubekey kubekey-v1.1.1-linux-amd64.tar.gz README.md test1.sql two_database.sql [root@node1 ~]# vi book.sql -- MySQL dump 10.13 Distrib 8.0.26, for Linux (x86_64) -- -- Host: localhost Database: atguigu -- ------------------------------------------------------ -- Server version 8.0.26 /*!40101 SET @OLD_CHARACTER_SET_CLIENT=@@CHARACTER_SET_CLIENT */; /*!40101 SET @OLD_CHARACTER_SET_RESULTS=@@CHARACTER_SET_RESULTS */; /*!40101 SET @OLD_COLLATION_CONNECTION=@@COLLATION_CONNECTION */; /*!50503 SET NAMES utf8mb4 */; /*!40103 SET @OLD_TIME_ZONE=@@TIME_ZONE */; /*!40103 SET TIME_ZONE='+00:00' */; /*!40014 SET @OLD_UNIQUE_CHECKS=@@UNIQUE_CHECKS, UNIQUE_CHECKS=0 */; /*!40014 SET @OLD_FOREIGN_KEY_CHECKS=@@FOREIGN_KEY_CHECKS, FOREIGN_KEY_CHECKS=0 */; /*!40101 SET @OLD_SQL_MODE=@@SQL_MODE, SQL_MODE='NO_AUTO_VALUE_ON_ZERO' */; /*!40111 SET @OLD_SQL_NOTES=@@SQL_NOTES, SQL_NOTES=0 */; -- -- Table structure for table `book` -- DROP TABLE IF EXISTS `book`; /*!40101 SET @saved_cs_client = @@character_set_client */; /*!50503 SET character_set_client = utf8mb4 */; CREATE TABLE `book` ( `bookid` int unsigned NOT NULL AUTO_INCREMENT, `card` int unsigned NOT NULL, `test` varchar(255) COLLATE utf8_bin DEFAULT NULL, PRIMARY KEY (`bookid`), KEY `Y` (`card`) ) ENGINE=InnoDB AUTO_INCREMENT=101 DEFAULT CHARSET=utf8mb3 COLLATE=utf8_bin; /*!40101 SET character_set_client = @saved_cs_client */; -- -- Dumping data for table `book` -- LOCK TABLES `book` WRITE; /*!40000 ALTER TABLE `book` DISABLE KEYS */; INSERT INTO `book` VALUES (1,9,NULL),(2,10,NULL),(3,4,NULL),(4,8,NULL),(5,7,NULL), (6,10,NULL),(7,11,NULL),(8,3,NULL),(9,1,NULL),(10,17,NULL),(11,19,NULL),(12,4,NULL), (13,1,NULL),(14,14,NULL),(15,5,NULL),(16,5,NULL),(17,8,NULL),(18,3,NULL),(19,12,NULL), (20,11,NULL),(21,9,NULL),(22,20,NULL),(23,13,NULL),(24,3,NULL),(25,18,NULL), (26,20,NULL),(27,5,NULL),(28,6,NULL),(29,15,NULL),(30,15,NULL),(31,12,NULL), (32,11,NULL),(33,20,NULL),(34,5,NULL),(35,4,NULL),(36,6,NULL),(37,17,NULL), (38,5,NULL),(39,16,NULL),(40,6,NULL),(41,18,NULL),(42,12,NULL),(43,6,NULL), (44,12,NULL),(45,2,NULL),(46,12,NULL),(47,15,NULL),(48,17,NULL),(49,2,NULL), (50,16,NULL),(51,13,NULL),(52,17,NULL),(53,7,NULL),(54,2,NULL),(55,9,NULL), (56,1,NULL),(57,14,NULL),(58,7,NULL),(59,15,NULL),(60,12,NULL),(61,13,NULL), (62,8,NULL),(63,2,NULL),(64,6,NULL),(65,2,NULL),(66,12,NULL),(67,12,NULL),(68,4,NULL), (69,5,NULL),(70,10,NULL),(71,16,NULL),(72,8,NULL),(73,14,NULL),(74,5,NULL), (75,4,NULL),(76,3,NULL),(77,2,NULL),(78,2,NULL),(79,2,NULL),(80,3,NULL),(81,8,NULL), (82,14,NULL),(83,5,NULL),(84,4,NULL),(85,2,NULL),(86,20,NULL),(87,12,NULL), (88,1,NULL),(89,8,NULL),(90,18,NULL),(91,3,NULL),(92,3,NULL),(93,6,NULL),(94,1,NULL), (95,4,NULL),(96,17,NULL),(97,15,NULL),(98,1,NULL),(99,20,NULL),(100,15,NULL); /*!40000 ALTER TABLE `book` ENABLE KEYS */; UNLOCK TABLES; /*!40103 SET TIME_ZONE=@OLD_TIME_ZONE */; 可以看到，book文件和备份的库文件类似。不同的是，book文件只包含book表的DROP、CREATE和 INSERT语句。 备份多张表使用下面的命令，比如备份book和account表： #备份多张表 mysqldump -uroot -p atguigu book account \u003e 2_tables_bak.sql ","date":"2022-10-22","objectID":"/mysql%E6%97%A5%E5%BF%97%E4%B8%8E%E5%A4%87%E4%BB%BD%E7%AF%87/:14:4","tags":[],"title":"MySQL日志与备份篇","uri":"/mysql%E6%97%A5%E5%BF%97%E4%B8%8E%E5%A4%87%E4%BB%BD%E7%AF%87/"},{"categories":["mysql"],"content":"2.5 备份单表的部分数据 有些时候一张表的数据量很大，我们只需要部分数据。这时就可以使用 –where 选项了。where后面附带需要满足的条件。 举例：备份student表中id小于10的数据： mysqldump -uroot -p atguigu student --where=\"id \u003c 10 \" \u003e student_part_id10_low_bak.sql 内容如下所示，insert语句只有id小于10的部分 LOCK TABLES `student` WRITE; /*!40000 ALTER TABLE `student` DISABLE KEYS */; INSERT INTO `student` VALUES (1,100002,'JugxTY',157,280),(2,100003,'QyUcCJ',251,277), (3,100004,'lATUPp',80,404),(4,100005,'BmFsXI',240,171),(5,100006,'mkpSwJ',388,476), (6,100007,'ujMgwN',259,124),(7,100008,'HBJTqX',429,168),(8,100009,'dvQSQA',61,504), (9,100010,'HljpVJ',234,185); ","date":"2022-10-22","objectID":"/mysql%E6%97%A5%E5%BF%97%E4%B8%8E%E5%A4%87%E4%BB%BD%E7%AF%87/:14:5","tags":[],"title":"MySQL日志与备份篇","uri":"/mysql%E6%97%A5%E5%BF%97%E4%B8%8E%E5%A4%87%E4%BB%BD%E7%AF%87/"},{"categories":["mysql"],"content":"2.6 排除某些表的备份 如果我们想备份某个库，但是某些表数据量很大或者与业务关联不大，这个时候可以考虑排除掉这些表，同样的，选项 --ignore-table 可以完成这个功能。 mysqldump -uroot -p atguigu --ignore-table=atguigu.student \u003e no_stu_bak.sql 通过如下指定判定文件中没有student表结构： grep \"student\" no_stu_bak.sql ","date":"2022-10-22","objectID":"/mysql%E6%97%A5%E5%BF%97%E4%B8%8E%E5%A4%87%E4%BB%BD%E7%AF%87/:14:6","tags":[],"title":"MySQL日志与备份篇","uri":"/mysql%E6%97%A5%E5%BF%97%E4%B8%8E%E5%A4%87%E4%BB%BD%E7%AF%87/"},{"categories":["mysql"],"content":"2.7 只备份结构或只备份数据 只备份结构的话可以使用 --no-data 简写为 -d 选项；只备份数据可以使用 --no-create-info 简写为 -t选项。 只备份结构 mysqldump -uroot -p atguigu --no-data \u003e atguigu_no_data_bak.sql #使用grep命令，没有找到insert相关语句，表示没有数据备份。 [root@node1 ~]# grep \"INSERT\" atguigu_no_data_bak.sql [root@node1 ~]# 只备份数据 mysqldump -uroot -p atguigu --no-create-info \u003e atguigu_no_create_info_bak.sql #使用grep命令，没有找到create相关语句，表示没有数据结构。 [root@node1 ~]# grep \"CREATE\" atguigu_no_create_info_bak.sql [root@node1 ~]# ","date":"2022-10-22","objectID":"/mysql%E6%97%A5%E5%BF%97%E4%B8%8E%E5%A4%87%E4%BB%BD%E7%AF%87/:14:7","tags":[],"title":"MySQL日志与备份篇","uri":"/mysql%E6%97%A5%E5%BF%97%E4%B8%8E%E5%A4%87%E4%BB%BD%E7%AF%87/"},{"categories":["mysql"],"content":"2.8 备份中包含存储过程、函数、事件 mysqldump备份默认是不包含存储过程，自定义函数及事件的。可以使用 --routines 或 -R 选项来备份存储过程及函数，使用 --events 或 -E 参数来备份事件。 举例：备份整个atguigu库，包含存储过程及事件： 使用下面的SQL可以查看当前库有哪些存储过程或者函数 mysql\u003e SELECT SPECIFIC_NAME,ROUTINE_TYPE ,ROUTINE_SCHEMA FROM information_schema.Routines WHERE ROUTINE_SCHEMA=\"atguigu\"; +---------------+--------------+----------------+ | SPECIFIC_NAME | ROUTINE_TYPE | ROUTINE_SCHEMA | +---------------+--------------+----------------+ | rand_num | FUNCTION | atguigu | | rand_string | FUNCTION | atguigu | | BatchInsert | PROCEDURE | atguigu | | insert_class | PROCEDURE | atguigu | | insert_order | PROCEDURE | atguigu | | insert_stu | PROCEDURE | atguigu | | insert_user | PROCEDURE | atguigu | | ts_insert | PROCEDURE | atguigu | +---------------+--------------+----------------+ 9 rows in set (0.02 sec) 下面备份atguigu库的数据，函数以及存储过程。 mysqldump -uroot -p -R -E --databases atguigu \u003e fun_atguigu_bak.sql 查询备份文件中是否存在函数，如下所示，可以看到确实包含了函数。 grep -C 5 \"rand_num\" fun_atguigu_bak.sql -- -- -- Dumping routines for database 'atguigu' -- /*!50003 DROP FUNCTION IF EXISTS `rand_num` */; /*!50003 SET @saved_cs_client = @@character_set_client */ ; /*!50003 SET @saved_cs_results = @@character_set_results */ ; /*!50003 SET @saved_col_connection = @@collation_connection */ ; /*!50003 SET character_set_client = utf8mb3 */ ; /*!50003 SET character_set_results = utf8mb3 */ ; /*!50003 SET collation_connection = utf8_general_ci */ ; /*!50003 SET @saved_sql_mode = @@sql_mode */ ; /*!50003 SET sql_mode = 'ONLY_FULL_GROUP_BY,STRICT_TRANS_TABLES,NO_ZERO_IN_DATE,NO_ZERO_DATE,ERROR_FOR_DIVISIO N_BY_ZERO,NO_ENGINE_SUBSTITUTION' */ ; DELIMITER ;; CREATE DEFINER=`root`@`%` FUNCTION `rand_num`(from_num BIGINT ,to_num BIGINT) RETURNS bigint BEGIN DECLARE i BIGINT DEFAULT 0; SET i = FLOOR(from_num +RAND()*(to_num - from_num+1)) ; RETURN i; END ;; -- BEGIN DECLARE i INT DEFAULT 0; SET autocommit = 0; REPEAT SET i = i + 1; INSERT INTO class ( classname,address,monitor ) VALUES (rand_string(8),rand_string(10),rand_num()); UNTIL i = max_num END REPEAT; COMMIT; END ;; DELIMITER ; -- BEGIN DECLARE i INT DEFAULT 0; SET autocommit = 0; #设置手动提交事务 REPEAT #循环 SET i = i + 1; #赋值 INSERT INTO order_test (order_id, trans_id ) VALUES (rand_num(1,7000000),rand_num(100000000000000000,700000000000000000)); UNTIL i = max_num END REPEAT; COMMIT; #提交事务 END ;; DELIMITER ; -- BEGIN DECLARE i INT DEFAULT 0; SET autocommit = 0; #设置手动提交事务 REPEAT #循环 SET i = i + 1; #赋值 INSERT INTO student (stuno, name ,age ,classId ) VALUES ((START+i),rand_string(6),rand_num(),rand_num()); UNTIL i = max_num END REPEAT; COMMIT; #提交事务 END ;; DELIMITER ; -- BEGIN DECLARE i INT DEFAULT 0; SET autocommit = 0; REPEAT SET i = i + 1; INSERT INTO `user` ( name,age,sex ) VALUES (\"atguigu\",rand_num(1,20),\"male\"); UNTIL i = max_num END REPEAT; COMMIT; END ;; DELIMITER ; ","date":"2022-10-22","objectID":"/mysql%E6%97%A5%E5%BF%97%E4%B8%8E%E5%A4%87%E4%BB%BD%E7%AF%87/:14:8","tags":[],"title":"MySQL日志与备份篇","uri":"/mysql%E6%97%A5%E5%BF%97%E4%B8%8E%E5%A4%87%E4%BB%BD%E7%AF%87/"},{"categories":["mysql"],"content":"2.9 mysqldump常用选项 mysqldump其他常用选项如下： --add-drop-database：在每个CREATE DATABASE语句前添加DROP DATABASE语句。 --add-drop-tables：在每个CREATE TABLE语句前添加DROP TABLE语句。 --add-locking：用LOCK TABLES和UNLOCK TABLES语句引用每个表转储。重载转储文件时插入得更快。 --all-database, -A：转储所有数据库中的所有表。与使用--database选项相同，在命令行中命名所有数据库。 --comment[=0|1]：如果设置为0，禁止转储文件中的其他信息，例如程序版本、服务器版本和主机。--skipcomments与--comments=0的结果相同。默认值为1，即包括额外信息。 --compact：产生少量输出。该选项禁用注释并启用--skip-add-drop-tables、--no-set-names、--skipdisable-keys和--skip-add-locking选项。 --compatible=name：产生与其他数据库系统或旧的MySQL服务器更兼容的输出，值可以为ansi、MySQL323、MySQL40、postgresql、oracle、mssql、db2、maxdb、no_key_options、no_table_options或者no_field_options。 --complete_insert, -c：使用包括列名的完整的INSERT语句。 --debug[=debug_options], -#[debug_options]：写调试日志。 --delete，-D：导入文本文件前清空表。 --default-character-set=charset：使用charsets默认字符集。如果没有指定，就使用utf8。 --delete--master-logs：在主复制服务器上，完成转储操作后删除二进制日志。该选项自动启用-masterdata。 --extended-insert，-e：使用包括几个VALUES列表的多行INSERT语法。这样使得转储文件更小，重载文件时可以加速插入。 --flush-logs，-F：开始转储前刷新MySQL服务器日志文件。该选项要求RELOAD权限。 --force，-f：在表转储过程中，即使出现SQL错误也继续。 --lock-all-tables，-x：对所有数据库中的所有表加锁。在整体转储过程中通过全局锁定来实现。该选项自动关闭--single-transaction和--lock-tables。 --lock-tables，-l：开始转储前锁定所有表。用READ LOCAL锁定表以允许并行插入MyISAM表。对于事务表（例如InnoDB和BDB），--single-transaction是一个更好的选项，因为它根本不需要锁定表。 --no-create-db，-n：该选项禁用CREATE DATABASE /*!32312 IF NOT EXIST*/db_name语句，如果给出--database或--all-database选项，就包含到输出中。 --no-create-info，-t：只导出数据，而不添加CREATE TABLE语句。 --no-data，-d：不写表的任何行信息，只转储表的结构。 --opt：该选项是速记，它可以快速进行转储操作并产生一个能很快装入MySQL服务器的转储文件。该选项默认开启，但可以用--skip-opt禁用。 --password[=password]，-p[password]：当连接服务器时使用的密码。 -port=port_num，-P port_num：用于连接的TCP/IP端口号。 --protocol={TCP|SOCKET|PIPE|MEMORY}：使用的连接协议。 --replace，-r –replace和--ignore：控制替换或复制唯一键值已有记录的输入记录的处理。如果指定--replace，新行替换有相同的唯一键值的已有行；如果指定--ignore，复制已有的唯一键值的输入行被跳过。如果不指定这两个选项，当发现一个复制键值时会出现一个错误，并且忽视文本文件的剩余部分。 --silent，-s：沉默模式。只有出现错误时才输出。 --socket=path，-S path：当连接localhost时使用的套接字文件（为默认主机）。 --user=user_name，-u user_name：当连接服务器时MySQL使用的用户名。 --verbose，-v：冗长模式，打印出程序操作的详细信息。 --xml，-X：产生XML输出。 运行帮助命令 mysqldump --help ，可以获得特定版本的完整选项列表。 提示 如果运行mysqldump没有–quick或–opt选项，mysqldump在转储结果前将整个结果集装入内 存。如果转储大数据库可能会出现问题，该选项默认启用，但可以用–skip-opt禁用。如果使用最 新版本的mysqldump程序备份数据，并用于恢复到比较旧版本的MySQL服务器中，则不要使用–opt 或-e选项。 ","date":"2022-10-22","objectID":"/mysql%E6%97%A5%E5%BF%97%E4%B8%8E%E5%A4%87%E4%BB%BD%E7%AF%87/:14:9","tags":[],"title":"MySQL日志与备份篇","uri":"/mysql%E6%97%A5%E5%BF%97%E4%B8%8E%E5%A4%87%E4%BB%BD%E7%AF%87/"},{"categories":["mysql"],"content":"3. mysql命令恢复数据 使用mysqldump命令将数据库中的数据备份成一个文本文件。需要恢复时，可以使用mysql命令来恢复备份的数据。 mysql命令可以执行备份文件中的CREATE语句和INSERT语句。通过CREATE语句来创建数据库和表。通过INSERT语句来插入备份的数据。 基本语法： mysql –u root –p [dbname] \u003c backup.sql 其中，dbname参数表示数据库名称。该参数是可选参数，可以指定数据库名，也可以不指定。指定数据库名时，表示还原该数据库下的表。此时需要确保MySQL服务器中已经创建了该名的数据库。不指定数据库名，表示还原文件中所有的数据库。此时sql文件中包含有CREATE DATABASE语句，不需要MySQL服务器中已存在的这些数据库。 ","date":"2022-10-22","objectID":"/mysql%E6%97%A5%E5%BF%97%E4%B8%8E%E5%A4%87%E4%BB%BD%E7%AF%87/:15:0","tags":[],"title":"MySQL日志与备份篇","uri":"/mysql%E6%97%A5%E5%BF%97%E4%B8%8E%E5%A4%87%E4%BB%BD%E7%AF%87/"},{"categories":["mysql"],"content":"3.1 单库备份中恢复单库 使用root用户，将之前练习中备份的atguigu.sql文件中的备份导入数据库中，命令如下： 如果备份文件中包含了创建数据库的语句，则恢复的时候不需要指定数据库名称，如下所示 mysql -uroot -p \u003c atguigu.sql 否则需要指定数据库名称，如下所示 mysql -uroot -p atguigu4\u003c atguigu.sql ","date":"2022-10-22","objectID":"/mysql%E6%97%A5%E5%BF%97%E4%B8%8E%E5%A4%87%E4%BB%BD%E7%AF%87/:15:1","tags":[],"title":"MySQL日志与备份篇","uri":"/mysql%E6%97%A5%E5%BF%97%E4%B8%8E%E5%A4%87%E4%BB%BD%E7%AF%87/"},{"categories":["mysql"],"content":"3.2 全量备份恢复 如果我们现在有昨天的全量备份，现在想整个恢复，则可以这样操作： mysql –u root –p \u003c all.sql mysql -uroot -pxxxxxx \u003c all.sql 执行完后，MySQL数据库中就已经恢复了all.sql文件中的所有数据库。 ","date":"2022-10-22","objectID":"/mysql%E6%97%A5%E5%BF%97%E4%B8%8E%E5%A4%87%E4%BB%BD%E7%AF%87/:15:2","tags":[],"title":"MySQL日志与备份篇","uri":"/mysql%E6%97%A5%E5%BF%97%E4%B8%8E%E5%A4%87%E4%BB%BD%E7%AF%87/"},{"categories":["mysql"],"content":"3.3 全量备份恢复 可能有这样的需求，比如说我们只想恢复某一个库，但是我们有的是整个实例的备份，这个时候我们可以从全量备份中分离出单个库的备份。 举例： sed -n '/^-- Current Database: `atguigu`/,/^-- Current Database: `/p' all_database.sql \u003e atguigu.sql #分离完成后我们再导入atguigu.sql即可恢复单个库 ","date":"2022-10-22","objectID":"/mysql%E6%97%A5%E5%BF%97%E4%B8%8E%E5%A4%87%E4%BB%BD%E7%AF%87/:15:3","tags":[],"title":"MySQL日志与备份篇","uri":"/mysql%E6%97%A5%E5%BF%97%E4%B8%8E%E5%A4%87%E4%BB%BD%E7%AF%87/"},{"categories":["mysql"],"content":"3.4 从单库备份中恢复单表 这个需求还是比较常见的。比如说我们知道哪个表误操作了，那么就可以用单表恢复的方式来恢复。 举例：我们有atguigu整库的备份，但是由于class表误操作，需要单独恢复出这张表。 cat atguigu.sql | sed -e '/./{H;$!d;}' -e 'x;/CREATE TABLE `class`/!d;q' \u003e class_structure.sql cat atguigu.sql | grep --ignore-case 'insert into `class`' \u003e class_data.sql #用shell语法分离出创建表的语句及插入数据的语句后 再依次导出即可完成恢复 use atguigu; mysql\u003e source class_structure.sql; Query OK, 0 rows affected, 1 warning (0.00 sec) mysql\u003e source class_data.sql; Query OK, 1 row affected (0.01 sec) ","date":"2022-10-22","objectID":"/mysql%E6%97%A5%E5%BF%97%E4%B8%8E%E5%A4%87%E4%BB%BD%E7%AF%87/:15:4","tags":[],"title":"MySQL日志与备份篇","uri":"/mysql%E6%97%A5%E5%BF%97%E4%B8%8E%E5%A4%87%E4%BB%BD%E7%AF%87/"},{"categories":["mysql"],"content":"4. 物理备份：直接复制整个数据库 直接将MySQL中的数据库文件复制出来。这种方法最简单，速度也最快。MySQL的数据库目录位置不一 定相同： 在Windows平台下，MySQL 8.0存放数据库的目录通常默认为 “ C:\\ProgramData\\MySQL\\MySQL Server 8.0\\Data ”或者其他用户自定义目录； 在Linux平台下，数据库目录位置通常为/var/lib/mysql/； 在MAC OSX平台下，数据库目录位置通常为“/usr/local/mysql/data” 但为了保证备份的一致性。需要保证： 方式1：备份前，将服务器停止。 方式2：备份前，对相关表执行 FLUSH TABLES WITH READ LOCK 操作。这样当复制数据库目录中 的文件时，允许其他客户继续查询表。同时，FLUSH TABLES语句来确保开始备份前将所有激活的索 引页写入硬盘。 这种方式方便、快速，但不是最好的备份方法，因为实际情况可能 不允许停止MySQL服务器 或者 锁住表 ，而且这种方法 对InnoDB存储引擎 的表不适用。对于MyISAM存储引擎的表，这样备份和还原很方便，但是还原时最好是相同版本的MySQL数据库，否则可能会存在文件类型不同的情况。 注意，物理备份完毕后，执行 UNLOCK TABLES 来结算其他客户对表的修改行为。 说明： 在MySQL版本号中，第一个数字表示主版本号，主版本号相同的MySQL数据库文件格式相同。 此外，还可以考虑使用相关工具实现备份。比如， MySQLhotcopy 工具。MySQLhotcopy是一个Perl脚本，它使用LOCK TABLES、FLUSH TABLES和cp或scp来快速备份数据库。它是备份数据库或单个表最快的途径，但它只能运行在数据库目录所在的机器上，并且只能备份MyISAM类型的表。多用于mysql5.5之前。 ","date":"2022-10-22","objectID":"/mysql%E6%97%A5%E5%BF%97%E4%B8%8E%E5%A4%87%E4%BB%BD%E7%AF%87/:16:0","tags":[],"title":"MySQL日志与备份篇","uri":"/mysql%E6%97%A5%E5%BF%97%E4%B8%8E%E5%A4%87%E4%BB%BD%E7%AF%87/"},{"categories":["mysql"],"content":"5. 物理恢复：直接复制到数据库目录 步骤： 1）演示删除备份的数据库中指定表的数据 2）将备份的数据库数据拷贝到数据目录下，并重启MySQL服务器 3）查询相关表的数据是否恢复。需要使用下面的 chown 操作。 要求： 必须确保备份数据的数据库和待恢复的数据库服务器的主版本号相同。 因为只有MySQL数据库主版本号相同时，才能保证这两个MySQL数据库文件类型是相同的。 这种方式对 MyISAM类型的表比较有效 ，对于InnoDB类型的表则不可用。 因为InnoDB表的表空间不能直接复制。 在Linux操作系统下，复制到数据库目录后，一定要将数据库的用户和组变成mysql，命令如下： chown -R mysql.mysql /var/lib/mysql/dbname 其中，两个mysql分别表示组和用户；“-R”参数可以改变文件夹下的所有子文件的用户和组；“dbname”参数表示数据库目录。 提示 Linux操作系统下的权限设置非常严格。通常情况下，MySQL数据库只有root用户和mysql用户 组下的mysql用户才可以访问，因此将数据库目录复制到指定文件夹后，一定要使用chown命令将 文件夹的用户组变为mysql，将用户变为mysql。 ","date":"2022-10-22","objectID":"/mysql%E6%97%A5%E5%BF%97%E4%B8%8E%E5%A4%87%E4%BB%BD%E7%AF%87/:17:0","tags":[],"title":"MySQL日志与备份篇","uri":"/mysql%E6%97%A5%E5%BF%97%E4%B8%8E%E5%A4%87%E4%BB%BD%E7%AF%87/"},{"categories":["mysql"],"content":"6. 表的导出与导入 ","date":"2022-10-22","objectID":"/mysql%E6%97%A5%E5%BF%97%E4%B8%8E%E5%A4%87%E4%BB%BD%E7%AF%87/:18:0","tags":[],"title":"MySQL日志与备份篇","uri":"/mysql%E6%97%A5%E5%BF%97%E4%B8%8E%E5%A4%87%E4%BB%BD%E7%AF%87/"},{"categories":["mysql"],"content":"6.1 表的导出 1. 使用SELECT…INTO OUTFILE导出文本文件 在MySQL中，可以使用SELECT…INTO OUTFILE语句将表的内容导出成一个文本文件。 **举例：**使用SELECT…INTO OUTFILE将atguigu数据库中account表中的记录导出到文本文件。 （1）选择数据库atguigu，并查询account表，执行结果如下所示。 use atguigu; select * from account; mysql\u003e select * from account; +----+--------+---------+ | id | name | balance | +----+--------+---------+ | 1 | 张三 | 90 | | 2 | 李四 | 100 | | 3 | 王五 | 0 | +----+--------+---------+ 3 rows in set (0.01 sec) （2）mysql默认对导出的目录有权限限制，也就是说使用命令行进行导出的时候，需要指定目录进行操作。 查询secure_file_priv值： mysql\u003e SHOW GLOBAL VARIABLES LIKE '%secure%'; +--------------------------+-----------------------+ | Variable_name | Value | +--------------------------+-----------------------+ | require_secure_transport | OFF | | secure_file_priv | /var/lib/mysql-files/ | +--------------------------+-----------------------+ 2 rows in set (0.02 sec) （3）上面结果中显示，secure_file_priv变量的值为/var/lib/mysql-files/，导出目录设置为该目录，SQL语句如下。 SELECT * FROM account INTO OUTFILE \"/var/lib/mysql-files/account.txt\"; （4）查看 /var/lib/mysql-files/account.txt`文件。 1 张三 90 2 李四 100 3 王五 0 2. 使用mysqldump命令导出文本文件 **举例1：**使用mysqldump命令将将atguigu数据库中account表中的记录导出到文本文件： mysqldump -uroot -p -T \"/var/lib/mysql-files/\" atguigu account mysqldump命令执行完毕后，在指定的目录/var/lib/mysql-files/下生成了account.sql和account.txt文件。 打开account.sql文件，其内容包含创建account表的CREATE语句。 [root@node1 mysql-files]# cat account.sql -- MySQL dump 10.13 Distrib 8.0.26, for Linux (x86_64) -- -- Host: localhost Database: atguigu -- ------------------------------------------------------ -- Server version 8.0.26 /*!40101 SET @OLD_CHARACTER_SET_CLIENT=@@CHARACTER_SET_CLIENT */; /*!40101 SET @OLD_CHARACTER_SET_RESULTS=@@CHARACTER_SET_RESULTS */; /*!40101 SET @OLD_COLLATION_CONNECTION=@@COLLATION_CONNECTION */; /*!50503 SET NAMES utf8mb4 */; /*!40103 SET @OLD_TIME_ZONE=@@TIME_ZONE */; /*!40103 SET TIME_ZONE='+00:00' */; /*!40101 SET @OLD_SQL_MODE=@@SQL_MODE, SQL_MODE='' */; /*!40111 SET @OLD_SQL_NOTES=@@SQL_NOTES, SQL_NOTES=0 */; -- -- Table structure for table `account` -- DROP TABLE IF EXISTS `account`; /*!40101 SET @saved_cs_client = @@character_set_client */; /*!50503 SET character_set_client = utf8mb4 */; CREATE TABLE `account` ( `id` int NOT NULL AUTO_INCREMENT, `name` varchar(255) NOT NULL, `balance` int NOT NULL, PRIMARY KEY (`id`) ) ENGINE=InnoDB AUTO_INCREMENT=4 DEFAULT CHARSET=utf8mb3; /*!40101 SET character_set_client = @saved_cs_client */; /*!40103 SET TIME_ZONE=@OLD_TIME_ZONE */; /*!40101 SET SQL_MODE=@OLD_SQL_MODE */; /*!40101 SET CHARACTER_SET_CLIENT=@OLD_CHARACTER_SET_CLIENT */; /*!40101 SET CHARACTER_SET_RESULTS=@OLD_CHARACTER_SET_RESULTS */; /*!40101 SET COLLATION_CONNECTION=@OLD_COLLATION_CONNECTION */; /*!40111 SET SQL_NOTES=@OLD_SQL_NOTES */; -- Dump completed on 2022-01-07 23:19:27 打开account.txt文件，其内容只包含account表中的数据。 [root@node1 mysql-files]# cat account.txt 1 张三 90 2 李四 100 3 王五 0 **举例2：**使用mysqldump将atguigu数据库中的account表导出到文本文件，使用FIELDS选项，要求字段之 间使用逗号“，”间隔，所有字符类型字段值用双引号括起来： mysqldump -uroot -p -T \"/var/lib/mysql-files/\" atguigu account --fields-terminatedby=',' --fields-optionally-enclosed-by='\\\"' 语句mysqldump语句执行成功之后，指定目录下会出现两个文件account.sql和account.txt。 打开account.sql文件，其内容包含创建account表的CREATE语句。 [root@node1 mysql-files]# cat account.sql -- MySQL dump 10.13 Distrib 8.0.26, for Linux (x86_64) -- -- Host: localhost Database: atguigu -- ------------------------------------------------------ -- Server version 8.0.26 /*!40101 SET @OLD_CHARACTER_SET_CLIENT=@@CHARACTER_SET_CLIENT */; /*!40101 SET @OLD_CHARACTER_SET_RESULTS=@@CHARACTER_SET_RESULTS */; /*!40101 SET @OLD_COLLATION_CONNECTION=@@COLLATION_CONNECTION */; /*!50503 SET NAMES utf8mb4 */; /*!40103 SET @OLD_TIME_ZONE=@@TIME_ZONE */; /*!40103 SET TIME_ZONE='+00:00' */; /*!40101 SET @OLD_SQL_MODE=@@SQL_MODE, SQL_MODE='' */; /*!40111 SET @OLD_SQL_NOTES=@@SQL_NOTES, SQL_NOTES=0 */; -- -- Table structure for table `account` -- DROP TABLE IF EXISTS `account`; /*!40101 SET @saved_cs_client = @@character_set_client */; /*!50503 SET character_set_client = utf8mb4 */; CREATE TABLE","date":"2022-10-22","objectID":"/mysql%E6%97%A5%E5%BF%97%E4%B8%8E%E5%A4%87%E4%BB%BD%E7%AF%87/:18:1","tags":[],"title":"MySQL日志与备份篇","uri":"/mysql%E6%97%A5%E5%BF%97%E4%B8%8E%E5%A4%87%E4%BB%BD%E7%AF%87/"},{"categories":["mysql"],"content":"6.2 表的导入 1. 使用LOAD DATA INFILE方式导入文本文件 举例1： 使用SELECT…INTO OUTFILE将atguigu数据库中account表的记录导出到文本文件 SELECT * FROM atguigu.account INTO OUTFILE '/var/lib/mysql-files/account_0.txt'; 删除account表中的数据： DELETE FROM atguigu.account; 从文本文件account.txt中恢复数据： LOAD DATA INFILE '/var/lib/mysql-files/account_0.txt' INTO TABLE atguigu.account; 查询account表中的数据： mysql\u003e select * from account; +----+--------+---------+ | id | name | balance | +----+--------+---------+ | 1 | 张三 | 90 | | 2 | 李四 | 100 | | 3 | 王五 | 0 | +----+--------+---------+ 3 rows in set (0.00 sec) 举例2： 选择数据库atguigu，使用SELECT…INTO OUTFILE将atguigu数据库account表中的记录导出到文本文件，使用FIELDS选项和LINES选项，要求字段之间使用逗号\"，“间隔，所有字段值用双引号括起来： SELECT * FROM atguigu.account INTO OUTFILE '/var/lib/mysql-files/account_1.txt' FIELDS TERMINATED BY ',' ENCLOSED BY '\\\"'; 删除account表中的数据： DELETE FROM atguigu.account; 从/var/lib/mysql-files/account.txt中导入数据到account表中： LOAD DATA INFILE '/var/lib/mysql-files/account_1.txt' INTO TABLE atguigu.account FIELDS TERMINATED BY ',' ENCLOSED BY '\\\"'; 查询account表中的数据，具体SQL如下： select * from account; mysql\u003e select * from account; +----+--------+---------+ | id | name | balance | +----+--------+---------+ | 1 | 张三 | 90 | | 2 | 李四 | 100 | | 3 | 王五 | 0 | +----+--------+---------+ 3 rows in set (0.00 sec) 2. 使用mysqlimport方式导入文本文件 举例： 导出文件account.txt，字段之间使用逗号”，“间隔，字段值用双引号括起来： SELECT * FROM atguigu.account INTO OUTFILE '/var/lib/mysql-files/account.txt' FIELDS TERMINATED BY ',' ENCLOSED BY '\\\"'; 删除account表中的数据： DELETE FROM atguigu.account; 使用mysqlimport命令将account.txt文件内容导入到数据库atguigu的account表中： mysqlimport -uroot -p atguigu '/var/lib/mysql-files/account.txt' --fields-terminated-by=',' --fields-optionally-enclosed-by='\\\"' 查询account表中的数据： select * from account; mysql\u003e select * from account; +----+--------+---------+ | id | name | balance | +----+--------+---------+ | 1 | 张三 | 90 | | 2 | 李四 | 100 | | 3 | 王五 | 0 | +----+--------+---------+ 3 rows in set (0.00 sec) ","date":"2022-10-22","objectID":"/mysql%E6%97%A5%E5%BF%97%E4%B8%8E%E5%A4%87%E4%BB%BD%E7%AF%87/:18:2","tags":[],"title":"MySQL日志与备份篇","uri":"/mysql%E6%97%A5%E5%BF%97%E4%B8%8E%E5%A4%87%E4%BB%BD%E7%AF%87/"},{"categories":["mysql"],"content":"7. 数据库迁移 ","date":"2022-10-22","objectID":"/mysql%E6%97%A5%E5%BF%97%E4%B8%8E%E5%A4%87%E4%BB%BD%E7%AF%87/:19:0","tags":[],"title":"MySQL日志与备份篇","uri":"/mysql%E6%97%A5%E5%BF%97%E4%B8%8E%E5%A4%87%E4%BB%BD%E7%AF%87/"},{"categories":["mysql"],"content":"7.1 概述 数据迁移（data migration）是指选择、准备、提取和转换数据，并将数据从一个计算机存储系统永久地传输到另一个计算机存储系统的过程。此外， 验证迁移数据的完整性 和 退役原来旧的数据存储 ，也被认为是整个数据迁移过程的一部分。 数据库迁移的原因是多样的，包括服务器或存储设备更换、维护或升级，应用程序迁移，网站集成，灾难恢复和数据中心迁移。 根据不同的需求可能要采取不同的迁移方案，但总体来讲，MySQL 数据迁移方案大致可以分为物理迁移和 逻辑迁移 两类。通常以尽可能 自动化 的方式执行，从而将人力资源从繁琐的任务中解放出来。 ","date":"2022-10-22","objectID":"/mysql%E6%97%A5%E5%BF%97%E4%B8%8E%E5%A4%87%E4%BB%BD%E7%AF%87/:19:1","tags":[],"title":"MySQL日志与备份篇","uri":"/mysql%E6%97%A5%E5%BF%97%E4%B8%8E%E5%A4%87%E4%BB%BD%E7%AF%87/"},{"categories":["mysql"],"content":"7.2 迁移方案 物理迁移 物理迁移适用于大数据量下的整体迁移。使用物理迁移方案的优点是比较快速，但需要停机迁移并且要 求 MySQL 版本及配置必须和原服务器相同，也可能引起未知问题。 物理迁移包括拷贝数据文件和使用 XtraBackup 备份工具两种。 不同服务器之间可以采用物理迁移，我们可以在新的服务器上安装好同版本的数据库软件，创建好相同目录，建议配置文件也要和原数据库相同，然后从原数据库方拷贝来数据文件及日志文件，配置好文件组权限，之后在新服务器这边使用 mysqld 命令启动数据库。 逻辑迁移 逻辑迁移适用范围更广，无论是 部分迁移 还是 全量迁移 ，都可以使用逻辑迁移。逻辑迁移中使用最多的就是通过 mysqldump 等备份工具。 ","date":"2022-10-22","objectID":"/mysql%E6%97%A5%E5%BF%97%E4%B8%8E%E5%A4%87%E4%BB%BD%E7%AF%87/:19:2","tags":[],"title":"MySQL日志与备份篇","uri":"/mysql%E6%97%A5%E5%BF%97%E4%B8%8E%E5%A4%87%E4%BB%BD%E7%AF%87/"},{"categories":["mysql"],"content":"7.3 迁移注意点 1. 相同版本的数据库之间迁移注意点 指的是在主版本号相同的MySQL数据库之间进行数据库移动。 方式1： 因为迁移前后MySQL数据库的 主版本号相同 ，所以可以通过复制数据库目录来实现数据库迁移，但是物理迁移方式只适用于MyISAM引擎的表。对于InnoDB表，不能用直接复制文件的方式备份数据库。 方式2： 最常见和最安全的方式是使用 mysqldump命令 导出数据，然后在目标数据库服务器中使用 MySQL命令导入。 举例： #host1的机器中备份所有数据库,并将数据库迁移到名为host2的机器上 mysqldump –h host1 –uroot –p –-all-databases| mysql –h host2 –uroot –p 在上述语句中，“|”符号表示管道，其作用是将mysqldump备份的文件给mysql命令；“–all-databases”表示要迁移所有的数据库。通过这种方式可以直接实现迁移。 2. 不同版本的数据库之间迁移注意点 例如，原来很多服务器使用5.7版本的MySQL数据库，在8.0版本推出来以后，改进了5.7版本的很多缺陷， 因此需要把数据库升级到8.0版本 旧版本与新版本的MySQL可能使用不同的默认字符集，例如有的旧版本中使用latin1作为默认字符集，而最新版本的MySQL默认字符集为utf8mb4。如果数据库中有中文数据，那么迁移过程中需要对 默认字符集 进行修改 ，不然可能无法正常显示数据。 高版本的MySQL数据库通常都会 兼容低版本 ，因此可以从低版本的MySQL数据库迁移到高版本的MySQL 数据库。 3. 不同数据库之间迁移注意点 不同数据库之间迁移是指从其他类型的数据库迁移到MySQL数据库，或者从MySQL数据库迁移到其他类 型的数据库。这种迁移没有普适的解决方法。 迁移之前，需要了解不同数据库的架构， 比较它们之间的差异 。不同数据库中定义相同类型的数据的 关键字可能会不同 。例如，MySQL中日期字段分为DATE和TIME两种，而ORACLE日期字段只有DATE；SQL Server数据库中有ntext、Image等数据类型，MySQL数据库没有这些数据类型；MySQL支持的ENUM和SET 类型，这些SQL Server数据库不支持。 另外，数据库厂商并没有完全按照SQL标准来设计数据库系统，导致不同的数据库系统的 SQL语句 有差别。例如，微软的SQL Server软件使用的是T-SQL语句，T-SQL中包含了非标准的SQL语句，不能和MySQL的SQL语句兼容。 不同类型数据库之间的差异造成了互相 迁移的困难 ，这些差异其实是商业公司故意造成的技术壁垒。但 是不同类型的数据库之间的迁移并 不是完全不可能 。例如，可以使用 MyODBC 实现MySQL和SQL Server之 间的迁移。MySQL官方提供的工具 MySQL Migration Toolkit 也可以在不同数据之间进行数据迁移。 MySQL迁移到Oracle时，需要使用mysqldump命令导出sql文件，然后， 手动更改 sql文件中的CREATE语句。 ","date":"2022-10-22","objectID":"/mysql%E6%97%A5%E5%BF%97%E4%B8%8E%E5%A4%87%E4%BB%BD%E7%AF%87/:19:3","tags":[],"title":"MySQL日志与备份篇","uri":"/mysql%E6%97%A5%E5%BF%97%E4%B8%8E%E5%A4%87%E4%BB%BD%E7%AF%87/"},{"categories":["mysql"],"content":"7.4 迁移小结 ","date":"2022-10-22","objectID":"/mysql%E6%97%A5%E5%BF%97%E4%B8%8E%E5%A4%87%E4%BB%BD%E7%AF%87/:19:4","tags":[],"title":"MySQL日志与备份篇","uri":"/mysql%E6%97%A5%E5%BF%97%E4%B8%8E%E5%A4%87%E4%BB%BD%E7%AF%87/"},{"categories":["mysql"],"content":"8. 删库了不敢跑，能干点啥？ ","date":"2022-10-22","objectID":"/mysql%E6%97%A5%E5%BF%97%E4%B8%8E%E5%A4%87%E4%BB%BD%E7%AF%87/:20:0","tags":[],"title":"MySQL日志与备份篇","uri":"/mysql%E6%97%A5%E5%BF%97%E4%B8%8E%E5%A4%87%E4%BB%BD%E7%AF%87/"},{"categories":["mysql"],"content":"8.1 delete：误删行 ","date":"2022-10-22","objectID":"/mysql%E6%97%A5%E5%BF%97%E4%B8%8E%E5%A4%87%E4%BB%BD%E7%AF%87/:20:1","tags":[],"title":"MySQL日志与备份篇","uri":"/mysql%E6%97%A5%E5%BF%97%E4%B8%8E%E5%A4%87%E4%BB%BD%E7%AF%87/"},{"categories":["mysql"],"content":"8.2 truncate/drop ：误删库/表 ","date":"2022-10-22","objectID":"/mysql%E6%97%A5%E5%BF%97%E4%B8%8E%E5%A4%87%E4%BB%BD%E7%AF%87/:20:2","tags":[],"title":"MySQL日志与备份篇","uri":"/mysql%E6%97%A5%E5%BF%97%E4%B8%8E%E5%A4%87%E4%BB%BD%E7%AF%87/"},{"categories":["mysql"],"content":"8.3 预防使用truncate/drop误删库/表 ","date":"2022-10-22","objectID":"/mysql%E6%97%A5%E5%BF%97%E4%B8%8E%E5%A4%87%E4%BB%BD%E7%AF%87/:20:3","tags":[],"title":"MySQL日志与备份篇","uri":"/mysql%E6%97%A5%E5%BF%97%E4%B8%8E%E5%A4%87%E4%BB%BD%E7%AF%87/"},{"categories":["mysql"],"content":"8.4 rm：误删MySQL实例 对于一个有高可用机制的MySQL集群来说，不用担心 rm删除数据 了。只是删掉了其中某一个节点的数据的话，HA系统就会开始工作，选出一个新的主库，从而保证整个集群的正常工作。我们要做的就是在这个节点上把数据恢复回来，再接入整个集群。 但如果是恶意地把整个集群删除，那就需要考虑跨机房备份，跨城市备份。 ","date":"2022-10-22","objectID":"/mysql%E6%97%A5%E5%BF%97%E4%B8%8E%E5%A4%87%E4%BB%BD%E7%AF%87/:20:4","tags":[],"title":"MySQL日志与备份篇","uri":"/mysql%E6%97%A5%E5%BF%97%E4%B8%8E%E5%A4%87%E4%BB%BD%E7%AF%87/"},{"categories":["mysql"],"content":"[toc] 第13章_事务基础知识 ","date":"2022-10-20","objectID":"/mysql%E4%BA%8B%E5%8A%A1%E7%AF%87/:0:0","tags":[],"title":"MySQL事务篇","uri":"/mysql%E4%BA%8B%E5%8A%A1%E7%AF%87/"},{"categories":["mysql"],"content":"1. 数据库事务概述 ","date":"2022-10-20","objectID":"/mysql%E4%BA%8B%E5%8A%A1%E7%AF%87/:1:0","tags":[],"title":"MySQL事务篇","uri":"/mysql%E4%BA%8B%E5%8A%A1%E7%AF%87/"},{"categories":["mysql"],"content":"1.1 存储引擎支持情况 SHOW ENGINES 命令来查看当前 MySQL 支持的存储引擎都有哪些，以及这些存储引擎是否支持事务。 能看出在 MySQL 中，只有InnoDB 是支持事务的。 ","date":"2022-10-20","objectID":"/mysql%E4%BA%8B%E5%8A%A1%E7%AF%87/:1:1","tags":[],"title":"MySQL事务篇","uri":"/mysql%E4%BA%8B%E5%8A%A1%E7%AF%87/"},{"categories":["mysql"],"content":"1.2 基本概念 **事务：**一组逻辑操作单元，使数据从一种状态变换到另一种状态。 **事务处理的原则：**保证所有事务都作为 一个工作单元 来执行，即使出现了故障，都不能改变这种执行方 式。当在一个事务中执行多个操作时，要么所有的事务都被提交( commit )，那么这些修改就 永久 地保 存下来；要么数据库管理系统将 放弃 所作的所有 修改 ，整个事务回滚( rollback )到最初状态。 # 案例：AA用户给BB用户转账100 update account set money = money - 100 where name = 'AA'; # 服务器宕机 update account set money = money + 100 where name = 'BB'; ","date":"2022-10-20","objectID":"/mysql%E4%BA%8B%E5%8A%A1%E7%AF%87/:1:2","tags":[],"title":"MySQL事务篇","uri":"/mysql%E4%BA%8B%E5%8A%A1%E7%AF%87/"},{"categories":["mysql"],"content":"1.3 事物的ACID特性 原子性（atomicity）： 原子性是指事务是一个不可分割的工作单位，要么全部提交，要么全部失败回滚。即要么转账成功，要么转账失败，是不存在中间的状态。如果无法保证原子性会怎么样？就会出现数据不一致的情形，A账户减去100元，而B账户增加100元操作失败，系统将无故丢失100元。 一致性（consistency）： （国内很多网站上对一致性的阐述有误，具体你可以参考 Wikipedia 对Consistency的阐述） 根据定义，**一致性是指事务执行前后，数据从一个 合法性状态 变换到另外一个 合法性状态 。**这种状态是 语义上 的而不是语法上的，跟具体的业务有关。 那什么是合法的数据状态呢？**满足 预定的约束 的状态就叫做合法的状态。**通俗一点，这状态是由你自己来定义的（比如满足现实世界中的约束）。满足这个状态，数据就是一致的，不满足这个状态，数据就 是不一致的！如果事务中的某个操作失败了，系统就会自动撤销当前正在执行的事务，返回到事务操作 之前的状态。 **举例1：**A账户有200元，转账300元出去，此时A账户余额为-100元。你自然就发现此时数据是不一致的，为什么呢？因为你定义了一个状态，余额这列必须\u003e=0。 **举例2：**A账户有200元，转账50元给B账户，A账户的钱扣了，但是B账户因为各种意外，余额并没有增加。你也知道此时的数据是不一致的，为什么呢？因为你定义了一个状态，要求A+B的总余额必须不变。 **举例3：**在数据表中我们将姓名字段设置为唯一性约束，这时当事务进行提交或者事务发生回滚的时候，如果数据表的姓名不唯一，就破坏了事物的一致性要求。 隔离型（isolation）： 事务的隔离性是指一个事务的执行**不能被其他事务干扰**，即一个事务内部的操作及使用的数据对并发的其他事务是隔离的，并发执行的各个事务之间不能相互干扰。 如果无法保证隔离性会怎么样？假设A账户有200元，B账户0元。A账户往B账户转账两次，每次金额为50元，分别在两个事务中执行。如果无法保证隔离性，会出现下面的情形： UPDATE accounts SET money = money - 50 WHERE NAME = 'AA'; UPDATE accounts SET money = money + 50 WHERE NAME = 'BB'; 持久性（durability）： 持久性是指一个事务一旦被提交，它对数据库中数据的改变就是永久性的 ，接下来的其他操作和数据库故障不应该对其有任何影响。 **持久性是通过 事务日志 来保证的。**日志包括了 重做日志 和 回滚日志 。当我们通过事务对数据进行修改 的时候，首先会将数据库的变化信息记录到重做日志中，然后再对数据库中对应的行进行修改。这样做的好处是，即使数据库系统崩溃，数据库重启后也能找到没有更新到数据库系统中的重做日志，重新执行，从而使事务具有持久性。 总结 ACID是事务的四大特征，在这四个特性中，原子性是基础，隔离性是手段，一致性是约束条件， 而持久性是我们的目的。 数据库事务，其实就是数据库设计者为了方便起见，把需要保证原子性、隔离性、一致性和持久性的一个或多个数据库操作称为一个事务。 ","date":"2022-10-20","objectID":"/mysql%E4%BA%8B%E5%8A%A1%E7%AF%87/:1:3","tags":[],"title":"MySQL事务篇","uri":"/mysql%E4%BA%8B%E5%8A%A1%E7%AF%87/"},{"categories":["mysql"],"content":"1.4 事务的状态 我们现在知道 事务 是一个抽象的概念，它其实对应着一个或多个数据库操作，MySQL根据这些操作所执 行的不同阶段把 事务 大致划分成几个状态： 活动的（active） 事务对应的数据库操作正在执行过程中时，我们就说该事务处在 活动的 状态。 部分提交的（partially committed） 当事务中的最后一个操作执行完成，但由于操作都在内存中执行，所造成的影响并 没有刷新到磁盘 时，我们就说该事务处在 部分提交的 状态。 失败的（failed） 当事务处在 活动的 或者 部分提交的 状态时，可能遇到了某些错误（数据库自身的错误、操作系统 错误或者直接断电等）而无法继续执行，或者人为的停止当前事务的执行，我们就说该事务处在 失败的 状态。 中止的（aborted） 如果事务执行了一部分而变为 失败的 状态，那么就需要把已经修改的事务中的操作还原到事务执行前的状态。换句话说，就是要撤销失败事务对当前数据库造成的影响。我们把这个撤销的过程称之为 回滚 。当 回滚 操作执行完毕时，也就是数据库恢复到了执行事务之前的状态，我们就说该事务处在了 中止的 状态。 举例： UPDATE accounts SET money = money - 50 WHERE NAME = 'AA'; UPDATE accounts SET money = money + 50 WHERE NAME = 'BB'; 提交的（committed） 当一个处在 部分提交的 状态的事务将修改过的数据都 同步到磁盘 上之后，我们就可以说该事务处在了 提交的 状态。 一个基本的状态转换图如下所示： 图中可见，只有当事物处于提交的或者中止的状态时，一个事务的生命周期才算是结束了。对于已经提交的事务来说，该事务对数据库所做的修改将永久生效，对于处于中止状态的事物，该事务对数据库所做的所有修改都会被回滚到没执行该事物之前的状态。 ","date":"2022-10-20","objectID":"/mysql%E4%BA%8B%E5%8A%A1%E7%AF%87/:1:4","tags":[],"title":"MySQL事务篇","uri":"/mysql%E4%BA%8B%E5%8A%A1%E7%AF%87/"},{"categories":["mysql"],"content":"2. 如何使用事务 使用事务有两种方式，分别为 显式事务 和 隐式事务 。 ","date":"2022-10-20","objectID":"/mysql%E4%BA%8B%E5%8A%A1%E7%AF%87/:2:0","tags":[],"title":"MySQL事务篇","uri":"/mysql%E4%BA%8B%E5%8A%A1%E7%AF%87/"},{"categories":["mysql"],"content":"2.1 显式事务 步骤1： START TRANSACTION 或者 BEGIN ，作用是显式开启一个事务。 mysql\u003e BEGIN; #或者 mysql\u003e START TRANSACTION; START TRANSACTION 语句相较于 BEGIN 特别之处在于，后边能跟随几个 修饰符 ： ① READ ONLY ：标识当前事务是一个 只读事务 ，也就是属于该事务的数据库操作只能读取数据，而不能修改数据。 补充：只读事务中只是不允许修改那些其他事务也能访问到的表中的数据，对于临时表来说（我们使用 CREATE TMEPORARY TABLE 创建的表），由于它们只能再当前会话中可见，所有只读事务其实也是可以对临时表进行增、删、改操作的。 ② READ WRITE ：标识当前事务是一个 读写事务 ，也就是属于该事务的数据库操作既可以读取数据， 也可以修改数据。 ③ WITH CONSISTENT SNAPSHOT ：启动一致性读。 比如： START TRANSACTION READ ONLY; # 开启一个只读事务 START TRANSACTION READ ONLY, WITH CONSISTENT SNAPSHOT # 开启只读事务和一致性读 START TRANSACTION READ WRITE, WITH CONSISTENT SNAPSHOT # 开启读写事务和一致性读 注意： READ ONLY和READ WRITE是用来设置所谓的事物访问模式的，就是以只读还是读写的方式来访问数据库中的数据，一个事务的访问模式不能同时即设置为只读的也设置为读写的，所以不能同时把READ ONLY和READ WRITE放到START TRANSACTION语句后边。 如果我们不显式指定事务的访问模式，那么该事务的访问模式就是读写模式 **步骤2：**一系列事务中的操作（主要是DML，不含DDL） **步骤3：**提交事务 或 中止事务（即回滚事务） # 提交事务。当提交事务后，对数据库的修改是永久性的。 mysql\u003e COMMIT; # 回滚事务。即撤销正在进行的所有没有提交的修改 mysql\u003e ROLLBACK; # 将事务回滚到某个保存点。 mysql\u003e ROLLBACK TO [SAVEPOINT] 其中关于SAVEPOINT相关操作有： # 在事务中创建保存点，方便后续针对保存点进行回滚。一个事务中可以存在多个保存点。 SAVEPOINT 保存点名称; # 删除某个保存点 RELEASE SAVEPOINT 保存点名称; ","date":"2022-10-20","objectID":"/mysql%E4%BA%8B%E5%8A%A1%E7%AF%87/:2:1","tags":[],"title":"MySQL事务篇","uri":"/mysql%E4%BA%8B%E5%8A%A1%E7%AF%87/"},{"categories":["mysql"],"content":"2.2 隐式事务 MySQL中有一个系统变量 autocommit ： mysql\u003e SHOW VARIABLES LIKE 'autocommit'; +---------------+-------+ | Variable_name | Value | +---------------+-------+ | autocommit | ON | +---------------+-------+ 1 row in set (0.01 sec) 当然，如果我们想关闭这种 自动提交 的功能，可以使用下边两种方法之一： 显式的的使用 START TRANSACTION 或者 BEGIN 语句开启一个事务。这样在本次事务提交或者回滚前会暂时关闭掉自动提交的功能。 把系统变量 autocommit 的值设置为 OFF ，就像这样： SET autocommit = OFF; #或 SET autocommit = 0; ","date":"2022-10-20","objectID":"/mysql%E4%BA%8B%E5%8A%A1%E7%AF%87/:2:2","tags":[],"title":"MySQL事务篇","uri":"/mysql%E4%BA%8B%E5%8A%A1%E7%AF%87/"},{"categories":["mysql"],"content":"2.3 隐式提交数据的情况 数据定义语言（Data definition language，缩写为：DDL） 数据库对象，指的就是数据库、表、视图、存储过程等结构。当我们CREATE、ALTER、DROP等语句去修改数据库对象时，就会隐式的提交前边语句所属于的事物。即： BEGIN; SELECT ... # 事务中的一条语句 UPDATE ... # 事务中的一条语句 ... # 事务中的其他语句 CREATE TABLE ... # 此语句会隐式的提交前边语句所属于的事务 隐式使用或修改mysql数据库中的表 当我们使用ALTER USER、CREATE USER、DROP USER、GRANT、RENAME USER、REVOKE、SET PASSWORD等语句时也会隐式的提交前边语句所属于的事务。 事务控制或关于锁定的语句 ① 当我们在一个事务还没提交或者回滚时就又使用 START TRANSACTION 或者 BEGIN 语句开启了另一个事务时，会隐式的提交上一个事务。即： BEGIN; SELECT ... # 事务中的一条语句 UPDATE ... # 事务中的一条语句 ... # 事务中的其他语句 BEGIN; # 此语句会隐式的提交前边语句所属于的事务 ② 当前的 autocommit 系统变量的值为 OFF ，我们手动把它调为 ON 时，也会隐式的提交前边语句所属的事务。 ③ 使用 LOCK TABLES 、 UNLOCK TABLES 等关于锁定的语句也会隐式的提交 前边语句所属的事务。 加载数据的语句 使用LOAD DATA语句来批量往数据库中导入数据时，也会隐式的提交前边语句所属的事务。 关于MySQL复制的一些语句 使用START SLAVE、STOP SLAVE、RESET SLAVE、CHANGE MASTER TO等语句会隐式的提交前边语句所属的事务 其他的一些语句 使用ANALYZE TABLE、CACHE INDEX、CAECK TABLE、FLUSH、LOAD INDEX INTO CACHE、OPTIMIZE TABLE、REPAIR TABLE、RESET等语句也会隐式的提交前边语句所属的事务。 ","date":"2022-10-20","objectID":"/mysql%E4%BA%8B%E5%8A%A1%E7%AF%87/:2:3","tags":[],"title":"MySQL事务篇","uri":"/mysql%E4%BA%8B%E5%8A%A1%E7%AF%87/"},{"categories":["mysql"],"content":"2.4 使用举例1：提交与回滚 我们看下在 MySQL 的默认状态下，下面这个事务最后的处理结果是什么。 情况1： CREATE TABLE user(name varchar(20), PRIMARY KEY (name)) ENGINE=InnoDB; BEGIN; INSERT INTO user SELECT '张三'; COMMIT; BEGIN; INSERT INTO user SELECT '李四'; INSERT INTO user SELECT '李四'; ROLLBACK; # 回滚到最近的一个提交的事务 SELECT * FROM user; 运行结果（1 行数据）： mysql\u003e commit; Query OK, 0 rows affected (0.00 秒) mysql\u003e BEGIN; Query OK, 0 rows affected (0.00 秒) mysql\u003e INSERT INTO user SELECT '李四'; Query OK, 1 rows affected (0.00 秒) mysql\u003e INSERT INTO user SELECT '李四'; Duplicate entry '李四' for key 'user.PRIMARY' mysql\u003e ROLLBACK; Query OK, 0 rows affected (0.01 秒) mysql\u003e select * from user; +--------+ | name | +--------+ | 张三 | +--------+ 1 行于数据集 (0.01 秒) 情况2： CREATE TABLE user (name varchar(20), PRIMARY KEY (name)) ENGINE=InnoDB; BEGIN; INSERT INTO user SELECT '张三'; COMMIT; INSERT INTO user SELECT '李四'; # 此条语句单独标识一个事务，自动开启事务并提交 INSERT INTO user SELECT '李四'; # 此条语句单独标识一个事务，自动开启事务并提交 ROLLBACK; # 回滚到最近的一个事务，也就是失败的插入语句前的状态。因此查询数据，没有变化 运行结果（2 行数据）： mysql\u003e SELECT * FROM user; +--------+ | name | +--------+ | 张三 | | 李四 | +--------+ 2 行于数据集 (0.01 秒) 情况3： CREATE TABLE user(name varchar(255), PRIMARY KEY (name)) ENGINE=InnoDB; SET @@completion_type = 1; BEGIN; INSERT INTO user SELECT '张三'; COMMIT; INSERT INTO user SELECT '李四'; INSERT INTO user SELECT '李四'; ROLLBACK; SELECT * FROM user; 运行结果（1 行数据）： mysql\u003e SELECT * FROM user; +--------+ | name | +--------+ | 张三 | +--------+ 1 行于数据集 (0.01 秒) 当我们设置 autocommit=0 时，不论是否采用 START TRANSACTION 或者 BEGIN 的方式来开启事务，都需要用 COMMIT 进行提交，让事务生效，使用 ROLLBACK 对事务进行回滚。 当我们设置 autocommit=1 时，每条 SQL 语句都会自动进行提交。 不过这时，如果你采用 START TRANSACTION 或者 BEGIN 的方式来显式地开启事务，那么这个事务只有在 COMMIT 时才会生效， 在 ROLLBACK 时才会回滚。 ","date":"2022-10-20","objectID":"/mysql%E4%BA%8B%E5%8A%A1%E7%AF%87/:2:4","tags":[],"title":"MySQL事务篇","uri":"/mysql%E4%BA%8B%E5%8A%A1%E7%AF%87/"},{"categories":["mysql"],"content":"2.5 使用举例2：测试不支持事务的engine CREATE TABLE test1(i INT) ENGINE=InnoDB; CREATE TABLE test2(i INT) ENGINE=MYISAM; 针对于InnoDB表 BEGIN; INSERT INTO test1 VALUES(1); ROLLBACK; SELECT * FROM test1; 结果：没有数据 针对于MYISAM表： BEGIN; INSERT INTO test2 VALUES(1); ROLLBACK; SELECT * FROM test2; 结果：有一条数据 ","date":"2022-10-20","objectID":"/mysql%E4%BA%8B%E5%8A%A1%E7%AF%87/:2:5","tags":[],"title":"MySQL事务篇","uri":"/mysql%E4%BA%8B%E5%8A%A1%E7%AF%87/"},{"categories":["mysql"],"content":"2.6 使用举例3：SAVEPOINT 创建表并添加数据： CREATE TABLE account( id INT PRIMARY KEY AUTO_INCREMENT, NAME VARCHAR(15), balance DECIMAL(10,2) ); INSERT INTO account(NAME,balance) VALUES ('张三',1000), ('李四',1000); BEGIN; UPDATE account SET balance = balance - 100 WHERE NAME = '张三'; UPDATE account SET balance = balance - 100 WHERE NAME = '张三'; SAVEPOINT s1; # 设置保存点 UPDATE account SET balance = balance + 1 WHERE NAME = '张三'; ROLLBACK TO s1; # 回滚到保存点 结果：张三：800.00 ROLLBACK; 结果：张三：1000.00 ","date":"2022-10-20","objectID":"/mysql%E4%BA%8B%E5%8A%A1%E7%AF%87/:2:6","tags":[],"title":"MySQL事务篇","uri":"/mysql%E4%BA%8B%E5%8A%A1%E7%AF%87/"},{"categories":["mysql"],"content":"3. 事务隔离级别 MySQL是一个 客户端／服务器 架构的软件，对于同一个服务器来说，可以有若干个客户端与之连接，每个客户端与服务器连接上之后，就可以称为一个会话（ Session ）。每个客户端都可以在自己的会话中 向服务器发出请求语句，一个请求语句可能是某个事务的一部分，也就是对于服务器来说可能同时处理多个事务。事务有 隔离性 的特性，理论上在某个事务 对某个数据进行访问 时，其他事务应该进行排队 ，当该事务提交之后，其他事务才可以继续访问这个数据。但是这样对 性能影响太大 ，我们既想保持事务的隔离性，又想让服务器在处理访问同一数据的多个事务时 性能尽量高些 ，那就看二者如何权衡取舍了。 ","date":"2022-10-20","objectID":"/mysql%E4%BA%8B%E5%8A%A1%E7%AF%87/:3:0","tags":[],"title":"MySQL事务篇","uri":"/mysql%E4%BA%8B%E5%8A%A1%E7%AF%87/"},{"categories":["mysql"],"content":"3.1 数据准备 CREATE TABLE student ( studentno INT, name VARCHAR(20), class varchar(20), PRIMARY KEY (studentno) ) Engine=InnoDB CHARSET=utf8; 然后向这个表里插入一条数据： INSERT INTO student VALUES(1, '小谷', '1班'); 现在表里的数据就是这样的： mysql\u003e select * from student; +-----------+--------+-------+ | studentno | name | class | +-----------+--------+-------+ | 1 | 小谷 | 1班 | +-----------+--------+-------+ 1 row in set (0.00 sec) ","date":"2022-10-20","objectID":"/mysql%E4%BA%8B%E5%8A%A1%E7%AF%87/:3:1","tags":[],"title":"MySQL事务篇","uri":"/mysql%E4%BA%8B%E5%8A%A1%E7%AF%87/"},{"categories":["mysql"],"content":"3.2 数据并发问题 针对事务的隔离性和并发性，我们怎么做取舍呢？先看一下访问相同数据的事务在不保证串行执行（也就是执行完一个再执行另一个）的情况下可能会出现哪些问题： 1. 脏写（ Dirty Write ） 对于两个事务 Session A、Session B，如果事务Session A 修改了 另一个 未提交 事务Session B 修改过 的数据，之后由于回滚，导致修改无效，那就意味着发生了 脏写–写无效，示意图如下： Session A 和 Session B 各开启了一个事务，Session B 中的事务先将studentno列为1的记录的name列更新为’李四’，然后Session A中的事务接着又把这条studentno列为1的记录的name列更新为’张三’。如果之后Session B中的事务进行了回滚，那么Session A中的更新也将不复存在，这种现象称之为脏写。这时Session A中的事务就没有效果了，明明把数据更新了，最后也提交事务了，最后看到的数据什么变化也没有。这里大家对事务的隔离性比较了解的话，会发现默认隔离级别下，上面Session A中的更新语句会处于等待状态，这里只是跟大家说明一下会出现这样的现象。 2. 脏读（ Dirty Read ） 对于两个事务 Session A、Session B，Session A 读取 了已经被 Session B 更新 但还 没有被提交 的字段。 之后若 Session B 回滚 ，Session A 读取 的内容就是 临时且无效 的–读无效。 Session A和Session B各开启了一个事务，Session B中的事务先将studentno列为1的记录的name列更新 为’张三’，然后Session A中的事务再去查询这条studentno为1的记录，如果读到列name的值为’张三’，而 Session B中的事务稍后进行了回滚，那么Session A中的事务相当于读到了一个不存在的数据，这种现象就称之为 脏读 。 3. 不可重复读（ Non-Repeatable Read ） 对于两个事务Session A、Session B，Session A 读取了一个字段，然后 Session B 更新了该字段。 之后 Session A 再次读取 同一个字段，值就不同了。那就意味着发生了不可重复读。–重复读值不同 我们在Session B中提交了几个 隐式事务 （注意是隐式事务，意味着语句结束事务就提交了），这些事务都修改了studentno列为1的记录的列name的值，每次事务提交之后，如果Session A中的事务都可以查看到最新的值，这种现象也被称之为 不可重复读 。 4. 幻读（ Phantom ） 对于两个事务Session A、Session B, Session A 从一个表中 读取 了一个字段, 然后 Session B 在该表中插入了一些新的行。之后, 如果 Session A 再次读取 同一个表, 就会多出几行–重复读多出数据行。那就意味着发生了幻读。 Session A 中的事务先根据条件 studentno \u003e 0 这个条件查询表 student ，得到了name列值为’张三’的记录； 之后Session B中提交了一个 隐式事务 ，该事务向表student中插入了一条新记录；之后Session A中的事务 再根据相同的条件 studentno \u003e 0查询表student，得到的结果集中包含Session B中的事务新插入的那条记录，这种现象也被称之为 幻读 。我们把新插入的那些记录称之为 幻影记录 。 ","date":"2022-10-20","objectID":"/mysql%E4%BA%8B%E5%8A%A1%E7%AF%87/:3:2","tags":[],"title":"MySQL事务篇","uri":"/mysql%E4%BA%8B%E5%8A%A1%E7%AF%87/"},{"categories":["mysql"],"content":"3.3 SQL中的四种隔离级别 上面介绍了几种并发事务执行过程中可能遇到的一些问题，这些问题有轻重缓急之分，我们给这些问题按照严重性来排一下序： 脏写 \u003e 脏读 \u003e 不可重复读 \u003e 幻读 我们愿意舍弃一部分隔离性来换取一部分性能在这里就体现在：设立一些隔离级别，隔离级别越低，并发问题发生的就越多。SQL标准中设立了4个隔离级别： READ UNCOMMITTED ：读未提交，在该隔离级别，所有事务都可以看到其他未提交事务的执行结果。不能避免脏读、不可重复读、幻读。 READ COMMITTED ：读已提交，它满足了隔离的简单定义：一个事务只能看见已经提交事务所做的改变。这是大多数数据库系统的默认隔离级别（但不是MySQL默认的）。可以避免脏读，但不可重复读、幻读问题仍然存在。 REPEATABLE READ ：可重复读，事务A在读到一条数据之后，此时事务B对该数据进行了修改并提交，那么事务A再读该数据，读到的还是原来的内容。可以避免脏读、不可重复读，但幻读问题仍然存在。MySQL InnoDB默认的隔离级别是Repeatable Reads（RR），但它通过MVCC+间隙锁解决了绝大部分幻读（后面会解释为什么是绝大部分而不是全部）的问题。 SERIALIZABLE ：可串行化，确保事务可以从一个表中读取相同的行。在这个事务持续期间，禁止其他事务对该表执行插入、更新和删除操作。所有的并发问题都可以避免，但性能十分低下（表级锁）。能避免脏读、不可重复读和幻读。 SQL标准中规定，针对不同的隔离级别，并发事务可以发生不同严重程度的问题，具体情况如下： 脏写 怎么没涉及到？因为脏写这个问题太严重了，不论是哪种隔离级别，都不允许脏写的情况发生。 不同的隔离级别有不同的现象，并有不同的锁和并发机制，隔离级别越高，数据库的并发性能就越差，4 种事务隔离级别与并发性能的关系如下： ","date":"2022-10-20","objectID":"/mysql%E4%BA%8B%E5%8A%A1%E7%AF%87/:3:3","tags":[],"title":"MySQL事务篇","uri":"/mysql%E4%BA%8B%E5%8A%A1%E7%AF%87/"},{"categories":["mysql"],"content":"3.4 MySQL支持的四种隔离级别 MySQL的默认隔离级别为REPEATABLE READ，我们可以手动修改一下事务的隔离级别。 # 查看隔离级别，MySQL 5.7.20的版本之前： mysql\u003e SHOW VARIABLES LIKE 'tx_isolation'; +---------------+-----------------+ | Variable_name | Value | +---------------+-----------------+ | tx_isolation | REPEATABLE-READ | +---------------+-----------------+ 1 row in set (0.00 sec) # MySQL 5.7.20版本之后，引入 transaction_isolation 来替换 tx_isolation # 查看隔离级别，MySQL 5.7.20的版本及之后： mysql\u003e SHOW VARIABLES LIKE 'transaction_isolation'; +-----------------------+-----------------+ | Variable_name | Value | +-----------------------+-----------------+ | transaction_isolation | REPEATABLE-READ | +-----------------------+-----------------+ 1 row in set (0.02 sec) #或者不同MySQL版本中都可以使用的： SELECT @@transaction_isolation; ","date":"2022-10-20","objectID":"/mysql%E4%BA%8B%E5%8A%A1%E7%AF%87/:3:4","tags":[],"title":"MySQL事务篇","uri":"/mysql%E4%BA%8B%E5%8A%A1%E7%AF%87/"},{"categories":["mysql"],"content":"3.5 如何设置事务的隔离级别 通过下面的语句修改事务的隔离级别： SET [GLOBAL|SESSION] TRANSACTION ISOLATION LEVEL 隔离级别; #其中，隔离级别格式： \u003e READ UNCOMMITTED \u003e READ COMMITTED \u003e REPEATABLE READ \u003e SERIALIZABLE 或者： SET [GLOBAL|SESSION] TRANSACTION_ISOLATION = '隔离级别' #其中，隔离级别格式： \u003e READ-UNCOMMITTED \u003e READ-COMMITTED \u003e REPEATABLE-READ \u003e SERIALIZABLE 关于设置时使用GLOBAL或SESSION的影响： 使用 GLOBAL 关键字（在全局范围影响）： SET GLOBAL TRANSACTION ISOLATION LEVEL SERIALIZABLE; #或 SET GLOBAL TRANSACTION_ISOLATION = 'SERIALIZABLE'; 则： 当前已经存在的会话无效 只对执行完该语句之后产生的会话起作用 使用 SESSION 关键字（在会话范围影响）： SET SESSION TRANSACTION ISOLATION LEVEL SERIALIZABLE; #或 SET SESSION TRANSACTION_ISOLATION = 'SERIALIZABLE'; 则： 对当前会话的所有后续的事务有效 如果在事务之间执行，则对后续的事务有效 该语句可以在已经开启的事务中间执行，但不会影响当前正在执行的事务 如果在服务器启动时想改变事务的默认隔离级别，可以修改启动参数transaction_isolation的值。比如，在启动服务器时指定了transaction_isolation=SERIALIZABLE，那么事务的默认隔离界别就从原来的REPEATABLE-READ变成了SERIALIZABLE。 小结： 数据库规定了多种事务隔离级别，不同隔离级别对应不同的干扰程度，隔离级别越高，数据一致性就越好，但并发性越弱。 ","date":"2022-10-20","objectID":"/mysql%E4%BA%8B%E5%8A%A1%E7%AF%87/:3:5","tags":[],"title":"MySQL事务篇","uri":"/mysql%E4%BA%8B%E5%8A%A1%E7%AF%87/"},{"categories":["mysql"],"content":"3.6 不同隔离级别举例 初始化数据： TRUNCATE TABLE account; INSERT INTO account VALUES (1,'张三','100'), (2,'李四','0'); 演示1. 读未提交之脏读 设置隔离级别为未提交读： 脏读就是指当前事务就在访问数据，并且对数据进行了修改，而这种修改还没有提交到数据库中，这时，另外一个事务也访问了这个数据，然后使用了这个数据。 演示2：读已提交 演示3. 不可重复读 设置隔离级别为可重复读，事务的执行流程如下： 当我们将当前会话的隔离级别设置为可重复读的时候，当前会话可以重复读，就是每次读取的结果集都相同，而不管其他事务有没有提交。但是在可重复读的隔离级别上会出现幻读的问题。 演示4：幻读 ","date":"2022-10-20","objectID":"/mysql%E4%BA%8B%E5%8A%A1%E7%AF%87/:3:6","tags":[],"title":"MySQL事务篇","uri":"/mysql%E4%BA%8B%E5%8A%A1%E7%AF%87/"},{"categories":["mysql"],"content":"4. 事务的常见分类 从事务理论的角度来看，可以把事务分为以下几种类型： 扁平事务（Flat Transactions） 带有保存点的扁平事务（Flat Transactions with Savepoints） 链事务（Chained Transactions） 嵌套事务（Nested Transactions） 分布式事务（Distributed Transactions） 第14章_MySQL事务日志 事务有4种特性：原子性、一致性、隔离性和持久性。那么事务的四种特性到底是基于什么机制实现呢？ 事务的隔离性由 锁机制 实现。 而事务的原子性、一致性和持久性由事务的 redo 日志和undo 日志来保证。 REDO LOG 称为 重做日志 ，提供再写入操作，恢复提交事务修改页的操作，用来保证事务的持久性。 UNDO LOG 称为 回滚日志 ，回滚行记录到某个特定版本，用来保证事务的原子性、一致性。=\u003e 回滚和多版本并发控制（MVCC） 有的DBA或许会认为 UNDO 是 REDO 的逆过程，其实不然。REDO 和 UNDO都可以视为是一种 恢复操作，但是： redo log: 是存储引擎层 (innodb) 生成的日志，记录的是\"物理级别\"上的页修改操作，比如页号xxx，偏移量yyy写入了’zzz’数据。主要为了保证数据的可靠性。这样内存的写入需要记录到redo log，一旦断电或死机等情况发生，那么就会导致内存数据的丢失，之后数据库恢复，就可以从redo log里再次操作上次失效的操作了。 undo log: 是存储引擎层 (innodb) 生成的日志，记录的是 逻辑操作 日志，比如对某一行数据进行了INSERT语句操作，那么undo log就记录一条与之相反的DELETE操作。主要用于 事务的回滚 (undo log 记录的是每个修改操作的 逆操作) 和 一致性非锁定读 (undo log 回滚行记录到某种特定的版本——MVCC，即多版本并发控制)。 ","date":"2022-10-20","objectID":"/mysql%E4%BA%8B%E5%8A%A1%E7%AF%87/:4:0","tags":[],"title":"MySQL事务篇","uri":"/mysql%E4%BA%8B%E5%8A%A1%E7%AF%87/"},{"categories":["mysql"],"content":"1. redo日志 InnoDB存储引擎是以页为单位来管理存储空间的。在真正访问页面之前，需要把在磁盘上的页缓存到内存中的Buffer Pool之后才可以访问。所有的变更都必须先更新缓冲池中的数据，然后缓冲池中的脏页会以一定的频率被刷入磁盘 (checkPoint机制)，通过缓冲池来优化CPU和磁盘之间的鸿沟，这样就可以保证整体的性能不会下降太快。 ","date":"2022-10-20","objectID":"/mysql%E4%BA%8B%E5%8A%A1%E7%AF%87/:5:0","tags":[],"title":"MySQL事务篇","uri":"/mysql%E4%BA%8B%E5%8A%A1%E7%AF%87/"},{"categories":["mysql"],"content":"1.1 为什么需要REDO日志 一方面，缓冲池可以帮助我们消除CPU和磁盘之间的鸿沟，checkpoint机制可以保证数据的最终落盘，然而由于checkpoint 并不是每次变更的时候就触发 的，而是master线程隔一段时间去处理的。所以最坏的情况就是事务提交后，刚写完缓冲池，数据库宕机了，那么这段数据就是丢失的，无法恢复。 另一方面，事务包含 持久性 的特性，就是说对于一个已经提交的事务，在事务提交后即使系统发生了崩溃，这个事务对数据库中所做的更改也不能丢失。 那么如何保证这个持久性呢？ 一个简单的做法 ：在事务提交完成之前把该事务所修改的所有页面都刷新到磁盘，但是这个简单粗暴的做法有些问题: 修改量与刷新磁盘工作量严重不成比例 有时候我们仅仅修改了某个页面中的一个字节，但是我们知道在InnoDB中是以页为单位来进行磁盘IO的，也就是说我们在该事务提交时不得不将一个完整的页面从内存中刷新到磁盘，我们又知道一个默认页面时16KB大小，只修改一个字节就要刷新16KB的数据到磁盘上显然是小题大做了。 随机IO刷新较慢 一个事务可能包含很多语句，即使是一条语句也可能修改许多页面，假如该事务修改的这些页面可能并不相邻，这就意味着在将某个事务修改的Buffer Pool中的页面刷新到磁盘时，需要进行很多的随机IO，随机IO比顺序IO要慢，尤其对于传统的机械硬盘来说。 另一个解决的思路 ：我们只是想让已经提交了的事务对数据库中数据所做的修改永久生效，即使后来系统崩溃，在重启后也能把这种修改恢复出来。所以我们其实没有必要在每次事务提交时就把该事务在内存中修改过的全部页面刷新到磁盘，只需要把修改了哪些东西记录一下就好。比如，某个事务将系统表空间中第10号页面中偏移量为 100 处的那个字节的值 1 改成 2 。我们只需要记录一下：将第0号表空间的10号页面的偏移量为100处的值更新为 2 InnoDB引擎的事务采用了WAL技术 (Write-Ahead Logging)，这种技术的思想就是先写日志，再写磁盘，只有日志写入成功，才算事务提交成功，这里的日志就是redo log。当发生宕机且数据未刷到磁盘的时候，可以通过redo log来恢复，保证ACID中的D（即持久性，事务的提交对数据库的影响是持久的），这就是redo log的作用。 ","date":"2022-10-20","objectID":"/mysql%E4%BA%8B%E5%8A%A1%E7%AF%87/:5:1","tags":[],"title":"MySQL事务篇","uri":"/mysql%E4%BA%8B%E5%8A%A1%E7%AF%87/"},{"categories":["mysql"],"content":"1.2 REDO日志的好处、特点 1. 好处 redo日志降低了刷盘频率 redo日志占用的空间非常小 存储表空间ID、页号、偏移量以及需要更新的值，所需的存储空间是很小的，刷盘快。 2. 特点 redo日志是顺序写入磁盘的 在执行事务的过程中，每执行一条语句，就可能产生若干条redo日志，这些日志是按照产生的顺序写入磁盘的，也就是使用顺序IO，效率比随机IO快。 事务执行过程中，redo log不断记录 redo log跟bin log的区别，redo log是存储引擎层产生的，而bin log是数据库层产生的。假设一个事务，对表做10万行的记录插入，在这个过程中，一直不断的往redo log顺序记录，而bin log不会记录，直到这个事务提交，才会一次写入到bin log文件中。 ","date":"2022-10-20","objectID":"/mysql%E4%BA%8B%E5%8A%A1%E7%AF%87/:5:2","tags":[],"title":"MySQL事务篇","uri":"/mysql%E4%BA%8B%E5%8A%A1%E7%AF%87/"},{"categories":["mysql"],"content":"1.3 redo的组成 Redo log可以简单分为以下两个部分： 重做日志的缓冲 (redo log buffer) ，保存在内存中，是易失的。 在服务器启动时就会向操作系统申请了一大片称之为 redo log buffer 的 连续内存 空间，翻译成中文就是redo日志缓冲区。这片内存空间被划分为若干个连续的redo log block。一个redo log block占用512字节大小。 参数设置：innodb_log_buffer_size： redo log buffer 大小，默认 16M ，最大值是4096M，最小值为1M。 mysql\u003e show variables like '%innodb_log_buffer_size%'; +------------------------+----------+ | Variable_name | Value | +------------------------+----------+ | innodb_log_buffer_size | 16777216 | +------------------------+----------+ 重做日志文件 (redo log file) ，保存在硬盘中，是持久的。 REDO日志文件如图所示，其中ib_logfile0和ib_logfile1即为REDO日志。 ","date":"2022-10-20","objectID":"/mysql%E4%BA%8B%E5%8A%A1%E7%AF%87/:5:3","tags":[],"title":"MySQL事务篇","uri":"/mysql%E4%BA%8B%E5%8A%A1%E7%AF%87/"},{"categories":["mysql"],"content":"1.4 redo的整体流程 以一个更新事务为例，redo log 流转过程，如下图所示： 第1步：先将原始数据从磁盘中读入内存中来，修改数据的内存拷贝 第2步：生成一条重做日志并写入redo log buffer，记录的是数据被修改后的值 第3步：当事务commit时，将redo log buffer中的内容刷新到 redo log file，对 redo log file采用追加写的方式（这样才算一个事务提交成功） 第4步：定期将内存中修改的数据刷新到磁盘中 体会： Write-Ahead Log(预先日志持久化)：在持久化一个数据页之前，先将内存中相应的日志页持久化。 ","date":"2022-10-20","objectID":"/mysql%E4%BA%8B%E5%8A%A1%E7%AF%87/:5:4","tags":[],"title":"MySQL事务篇","uri":"/mysql%E4%BA%8B%E5%8A%A1%E7%AF%87/"},{"categories":["mysql"],"content":"1.5 redo log的刷盘策略 redo log的写入并不是直接写入磁盘的，InnoDB引擎会在写redo log的时候先写redo log buffer，之后以一 定的频率刷入到真正的redo log file 中。这里的一定频率怎么看待呢？这就是我们要说的刷盘策略。 注意，redo log buffer刷盘到redo log file的过程并不是真正的刷到磁盘中去，只是刷入到 文件系统缓存 （page cache）中去（这是现代操作系统为了提高文件写入效率做的一个优化），真正的写入会交给系统自己来决定（比如page cache足够大了）。那么对于InnoDB来说就存在一个问题，如果交给系统来同步，同样如果系统宕机，那么数据也丢失了（虽然整个系统宕机的概率还是比较小的）。 针对这种情况，InnoDB给出 innodb_flush_log_at_trx_commit 参数，该参数控制 commit提交事务 时，如何将 redo log buffer 中的日志刷新到 redo log file 中。它支持三种策略：（系统默认master thread每隔1s进行一次重做日志的同步） 设置为0 ：表示每次事务提交时不进行刷盘操作，也就是只把 redo log 留在 redo log buffer 内存里中 第1步：先将原始数据从磁盘中读入内存中来，修改数据的内存拷贝 第2步：生成一条重做日志并写入redo log buffer，记录的是数据被修改后的值 第3步：当事务commit时，将redo log buffer中的内容刷新到 redo log file，对 redo log file采用追加写的方式 第4步：定期将内存中修改的数据刷新到磁盘中 这种方式存在缺陷：redo log持久化磁盘的策略只有一种：系统master线程来做定期的write+fsync操作。如果master线程挂了或者说mysql宕机了，那么这宕机之后的所有数据（前一秒在redo log buffer里记录的）就会丢失。虽然这种方式是最快的，但是也是最危险的。所以，一般不会采用这种策略。 设置为1 ：表示每次事务提交时都将进行同步，刷盘持久化到操作（ 默认值 ），也就是每次提交事务都将redo log 持久化到磁盘上，也就是write+fsync write：刷盘 fsync：持久化到磁盘 write(刷盘)指的是MySQL从buffer pool中将内容写到系统的page cache中，并没有持久化到系统磁盘上。这个速度其实是很快的。(磁盘读写很慢，算是现代操作系统这一层做的优化) fsync指的是从系统的cache中将数据持久化到系统磁盘上。这个速度可以认为比较慢，而且也是IOPS升高的真正原因。 设置为2 ：表示每次事务提交时都只把 redo log buffer 内容写入 page cache，不进行同步。由os自己决定什么时候同步到磁盘文件（fsync）。 另外，InnoDB存储引擎有一个后台线程，每隔1秒，就会把redo log buffer中的内容写到文件系统缓存(page cache)，然后调用刷盘操作。 也就是说，一个没有提交事务的redo log记录，也可能会刷盘。因为在事务执行过程 redo log 记录是会写入 redo log buffer中，这些redo log 记录会被后台线程刷盘。事务执行过程中，而且在提交之前，中间的执行语句首先会被redo log记录，之后，后台线程再每隔1秒进行异步地刷盘。 除了后台线程每秒1次的轮询操作，还有一种情况，当redo log buffer占用的空间即将达到innodb_log_buffer_size（这个参数默认是16M）的一半的时候，后台线程会主动刷盘。 ","date":"2022-10-20","objectID":"/mysql%E4%BA%8B%E5%8A%A1%E7%AF%87/:5:5","tags":[],"title":"MySQL事务篇","uri":"/mysql%E4%BA%8B%E5%8A%A1%E7%AF%87/"},{"categories":["mysql"],"content":"1.6 不同刷盘策略演示 1. 流程图 注：为1的情况下，事务提交成功的前提之一：需要将redo log持久化（write+fsync）到redo log file。如果刷盘失败，事务不会提交成功，这样就保证了提交事务前要持久化redo log，这样就保证了ACID里的D特性。 2. 举例 比较innodb_flush_log_at_trx_commit对事务的影响。 CREATE TABLE test_load( a INT, b CHAR(80) )ENGINE=INNODB; DELIMITER// CREATE PROCEDURE p_load(COUNT INT UNSIGNED) BEGIN DECLARE s INT UNSIGNED DEFAULT 1; DECLARE c CHAR(80) DEFAULT REPEAT('a',80); WHILE s\u003c=COUNT DO INSERT INTO test_load SELECT NULL, c; COMMIT; SET s=s+1; END WHILE; END // DELIMITER; mysql\u003e CALL p_load(30000); Query OK, 0 rows affected(1 min 23 sec) 1 min 23 sec的时间显然是不能接受的。而造成时间比较长的原因就在于fsync操作所需要的时间。 修改参数innodb_flush_log_at_trx_commit，设置为0： mysql\u003e set global innodb_flush_log_at_trx_commit = 0; mysql\u003e CALL p_load(30000); Query OK, 0 rows affected(38 sec) 修改参数innodb_flush_log_at_trx_commit，设置为2： mysql\u003e set global innodb_flush_log_at_trx_commit = 2; mysql\u003e CALL p_load(30000); Query OK, 0 rows affected(46 sec) 总结：innodb_flush_log_at_trx_commit为0或2时，虽然可以显著提升mysql事务的性能，但是在某些情况下就无法完全保证ACID的D特性了 当为0的时候，mysql只有master线程来做持久化操作（fsync），但是如果mysql宕机或系统宕机，显然会丢掉redo log buffer里记录的前一秒的数据。 当为2的时候，mysql除了master线程来定期做持久化操作之外，还会有一个主动刷盘的策略（write）。主要是每次事务提交成功的时候，会将redo log写入系统的page cache（系统缓存）里，然后再由系统决定何时真正地放到硬盘里。显然在这个过程里，如果mysql宕机，系统没宕机，还是满足D特性；但是如果系统宕机了，那么D特性就无法满足了，系统会丢掉没有来得及刷入磁盘地缓存数据。 当为1的时候（默认），mysql有两种策略：一种还是有一个mysql的master线程来定期扫描持久化；另一种就是事务提交前会强制持久化redo log（write+fsync）操作。第二种策略才是真的保证了事务的D特性。 ","date":"2022-10-20","objectID":"/mysql%E4%BA%8B%E5%8A%A1%E7%AF%87/:5:6","tags":[],"title":"MySQL事务篇","uri":"/mysql%E4%BA%8B%E5%8A%A1%E7%AF%87/"},{"categories":["mysql"],"content":"1.7 写入redo log buffer 过程 1. 补充概念：Mini-Transaction MySQL把对底层页面中的一次原子访问过程称之为一个Mini-Transaction，简称mtr，比如，向某个索引对应的B+树中插入一条记录的过程就是一个Mini-Transaction。一个所谓的mtr可以包含一组redo日志，在进行崩溃恢复时这一组redo日志可以作为一个不可分割的整体。 一个事务可以包含若干条语句，每一条语句其实是由若干个 mtr 组成，每一个 mtr 又可以包含若干条 redo 日志，画个图表示它们的关系就是这样： 2. redo 日志写入log buffer 不同的事务可能是 并发 执行的，所以 T1 、 T2 之间的 mtr 可能是 交替执行 的。每当一个mtr执行完成时，伴随该mtr生成的一组redo日志就需要被复制到log buffer中，也就是说不同事务的mtr可能是交替写入log buffer的，我们画个示意图（为了美观，我们把一个mtr中产生的所有redo日志当做一个整体来画）： 有的mtr产生的redo日志量非常大，比如mtr_t1_2产生的redo日志占用空间比较大，占用了3个block来存储。 3. redo log block的结构图 一个redo log block是由日志头、日志体、日志尾组成。日志头占用12字节，日志尾占用8字节，所以一个block真正能存储的数据是512-12-8=492字节。 真正的redo日志都是存储到占用496字节大小的log block body中，图中的log block header和log block trailer存储的是一些管理信息。我们来看看这些所谓管理信息都有什么。 ","date":"2022-10-20","objectID":"/mysql%E4%BA%8B%E5%8A%A1%E7%AF%87/:5:7","tags":[],"title":"MySQL事务篇","uri":"/mysql%E4%BA%8B%E5%8A%A1%E7%AF%87/"},{"categories":["mysql"],"content":"1.8 redo log file 1. 相关参数设置 innodb_log_group_home_dir ：指定 redo log 文件组所在的路径，默认值为 ./ ，表示在数据库的数据目录下。MySQL的默认数据目录（ var/lib/mysql）下默认有两个名为 ib_logfile0 和 ib_logfile1 的文件，log buffer中的日志默认情况下就是刷新到这两个磁盘文件中。此redo日志文件位置还可以修改。 innodb_log_files_in_group：指明redo log file的个数，命名方式如：ib_logfile0，iblogfile1… iblogfilen。默认2个，最大100个。 mysql\u003e show variables like 'innodb_log_files_in_group'; +---------------------------+-------+ | Variable_name | Value | +---------------------------+-------+ | innodb_log_files_in_group | 2 | +---------------------------+-------+ #ib_logfile0 #ib_logfile1 innodb_flush_log_at_trx_commit：控制 redo log 刷新到磁盘的策略，默认为1，也是最慢最安全的持久化策略。 innodb_log_file_size：单个 redo log 文件设置大小，默认值为 48M 。最大值为512G，注意最大值指的是整个 redo log 系列文件之和，即（innodb_log_files_in_group * innodb_log_file_size ）不能大于最大值512G。 mysql\u003e show variables like 'innodb_log_file_size'; +----------------------+----------+ | Variable_name | Value | +----------------------+----------+ | innodb_log_file_size | 50331648 | +----------------------+----------+ 根据业务修改其大小，以便容纳较大的事务。编辑my.cnf文件并重启数据库生效，如下所示 [root@localhost ~]# vim /etc/my.cnf innodb_log_file_size=200M 在数据库实例更新比较频繁的情况下，可以适当加大 redo log 数组和大小。但也不推荐 redo log 设置过大，在MySQL崩溃时会重新执行REDO日志中的记录。 2. 日志文件组 总共的redo日志文件大小其实就是： innodb_log_file_size × innodb_log_files_in_group 。 采用循环使用的方式向redo日志文件组里写数据的话，会导致后写入的redo日志覆盖掉前边写的redo日志？当然！所以InnoDB的设计者提出了checkpoint的概念。 3. checkpoint 在整个日志文件组中还有两个重要的属性，分别是 write pos、checkpoint write pos是当前记录的位置，一边写一边后移 checkpoint是当前要擦除的位置，也是往后推移 每次刷盘 redo log 记录到日志文件组中，write pos 位置就会后移更新。每次MySQL加载日志文件组恢复数据时，会清空加载过的 redo log 记录，并把check point后移更新。write pos 和 checkpoint 之间的还空着的部分可以用来写入新的 redo log 记录。 如果 write pos 追上 checkpoint ，表示日志文件组满了，这时候不能再写入新的 redo log记录，MySQL 得停下来，清空一些记录，把 checkpoint 推进一下。 ","date":"2022-10-20","objectID":"/mysql%E4%BA%8B%E5%8A%A1%E7%AF%87/:5:8","tags":[],"title":"MySQL事务篇","uri":"/mysql%E4%BA%8B%E5%8A%A1%E7%AF%87/"},{"categories":["mysql"],"content":"1.9 redo log 小结 ","date":"2022-10-20","objectID":"/mysql%E4%BA%8B%E5%8A%A1%E7%AF%87/:5:9","tags":[],"title":"MySQL事务篇","uri":"/mysql%E4%BA%8B%E5%8A%A1%E7%AF%87/"},{"categories":["mysql"],"content":"2. Undo日志 redo log是事务持久性的保证，undo log是事务原子性的保证。在事务中更新数据的前置操作其实是要先写入一个undo log。 ","date":"2022-10-20","objectID":"/mysql%E4%BA%8B%E5%8A%A1%E7%AF%87/:6:0","tags":[],"title":"MySQL事务篇","uri":"/mysql%E4%BA%8B%E5%8A%A1%E7%AF%87/"},{"categories":["mysql"],"content":"2.1 如何理解Undo日志 事务需要保证原子性，也就是事务中的操作要么全部完成，要么什么也不做。但有时候事务执行到一半会出现一些情况，比如： 情况一：事务执行过程中可能遇到各种错误，比如服务器本身的错误，操作系统错误，甚至是突然断电导致的错误。 情况二：程序员可以在事务执行过程中手动输入ROLLBACK语句结束当前事务的执行。 以上情况出现，我们需要把数据改回原先的样子，这个过程称之为回滚，这样就可以造成一个假象：这个事务看起来什么都没做，所以符合原子性要求。 ","date":"2022-10-20","objectID":"/mysql%E4%BA%8B%E5%8A%A1%E7%AF%87/:6:1","tags":[],"title":"MySQL事务篇","uri":"/mysql%E4%BA%8B%E5%8A%A1%E7%AF%87/"},{"categories":["mysql"],"content":"2.2 Undo日志的作用 作用1：回滚数据 回滚只是使用逻辑反操作 作用2：MVCC undo的另一个作用是MVCC，即在InnoDB存储引擎中MVCC的实现是通过undo来完成。当用户读取一行记录时，若该记录以及被其他事务占用，当前事务可以通过undo读取之前的行版本信息，以此实现非锁定读取。 ","date":"2022-10-20","objectID":"/mysql%E4%BA%8B%E5%8A%A1%E7%AF%87/:6:2","tags":[],"title":"MySQL事务篇","uri":"/mysql%E4%BA%8B%E5%8A%A1%E7%AF%87/"},{"categories":["mysql"],"content":"2.3 undo的存储结构 1. 回滚段与undo页 InnoDB对undo log的管理采用段的方式，也就是回滚段（rollback segment）。每个回滚段记录了 1024 个 undo log segment ，而在每个undo log segment段中进行undo页的申请。 在InnoDB1.1版本之前（不包括1.1版本），只有一个rollback segment，因此支持同时在线的事务限制为1024。虽然对绝大多数的应用来说都已经够用。 从1.1版本开始InnoDB支持最大 128个rollback segment ，故其支持同时在线的事务限制提高到了 128*1024 。 mysql\u003e show variables like 'innodb_undo_logs'; +------------------+-------+ | Variable_name | Value | +------------------+-------+ | innodb_undo_logs | 128 | +------------------+-------+ 2. 回滚段与事务 每个事务只会使用一个回滚段，一个回滚段在同一时刻可能会服务于多个事务。 当一个事务开始的时候，会制定一个回滚段，在事务进行的过程中，当数据被修改时，原始的数据会被复制到回滚段。 在回滚段中，事务会不断填充盘区，直到事务结束或所有的空间被用完。如果当前的盘区不够用，事务会在段中请求扩展下一个盘区，如果所有已分配的盘区都被用完，事务会覆盖最初的盘 区或者在回滚段允许的情况下扩展新的盘区来使用。 回滚段存在于undo表空间中，在数据库中可以存在多个undo表空间，但同一时刻只能使用一个undo表空间。 mysql\u003e show variables like 'innodb_undo_tablespaces'; +-------------------------+-------+ | Variable_name | Value | +-------------------------+-------+ | innodb_undo_tablespaces | 2 | +-------------------------+-------+ # undo log的数量，最少为2. undo log的truncate操作有purge协调线程发起。在truncate某个undo log表空间的过程中，保证有一个可用的undo log可用。 当事务提交时，InnoDB存储引擎会做以下两件事情： 将undo log放入列表中，以供之后的purge操作 判断undo log所在的页是否可以重用，若可以分配给下个事务使用 3. 回滚段中的数据分类 未提交的回滚数据(uncommitted undo information)：该数据所关联的事务并未提交，用于实现读一致性，所以该数据不能被其他事务的数据覆盖。 已经提交但未过期的回滚数据(committed undo information)：该数据关联的事务已经提交，但是仍受到undo retention参数的保持时间的影响。 事务已经提交并过期的数据(expired undo information)：事务已经提交，而且数据保存时间已经超过 undo retention参数指定的时间，属于已经过期的数据。当回滚段满了之后，就优先覆盖“事务已经提交并过期的数据\"。 事务提交后不能马上删除undo log及undo log所在的页。这是因为可能还有其他事务需要通过undo log来得到行记录之前的版本。故事务提交时将undo log放入一个链表中，是否可以最终删除undo log以undo log所在页由purge线程来判断。 ","date":"2022-10-20","objectID":"/mysql%E4%BA%8B%E5%8A%A1%E7%AF%87/:6:3","tags":[],"title":"MySQL事务篇","uri":"/mysql%E4%BA%8B%E5%8A%A1%E7%AF%87/"},{"categories":["mysql"],"content":"2.4 undo的类型 在InnoDB存储引擎中，undo log分为： insert undo log insert undo log是指insert操作中产生的undo log。因为insert操作的记录，只对事务本身可见，对其他事务不可见（这是事务隔离性的要求），故该undo log可以在事务提交后直接删除。不需要进行purge操作。（事务里的insert操作不需要旧版本数据 update undo log update undo log记录的是对delete和update操作产生的undo log。该undo log可能需要提供MVCC机制，因此不能在事务提交时就进行删除。提交时放入undo log链表，等待purge线程进行最后的删除。（事务里的delete和update操作提交后的undo日志就相当于旧版本数据 当读取的某一行被其他事务锁定时，它可以从undo log中分析出该行记录以前的数据是什么，从而提供该行版本信息，让用户实现非锁定一致性读取。 ","date":"2022-10-20","objectID":"/mysql%E4%BA%8B%E5%8A%A1%E7%AF%87/:6:4","tags":[],"title":"MySQL事务篇","uri":"/mysql%E4%BA%8B%E5%8A%A1%E7%AF%87/"},{"categories":["mysql"],"content":"2.5 undo log的生命周期 1. 简要生成过程 以下是undo+redo事务的简化过程 假设有两个数值，分别为A=1和B=2，然后将A修改为3，B修改为4 只有Buffer Pool的流程： 有了Redo Log和Undo Log之后： 在更新Buffer Pool中的数据之前，我们需要先将该数据事务开始之前的状态写入Undo Log中。假设更新到一半出错了，我们就可以通过Undo Log来回滚到事务开始前。 2. 详细生成过程 当我们执行INSERT时： begin; INSERT INTO user (name) VALUES (\"tom\"); 插入的数据都会生成一条insert undo log，并且数据的回滚指针会指向它。undo log会记录undo log的序号、插入主键的列和值…，那么在进行rollback的时候，通过主键直接把对应的数据删除即可。 当我们执行UPDATE时： 对应更新的操作会产生update undo log，并且会分更新主键和不更新主键的，假设现在执行： UPDATE user SET name=\"Sun\" WHERE id=1; 这时会把老的记录写入新的undo log，让回滚指针指向新的undo log，它的undo no是1，并且新的undo log会指向老的undo log（undo no=0）。(单链表) 假设现在执行： UPDATE user SET id=2 WHERE id=1; # 更新主键 对于更新主键的操作，会先把原来的数据deletemark标识打开，这时并没有真正的删除数据，真正的删除会交给清理线程去判断，然后在后面插入一条新的数据，新的数据也会产生undo log，并且undo log的序号会递增。 可以发现每次对数据的变更都会产生一个undo log，当一条记录被变更多次时，那么就会产生多条undo log，undo log记录的是变更前的日志，并且每个undo log的序号是递增的，那么当要回滚的时候，按照序号依次向前推，就可以找到我们的原始数据了。(有点类似于git) 3. undo log是如何回滚的 以上面的例子来说，假设执行rollback，那么对应的流程应该是这样： 通过undo no=3的日志把id=2的数据删除 通过undo no=2的日志把id=1的数据的deletemark还原成0 通过undo no=1的日志把id=1的数据的name还原成Tom 通过undo no=0的日志把id=1的数据删除 4. undo log的删除 针对于insert undo log 因为insert操作的记录，只对事务本身可见，对其他事务不可见。故该undo log可以在事务提交后直接删除，不需要进行purge操作。 针对于update undo log 该undo log可能需要提供MVCC机制，因此不能在事务提交时就进行删除。提交时放入undo log链表，等待purge线程进行最后的删除。 补充： purge线程两个主要作用是：清理undo页和清理page里面带有Delete_Bit标识的数据行。在InnoDB中，事务中的Delete操作实际上并不是真正的删除掉数据行，而是一种Delete Mark操作，在记录上标识Delete_Bit，而不删除记录。是一种“假删除”，只是做了个标记，真正的删除工作需要后台purge线程去完成。 ","date":"2022-10-20","objectID":"/mysql%E4%BA%8B%E5%8A%A1%E7%AF%87/:6:5","tags":[],"title":"MySQL事务篇","uri":"/mysql%E4%BA%8B%E5%8A%A1%E7%AF%87/"},{"categories":["mysql"],"content":"2.6 小结 undo log是逻辑日志，对事务回滚时，只是将数据库逻辑地恢复到原来的样子。 redo log是物理日志，记录的是数据页的物理变化，undo log不是redo log的逆过程。 第15章_锁 ","date":"2022-10-20","objectID":"/mysql%E4%BA%8B%E5%8A%A1%E7%AF%87/:6:6","tags":[],"title":"MySQL事务篇","uri":"/mysql%E4%BA%8B%E5%8A%A1%E7%AF%87/"},{"categories":["mysql"],"content":"1. 概述 在数据库中，除传统的计算资源（如CPU、RAM、I/O等）的争用以外，数据也是一种供许多用户共享的资源。为保证数据的一致性，需要对 并发操作进行控制 ，因此产生了 锁 。同时 锁机制 也为实现MySQL 的各个隔离级别提供了保证。锁冲突也是影响数据库并发访问性能的一个重要因素。所以锁对数据库而言显得尤其重要，也更加复杂。 ","date":"2022-10-20","objectID":"/mysql%E4%BA%8B%E5%8A%A1%E7%AF%87/:7:0","tags":[],"title":"MySQL事务篇","uri":"/mysql%E4%BA%8B%E5%8A%A1%E7%AF%87/"},{"categories":["mysql"],"content":"2. MySQL并发事务访问相同记录 并发事务访问相同记录的情况大致可以划分为3种： ","date":"2022-10-20","objectID":"/mysql%E4%BA%8B%E5%8A%A1%E7%AF%87/:8:0","tags":[],"title":"MySQL事务篇","uri":"/mysql%E4%BA%8B%E5%8A%A1%E7%AF%87/"},{"categories":["mysql"],"content":"2.1 读-读情况 读-读情况，即并发事务相继读取相同的记录。读取操作本身不会对记录有任何影响，并不会引起什么问题，所以允许这种情况的发生。 ","date":"2022-10-20","objectID":"/mysql%E4%BA%8B%E5%8A%A1%E7%AF%87/:8:1","tags":[],"title":"MySQL事务篇","uri":"/mysql%E4%BA%8B%E5%8A%A1%E7%AF%87/"},{"categories":["mysql"],"content":"2.2 写-写情况 写-写情况，即并发事务相继对相同的记录做出改动。 在这种情况下可能会发生 脏写 的问题，任何一种隔离级别都不允许这种问题的发生。所以在多个未提交事务相继对一条记录做改动时，需要让它们 排队执行 ，这个排队的过程其实是通过 锁 来实现的。这个所谓的锁其实是一个内存中的结构 ，在事务执行前本来是没有锁的，也就是说一开始是没有 锁结构 和记录进行关联的，如图所示： 当一个事务想对这条记录做改动时，首先会看看内存中有没有与这条记录关联的锁结构，当没有的时候就会在内存中生成一个锁结构与之关联。比如，事务 T1要对这条记录做改动，就需要生成一个锁结构与之关联： 在锁结构里有很多信息，为了简化理解，只把两个比较重要的属性拿了出来： trx信息：代表这个锁结构是哪个事务生成的。 is_waiting：代表当前事务是否在等待。 在事务T1改动了这条记录后，就生成了一个锁结构与该记录关联，因为之前没有别的事务为这条记录加锁，所以is_waiting属性就是false，我们把这个场景就称值为获取锁成功，或者加锁成功，然后就可以继续执行操作了。 在事务T1提交之前，另一个事务T2也想对该记录做改动，那么先看看有没有锁结构与这条记录关联，发现有一个锁结构与之关联后，然后也生成了一个锁结构与这条记录关联，不过锁结构的is_waiting属性值为true，表示当前事务需要等待，我们把这个场景就称之为获取锁失败，或者加锁失败，图示： 在事务T1提交之后，就会把该事务生成的锁结构释放掉，然后看看还有没有别的事务在等待获取锁，发现了事务T2还在等待获取锁，所以把事务T2对应的锁结构的is_waiting属性设置为false，然后把该事务对应的线程唤醒，让它继续执行，此时事务T2就算获取到锁了。效果就是这样。 小结几种说法： 不加锁 意思就是不需要在内存中生成对应的 锁结构 ，可以直接执行操作。 获取锁成功，或者加锁成功 意思就是在内存中生成了对应的 锁结构 ，而且锁结构的 is_waiting 属性为 false ，也就是事务可以继续执行操作。 获取锁失败，或者加锁失败，或者没有获取到锁 意思就是在内存中生成了对应的 锁结构 ，不过锁结构的 is_waiting 属性为 true ，也就是事务需要等待，不可以继续执行操作。 ","date":"2022-10-20","objectID":"/mysql%E4%BA%8B%E5%8A%A1%E7%AF%87/:8:2","tags":[],"title":"MySQL事务篇","uri":"/mysql%E4%BA%8B%E5%8A%A1%E7%AF%87/"},{"categories":["mysql"],"content":"2.3 读-写或写-读情况 读-写 或 写-读 ，即一个事务进行读取操作，另一个进行改动操作。这种情况下可能发生脏读、不可重复读、幻读的问题。 各个数据库厂商对 SQL标准 的支持都可能不一样。比如MySQL在 REPEATABLE READ 隔离级别上就已经解决了 幻读 问题。 ","date":"2022-10-20","objectID":"/mysql%E4%BA%8B%E5%8A%A1%E7%AF%87/:8:3","tags":[],"title":"MySQL事务篇","uri":"/mysql%E4%BA%8B%E5%8A%A1%E7%AF%87/"},{"categories":["mysql"],"content":"2.4 并发问题的解决方案 怎么解决 脏读 、 不可重复读 、 幻读 这些问题呢？其实有两种可选的解决方案： 方案一：读操作利用多版本并发控制（ MVCC ，下章讲解），写操作进行 加锁 。 普通的SELECT语句在READ COMMITTED和REPEATABLE READ隔离级别下会使用到MVCC读取记录。 在 READ COMMITTED 隔离级别下，一个事务在执行过程中每次执行SELECT操作时都会生成一个ReadView，ReadView的存在本身就保证了事务不可以读取到未提交的事务所做的更改，也就是避免了脏读现象； 在 REPEATABLE READ 隔离级别下，一个事务在执行过程中只有 第一次执行SELECT操作 才会生成一个ReadView，之后的SELECT操作都 复用 这个ReadView，这样也就避免了不可重复读和幻读的问题。 方案二：读、写操作都采用 加锁 的方式。 小结对比发现： 采用 MVCC 方式的话， 读-写 操作彼此并不冲突性能更高 。 采用 加锁 方式的话， 读-写 操作彼此需要 排队执行 ，影响性能。 一般情况下我们当然愿意采用 MVCC 来解决 读-写 操作并发执行的问题，但是业务在某些特殊情况下，要求必须采用加锁的方式执行。下面就讲解下MySQL中不同类别的锁。 ","date":"2022-10-20","objectID":"/mysql%E4%BA%8B%E5%8A%A1%E7%AF%87/:8:4","tags":[],"title":"MySQL事务篇","uri":"/mysql%E4%BA%8B%E5%8A%A1%E7%AF%87/"},{"categories":["mysql"],"content":"3. 锁的不同角度分类 锁的分类图，如下： ","date":"2022-10-20","objectID":"/mysql%E4%BA%8B%E5%8A%A1%E7%AF%87/:9:0","tags":[],"title":"MySQL事务篇","uri":"/mysql%E4%BA%8B%E5%8A%A1%E7%AF%87/"},{"categories":["mysql"],"content":"3.1 从数据操作的类型划分：读锁、写锁 增删改查：查可以是加X或S锁，但是删改只能是加X锁，增是隐式锁 读锁 ：也称为共享锁、英文用 S 表示。针对同一份数据，多个事务的读操作可以同时进行而不会互相影响，相互不阻塞的，但是会阻塞另外一个事务的写锁获取。 写锁 ：也称为排他锁、英文用 X 表示。当前写操作没有完成前，它会阻断其他写和读。这样就能确保在给定的时间里，只有一个事务能执行写入，并防止其他用户读取或写入正在写入的同一资源。 需要注意的是对于 InnoDB 引擎来说，读锁和写锁可以加在表上，也可以加在行上。 1. 锁定读 当前事务select...持有S锁，不阻塞持有S锁的事务来读，但是阻塞持有X锁的写；当前事务select...持有X锁，阻塞持有X锁读和写或阻塞S锁的事务读 2. 写操作 ","date":"2022-10-20","objectID":"/mysql%E4%BA%8B%E5%8A%A1%E7%AF%87/:9:1","tags":[],"title":"MySQL事务篇","uri":"/mysql%E4%BA%8B%E5%8A%A1%E7%AF%87/"},{"categories":["mysql"],"content":"3.2 从数据操作的粒度划分：表级锁、页级锁、行锁 1. 表锁（Table Lock） ① 表级别的S锁、X锁 在对某个表执行SELECT、INSERT、DELETE、UPDATE语句时，InnoDB存储引擎是不会为这个表添加表级别的 S锁 或者 X锁 的。在对某个表执行一些诸如 ALTER TABLE 、 DROP TABLE 这类的 DDL 语句时，其 他事务对这个表并发执行诸如SELECT、INSERT、DELETE、UPDATE的语句会发生阻塞。同理，某个事务中对某个表执行SELECT、INSERT、DELETE、UPDATE语句时，在其他会话中对这个表执行 DDL 语句也会 发生阻塞。这个过程其实是通过在 server层使用一种称之为 元数据锁 （英文名： Metadata Locks ， 简称 MDL ）结构来实现的。 一般情况下，不会使用InnoDB存储引擎提供的表级别的 S锁 和 X锁 。只会在一些特殊情况下，比方说 崩溃恢复 过程中用到。比如，在系统变量 autocommit=0，innodb_table_locks = 1 时，手动获取InnoDB存储引擎提供的表t的 S锁 或者 X锁 可以这么写： LOCK TABLES t READ ：InnoDB存储引擎会对表 t 加表级别的 S锁 。 LOCK TABLES t WRITE ：InnoDB存储引擎会对表 t 加表级别的 X锁。 不过尽量避免在使用InnoDB存储引擎的表上使用 LOCK TABLES 这样的手动锁表语句，它们并不会提供什么额外的保护，只是会降低并发能力而已。InnoDB的厉害之处还是实现了更细粒度的 行锁 ，关于 InnoDB表级别的 S锁 和 X锁 大家了解一下就可以了。 **举例：**下面我们讲解MyISAM引擎下的表锁。 步骤1：创建表并添加数据 CREATE TABLE mylock( id INT NOT NULL PRIMARY KEY auto_increment, NAME VARCHAR(20) )ENGINE myisam; # 插入一条数据 INSERT INTO mylock(NAME) VALUES('a'); # 查询表中所有数据 SELECT * FROM mylock; +----+------+ | id | Name | +----+------+ | 1 | a | +----+------+ 步骤二：查看表上加过的锁 SHOW OPEN TABLES; # 主要关注In_use字段的值 或者 SHOW OPEN TABLES where In_use \u003e 0; 或者 上面的结果表明，当前数据库中没有被锁定的表 步骤3：手动增加表锁命令 LOCK TABLES t READ; # 存储引擎会对表t加表级别的共享锁。共享锁也叫读锁或S锁（Share的缩写） LOCK TABLES t WRITE; # 存储引擎会对表t加表级别的排他锁。排他锁也叫独占锁、写锁或X锁（exclusive的缩写） 比如： 步骤4：释放表锁 UNLOCK TABLES; # 使用此命令解锁当前加锁的表 比如： 步骤5：加读锁 我们为mylock表加read锁（读阻塞写），观察阻塞的情况，流程如下： 步骤6：加写锁 为mylock表加write锁，观察阻塞的情况，流程如下： 总结： MyISAM在执行查询语句（SELECT）前，会给涉及的所有表加读锁，在执行增删改操作前，会给涉及的表加写锁。InnoDB存储引擎是不会为这个表添加表级别的读锁和写锁的。 MySQL的表级锁有两种模式：（以MyISAM表进行操作的演示） 表共享读锁（Table Read Lock） 表独占写锁（Table Write Lock） ② 意向锁 （intention lock） InnoDB 支持 多粒度锁（multiple granularity locking） ，它允许 行级锁 与 表级锁 共存，而意向锁就是其中的一种 表锁 。 意向锁的存在是为了协调行锁和表锁的关系，支持多粒度（表锁和行锁）的锁并存。 意向锁是一种不与行级锁、表级锁冲突，这一点非常重要。 表明“某个事务正在某些行持有了锁或该事务准备去持有锁” 意向锁分为两种： 意向共享锁（intention shared lock, IS）：事务有意向对表中的某些行加共享锁（S锁） # 事务要获取某些行的 S 锁，必须先获得表的 IS 锁。 SELECT column FROM table ... LOCK IN SHARE MODE; 意向排他锁（intention exclusive lock, IX）：事务有意向对表中的某些行加排他锁（X锁） -- 事务要获取某些行的 X 锁，必须先获得表的 IX 锁。 SELECT column FROM table ... FOR UPDATE; 即：意向锁是由存储引擎 自己维护的 ，用户无法手动操作意向锁，在为数据行加共享 / 排他锁之前， InooDB 会先获取该数据行 所在数据表的对应意向锁 。 1. 意向锁要解决的问题 **举例：**创建表teacher,插入6条数据，事务的隔离级别默认为Repeatable-Read，如下所示。 CREATE TABLE `teacher` ( `id` int NOT NULL, `name` varchar(255) NOT NULL, PRIMARY KEY (`id`) )ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_0900_ai_ci; INSERT INTO `teacher` VALUES ('1', 'zhangsan'), ('2', 'lisi'), ('3', 'wangwu'), ('4', 'zhaoliu'), ('5', 'songhongkang'), ('6', 'leifengyang'); mysql\u003e SELECT @@transaction_isolation; +-------------------------+ | @@transaction_isolation | +-------------------------+ | REPEATABLE-READ | +-------------------------+ 假设事务A获取了某一行的排他锁，并未提交，语句如下所示: BEGIN; SELECT * FROM teacher WHERE id = 6 FOR UPDATE; 事务B想要获取teacher表的表读锁，语句如下： BEGIN; LOCK TABLES teacher READ; BEGIN; SELECT * FROM teacher WHERE id = 6 FOR UPDATE; 此时teacher表存在两把锁：teacher表上的意向排他锁与id未6的数据行上的排他锁。事务B想要获取teacher表的共享锁。 BEGIN; LOCK TABLES teacher READ; 此时事务B检测事务A持有teacher表的意向排他锁，就可以得知事务A必须持有该表中某些数据行的排他锁，那么事务B对teacher表的加锁请求就会被排斥（阻塞），而无需去检测表中的每一行数据是否存在排他锁。 意向锁的并发性 意向锁不会与行级的共享 / 排他锁互斥！正因为如此，意向锁并不会影响到多个事务对不同数据行加排他锁时的并发性。（不然我们直接用普通的表锁就行了） 我们扩展一下上面 teacher表的例子来概括一下意向锁的作用（一条数据从被锁定到被释放的过程中，可 能存在多种不同锁，但是这里我们只着重表现意向锁）。 事务A先获得了某一行的排他锁，并未提交： BEGIN; SELECT * FROM teacher WHERE id = 6 FOR UPDATE; 事务A获取了teacher表上的意向排他锁。事务A获取了id为6的数据行上的排他锁。之后事务B想要获取teacher表上的共享锁。 BEGIN; LOCK TABLES teacher READ; 事务B检测到事务A持有teacher表的意向排他锁。事务B对teacher表的加锁请求被阻塞（排斥）。最后事务C也想获取teacher表中某一行的排他锁。 BEGIN; SELECT * FROM teacher WHERE id = 5 FOR UPDATE; 事务C申请teacher表的意向排他锁。事务C检测到事务A持有teacher表的意向排他锁。因为意向锁之间并不互斥，所以事务C获取到了teacher表的意向排他锁。因为id为5的数据行上不存在任何排他锁，最终事务C成功获取到了该数据行上的排他锁。 从上面的案例可以得到如下结论： InnoDB 支持 多粒度锁 ，特定场景下，行级锁可以与表级锁共存。 意向锁之间互不排斥，但除了 IS 与 S 兼容外， 意向锁会与 共享锁 / 排他锁 互斥 。 IX，IS是表级锁，不会和行级的X，S锁发生冲突。只会和表级的X，S发生冲突。 意向锁在保证并发性的前提下，实现了 行锁和表锁共存 且 满足事务隔离性 的要求。 ③ 自增锁（AUTO-INC锁） 在使用MySQL过程中，我们可以为表的某个列添加 AUTO_INCREMENT 属性。举例： CREATE TABLE `teacher` ( `id` int NOT NULL AUTO_INCREMENT, `name` varchar(255) NOT NULL, PRIMAR","date":"2022-10-20","objectID":"/mysql%E4%BA%8B%E5%8A%A1%E7%AF%87/:9:2","tags":[],"title":"MySQL事务篇","uri":"/mysql%E4%BA%8B%E5%8A%A1%E7%AF%87/"},{"categories":["mysql"],"content":"3.3 从对待锁的态度划分:乐观锁、悲观锁 从对待锁的态度来看锁的话，可以将锁分成乐观锁和悲观锁，从名字中也可以看出这两种锁是两种看待 数据并发的思维方式 。需要注意的是，乐观锁和悲观锁并不是锁，而是锁的 设计思想 。 1. 悲观锁（Pessimistic Locking） 悲观锁是一种思想，顾名思义，就是很悲观，对数据被其他事务的修改持保守态度，会通过数据库自身的锁机制来实现，从而保证数据操作的排它性。 悲观锁总是假设最坏的情况，每次去拿数据的时候都认为别人会修改，所以每次在拿数据的时候都会上锁，这样别人想拿这个数据就会 阻塞 直到它拿到锁（共享资源每次只给一个线程使用，其它线程阻塞， 用完后再把资源转让给其它线程）。比如行锁，表锁等，读锁，写锁等，都是在做操作之前先上锁，当其他线程想要访问数据时，都需要阻塞挂起。Java中 synchronized 和 ReentrantLock 等独占锁就是悲观锁思想的实现。 秒杀案例1： 2. 乐观锁（Optimistic Locking） 乐观锁认为对同一数据的并发操作不会总发生，属于小概率事件，不用每次都对数据上锁，但是在更新的时候会判断一下在此期间别人有没有去更新这个数据，也就是不采用数据库自身的锁机制，而是通过程序来实现。在程序上，我们可以采用 版本号机制 或者 CAS机制 实现。乐观锁适用于多读的应用类型， 这样可以提高吞吐量。在Java中 java.util.concurrent.atomic 包下的原子变量类就是使用了乐观锁的一种实现方式：CAS实现的。 1. 乐观锁的版本号机制 在表中设计一个 版本字段 version ，第一次读的时候，会获取 version 字段的取值。然后对数据进行更新或删除操作时，会执行 UPDATE ... SET version=version+1 WHERE version=version 。此时如果已经有事务对这条数据进行了更改，修改就不会成功。 这种方式类似我们熟悉的SVN、CVS版本管理系统，当我们修改了代码进行提交时，首先会检查当前版本号与服务器上的版本号是否一致，如果一致就可以直接提交，如果不一致就需要更新服务器上的最新代码，然后再进行提交。 2. 乐观锁的时间戳机制 时间戳和版本号机制一样，也是在更新提交的时候，将当前数据的时间戳和更新之前取得的时间戳进行 比较，如果两者一致则更新成功，否则就是版本冲突。 你能看到乐观锁就是程序员自己控制数据并发操作的权限，基本是通过给数据行增加一个戳（版本号或者时间戳），从而证明当前拿到的数据是否最新。 3. 两种锁的适用场景 从这两种锁的设计思想中，我们总结一下乐观锁和悲观锁的适用场景： 乐观锁 适合 读操作多 的场景，相对来说写的操作比较少。它的优点在于程序实现，不存在死锁问题，不过适用场景也会相对乐观，因为它阻止不了除了程序以外的数据库操作。而且，如果我们对同一条数据又很多个事务进行修改，那么只有其中一个事务成功。因为一个事务更新成功就会修改当前数据行的版本号，那么其他事务的修改就会失败，需要读取数据新的版本从而基于这个新版本更新。这样做可以更好地保证数据库的并发正确性，但是如果修改操作过多，可能会引起雪崩式的反应：长时间会有大量的事务失败返回。虽然保证了并发场景下的修改正确性，但是在修改较多的场景下并发性能也下降了。所以，乐观锁不适合在写操作较多的情况下使用。 悲观锁 适合 写操作多 的场景，因为写的操作具有 排它性 。采用悲观锁的方式，可以在数据库层面阻止其他事务对该数据的操作权限，防止 读 - 写 和 写 - 写 的冲突。 ","date":"2022-10-20","objectID":"/mysql%E4%BA%8B%E5%8A%A1%E7%AF%87/:9:3","tags":[],"title":"MySQL事务篇","uri":"/mysql%E4%BA%8B%E5%8A%A1%E7%AF%87/"},{"categories":["mysql"],"content":"3.4 按加锁的方式划分：显式锁、隐式锁 1. 隐式锁 情景一：对于聚簇索引记录来说，有一个 trx_id 隐藏列，该隐藏列记录着最后改动该记录的 事务 id 。那么如果在当前事务中新插入一条聚簇索引记录后，该记录的 trx_id 隐藏列代表的的就是 当前事务的 事务id ，如果其他事务此时想对该记录添加 S锁 或者 X锁 时，首先会看一下该记录的 trx_id 隐藏列代表的事务是否是当前的活跃事务，如果是的话，那么就帮助当前事务创建一个 X 锁 （也就是为当前事务创建一个锁结构， is_waiting 属性是 false ），然后自己进入等待状态 （也就是为自己也创建一个锁结构， is_waiting 属性是 true ）。 情景二：对于二级索引记录来说，本身并没有 trx_id 隐藏列，但是在二级索引页面的 Page Header 部分有一个 PAGE_MAX_TRX_ID 属性，该属性代表对该页面做改动的最大的 事务id ，如 果 PAGE_MAX_TRX_ID 属性值小于当前最小的活跃 事务id ，那么说明对该页面做修改的事务都已 经提交了，否则就需要在页面中定位到对应的二级索引记录，然后回表找到它对应的聚簇索引记 录，然后再重复 情景一 的做法。 session 1: mysql\u003e begin; Query OK, 0 rows affected (0.00 sec) mysql\u003e insert INTO student VALUES(34,\"周八\",\"二班\"); # 当前行的聚簇索引行格式里的trx_id为当前事务的id，这里算一个隐式锁（还没形成锁结构，一旦另外一个事务检测到了当前行，当前行的隐式锁回转化为显示的锁结构。这样另外一个事务就会进入阻塞等待状态） Query OK, 1 row affected (0.00 sec) session 2: mysql\u003e begin; Query OK, 0 rows affected (0.00 sec) mysql\u003e select * from student lock in share mode; # 执行完，当前事务被阻塞，因为当前操作检测到的行出现了隐式锁，然后隐式锁会转化为显示锁 执行下述语句，输出结果： mysql\u003e SELECT * FROM performance_schema.data_lock_waits\\G; *************************** 1. row *************************** ENGINE: INNODB REQUESTING_ENGINE_LOCK_ID: 140562531358232:7:4:9:140562535668584 REQUESTING_ENGINE_TRANSACTION_ID: 422037508068888 REQUESTING_THREAD_ID: 64 REQUESTING_EVENT_ID: 6 REQUESTING_OBJECT_INSTANCE_BEGIN: 140562535668584 BLOCKING_ENGINE_LOCK_ID: 140562531351768:7:4:9:140562535619104 BLOCKING_ENGINE_TRANSACTION_ID: 15902 BLOCKING_THREAD_ID: 64 BLOCKING_EVENT_ID: 6 BLOCKING_OBJECT_INSTANCE_BEGIN: 140562535619104 1 row in set (0.00 sec) 隐式锁的逻辑过程如下： A. InnoDB的每条记录中都一个隐含的trx_id字段，这个字段存在于聚簇索引的B+Tree中。 B. 在操作一条记录前，首先根据记录中的trx_id检查该事务是否是活动的事务(未提交或回滚)。如果是活动的事务，首先将 隐式锁 转换为 显式锁 (就是为该事务添加一个锁)。 C. 检查是否有锁冲突，如果有冲突，创建锁，并设置为 is waiting 状态（给自己加一个意向锁）。如果没有冲突不加锁，跳到E。 D. 等待加锁成功，被唤醒，或者超时。 E. 写数据，并将自己的trx_id写入trx_id字段。 2. 显式锁 通过特定的语句进行加锁，我们一般称之为显示加锁，例如： 显示加共享锁： select .... lock in share mode 显示加排它锁： select .... for update ","date":"2022-10-20","objectID":"/mysql%E4%BA%8B%E5%8A%A1%E7%AF%87/:9:4","tags":[],"title":"MySQL事务篇","uri":"/mysql%E4%BA%8B%E5%8A%A1%E7%AF%87/"},{"categories":["mysql"],"content":"3.5 其它锁之：全局锁 全局锁就是对 整个数据库实例 加锁。当你需要让整个库处于 只读状态 的时候，可以使用这个命令，之后 其他线程的以下语句会被阻塞：数据更新语句（数据的增删改）、数据定义语句（包括建表、修改表结 构等）和更新类事务的提交语句。全局锁的典型使用 场景 是：做 全库逻辑备份 。 全局锁的命令： Flush tables with read lock ","date":"2022-10-20","objectID":"/mysql%E4%BA%8B%E5%8A%A1%E7%AF%87/:9:5","tags":[],"title":"MySQL事务篇","uri":"/mysql%E4%BA%8B%E5%8A%A1%E7%AF%87/"},{"categories":["mysql"],"content":"3.6 其它锁之：死锁 1. 概念 两个事务都持有对方需要的锁，并且在等待对方释放，并且双方都不会释放自己的锁。 举例1： 下面这个例子加的锁显然是隐式锁 举例2： 用户A给用户B转账100，再次同时，用户B也给用户A转账100。这个过程，可能导致死锁。 2. 产生死锁的必要条件 两个或者两个以上事务 每个事务都已经持有锁并且申请新的锁 锁资源同时只能被同一个事务持有或者不兼容 事务之间因为持有锁和申请锁导致彼此循环等待 死锁的关键在于：两个（或以上）的Session加锁的顺序不一致。 3. 如何处理死锁 **方式1：**等待，直到超时（innodb_lock_wait_timeout=50s) **方式2：**使用死锁检测处理死锁程序 方式1检测死锁太过被动，innodb还提供了wait-for graph算法来主动进行死锁检测，每当加锁请求无法立即满足需要并进入等待时，wait-for graph 算法都会被触发。 这是一种较为主动的死锁检测机制，要求数据库保存锁的信息链表和事物等待链表两部分信息。 基于这两个信息，可以绘制wait-for graph（等待图） 死锁检测的原理是构建一个以事务为顶点，锁为边的有向图，判断有向图是否存在环，存在既有死锁。 一旦检测到回路、有死锁，这时候InnoDB存储引擎会选择回滚undo量最小的事务，让其他事务继续执行（innodb_deadlock_detect=on表示开启这个逻辑）。 缺点：每个新的被阻塞的线程，都要判断是不是由于自己的加入导致了死锁，这个操作时间复杂度是O(n)。如果100个并发线程同时更新同一行，意味着要检测100*100=1万次，1万个线程就会有1千万次检测。 如何解决？ 方式1：关闭死锁检测，但意味着可能会出现大量的超时，会导致业务有损。 方式2：控制并发访问的数量。比如在中间件中实现对于相同行的更新，在进入引擎之前排队，这样在InnoDB内部就不会有大量的死锁检测工作。 进一步的思路： 可以考虑通过将一行改成逻辑上的多行来减少锁冲突。比如，连锁超市账户总额的记录，可以考虑放到多条记录上。账户总额等于这多个记录的值的总和。 4. 如何避免死锁 ","date":"2022-10-20","objectID":"/mysql%E4%BA%8B%E5%8A%A1%E7%AF%87/:9:6","tags":[],"title":"MySQL事务篇","uri":"/mysql%E4%BA%8B%E5%8A%A1%E7%AF%87/"},{"categories":["mysql"],"content":"4. 锁的内部结构 我们前边说对一条记录加锁的本质就是在内存中创建一个锁结构与之关联，那么是不是一个事务对多条记录加锁，就要创建多个锁结构呢？比如： # 事务T1 SELECT * FROM user LOCK IN SHARE MODE; 理论上创建多个锁结构没问题，但是如果一个事务要获取10000条记录的锁，生成10000个锁结构也太崩溃了！所以决定在对不同记录加锁时，如果符合下边这些条件的记录会放在一个锁结构中。 在同一个事务中进行加锁操作 被加锁的记录在同一个页面中 加锁的类型是一样的 等待状态是一样的 InnoDB 存储引擎中的 锁结构 如下： 结构解析： 1. 锁所在的事务信息 ： 不论是 表锁 还是 行锁 ，都是在事务执行过程中生成的，哪个事务生成了这个锁结构 ，这里就记录这个事务的信息。 此 锁所在的事务信息 在内存结构中只是一个指针，通过指针可以找到内存中关于该事务的更多信息，比方说事务id等。 2. 索引信息 ： 对于 行锁 来说，需要记录一下加锁的记录是属于哪个索引的。这里也是一个指针。 3. 表锁／行锁信息 ： 表锁结构 和 行锁结构 在这个位置的内容是不同的： 表锁： 记载着是对哪个表加的锁，还有其他的一些信息。 行锁： 记载了三个重要的信息： Space ID ：记录所在表空间。 Page Number ：记录所在页号。 n_bits ：对于行锁来说，一条记录就对应着一个比特位，一个页面中包含很多记录，用不同 的比特位来区分到底是哪一条记录加了锁。为此在行锁结构的末尾放置了一堆比特位，这个n_bis属性代表使用了多少比特位。 n_bits的值一般都比页面中记录条数多一些。主要是为了之后在页面中插入了新记录后 也不至于重新分配锁结构 4. type_mode ： 这是一个32位的数，被分成了 lock_mode 、 lock_type 和 rec_lock_type 三个部分，如图所示： 锁的模式（ lock_mode ），占用低4位，可选的值如下： LOCK_IS （十进制的 0 ）：表示共享意向锁，也就是 IS锁 。 LOCK_IX （十进制的 1 ）：表示独占意向锁，也就是 IX锁 。 LOCK_S （十进制的 2 ）：表示共享锁，也就是 S锁 。 LOCK_X （十进制的 3 ）：表示独占锁，也就是 X锁 。 LOCK_AUTO_INC （十进制的 4 ）：表示 AUTO-INC锁 。 在InnoDB存储引擎中，LOCK_IS，LOCK_IX，LOCK_AUTO_INC都算是表级锁的模式，LOCK_S和 LOCK_X既可以算是表级锁的模式，也可以是行级锁的模式。 锁的类型（ lock_type ），占用第5～8位，不过现阶段只有第5位和第6位被使用： LOCK_TABLE （十进制的 16 ），也就是当第5个比特位置为1时，表示表级锁。 LOCK_REC （十进制的 32 ），也就是当第6个比特位置为1时，表示行级锁。 行锁的具体类型（ rec_lock_type ），使用其余的位来表示。只有在 lock_type 的值为 LOCK_REC 时，也就是只有在该锁为行级锁时，才会被细分为更多的类型： LOCK_ORDINARY （十进制的 0 ）：表示 next-key锁 。 LOCK_GAP （十进制的 512 ）：也就是当第10个比特位置为1时，表示 gap锁 。 LOCK_REC_NOT_GAP （十进制的 1024 ）：也就是当第11个比特位置为1时，表示正经 记录锁 。 LOCK_INSERT_INTENTION （十进制的 2048 ）：也就是当第12个比特位置为1时，表示插入意向锁。其他的类型：还有一些不常用的类型我们就不多说了。 is_waiting 属性呢？基于内存空间的节省，所以把 is_waiting 属性放到了 type_mode 这个32 位的数字中： LOCK_WAIT （十进制的 256 ） ：当第9个比特位置为 1 时，表示 is_waiting 为 true ，也 就是当前事务尚未获取到锁，处在等待状态；当这个比特位为 0 时，表示 is_waiting 为 false ，也就是当前事务获取锁成功。 5. 其他信息 ： 为了更好的管理系统运行过程中生成的各种锁结构而设计了各种哈希表和链表。 6. 一堆比特位 ： 如果是 行锁结构 的话，在该结构末尾还放置了一堆比特位，比特位的数量是由上边提到的 n_bits 属性 表示的。InnoDB数据页中的每条记录在 记录头信息 中都包含一个 heap_no 属性，伪记录 Infimum 的 heap_no 值为 0 ， Supremum 的 heap_no 值为 1 ，之后每插入一条记录， heap_no 值就增1。 锁结构最后的一堆比特位就对应着一个页面中的记录，一个比特位映射一个 heap_no ，即一个比特位映射 到页内的一条记录。 ","date":"2022-10-20","objectID":"/mysql%E4%BA%8B%E5%8A%A1%E7%AF%87/:10:0","tags":[],"title":"MySQL事务篇","uri":"/mysql%E4%BA%8B%E5%8A%A1%E7%AF%87/"},{"categories":["mysql"],"content":"5. 锁监控 关于MySQL锁的监控，我们一般可以通过检查 InnoDB_row_lock 等状态变量来分析系统上的行锁的争夺情况 mysql\u003e show status like 'innodb_row_lock%'; +-------------------------------+-------+ | Variable_name | Value | +-------------------------------+-------+ | Innodb_row_lock_current_waits | 0 | | Innodb_row_lock_time | 0 | | Innodb_row_lock_time_avg | 0 | | Innodb_row_lock_time_max | 0 | | Innodb_row_lock_waits | 0 | +-------------------------------+-------+ 5 rows in set (0.01 sec) 对各个状态量的说明如下： Innodb_row_lock_current_waits：当前正在等待锁定的数量； Innodb_row_lock_time：从系统启动到现在锁定总时间长度；（等待总时长） Innodb_row_lock_time_avg：每次等待所花平均时间；（等待平均时长） Innodb_row_lock_time_max：从系统启动到现在等待最常的一次所花的时间； Innodb_row_lock_waits：系统启动后到现在总共等待的次数；（等待总次数） 对于这5个状态变量，比较重要的3个见上面（灰色）。 尤其是当等待次数很高，而且每次等待时长也不小的时候，我们就需要分析系统中为什么会有如此多的等待，然后根据分析结果着手指定优化计划。 其他监控方法： MySQL把事务和锁的信息记录在了 information_schema 库中，涉及到的三张表分别是 INNODB_TRX 、 INNODB_LOCKS 和 INNODB_LOCK_WAITS 。 MySQL5.7及之前 ，可以通过information_schema.INNODB_LOCKS查看事务的锁情况，但只能看到阻塞事务的锁；如果事务并未被阻塞，则在该表中看不到该事务的锁情况。 MySQL8.0删除了information_schema.INNODB_LOCKS，添加了 performance_schema.data_locks，可以通过performance_schema.data_locks查看事务的锁情况，和MySQL5.7及之前不同，performance_schema.data_locks不但可以看到阻塞该事务的锁，还可以看到该事务所持有的锁。 同时，information_schema.INNODB_LOCK_WAITS也被 performance_schema.data_lock_waits所代替。 我们模拟一个锁等待的场景，以下是从这三张表收集的信息锁等待场景，我们依然使用记录锁中的案例，当事务2进行等待时，查询情况如下： （1）查询正在被锁阻塞的sql语句。 SELECT * FROM information_schema.INNODB_TRX\\G; 重要属性代表含义已在上述中标注。 （2）查询锁等待情况 SELECT * FROM data_lock_waits\\G; *************************** 1. row *************************** ENGINE: INNODB REQUESTING_ENGINE_LOCK_ID: 139750145405624:7:4:7:139747028690608 REQUESTING_ENGINE_TRANSACTION_ID: 13845 #被阻塞的事务ID REQUESTING_THREAD_ID: 72 REQUESTING_EVENT_ID: 26 REQUESTING_OBJECT_INSTANCE_BEGIN: 139747028690608 BLOCKING_ENGINE_LOCK_ID: 139750145406432:7:4:7:139747028813248 BLOCKING_ENGINE_TRANSACTION_ID: 13844 #正在执行的事务ID，阻塞了13845 BLOCKING_THREAD_ID: 71 BLOCKING_EVENT_ID: 24 BLOCKING_OBJECT_INSTANCE_BEGIN: 139747028813248 1 row in set (0.00 sec) （3）查询锁的情况 mysql \u003e SELECT * from performance_schema.data_locks\\G; *************************** 1. row *************************** ENGINE: INNODB ENGINE_LOCK_ID: 139750145405624:1068:139747028693520 ENGINE_TRANSACTION_ID: 13847 THREAD_ID: 72 EVENT_ID: 31 OBJECT_SCHEMA: atguigu OBJECT_NAME: user PARTITION_NAME: NULL SUBPARTITION_NAME: NULL INDEX_NAME: NULL OBJECT_INSTANCE_BEGIN: 139747028693520 LOCK_TYPE: TABLE LOCK_MODE: IX LOCK_STATUS: GRANTED LOCK_DATA: NULL *************************** 2. row *************************** ENGINE: INNODB ENGINE_LOCK_ID: 139750145405624:7:4:7:139747028690608 ENGINE_TRANSACTION_ID: 13847 THREAD_ID: 72 EVENT_ID: 31 OBJECT_SCHEMA: atguigu OBJECT_NAME: user PARTITION_NAME: NULL SUBPARTITION_NAME: NULL INDEX_NAME: PRIMARY OBJECT_INSTANCE_BEGIN: 139747028690608 LOCK_TYPE: RECORD LOCK_MODE: X,REC_NOT_GAP LOCK_STATUS: WAITING LOCK_DATA: 1 *************************** 3. row *************************** ENGINE: INNODB ENGINE_LOCK_ID: 139750145406432:1068:139747028816304 ENGINE_TRANSACTION_ID: 13846 THREAD_ID: 71 EVENT_ID: 28 OBJECT_SCHEMA: atguigu OBJECT_NAME: user PARTITION_NAME: NULL SUBPARTITION_NAME: NULL INDEX_NAME: NULL OBJECT_INSTANCE_BEGIN: 139747028816304 LOCK_TYPE: TABLE LOCK_MODE: IX LOCK_STATUS: GRANTED LOCK_DATA: NULL *************************** 4. row *************************** ENGINE: INNODB ENGINE_LOCK_ID: 139750145406432:7:4:7:139747028813248 ENGINE_TRANSACTION_ID: 13846 THREAD_ID: 71 EVENT_ID: 28 OBJECT_SCHEMA: atguigu OBJECT_NAME: user PARTITION_NAME: NULL SUBPARTITION_NAME: NULL INDEX_NAME: PRIMARY OBJECT_INSTANCE_BEGIN: 139747028813248 LOCK_TYPE: RECORD LOCK_MODE: X,REC_NOT_GAP LOCK_STATUS: GRANTED LOCK_DATA: 1 4 rows in set (0.00 sec) ERROR: No query specified 从锁的情况可以看出来，两个事务分别获取了IX锁，我们从意向锁章节可以知道，IX锁互相时兼容的。所 以这里不会等待，但是事务1同样持有X锁，此时事务2也要去同一行记录获取X锁，他们之间不兼容，导致等待的情况发生。 ","date":"2022-10-20","objectID":"/mysql%E4%BA%8B%E5%8A%A1%E7%AF%87/:11:0","tags":[],"title":"MySQL事务篇","uri":"/mysql%E4%BA%8B%E5%8A%A1%E7%AF%87/"},{"categories":["mysql"],"content":"6. 附录 间隙锁加锁规则（共11个案例） 间隙锁是在可重复读隔离级别下才会生效的：next-key lock 实际上是由间隙锁加行锁实现的，如果切换到读提交隔离级别 (read-committed) 的话，就好理解了，过程中去掉间隙锁的部分，也就是只剩下行锁的部分。而在读提交隔离级别下间隙锁就没有了，为了解决可能出现的数据和日志不一致问题，需要把 binlog 格式设置为 row 。也就是说，许多公司的配置为：读提交隔离级别加 binlog_format=row。业务不需要可重复读的保证，这样考虑到读提交下操作数据的锁范围更小（没有间隙锁），这个选择是合理的。 next-key lock 的加锁规则（临建锁），总结的加锁规则里面，包含了两个原则、两个优化和一个bug： 原则 1 ：加锁的基本单位是 next-key lock 。 next-key lock 是前开后闭区间。 原则 2 ：查找过程中访问到的对象才会加锁。任何辅助索引上的锁，或者非索引列上的锁，最终都要回溯到主键上，在主键上也要加一把锁。 优化 1 ：索引上的等值查询，给唯一索引加锁的时候， next-key lock 退化为行锁。也就是说如果 InnoDB扫描的是一个主键、或是一个唯一索引的话，那InnoDB只会采用行锁方式来加锁 优化 2 ：索引上（不一定是唯一索引）的等值查询，向右遍历时且最后一个值不满足等值条件的时候，next-key lock 退化为间隙锁。 一个 bug ：唯一索引上的范围查询会访问到不满足条件的第一个值为止。 我们以表test作为例子，建表语句和初始化语句如下：其中id为主键索引。 CREATE TABLE `test` ( `id` int(11) NOT NULL, `col1` int(11) DEFAULT NULL, `col2` int(11) DEFAULT NULL, PRIMARY KEY (`id`), KEY `col1` (`col1`) ) ENGINE=InnoDB; insert into test values(0,0,0),(5,5,5), (10,10,10),(15,15,15),(20,20,20),(25,25,25); 案例一：唯一索引等值查询间隙锁 由于表 test 中没有 id=7 的记录 根据原则 1 ，加锁单位是 next-key lock ， session A 加锁范围就是 (5,10] ； 同时根据优化 2 ，这是一个等值查询 (id=7) ，而 id=10 不满足查询条件，next-key lock 退化成间隙锁，因此最终加锁的范围是 (5,10) 案例二：非唯一索引等值查询锁 这里 session A 要给索引 col1 上 col1=5 的这一行加上读锁。 根据原则 1 ，加锁单位是 next-key lock ，左开右闭，5是闭上的，因此会给 (0,5] 加上 next-key lock。 要注意 c 是普通索引，因此仅访问 c=5 这一条记录是不能马上停下来的（可能有col1=5的其他记录），需要向右遍历，查到c=10才放弃。根据原则 2 ，访问到的都要加锁，因此要给 (5,10] 加 next-key lock 。 但是同时这个符合优化 2 ：等值判断，向右遍历，最后一个值不满足 col1=5 这个等值条件，因此退化成间隙锁 (5,10) 。 根据原则 2 ， 只有访问到的对象才会加锁，这个查询使用覆盖索引，并不需要访问主键索引，所以主键索引上没有加任何锁，这就是为什么 session B 的 update 语句可以执行完成。 但 session C 要插入一个 (7,7,7) 的记录，就会被 session A 的间隙锁 (5,10) 锁住。这个例子说明，锁是加在索引上的。 执行 for update 时，系统会认为你接下来要更新数据，因此会顺便给主键索引上满足条件的行加上行锁。 如果你要用 lock in share mode来给行加读锁避免数据被更新的话，就必须得绕过覆盖索引的优化，因为覆盖索引不会访问主键索引，不会给主键索引上加锁 案例三：主键索引范围查询锁 上面两个例子是等值查询的，这个例子是关于范围查询的，也就是说下面的语句 select * from test where id=10 for update; # 此处只加记录锁，没有间隙锁、临键锁 select * from tets where id\u003e=10 and id\u003c11 for update; 这两条查语句肯定是等价的，但是它们的加锁规则不太一样 开始执行的时候，要找到第一个 id=10 的行，因此本该是 next-key lock (5,10] 。 根据优化 1 ，主键 id 上的等值条件，退化成行锁，只加了 id=10 这一行的行锁。 它是范围查询，范围查找就往后继续找，找到 id=15 这一行停下来，不满足条件，因此需要加 next-key lock (10,15]。 session A 这时候锁的范围就是主键索引上，行锁 id=10 和 next-key lock(10,15] 。首次 session A 定位查找 id=10 的行的时候，是当做等值查询来判断的，而向右扫描到 id=15 的时候，用的是范围查询判断。 案例四：非唯一索引范围查询锁 与案例三不同的是，案例四中查询语句的 where 部分用的是字段 col1 ，它是普通索引 这两条查语句肯定是等价的，但是它们的加锁规则不太一样 在第一次用 col1=10 定位记录的时候，索引 c 上加了 (5,10] 这个 next-key lock 后，由于索引 col1 是非唯一索引，没有优化规则，也就是说不会蜕变为行锁，因此最终 sesion A 加的锁是，索引 c 上的 (5,10] 和 (10,15] 这两个 next-keylock 。 这里需要扫描到 col1=15 才停止扫描，是合理的，因为 InnoDB 要扫到 col1=15 ，才知道不需要继续往后找了。 案例五：唯一索引范围查询锁bug session A 是一个范围查询，按照原则 1 的话，应该是索引 id 上只加 (10,15] 这个 next-key lock ，并且因为 id 是唯一键，所以循环判断到 id=15 这一行就应该停止了。 **但是实现上， InnoDB 会往前扫描到第一个不满足条件的行为止，也就是 id=20 。而且由于这是个范围扫描，因此索引 id 上的 (15,20] 这个 next-key lock 也会被锁上。**照理说，这里锁住 id=20 这一行的行为，其实是没有必要的。因为扫描到 id=15 ，就可以确定不用往后再找了。 案例六：非唯一索引上存在等值的例子 这里，我给表 t 插入一条新记录：insert into t values(30,10,30);也就是说，现在表里面有两个c=10的行 但是它们的主键值 id 是不同的（分别是 10 和 30 ），因此这两个c=10 的记录之间，也是有间隙的。 这次我们用 delete 语句来验证。注意， delete 语句加锁的逻辑，其实跟 select … for update 是类似的， 也就是我在文章开始总结的两个 “ 原则 ” 、两个 “ 优化 ” 和一个 “bug” 。 这时， session A 在遍历的时候，先访问第一个 col1=10 的记录。同样地，根据原则 1 ，这里加的是 (col1=5,id=5) 到 (col1=10,id=10) 这个 next-key lock 。 由于c是普通索引，所以继续向右查找，直到碰到 (col1=15,id=15) 这一行循环才结束。根据优化 2 ，这是 一个等值查询，向右查找到了不满足条件的行，所以会退化成 (col1=10,id=10) 到 (col1=15,id=15) 的间隙锁。 这个 delete 语句在索引 c 上的加锁范围，就是上面图中蓝色区域覆盖的部分。这个蓝色区域左右两边都 是虚线，表示开区间，即 (col1=5,id=5) 和 (col1=15,id=15) 这两行上都没有锁 案例七： limit 语句加锁 例子 6 也有一个对照案例，场景如下所示： session A 的 delete 语句加了 limit 2 。你知道表 t 里 c=10 的记录其实只有两条，因此加不加 limit 2 ，删除的效果都是一样的。但是加锁效果却不一样 这是因为，案例七里的 delete 语句明确加了 limit 2 的限制，因此在遍历到 (col1=10, id=30) 这一行之后， 满足条件的语句已经有两条，循环就结束了。因此，索引 col1 上的加锁范围就变成了从（ col1=5,id=5) 到（ col1=10,id=30) 这个前开后闭区间，如下图所示： 这个例子对我们实践的指导意义就是， 在删除数据的时候尽量加 limit 。 这样不仅可以控制删除数据的条数，让操作更安全，还可以减小加锁的范围。 案例八：一个死锁的例子 session A 启动事务后执行查询语句加 lock in share mode ，在索引 col1 上加了 next-keylock(5,10] 和 间隙锁 (10,15) （索引向右遍历退化为间隙锁）； session B 的 update 语句也要在索引 c 上加 next-key lock(5,10] ，进入锁等待； 实际上分成了两步， 先是加 (5,10) 的间隙锁，加锁成功；然后加 col1=10 的行锁，因为se","date":"2022-10-20","objectID":"/mysql%E4%BA%8B%E5%8A%A1%E7%AF%87/:12:0","tags":[],"title":"MySQL事务篇","uri":"/mysql%E4%BA%8B%E5%8A%A1%E7%AF%87/"},{"categories":["mysql"],"content":"1. 什么是MVCC MVCC （Multiversion Concurrency Control），多版本并发控制。顾名思义，MVCC 是通过数据行的多个版本管理来实现数据库的 并发控制 。这项技术使得在InnoDB的事务隔离级别下执行 一致性读 操作有了保证。换言之，就是为了查询一些正在被另一个事务更新的行，并且可以看到它们被更新之前的值，这样 在做查询的时候就不用等待另一个事务释放锁。 MVCC没有正式的标准，在不同的DBMS中MVCC的实现方式可能是不同的，也不是普遍使用的（大家可以参考相关的DBMS文档）。这里讲解InnoDB中MVCC的实现机制（MySQL其他的存储引擎并不支持它）。 ","date":"2022-10-20","objectID":"/mysql%E4%BA%8B%E5%8A%A1%E7%AF%87/:13:0","tags":[],"title":"MySQL事务篇","uri":"/mysql%E4%BA%8B%E5%8A%A1%E7%AF%87/"},{"categories":["mysql"],"content":"2. 快照读与当前读 MVCC在MySQL InnoDB中的实现主要是为了提高数据库并发性能，用更好的方式去处理 读-写冲突 ，做到 即使有读写冲突时，也能做到 不加锁 ， 非阻塞并发读 ，而这个读指的就是 快照读 , 而非 当前读 。当前 读实际上是一种加锁的操作，是悲观锁的实现。而MVCC本质是采用乐观锁思想的一种方式。 ","date":"2022-10-20","objectID":"/mysql%E4%BA%8B%E5%8A%A1%E7%AF%87/:14:0","tags":[],"title":"MySQL事务篇","uri":"/mysql%E4%BA%8B%E5%8A%A1%E7%AF%87/"},{"categories":["mysql"],"content":"2.1 快照读 快照读又叫一致性读，读取的是快照数据。不加锁的简单的 SELECT 都属于快照读，即不加锁的非阻塞读；比如这样： SELECT * FROM player WHERE ... 之所以出现快照读的情况，是基于提高并发性能的考虑，快照读的实现是基于MVCC，它在很多情况下， 避免了加锁操作，降低了开销。 既然是基于多版本，那么快照读可能读到的并不一定是数据的最新版本，而有可能是之前的历史版本。 快照读的前提是隔离级别不是串行级别，串行级别下的快照读会退化成当前读。 ","date":"2022-10-20","objectID":"/mysql%E4%BA%8B%E5%8A%A1%E7%AF%87/:14:1","tags":[],"title":"MySQL事务篇","uri":"/mysql%E4%BA%8B%E5%8A%A1%E7%AF%87/"},{"categories":["mysql"],"content":"2.2 当前读 当前读读取的是记录的最新版本（最新数据，而不是历史版本的数据），读取时还要保证其他并发事务不能修改当前记录，会对读取的记录进行加锁。加锁的 SELECT，或者对数据进行增删改都会进行当前读。比如： SELECT * FROM student LOCK IN SHARE MODE; # 共享锁 SELECT * FROM student FOR UPDATE; # 排他锁 INSERT INTO student values ... # 排他锁 DELETE FROM student WHERE ... # 排他锁 UPDATE student SET ... # 排他锁 ","date":"2022-10-20","objectID":"/mysql%E4%BA%8B%E5%8A%A1%E7%AF%87/:14:2","tags":[],"title":"MySQL事务篇","uri":"/mysql%E4%BA%8B%E5%8A%A1%E7%AF%87/"},{"categories":["mysql"],"content":"3. 复习 ","date":"2022-10-20","objectID":"/mysql%E4%BA%8B%E5%8A%A1%E7%AF%87/:15:0","tags":[],"title":"MySQL事务篇","uri":"/mysql%E4%BA%8B%E5%8A%A1%E7%AF%87/"},{"categories":["mysql"],"content":"3.1 再谈隔离级别 我们知道事务有 4 个隔离级别，可能存在三种并发问题： ","date":"2022-10-20","objectID":"/mysql%E4%BA%8B%E5%8A%A1%E7%AF%87/:15:1","tags":[],"title":"MySQL事务篇","uri":"/mysql%E4%BA%8B%E5%8A%A1%E7%AF%87/"},{"categories":["mysql"],"content":"3.2 隐藏字段、Undo Log版本链 回顾一下undo日志的版本链，对于使用 InnoDB 存储引擎的表来说，它的聚簇索引记录中都包含两个必要的隐藏列。 trx_id ：每次一个事务对某条聚簇索引记录进行改动时，都会把该事务的 事务id 赋值给 trx_id 隐藏列。(类似于一个隐式锁，当有另外一个事务来查询当前行时，如果发现存在一个trx_id，隐式锁会蜕变为显示锁) roll_pointer ：每次对某条聚簇索引记录进行改动时，都会把旧的版本写入到 undo日志 中，然后这个隐藏列就相当于一个指针，可以通过它来找到该记录修改前的信息。 假设插入该记录的事务id为8，那么此刻该条记录的示意图如下所示： insert undo只在事务回滚时起作用，当事务提交后，该类型的undo日志就没用了，它占用的Undo Log Segment也会被系统回收（也就是该undo日志占用的Undo页面链表要么被重用，要么被释放）。 假设之后两个事务id分别为10、20的事务对这条记录进行 UPDATE 操作，操作流程如下： 每次对记录进行改动，都会记录一条undo日志，每条undo日志也都有一个roll_pointer属性（INSERT操作对应的undo日志没有该属性，因为该记录并没有更早的版本），可以将这些undo日志都连起来，串成一个链表： 对该记录每次更新后，都会将旧值放到一条 undo日志 中，就算是该记录的一个旧版本，随着更新次数的增多，所有的版本都会被 roll_pointer 属性连接成一个链表，我们把这个链表称之为 版本链 ，版本链的头节点就是当前记录最新的值。 每个版本中还包含生成该版本时对应的事务id。 ","date":"2022-10-20","objectID":"/mysql%E4%BA%8B%E5%8A%A1%E7%AF%87/:15:2","tags":[],"title":"MySQL事务篇","uri":"/mysql%E4%BA%8B%E5%8A%A1%E7%AF%87/"},{"categories":["mysql"],"content":"4. MVCC实现原理之ReadView MVCC 的实现依赖于：隐藏字段、Undo Log、Read View。 ","date":"2022-10-20","objectID":"/mysql%E4%BA%8B%E5%8A%A1%E7%AF%87/:16:0","tags":[],"title":"MySQL事务篇","uri":"/mysql%E4%BA%8B%E5%8A%A1%E7%AF%87/"},{"categories":["mysql"],"content":"4.1 什么是ReadView ","date":"2022-10-20","objectID":"/mysql%E4%BA%8B%E5%8A%A1%E7%AF%87/:16:1","tags":[],"title":"MySQL事务篇","uri":"/mysql%E4%BA%8B%E5%8A%A1%E7%AF%87/"},{"categories":["mysql"],"content":"4.2 设计思路 使用 READ UNCOMMITTED 隔离级别的事务，由于可以读到未提交事务修改过的记录，所以直接读取记录的最新版本就好了。 使用 SERIALIZABLE 隔离级别的事务，InnoDB规定使用加锁的方式来访问记录。 使用 READ COMMITTED 和 REPEATABLE READ 隔离级别的事务，都必须保证读到 已经提交了的 事务修改过的记录。假如另一个事务已经修改了记录但是尚未提交，是不能直接读取最新版本的记录的，核心问题就是需要判断一下版本链中的哪个版本是当前事务可见的，这是ReadView要解决的主要问题。 这个ReadView中主要包含4个比较重要的内容，分别如下： creator_trx_id，创建这个 Read View 的事务 ID。 说明：只有在对表中的记录做改动时（执行INSERT、DELETE、UPDATE这些语句时）才会为事务分配事务id，否则在一个只读事务中的事务id值都默认为0。 trx_ids ，表示在生成ReadView时当前系统中活跃的读写事务的 事务id列表 。 up_limit_id ，活跃的事务中最小的事务 ID。 low_limit_id ，表示生成ReadView时系统中应该分配给下一个事务的 id 值。low_limit_id 是系统最大的事务id值，这里要注意是系统中的事务id，需要区别于正在活跃的事务ID。 注意：low_limit_id并不是trx_ids中的最大值，事务id是递增分配的。比如，现在有id为1， 2，3这三个事务，之后id为3的事务提交了。那么一个新的读事务在生成ReadView时，trx_ids就包括1和2,up_limit_id的值就是1，low_limit_id的值就是4。 ","date":"2022-10-20","objectID":"/mysql%E4%BA%8B%E5%8A%A1%E7%AF%87/:16:2","tags":[],"title":"MySQL事务篇","uri":"/mysql%E4%BA%8B%E5%8A%A1%E7%AF%87/"},{"categories":["mysql"],"content":"4.3 ReadView的规则 有了这个ReadView，这样在访问某条记录时，只需要按照下边的步骤判断记录的某个版本是否可见。 如果被访问版本的trx_id属性值与ReadView中的 creator_trx_id 值相同，意味着当前事务在访问它自己修改过的记录，所以该版本可以被当前事务访问。 如果被访问版本的trx_id属性值小于ReadView中的 up_limit_id 值，表明生成该版本的事务在当前事务生成ReadView前已经提交，所以该版本可以被当前事务访问。 如果被访问版本的trx_id属性值大于或等于ReadView中的 low_limit_id 值，表明生成该版本的事务在当前事务生成ReadView后才开启，所以该版本不可以被当前事务访问。 如果被访问版本的trx_id属性值在ReadView的 up_limit_id 和 low_limit_id 之间，那就需要判断一下trx_id属性值是不是在 trx_ids 列表中。 如果在，说明创建ReadView时生成该版本的事务还是活跃的，该版本不可以被访问。 如果不在，说明创建ReadView时生成该版本的事务已经被提交，该版本可以被访问。 ","date":"2022-10-20","objectID":"/mysql%E4%BA%8B%E5%8A%A1%E7%AF%87/:16:3","tags":[],"title":"MySQL事务篇","uri":"/mysql%E4%BA%8B%E5%8A%A1%E7%AF%87/"},{"categories":["mysql"],"content":"4.4 MVCC整体操作流程 了解了这些概念之后，我们来看下当查询一条记录的时候，系统如何通过MVCC找到它： 首先获取事务自己的版本号，也就是事务ID； 获取 ReadView； 查询得到的数据，然后与 ReadView 中的事务版本号进行比较； 如果不符合 ReadView 规则，就需要从 Undo Log 中获取历史快照； 最后返回符合规则的数据。 在隔离级别为读已提交（Read Committed）时，一个事务中的每一次 SELECT 查询都会重新获取一次 Read View。 如表所示： 注意，此时同样的查询语句都会重新获取一次 Read View，这时如果 Read View 不同，就可能产生不可重复读或者幻读的情况。 当隔离级别为可重复读的时候，就避免了不可重复读，这是因为一个事务只在第一次 SELECT 的时候会获取一次 Read View，而后面所有的 SELECT 都会复用这个 Read View，如下表所示： ","date":"2022-10-20","objectID":"/mysql%E4%BA%8B%E5%8A%A1%E7%AF%87/:16:4","tags":[],"title":"MySQL事务篇","uri":"/mysql%E4%BA%8B%E5%8A%A1%E7%AF%87/"},{"categories":["mysql"],"content":"5. 举例说明 ","date":"2022-10-20","objectID":"/mysql%E4%BA%8B%E5%8A%A1%E7%AF%87/:17:0","tags":[],"title":"MySQL事务篇","uri":"/mysql%E4%BA%8B%E5%8A%A1%E7%AF%87/"},{"categories":["mysql"],"content":"5.1 READ COMMITTED隔离级别下 READ COMMITTED ：每次读取数据前都生成一个ReadView。 现在有两个事务id分别为10、20的事务在执行: # Transaction 10 BEGIN; UPDATE student SET name=\"李四\" WHERE id=1; UPDATE student SET name=\"王五\" WHERE id=1; # Transaction 20 BEGIN; # 更新了一些别的表的记录 ... 说明：事务执行过程中，只有在第一次真正修改记录时（比如使用INSERT、DELETE、UPDATE语句），才会被分配一个单独的事务id，这个事务id是递增的。所以我们才在事务2中更新一些别的表的记录，目的是让它分配事务id。 此刻，表student 中 id 为 1 的记录得到的版本链表如下所示： 假设现在有一个使用 READ COMMITTED 隔离级别的事务开始执行： # 使用READ COMMITTED隔离级别的事务 BEGIN; # SELECT1：Transaction 10、20未提交 SELECT * FROM student WHERE id = 1; # 得到的列name的值为'张三' 之后，我们把 事务id 为 10 的事务提交一下： # Transaction 10 BEGIN; UPDATE student SET name=\"李四\" WHERE id=1; UPDATE student SET name=\"王五\" WHERE id=1; COMMIT; 然后再到 事务id 为 20 的事务中更新一下表 student 中 id 为 1 的记录： # Transaction 20 BEGIN; # 更新了一些别的表的记录 ... UPDATE student SET name=\"钱七\" WHERE id=1; UPDATE student SET name=\"宋八\" WHERE id=1; 此刻，表student中 id 为 1 的记录的版本链就长这样： 然后再到刚才使用 READ COMMITTED 隔离级别的事务中继续查找这个 id 为 1 的记录，如下： # 使用READ COMMITTED隔离级别的事务 BEGIN; # SELECT1：Transaction 10、20均未提交 SELECT * FROM student WHERE id = 1; # 得到的列name的值为'张三' # SELECT2：Transaction 10提交，Transaction 20未提交 SELECT * FROM student WHERE id = 1; # 得到的列name的值为'王五' -- 出现不可重复读 ","date":"2022-10-20","objectID":"/mysql%E4%BA%8B%E5%8A%A1%E7%AF%87/:17:1","tags":[],"title":"MySQL事务篇","uri":"/mysql%E4%BA%8B%E5%8A%A1%E7%AF%87/"},{"categories":["mysql"],"content":"5.2 REPEATABLE READ隔离级别下 使用 REPEATABLE READ 隔离级别的事务来说，只会在第一次执行查询语句时生成一个 ReadView ，之后的查询就不会重复生成了。 比如，系统里有两个 事务id 分别为 10 、 20 的事务在执行： # Transaction 10 BEGIN; UPDATE student SET name=\"李四\" WHERE id=1; UPDATE student SET name=\"王五\" WHERE id=1; # Transaction 20 BEGIN; # 更新了一些别的表的记录 ... 此刻，表student 中 id 为 1 的记录得到的版本链表如下所示： 假设现在有一个使用 REPEATABLE READ 隔离级别的事务开始执行： # 使用 REPEATABLE READ 隔离级别的事务 BEGIN; # SELECT1：Transaction 10、20未提交 SELECT * FROM student WHERE id = 1; # 得到的列name的值为'张三' 之后，我们把 事务id 为 10 的事务提交一下，就像这样： # Transaction 10 BEGIN; UPDATE student SET name=\"李四\" WHERE id=1; UPDATE student SET name=\"王五\" WHERE id=1; COMMIT; 然后再到 事务id 为 20 的事务中更新一下表 student 中 id 为 1 的记录： # Transaction 20 BEGIN; # 更新了一些别的表的记录 ... UPDATE student SET name=\"钱七\" WHERE id=1; UPDATE student SET name=\"宋八\" WHERE id=1; 此刻，表 student 中id为1的记录的版本链长这样： 然后再到刚才使用 REPEATABLE READ 隔离级别的事务中继续查找这个 id 为 1 的记录，如下： # 使用REPEATABLE READ隔离级别的事务 BEGIN; # SELECT1：Transaction 10、20均未提交 SELECT * FROM student WHERE id = 1; # 得到的列name的值为'张三' # SELECT2：Transaction 10提交，Transaction 20未提交 SELECT * FROM student WHERE id = 1; # 得到的列name的值仍为'张三' -- 解决了不可重复读问题 这次SELECT查询得到的结果是重复的，记录的列c值都是张三，这就是可重复读的含义。如果我们之后再把事务id为20的记录提交了，然后再到刚才使用REPEATABLE READ隔离级别的事务中继续查找这个id为1的记录，得到的结果还是张三，具体执行过程大家可以自己分析一下。 ","date":"2022-10-20","objectID":"/mysql%E4%BA%8B%E5%8A%A1%E7%AF%87/:17:2","tags":[],"title":"MySQL事务篇","uri":"/mysql%E4%BA%8B%E5%8A%A1%E7%AF%87/"},{"categories":["mysql"],"content":"5.3 如何解决幻读 接下来说明 InnoDB 是如何解决幻读的。 假设现在表 student 中只有一条数据，数据内容中，主键 id=1，隐藏的 trx_id=10，它的 undo log 如下图所示。 假设现在有事务 A 和事务 B 并发执行，事务 A 的事务 id 为 20 ， 事务 B 的事务 id 为 30 。 步骤1：事务 A 开始第一次查询数据，查询的 SQL 语句如下。 select * from student where id \u003e= 1; 在开始查询之前，MySQL 会为事务 A 产生一个 ReadView，此时 ReadView 的内容如下： trx_ids= [20,30] ， up_limit_id=20 ， low_limit_id=31 ， creator_trx_id=20 。 由于此时表 student 中只有一条数据，且符合 where id\u003e=1 条件，因此会查询出来。然后根据 ReadView 机制，发现该行数据的trx_id=10，小于事务 A 的 ReadView 里 up_limit_id，这表示这条数据是事务 A 开启之前，其他事务就已经提交了的数据，因此事务 A 可以读取到。 结论：事务 A 的第一次查询，能读取到一条数据，id=1。 步骤2：接着事务 B(trx_id=30)，往表 student 中新插入两条数据，并提交事务。 insert into student(id,name) values(2,'李四'); insert into student(id,name) values(3,'王五'); 此时表student 中就有三条数据了，对应的 undo 如下图所示： 步骤3：接着事务 A 开启第二次查询，根据可重复读隔离级别的规则，此时事务 A 并不会再重新生成 ReadView。此时表 student 中的 3 条数据都满足 where id\u003e=1 的条件，因此会先查出来。然后根据 ReadView 机制，判断每条数据是不是都可以被事务 A 看到。 1）首先 id=1 的这条数据，前面已经说过了，可以被事务 A 看到。 2）然后是 id=2 的数据，它的 trx_id=30，此时事务 A 发现，这个值处于 up_limit_id 和 low_limit_id 之间，因此还需要再判断 30 是否处于 trx_ids 数组内。由于事务 A 的 trx_ids=[20,30]，因此在数组内，这表示 id=2 的这条数据是与事务 A 在同一时刻启动的其他事务未提交的，所以这条数据不能让事务 A 看到。 3）同理，id=3 的这条数据，trx_id 也为 30，因此也不能被事务 A 看见。 结论：最终事务 A 的第二次查询，只能查询出 id=1 的这条数据。这和事务 A 的第一次查询的结果是一样的，因此没有出现幻读现象，所以说在 MySQL 的可重复读隔离级别下，不存在幻读问题。 ","date":"2022-10-20","objectID":"/mysql%E4%BA%8B%E5%8A%A1%E7%AF%87/:17:3","tags":[],"title":"MySQL事务篇","uri":"/mysql%E4%BA%8B%E5%8A%A1%E7%AF%87/"},{"categories":["mysql"],"content":"6. 总结 这里介绍了 MVCC 在 READ COMMITTD 、 REPEATABLE READ 这两种隔离级别的事务在执行快照读操作时访问记录的版本链的过程。这样使不同事务的 读-写 、 写-读 操作并发执行，从而提升系统性能。 核心点在于 ReadView 的原理，READ COMMITTD、REPEATABLE READ这两个隔离级别的一个很大不同就是生成ReadView的时机不同： READ COMMITTD在每一次进行普通SELECT操作前都会生成一个ReadView，显然这样不能避免幻读 REPEATABLE READ只在第一次进行普通SELECT操作前生成一个ReadView，之后的查询操作都重复使用这个ReadView就好了，这样就解决了幻读问题。 通过MVCC我们可以解决： ","date":"2022-10-20","objectID":"/mysql%E4%BA%8B%E5%8A%A1%E7%AF%87/:18:0","tags":[],"title":"MySQL事务篇","uri":"/mysql%E4%BA%8B%E5%8A%A1%E7%AF%87/"},{"categories":["杂"],"content":"burpsuitepro的模块 Proxy 提供一个直观、友好的用户界面，它的代理服务器包含非常详细的拦截规则，并能准确分析 HTTP 消息的结构与内容。 Spide 爬行蜘蛛工具，可以用来抓取目标网站，以显示网站的内容，基本结构，和其他功能。 Scanner Web 应用程序的安全漏洞进行自动发现工具。它被设计用于渗透测试，并密切与您现有的技术和方法，以适应执行手动和半自动化的 Web 应用程序渗透测试。这个对于抓包分析来说用处不大。 Repeater 可让您手动重新发送单个HTTP请求以查看服务器回应。这个对于重新构造请求头和请求体比较方便。 Intruder 是burp套件的优势,它提供一组特别有用的功能。它可以自动实施各种定制攻击，包括资源枚举、数据提取、模糊测试等常见漏洞等。在各种有效的扫描工具中，它能够以最细化、最简单的方式访问它生产的请求与响应，允许组合利用个人智能与该工具的控制优点。 Sequencer 对会话令牌，会话标识符或其他出于安全原因需要随机产生的键值的可预测性进行分析。 Comparer 是一个简单的工具，执行比较数据之间的任何两个项目（一个可视化的“差异”）。在攻击一个Web 应用程序的情况下，这一要求通常会出现当你想快速识别两个应用程序的响应之间的差异（例如，入侵者攻击的过程中收到的两种反应之间之间，或登录失败的反应使用有效的和无效的用户名）之间，或两个应用程序请求（例如，确定不同的行为引起不同的请求参数）。 Decoder 使用各种编码绕过服务器端输入过滤，smart decode 自动识别编码格式。 ","date":"2022-10-05","objectID":"/burpsuitepro%E6%8A%93%E5%8C%85%E4%B9%8B%E6%97%85/:1:0","tags":[],"title":"BurpsuitePro抓包之旅","uri":"/burpsuitepro%E6%8A%93%E5%8C%85%E4%B9%8B%E6%97%85/"},{"categories":["杂"],"content":"google浏览器网页抓包 ","date":"2022-10-05","objectID":"/burpsuitepro%E6%8A%93%E5%8C%85%E4%B9%8B%E6%97%85/:2:0","tags":[],"title":"BurpsuitePro抓包之旅","uri":"/burpsuitepro%E6%8A%93%E5%8C%85%E4%B9%8B%E6%97%85/"},{"categories":["杂"],"content":"下载证书 burpsuitepro默认的证书地址在120.0.0.1:8080端口，当然也可以更改。这样在本地就可以访问127.0.0.1:8080地址获取CA证书了。如果发生端口冲突，可以手动修改burpsuitepro的代理地址端口，这样只需要在对应代理端口访问即可。然后点击 CA Certificate即可下载证书 ","date":"2022-10-05","objectID":"/burpsuitepro%E6%8A%93%E5%8C%85%E4%B9%8B%E6%97%85/:2:1","tags":[],"title":"BurpsuitePro抓包之旅","uri":"/burpsuitepro%E6%8A%93%E5%8C%85%E4%B9%8B%E6%97%85/"},{"categories":["杂"],"content":"安装证书 设置—\u003e隐私设置和安全性—\u003e更多—\u003e管理证书 导入证书 下一步到浏览本地证书位置。选所有文件，不然可能你找不到你的证书!! 选择证书后打开进入下一步。 按下图位置设置进入下一步，完成。 最后设置证书信任 按图示操作找到刚才安装的证书。 选中证书点高级 按下图勾选，确认。最后重启浏览器即可。 ","date":"2022-10-05","objectID":"/burpsuitepro%E6%8A%93%E5%8C%85%E4%B9%8B%E6%97%85/:2:2","tags":[],"title":"BurpsuitePro抓包之旅","uri":"/burpsuitepro%E6%8A%93%E5%8C%85%E4%B9%8B%E6%97%85/"},{"categories":["杂"],"content":"抓包 挂代理 需要使用google浏览器抓包时，需要将burpsuitepro的proxy端口地址开个代理。可以全局代理，也建议局部代理，这里推荐一款好用的插件 SwitchyOmega ,它可以定制代理规则，将浏览器的请求转发到对应地址上。这里我们需要将burpsuitepro监听的端口地址，使用代理进行转发，这样浏览器发出的请求就会被转到代理地址，就可以收到请求报文信息。 打开拦截 proxy-\u003eintercept–\u003eintercept is off=\u003eintercept is on 打开代理和拦截后，那么对满足代理规则的网址的请求都会拦截并解析。 选项解释 这里说说，Forward Drop Action intercept is off/on 这几个选项 Forward 拦截并编辑请求之后，发送请求到服务器或浏览器 Drop 不想要发送本次请求，可以点击Drop放弃这个拦截请求 intercept is off/on 关闭/开启所有拦截.如果时 on ，表示请求和响应将被拦截或自动转发根据配置的拦截规则配置的代理选项。如果显示 off 则显示拦截之后的所有信息将自动转发。相当于没有对这个请求做任何事情。 Action 对当前拦截的请求做一些操作，如Repeater可以篡改请求之后再发送请求。 ","date":"2022-10-05","objectID":"/burpsuitepro%E6%8A%93%E5%8C%85%E4%B9%8B%E6%97%85/:2:3","tags":[],"title":"BurpsuitePro抓包之旅","uri":"/burpsuitepro%E6%8A%93%E5%8C%85%E4%B9%8B%E6%97%85/"},{"categories":["杂"],"content":"微信小程序抓包 由于目前微信升级了。最新版本的微信小程序应用使用代理，请求却无法走代理。目前好像不能抓包。微信小程序的请求不走代理端口地址。 ","date":"2022-10-05","objectID":"/burpsuitepro%E6%8A%93%E5%8C%85%E4%B9%8B%E6%97%85/:3:0","tags":[],"title":"BurpsuitePro抓包之旅","uri":"/burpsuitepro%E6%8A%93%E5%8C%85%E4%B9%8B%E6%97%85/"},{"categories":["杂"],"content":"APP抓包 todo ","date":"2022-10-05","objectID":"/burpsuitepro%E6%8A%93%E5%8C%85%E4%B9%8B%E6%97%85/:4:0","tags":[],"title":"BurpsuitePro抓包之旅","uri":"/burpsuitepro%E6%8A%93%E5%8C%85%E4%B9%8B%E6%97%85/"},{"categories":["golang"],"content":"sync.Pool使用及源码浅析 ","date":"2022-09-24","objectID":"/sync.pool%E7%9A%84%E4%BD%BF%E7%94%A8%E5%8F%8A%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/:1:0","tags":["go编程技巧"],"title":"Sync.Pool的使用及源码分析","uri":"/sync.pool%E7%9A%84%E4%BD%BF%E7%94%A8%E5%8F%8A%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/"},{"categories":["golang"],"content":"sync.Pool使用 背景 “频繁创建对象，频繁销毁对象”是在项目开发里算比较常见。sync.Pool的出现就是为了解决这个问题。 Go语言从1.3版本开始提供了对象重用的机制，即sync.Pool。sync.Pool是可伸缩的，同时也是并发安全的，其大小仅受限于内存的大小。sync.Pool用于存储那些被分配了但是没有被使用，而未来可能会使用的值。这样就可以不用再次经过内存分配，可直接复用已有对象，减轻GC的压力，从而提升系统的性能。 sync.Pool的大小是可伸缩的，高负载时会动态扩容，存放在池中的对象如果不活跃了会被自动清理。 GC是一种自动内存管理机制，回收不再使用的对象的内存。 需要注意的是，sync.Pool 缓存的对象随时可能被无通知的清除，因此不能将 sync.Pool 用于存储持久对象的场景。 声明对象池 只需要实现New函数即可。对象池中没有对象时，将会调用New函数创建。 var machinePool = sync.Pool{ New: func() interface{} { return new(Machine) }, } Get\u0026Put m := machinePool.Get().(*Machine) json.Unmarshal(buf, m) machinePool.Put(m) Get() 用于从对象池中获取对象，因为返回值是 interface{}，因此需要类型转换。 Put() 则是在对象使用完毕后，返回对象池。 性能测试 struct反序列化 func BenchmarkUnmarshal(b *testing.B) { for n := 0; n \u003c b.N; n++ { m := \u0026Machine{} if err := json.Unmarshal([]byte(\"{\\\"A\\\":2}\"), m); err != nil { log.Println(err) return } } } func BenchmarkUnmarshalWithPool(b *testing.B) { for n := 0; n \u003c b.N; n++ { m := machinePool.Get().(*Machine) if err := json.Unmarshal([]byte(\"{\\\"A\\\":2}\"), m); err != nil { log.Println(err) return } machinePool.Put(m) } } 测试： $ go test -bench . -benchmem goos: windows goarch: amd64 pkg: demo cpu: Intel(R) Core(TM) i5-10210U CPU @ 1.60GHz BenchmarkUnmarshal-8 2188940 528.5 ns/op 232 B/op 6 allocs/op BenchmarkUnmarshalWithPool-8 2264995 518.5 ns/op 224 B/op 5 allocs/op PASS ok demo 3.805s 可以从结果看到：使用池化复用对象的方式要比不使用具有更好的性能。当然，本例之中的结构体比较小，没能凸显出较大区别。 bytes.Buffer var bufferPool = sync.Pool{ New: func() interface{} { return \u0026bytes.Buffer{} }, } var data = make([]byte, 10000) func BenchmarkBufferWithPool(b *testing.B) { for n := 0; n \u003c b.N; n++ { buf := bufferPool.Get().(*bytes.Buffer) buf.Write(data) buf.Reset() bufferPool.Put(buf) } } func BenchmarkBuffer(b *testing.B) { for n := 0; n \u003c b.N; n++ { var buf bytes.Buffer buf.Write(data) } } 测试： $ go test -bench . -benchmem goos: windows goarch: amd64 pkg: demo cpu: Intel(R) Core(TM) i5-10210U CPU @ 1.60GHz BenchmarkBufferWithPool-8 10175354 109.2 ns/op 0 B/op 0 allocs/op BenchmarkBuffer-8 750318 1669 ns/op 10240 B/op 1 allocs/op PASS ok demo 2.879s 使用池化进行对象的复用和不使用有明显的性能差异 标准库应用 json.Marshal encodeState对象的复用 fmt.Printf 对pp的复用 ","date":"2022-09-24","objectID":"/sync.pool%E7%9A%84%E4%BD%BF%E7%94%A8%E5%8F%8A%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/:1:1","tags":["go编程技巧"],"title":"Sync.Pool的使用及源码分析","uri":"/sync.pool%E7%9A%84%E4%BD%BF%E7%94%A8%E5%8F%8A%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/"},{"categories":["golang"],"content":"sync.Pool浅析 参考博客 -\u003e https://www.cyhone.com/articles/think-in-sync-pool/ // A Pool must not be copied after first use. type Pool struct { noCopy noCopy local unsafe.Pointer // local fixed-size per-P pool, actual type is [P]poolLocal localSize uintptr // size of the local array victim unsafe.Pointer // local from previous cycle victimSize uintptr // size of victims array // New optionally specifies a function to generate // a value when Get would otherwise return nil. // It may not be changed concurrently with calls to Get. New func() interface{} } no copy no copy的原因是为了安全，因为结构体对象中包含引用类型的话，直接赋值拷贝是浅拷贝，是不安全的。因为浅拷贝之后，就相当于有两个指针指向同一个地址上的对象，任意一个指针引起的更新删除等操作都会影响到另一个指针。 no copy的实现也很简单，只需要有实现sync.Locker接口，然后再把实现的类型嵌入目标结构体，就可以实现。这种实现不是直接禁掉复制这个功能，嵌入了no copy字段的程序依然可以正常执行。通过go vet分析，拷贝了嵌入no copy字段的类型时会报错，提示不能对当前类型进行值拷贝（goland也能在一定程度上提示，变黄）。 当然除了使用no copy字段来约束类型不能出现复制以外，还可以在代码逻辑层面实现（不是范式，不能总结）。 local \u0026 local size local 是个数组，长度为 P 的个数。其元素类型是 poolLocal。这里面存储着各个 P 对应的本地对象池。可以近似的看做 [P]poolLocal。（P，指的是GMP里的Processor） localSize。代表 local 数组的长度。因为 P 可以在运行时通过调用 runtime.GOMAXPROCS 进行修改, 因此我们还是得通过 localSize 来对应 local 数组的长度。 由于每个 P 都有自己的一个本地对象池 poolLocal，Get 和 Put 操作都会优先存取本地对象池。由于 P 的特性，操作本地对象池的时候整个并发问题就简化了很多，可以尽量避免并发冲突。 我们再看下本地对象池 poolLocal 的定义，如下： // 每个 P 都会有一个 poolLocal 的本地 type poolLocal struct { poolLocalInternal pad [128 - unsafe.Sizeof(poolLocalInternal{})%128]byte // 128 - unsafe.Sizeof(poolLocalInternal{})%128 + unsafe.Sizeof(poolLocalInternal{}) = n*128 } type poolLocalInternal struct { private interface{} shared poolChain } pad 变量的作用在下文会讲到，这里暂时不展开讨论。我们可以直接看 poolLocalInternal 的定义，其中每个本地对象池，都会包含两项： private 私有变量。Get 和 Put 操作都会优先存取 private 变量，如果 private 变量可以满足情况，则不再深入进行其他的复杂操作。 shared。其类型为 poolChain，这个是链表结构，这个就是 P 的本地对象池了。 Get方法 func (p *Pool) Get() interface{} { if race.Enabled { race.Disable() } // 禁掉M调度，固定住P，并拿到当前P的poolLocal数组 l, pid := p.pin() // 途径一：拿私有 x := l.private l.private = nil // 途径二：私有没有，就拿公共存储区shared双端队列缓存 if x == nil { x, _ = l.shared.popHead() if x == nil { // 途径三：还没有，在当前P进行地址偏移，获取数组里其他所有P的公共存储区shared双端队列缓存；还没有就取 pool.victim x = p.getSlow(pid) } } // 取消P的固定 runtime_procUnpin() if race.Enabled { race.Enable() if x != nil { race.Acquire(poolRaceAddr(x)) } } // 途径四：实在没得，就内存分配一个对象 if x == nil \u0026\u0026 p.New != nil { x = p.New() } return x } Put方法 // Put adds x to the pool. func (p *Pool) Put(x interface{}) { if x == nil { return } // 竟态检测 if race.Enabled { if fastrand()%4 == 0 { // Randomly drop x on floor. return } race.ReleaseMerge(poolRaceAddr(x)) race.Disable() } // 先禁用M调度，固定当前G l, _ := p.pin() // 如果私有为空，就放到私有上 if l.private == nil { l.private = x x = nil } // 私有已经有了，就放到公共缓存区 if x != nil { l.shared.pushHead(x) } // 取消固定 runtime_procUnpin() if race.Enabled { race.Enable() } } 清理对象 每个被使用的 sync.Pool，都会在初始化阶段被添加到全局变量 allPools []*Pool 对象中。Golang 的 runtime 将会在 每轮 GC 前，触发调用 poolCleanup 函数，清理 allPools。 func poolCleanup() { // Drop victim caches from all pools. for _, p := range oldPools { p.victim = nil p.victimSize = 0 } // Move primary cache to victim cache. for _, p := range allPools { p.victim = p.local p.victimSize = p.localSize p.local = nil p.localSize = 0 } oldPools, allPools = allPools, nil } var ( allPoolsMu Mutex allPools []*Pool // get 或 put时，会将pool对象放到allPools里 oldPools []*Pool ) func init() { runtime_registerPoolCleanup(poolCleanup) } 可以看到，每次GC前，都会将当前p里的local放到victim里，这样，需要两次GC才能将sync.Pool里的对象池的对象，完全清掉。 ","date":"2022-09-24","objectID":"/sync.pool%E7%9A%84%E4%BD%BF%E7%94%A8%E5%8F%8A%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/:1:2","tags":["go编程技巧"],"title":"Sync.Pool的使用及源码分析","uri":"/sync.pool%E7%9A%84%E4%BD%BF%E7%94%A8%E5%8F%8A%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/"},{"categories":["golang"],"content":"sync.Pool结构 ","date":"2022-09-24","objectID":"/sync.pool%E7%9A%84%E4%BD%BF%E7%94%A8%E5%8F%8A%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/:1:3","tags":["go编程技巧"],"title":"Sync.Pool的使用及源码分析","uri":"/sync.pool%E7%9A%84%E4%BD%BF%E7%94%A8%E5%8F%8A%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/"},{"categories":["mysql"],"content":"[toc] 第06章_索引的数据结构 ","date":"2022-09-24","objectID":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/:0:0","tags":[],"title":"MySQL索引及调优篇","uri":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/"},{"categories":["mysql"],"content":"1. 为什么使用索引 索引是存储引擎用于快速找到数据记录的一种数据结构，就好比一本教科书的目录部分，通过目录中找到对应文章的页码，便可快速定位到需要的文章。MySQL中也是一样的道理，进行数据查找时，首先查看查询条件是否命中某条索引，符合则通过索引查找相关数据，如果不符合则需要全表扫描，即需要一条一条地查找记录，直到找到与条件符合的记录。 如上图所示，数据库没有索引的情况下，数据分布在硬盘不同的位置上面，读取数据时，摆臂需要前后摆动查询数据，这样操作非常消耗时间。如果数据顺序摆放，那么也需要从1到6行按顺序读取，这样就相当于进行了6次IO操作，依旧非常耗时。如果我们不借助任何索引结构帮助我们快速定位数据的话，我们查找 Col 2 = 89 这条记录，就要逐行去查找、去比较。从Col 2 = 34 开始，进行比较，发现不是，继续下一行。我们当前的表只有不到10行数据，但如果表很大的话，有上千万条数据，就意味着要做很多很多次硬盘I/0才能找到。现在要查找 Col 2 = 89 这条记录。CPU必须先去磁盘查找这条记录，找到之后加载到内存，再对数据进行处理。这个过程最耗时间就是磁盘I/O（涉及到磁盘的旋转时间（速度较快），磁头的寻道时间(速度慢、费时)） 假如给数据使用 二叉树 这样的数据结构进行存储，如下图所示 对字段 Col 2 添加了索引，就相当于在硬盘上为 Col 2 维护了一个索引的数据结构，即这个 二叉搜索树。二叉搜索树的每个结点存储的是 (K, V) 结构，key 是 Col 2，value 是该 key 所在行的文件指针（地址）。比如：该二叉搜索树的根节点就是：(34, 0x07)。现在对 Col 2 添加了索引，这时再去查找 Col 2 = 89 这条记录的时候会先去查找该二叉搜索树（二叉树的遍历查找）。读 34 到内存，89 \u003e 34; 继续右侧数据，读 89 到内存，89==89；找到数据返回。找到之后就根据当前结点的 value 快速定位到要查找的记录对应的地址。我们可以发现，只需要 查找两次 就可以定位到记录的地址，查询速度就提高了。 这就是我们为什么要建索引，目的就是为了 减少磁盘I/O的次数，加快查询速率。 ","date":"2022-09-24","objectID":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/:1:0","tags":[],"title":"MySQL索引及调优篇","uri":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/"},{"categories":["mysql"],"content":"2. 索引及其优缺点 ","date":"2022-09-24","objectID":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/:2:0","tags":[],"title":"MySQL索引及调优篇","uri":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/"},{"categories":["mysql"],"content":"2.1 索引概述 MySQL官方对索引的定义为：索引（Index）是帮助MySQL高效获取数据的数据结构。 索引的本质：索引是数据结构。你可以简单理解为“排好序的快速查找数据结构”，满足特定查找算法。 这些数据结构以某种方式指向数据， 这样就可以在这些数据结构的基础上实现 高级查找算法 。 索引是在存储引擎中实现的，因此每种存储引擎的索引不一定完全相同，并且每种存储引擎不一定支持所有索引类型。同时，存储引擎可以定义每个表的 最大索引数和 最大索引长度。所有存储引擎支持每个表至少16个索引，总索引长度至少为256字节。有些存储引擎支持更多的索引数和更大的索引长度。 ","date":"2022-09-24","objectID":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/:2:1","tags":[],"title":"MySQL索引及调优篇","uri":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/"},{"categories":["mysql"],"content":"2.2 优点 （1）类似大学图书馆建书目索引，提高数据检索的效率，降低 数据库的IO成本 ，这也是创建索引最主 要的原因。 （2）通过创建唯一索引，可以保证数据库表中每一行 数据的唯一性 。 （3）在实现数据的 参考完整性方面，可以 加速表和表之间的连接 。换句话说，对于有依赖关系的子表和父表联合查询时， 可以提高查询速度。 （4）在使用分组和排序子句进行数据查询时，可以显著 减少查询中分组和排序的时间 ，降低了CPU的消耗。 ","date":"2022-09-24","objectID":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/:2:2","tags":[],"title":"MySQL索引及调优篇","uri":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/"},{"categories":["mysql"],"content":"2.3 缺点 增加索引也有许多不利的方面，主要表现在如下几个方面： （1）创建索引和维护索引要 耗费时间 ，并 且随着数据量的增加，所耗费的时间也会增加。 （2）索引需要占 磁盘空间 ，除了数据表占数据空间之 外，每一个索引还要占一定的物理空间， 存储在磁盘上 ，如果有大量的索引，索引文件就可能比数据文 件更快达到最大文件尺寸。 （3）虽然索引大大提高了查询速度，同时却会 降低更新表的速度 。当对表 中的数据进行增加、删除和修改的时候，索引也要动态地维护，这样就降低了数据的维护速度。 因此，选择使用索引时，需要综合考虑索引的优点和缺点。 因此，选择使用索引时，需要综合考虑索引的优点和缺点。 提示： 索引可以提高查询的速度，但是会影响插入记录的速度。这种情况下，最好的办法是先删除表中的索引，然后插入数据，插入完成后再创建索引。 ","date":"2022-09-24","objectID":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/:2:3","tags":[],"title":"MySQL索引及调优篇","uri":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/"},{"categories":["mysql"],"content":"3. InnoDB中索引的推演 ","date":"2022-09-24","objectID":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/:3:0","tags":[],"title":"MySQL索引及调优篇","uri":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/"},{"categories":["mysql"],"content":"3.1 索引之前的查找 先来看一个精确匹配的例子： SELECT [列名列表] FROM 表名 WHERE 列名 = xxx; 1. 在一个页中的查找 假设目前表中的记录比较少，所有的记录都可以被存放到一个页中，在查找记录的时候可以根据搜索条件的不同分为两种情况： 以主键为搜索条件 可以在页目录中使用 二分法 快速定位到对应的槽，然后再遍历该槽对用分组中的记录即可快速找到指定记录。 以其他列作为搜索条件 因为在数据页中并没有对非主键列简历所谓的页目录，所以我们无法通过二分法快速定位相应的槽。这种情况下只能从 最小记录 开始 依次遍历单链表中的每条记录， 然后对比每条记录是不是符合搜索条件。很显然，这种查找的效率是非常低的。 2. 在很多页中查找 在很多页中查找记录的活动可以分为两个步骤： 定位到记录所在的页。 从所在的页内中查找相应的记录。 在没有索引的情况下，不论是根据主键列或者其他列的值进行查找，由于我们并不能快速的定位到记录所在的页，所以只能 从第一个页沿着双向链表 一直往下找，在每一个页中根据我们上面的查找方式去查 找指定的记录。因为要遍历所有的数据页，所以这种方式显然是 超级耗时 的。如果一个表有一亿条记录呢？此时 索引 应运而生。 ","date":"2022-09-24","objectID":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/:3:1","tags":[],"title":"MySQL索引及调优篇","uri":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/"},{"categories":["mysql"],"content":"3.2 设计索引 建一个表： mysql\u003e CREATE TABLE index_demo( -\u003e c1 INT, -\u003e c2 INT, -\u003e c3 CHAR(1), -\u003e PRIMARY KEY(c1) -\u003e ) ROW_FORMAT = Compact; 这个新建的 index_demo 表中有2个INT类型的列，1个CHAR(1)类型的列，而且我们规定了c1列为主键， 这个表使用 Compact 行格式来实际存储记录的。这里我们简化了index_demo表的行格式示意图： 我们只在示意图里展示记录的这几个部分： record_type ：记录头信息的一项属性，表示记录的类型， 0 表示普通记录、 2 表示最小记 录、 3 表示最大记录、 1 暂时还没用过，下面讲。 mysql\u003e CREATE TABLE index_demo( -\u003e c1 INT, -\u003e c2 INT, -\u003e c3 CHAR(1), -\u003e PRIMARY KEY(c1) -\u003e ) ROW_FORMAT = Compact; next_record ：记录头信息的一项属性，表示下一条地址相对于本条记录的地址偏移量，我们用 箭头来表明下一条记录是谁。 各个列的值 ：这里只记录在 index_demo 表中的三个列，分别是 c1 、 c2 和 c3 。 其他信息 ：除了上述3种信息以外的所有信息，包括其他隐藏列的值以及记录的额外信息。 将记录格式示意图的其他信息项暂时去掉并把它竖起来的效果就是这样： 把一些记录放到页里的示意图就是： 1. 一个简单的索引设计方案 我们在根据某个搜索条件查找一些记录时为什么要遍历所有的数据页呢？因为各个页中的记录并没有规律，我们并不知道我们的搜索条件匹配哪些页中的记录，所以不得不依次遍历所有的数据页。所以如果我们 想快速的定位到需要查找的记录在哪些数据页 中该咋办？我们可以为快速定位记录所在的数据页而建立一个目录 ，建这个目录必须完成下边这些事： 下一个数据页中用户记录的主键值必须大于上一个页中用户记录的主键值。 假设：每个数据结构最多能存放3条记录（实际上一个数据页非常大，可以存放下好多记录）。 INSERT INTO index_demo VALUES(1, 4, 'u'), (3, 9, 'd'), (5, 3, 'y'); 那么这些记录以及按照主键值的大小串联成一个单向链表了，如图所示： 从图中可以看出来， index_demo 表中的3条记录都被插入到了编号为10的数据页中了。此时我们再来插入一条记录 INSERT INTO index_demo VALUES(4, 4, 'a'); 因为 页10 最多只能放3条记录，所以我们不得不再分配一个新页： 注意：新分配的 数据页编号可能并不是连续的。它们只是通过维护者上一个页和下一个页的编号而建立了 链表 关系。另外，页10中用户记录最大的主键值是5，而页28中有一条记录的主键值是4，因为5\u003e4，所以这就不符合下一个数据页中用户记录的主键值必须大于上一个页中用户记录的主键值的要求，所以在插入主键值为4的记录的时候需要伴随着一次 记录移动，也就是把主键值为5的记录移动到页28中，然后再把主键值为4的记录插入到页10中，这个过程的示意图如下： 这个过程表明了在对页中的记录进行增删改查操作的过程中，我们必须通过一些诸如 记录移动 的操作来始终保证这个状态一直成立：下一个数据页中用户记录的主键值必须大于上一个页中用户记录的主键值。这个过程称为 页分裂。 给所有的页建立一个目录项。 由于数据页的 编号可能是不连续 的，所以在向 index_demo 表中插入许多条记录后，可能是这样的效果： 我们需要给它们做个 目录，每个页对应一个目录项，每个目录项包括下边两个部分： 1）页的用户记录中最小的主键值，我们用 key 来表示。 2）页号，我们用 page_on 表示。 以 页28 为例，它对应 目录项2 ，这个目录项中包含着该页的页号 28 以及该页中用户记录的最小主 键值 5 。我们只需要把几个目录项在物理存储器上连续存储（比如：数组），就可以实现根据主键 值快速查找某条记录的功能了。比如：查找主键值为 20 的记录，具体查找过程分两步： 先从目录项中根据 二分法 快速确定出主键值为 20 的记录在 目录项3 中（因为 12 \u003c 20 \u003c 209 ），它对应的页是 页9 。 再根据前边说的在页中查找记录的方式去 页9 中定位具体的记录。 至此，针对数据页做的简易目录就搞定了。这个目录有一个别名，称为 索引 。 2. InnoDB中的索引方案 ① 迭代1次：目录项纪录的页 InnoDB怎么区分一条记录是普通的 用户记录 还是 目录项记录 呢？使用记录头信息里的 record_type 属性，它的各自取值代表的意思如下： 0：普通的用户记录 1：目录项记录 2：最小记录 3：最大记录 我们把前边使用到的目录项放到数据页中的样子就是这样： 从图中可以看出来，我们新分配了一个编号为30的页来专门存储目录项记录。这里再次强调 目录项记录 和普通的 用户记录 的不同点： 目录项记录 的 record_type 值是1，而 普通用户记录 的 record_type 值是0。 目录项记录只有 主键值和页的编号 两个列，而普通的用户记录的列是用户自己定义的，可能包含 很多列 ，另外还有InnoDB自己添加的隐藏列。 了解：记录头信息里还有一个叫 min_rec_mask 的属性，只有在存储 目录项记录 的页中的主键值最小的 目录项记录 的 min_rec_mask 值为 1 ，其他别的记录的 min_rec_mask 值都是 0 。 相同点：两者用的是一样的数据页，都会为主键值生成 Page Directory （页目录），从而在按照主键值进行查找时可以使用 二分法 来加快查询速度。 现在以查找主键为 20 的记录为例，根据某个主键值去查找记录的步骤就可以大致拆分成下边两步： 先到存储 目录项记录 的页，也就是页30中通过 二分法 快速定位到对应目录项，因为 12 \u003c 20 \u003c 209 ，所以定位到对应的记录所在的页就是页9。 再到存储用户记录的页9中根据 二分法 快速定位到主键值为 20 的用户记录。 ② 迭代2次：多个目录项纪录的页 从图中可以看出，我们插入了一条主键值为320的用户记录之后需要两个新的数据页： 为存储该用户记录而新生成了 页31 。 因为原先存储目录项记录的 页30的容量已满 （我们前边假设只能存储4条目录项记录），所以不得 不需要一个新的 页32 来存放 页31 对应的目录项。 现在因为存储目录项记录的页不止一个，所以如果我们想根据主键值查找一条用户记录大致需要3个步骤，以查找主键值为 20 的记录为例： 确定 目录项记录页 我们现在的存储目录项记录的页有两个，即 页30 和 页32 ，又因为页30表示的目录项的主键值的 范围是 [1, 320) ，页32表示的目录项的主键值不小于 320 ，所以主键值为 20 的记录对应的目 录项记录在 页30 中。 通过目录项记录页 确定用户记录真实所在的页 。 在一个存储 目录项记录 的页中通过主键值定位一条目录项记录的方式说过了。 在真实存储用户记录的页中定位到具体的记录。 ③ 迭代3次：目录项记录页的目录页 如果我们表中的数据非常多则会产生很多存储目录项记录的页，那我们怎么根据主键值快速定位一个存储目录项记录的页呢？那就为这些存储目录项记录的页再生成一个更高级的目录，就像是一个多级目录一样，大目录里嵌套小目录，小目录里才是实际的数据，所以现在各个页的示意图就是这样子： 如图，我们生成了一个存储更高级目录项的 页33 ，这个页中的两条记录分别代表页30和页32，如果用 户记录的主键值在 [1, 320) 之间，则到页30中查找更详细的目录项记录，如果主键值 不小于320 的 话，就到页32中查找更详细的目录项记录。 我们可以用下边这个图来描述它： 这个数据结构，它的名称是 B+树 。 ④ B+Tree 一个B+树的节点其实可以分成好多层，规定最下边的那层，也就是存放我们用户记录的那层为第 0 层， 之后依次往上加。之前我们做了一个非常极端的假设：存放用户记录的页 最多存放3条记录 ，存放目录项 记录的页 最多存放4条记录 。其实真实环境中一个页存放的记录数量是非常大的，假设所有存放用户记录 的叶子节点代表的数据页可以存放 100条用户记录 ，所有存放目录项记录的内节点代表的数据页可以存 放 1000条目录项记录 ，那么： 如果B+树只有1层，也就是只有1个用于存放用户记录的节点，最多能存放 100 条记录。 如果B+树有2层，最多能存放 1000×100=10,0000 条记录。 如果B+树有3层，最多能存放 1000×1000×100=1,0000,0000 条记录。 如果B+树有4层，最多能存放 1000×1000×1000×100=1000,0000,0000 条记录。相当多的记录！ 你的表里能存放 100000000000 条记录吗？所以一般情况下，我们用到的 B+树都不会超过4层 ，那我们通过主键值去查找某条记录最多只需要做4个页面内的查找（查找3个目录项页和一个用户记录页），又因为在每个页面内有所谓的 Page Directory （页目录），所以在页面内也可以通过 二分法 实现快速 定位记录。 而且最重要的是：应当尽可能减少B+树的高度，因为每深入B+树的下一层寻找页都会将该页从磁盘加载到内存，这个过程是IO入内存，非常非常慢 ","date":"2022-09-24","objectID":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/:3:2","tags":[],"title":"MySQL索引及调优篇","uri":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/"},{"categories":["mysql"],"content":"3.3 常见索引概念 索引按照物理实现方式，索引可以分为 2 种：聚簇（聚集）和非聚簇（非聚集）索引。我们也把非聚集 索引称为二级索引或者辅助索引。 1. 聚簇索引 聚簇索引并不是一种单独的索引类型，而是一种数据存储方式（所有的用户记录都存储在了叶子结点），也就是所谓的 索引即数据，数据即索引。一般情况下主键就是默认的聚簇索引 聚簇索引默认是主键，如果表中没有定义主键，InnoDB 会选择一个唯一的非空索引代替（“唯一的非空索引”是指列不能出现null值的唯一索引，跟主键性质一样）。如果没有这样的索引，InnoDB会隐式地定义一个主键来作为聚簇索引。 术语\"聚簇\"表示当前数据行和相邻的键值聚簇的存储在一起 特点： 使用记录主键值的大小进行记录和页的排序，这包括三个方面的含义： 页内的记录是按照主键的大小顺序排成一个 单向链表 。 各个存放 用户记录的页 也是根据页中用户记录的主键大小顺序排成一个 双向链表 。 存放 目录项记录的页 分为不同的层次，在同一层次中的页也是根据页中目录项记录的主键大小顺序排成一个 双向链表 。 B+树的 叶子节点 存储的是完整的用户记录。 所谓完整的用户记录，就是指这个记录中存储了所有列的值（包括隐藏列）。 我们把具有这两种特性的 B+树 称为聚簇索引，所有完整的用户记录都存放在这个聚簇索引的叶子节点处。这种聚簇索引并不需要我们在MySQL语句中显式的使用 INDEX 语句去创建， InnDB 存储引擎会 自动 的为我们创建聚簇索引。 优点： 数据访问更快 ，因为聚簇索引将索引和数据保存在同一个B+树中，因此从聚簇索引中获取数据比非聚簇索引更快 聚簇索引对于主键的 排序查找 和 范围查找 速度非常快 按照聚簇索引排列顺序，查询显示一定范围数据的时候，由于数据都是紧密相连，数据库不用从多个数据块中提取数据，所以 节省了大量的io操作 。 缺点： 插入速度严重依赖于插入顺序 ，按照主键的顺序插入是最快的方式，否则将会出现页分裂，严重影响性能。因此，对于InnoDB表，我们一般都会定义一个自增的ID列为主键。自增ID列，这样就可以保证插入时一定是按照主键顺序插入，这样就不会存在页分裂、重新构造上层页目录的情况，毕竟这样做可能就是又会出现多次磁盘IO才能办到，大大拖低了MySQL服务器的性能 更新主键的代价很高 ，因为将会导致被更新的行移动。因此，对于InnoDB表，我们一般定义主键为不可更新 二级索引访问需要两次索引查找 ，第一次找到主键值，第二次根据主键值找到行数据 –\u003e 回表，因为二级索引，不会将一行数据全部放到B+树得叶子节点上 2. 二级索引（辅助索引、非聚簇索引） 如果我们想以别的列作为搜索条件该怎么办？肯定不能是从头到尾沿着链表依次遍历记录一遍。 答案：我们可以多建几颗B+树，不同的B+树中的数据采用不同的排列规则。比方说我们用c2列的大小作为数据页、页中记录的排序规则，再建一课B+树，效果如下图所示： 这个B+树与上边介绍的聚簇索引有几处不同： 下图里的“大小”表示数据值 **概念：回表 ** 我们根据这个以c2列大小排序的B+树只能确定我们要查找记录的主键值，所以如果我们想根据c2列的值查找到完整的用户记录的话，仍然需要到聚簇索引中再查一遍，这个过程称为回表 。也就是根据c2列的值查询一条完整的用户记录需要使用到2棵B+树！ 问题：为什么我们还需要一次回表操作呢？直接把完整的用户记录放到叶子节点不OK吗？ 回答： 如果把完整的用户记录放到叶子结点是可以不用回表。但是太占地方了，相当于每建立一课B+树都需要把所有的用户记录再都拷贝一遍，这就有点太浪费存储空间了。 因为这种按照非主键列建立的B+树需要一次回表操作才可以定位到完整的用户记录，所以这种B+树也被称为二级索引，或者辅助索引。由于使用的是c2列的大小作为B+树的排序规则，所以我们也称这个B+树为c2列建立的索引。 非聚簇索引的存在不影响数据在聚簇索引中的组织，所以一张表可以有多个非聚簇索引。 小结：聚簇索引与非聚簇索引的原理不同，在使用上也有一些区别： 聚簇索引的叶子节点存储的就是我们的数据记录, 非聚簇索引的叶子节点存储的是数据位置。非聚簇索引不会影响数据表的物理存储顺序。 一个表只能有一个聚簇索引，因为只能有一种排序存储的方式，但可以有多个非聚簇索引，也就是多个索引目录提供数据检索。 使用聚簇索引的时候，数据的查询效率高，但如果对数据进行插入，删除，更新等操作，效率会比非聚簇索引低。 3.联合索引 我们也可以同时以多个列的大小作为排序规则，也就是同时为多个列建立索引，比方说我们想让B+树按照c2和c3列的大小进行排序，这个包含两层含义： 先把各个记录和页按照c2列进行排序。 在记录的c2列相同的情况下，采用c3列进行排序 为c2和c3建立的索引的示意图如下： 如图所示，我们需要注意以下几点： 每条目录项都有c2、c3、页号这三个部分组成，各条记录先按照c2列的值进行排序，如果记录的c2列相同，则按照c3列的值进行排序 B+树叶子节点处的用户记录由c2、c3和主键c1列组成 注意一点，以c2和c3列的大小为排序规则建立的B+树称为 联合索引 ，本质上也是一个二级索引。它的意思与分别为c2和c3列分别建立索引的表述是不同的，不同点如下： 建立 联合索引 只会建立如上图一样的1棵B+树。 为c2和c3列分别建立索引会分别以c2和c3列的大小为排序规则建立2棵B+树。 ","date":"2022-09-24","objectID":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/:3:3","tags":[],"title":"MySQL索引及调优篇","uri":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/"},{"categories":["mysql"],"content":"3.4 InnoDB的B+树索引的注意事项 1. 根页面位置万年不动 实际上B+树的形成过程是这样的： 每当为某个表创建一个B+树索引（聚簇索引不是人为创建的，默认就有）的时候，都会为这个索引创建一个 根结点 页面。最开始表中没有数据的时候，每个B+树索引对应的 根结点 中即没有用户记录，也没有目录项记录。 随后向表中插入用户记录时，先把用户记录存储到这个根节点 中。 当根节点中的可用 空间用完时 继续插入记录，此时会将根节点中的所有记录复制到一个新分配的页，比如 页a 中，然后对这个新页进行 页分裂 的操作，得到另一个新页，比如页b 。这时新插入的记录根据键值（也就是聚簇索引中的主键值，二级索引中对应的索引列的值）的大小就会被分配到 页a 或者 页b 中，而 根节点 便升级为存储目录项记录的页。 这个过程特别注意的是：一个B+树索引的根节点自诞生之日起，便不会再移动。这样只要我们对某个表建议一个索引，那么它的根节点的页号便会被记录到某个地方。然后凡是 InnoDB 存储引擎需要用到这个索引的时候，都会从哪个固定的地方取出根节点的页号，从而来访问这个索引。 2. 内节点中目录项记录的唯一性 我们知道B+树索引的内节点中目录项记录的内容是 索引列 + 页号 的搭配，但是这个搭配对于二级索引来说有点不严谨。还拿 index_demo 表为例，假设这个表中的数据是这样的： 如果二级索引中目录项记录的内容只是 索引列 + 页号 的搭配的话，那么为 c2 列简历索引后的B+树应该长这样： 如果我们想新插入一行记录，其中 c1 、c2 、c3 的值分别是: 9、1、c, 那么在修改这个为 c2 列建立的二级索引对应的 B+ 树时便碰到了个大问题：由于 页3 中存储的目录项记录是由 c2列 + 页号 的值构成的，页3 中的两条目录项记录对应的 c2 列的值都是1，而我们 新插入的这条记录 的 c2 列的值也是 1，那我们这条新插入的记录到底应该放在 页4 中，还是应该放在 页5 中？答案：对不起，懵了 为了让新插入记录找到自己在那个页面，我们需要保证在B+树的同一层页节点的目录项记录除页号这个字段以外是唯一的。所以对于二级索引的内节点的目录项记录的内容实际上是由三个部分构成的： 索引列的值 主键值 页号 也就是我们把主键值也添加到二级索引内节点中的目录项记录，这样就能保住 B+ 树每一层节点中各条目录项记录除页号这个字段外是唯一的，所以我们为c2建立二级索引后的示意图实际上应该是这样子的：（事实上，叶子节点也应该带上主键和索引列，下图画的有点毛病… 这样我们再插入记录(9, 1, 'c') 时，由于 页3 中存储的目录项记录是由 c2列 + 主键 + 页号 的值构成的，可以先把新纪录的 c2 列的值和 页3 中各目录项记录的 c2 列的值作比较，如果 c2 列的值相同的话，可以接着比较主键值，因为B+树同一层中不同目录项记录的 c2列 + 主键的值肯定是不一样的，所以最后肯定能定位唯一的一条目录项记录，在本例中最后确定新纪录应该被插入到 页5 中。 3. 一个页面最少存储 2 条记录 一个B+树只需要很少的层级就可以轻松存储数亿条记录，查询速度相当不错！这是因为B+树本质上就是一个大的多层级目录，每经过一个目录时都会过滤掉许多无效的子目录，直到最后访问到存储真实数据的目录。那如果一个大的目录中只存放一个子目录是个啥效果呢？那就是目录层级非常非常多，而且最后的那个存放真实数据的目录中只存放一条数据。所以 InnoDB 的一个数据页至少可以存放两条记录。 ","date":"2022-09-24","objectID":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/:3:4","tags":[],"title":"MySQL索引及调优篇","uri":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/"},{"categories":["mysql"],"content":"4. MyISAM中的索引方案 B树索引使用存储引擎如表所示： 索引 / 存储引擎 MyISAM InnoDB Memory B-Tree索引 支持 支持 支持 即使多个存储引擎支持同一种类型的索引，但是他们的实现原理也是不同的。Innodb和MyISAM默认的索 引是Btree索引；而Memory默认的索引是Hash索引。 MyISAM引擎使用 B+Tree 作为索引结构，叶子节点的data域存放的是数据记录的地址。 ","date":"2022-09-24","objectID":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/:4:0","tags":[],"title":"MySQL索引及调优篇","uri":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/"},{"categories":["mysql"],"content":"4.1 MyISAM索引的原理 ","date":"2022-09-24","objectID":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/:4:1","tags":[],"title":"MySQL索引及调优篇","uri":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/"},{"categories":["mysql"],"content":"4.2 MyISAM 与 InnoDB对比 MyISAM的索引方式都是“非聚簇”的，与InnoDB包含1个聚簇索引是不同的。小结两种引擎中索引的区别： ① 在InnoDB存储引擎中，我们只需要根据主键值对 聚簇索引 进行一次查找就能找到对应的记录，而在 MyISAM 中却需要进行一次 回表 操作，意味着MyISAM中建立的索引相当于全部都是 二级索引 。 ② InnoDB的数据文件本身就是索引文件，而MyISAM索引文件和数据文件是 分离的 ，索引文件仅保存数 据记录的地址。 ③ InnoDB的非聚簇索引data域存储相应记录 主键的值 ，而MyISAM索引记录的是 地址 。换句话说， InnoDB的所有非聚簇索引都引用主键作为data域。 ④ MyISAM的回表操作是十分 快速 的，因为是拿着地址偏移量直接到文件中取数据的，反观InnoDB是通 过获取主键之后再去聚簇索引里找记录，虽然说也不慢，但还是比不上直接用地址去访问。 ⑤ InnoDB要求表 必须有主键 （ MyISAM可以没有 ）。如果没有显式指定，则MySQL系统会自动选择一个 可以非空且唯一标识数据记录的列作为主键。如果不存在这种列，则MySQL自动为InnoDB表生成一个隐 含字段作为主键，这个字段长度为6个字节，类型为长整型。 小结： ","date":"2022-09-24","objectID":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/:4:2","tags":[],"title":"MySQL索引及调优篇","uri":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/"},{"categories":["mysql"],"content":"5. 索引的代价 索引是个好东西，可不能乱建，它在空间和时间上都会有消耗： 空间上的代价 每建立一个索引都要为它建立一棵B+树，每一棵B+树的每一个节点都是一个数据页，一个页默认会 占用 16KB 的存储空间，一棵很大的B+树由许多数据页组成，那就是很大的一片存储空间。 时间上的代价 每次对表中的数据进行 增、删、改 操作时，都需要去修改各个B+树索引。而且我们讲过，B+树每 层节点都是按照索引列的值 从小到大的顺序排序 而组成了 双向链表 。不论是叶子节点中的记录，还 是内节点中的记录（也就是不论是用户记录还是目录项记录）都是按照索引列的值从小到大的顺序 而形成了一个单向链表。而增、删、改操作可能会对节点和记录的排序造成破坏，所以存储引擎需 要额外的时间进行一些 记录移位 ， 页面分裂 、 页面回收 等操作来维护好节点和记录的排序。如果 我们建了许多索引，每个索引对应的B+树都要进行相关的维护操作，会给性能拖后腿。 一个表上索引建的越多，就会占用越多的存储空间，在增删改记录的时候性能就越差。为了能建立又好又少的索引，我们得学学这些索引在哪些条件下起作用的。 ","date":"2022-09-24","objectID":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/:5:0","tags":[],"title":"MySQL索引及调优篇","uri":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/"},{"categories":["mysql"],"content":"6. MySQL数据结构选择的合理性 ","date":"2022-09-24","objectID":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/:6:0","tags":[],"title":"MySQL索引及调优篇","uri":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/"},{"categories":["mysql"],"content":"6.1 全表查询 这里都懒得说了。 ","date":"2022-09-24","objectID":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/:6:1","tags":[],"title":"MySQL索引及调优篇","uri":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/"},{"categories":["mysql"],"content":"6.2 Hash查询 加快查找速度的数据结构，常见的有两类： (1) 树，例如平衡二叉搜索树，查询/插入/修改/删除的平均时间复杂度都是 O(log2N); (2)哈希，例如HashMap，查询/插入/修改/删除的平均时间复杂度都是 O(1); (key, value) 上图中哈希函数h有可能将两个不同的关键字映射到相同的位置，这叫做 碰撞 ，在数据库中一般采用 链 接法 来解决。在链接法中，将散列到同一槽位的元素放在一个链表中，如下图所示： 实验：体会数组和hash表的查找方面的效率区别 // 算法复杂度为 O(n) @Test public void test1(){ int[] arr = new int[100000]; for(int i = 0;i \u003c arr.length;i++){ arr[i] = i + 1; } long start = System.currentTimeMillis(); for(int j = 1; j\u003c=100000;j++){ int temp = j; for(int i = 0;i \u003c arr.length;i++){ if(temp == arr[i]){ break; } } } long end = System.currentTimeMillis(); System.out.println(\"time： \" + (end - start)); //time： 823 } // 算法复杂度为 O(1) @Test public void test2(){ HashSet\u003cInteger\u003e set = new HashSet\u003c\u003e(100000); for(int i = 0;i \u003c 100000;i++){ set.add(i + 1); } long start = System.currentTimeMillis(); for(int j = 1; j\u003c=100000;j++) { int temp = j; boolean contains = set.contains(temp); } long end = System.currentTimeMillis(); System.out.println(\"time： \" + (end - start)); //time： 5 } Hash结构效率高，那为什么索引结构要设计成树型呢？ Hash索引适用存储引擎如表所示： 索引 / 存储引擎 MyISAM InnoDB Memory HASH索引 不支持 不支持 支持 Hash索引的适用性： 采用自适应 Hash 索引目的是方便根据 SQL 的查询条件加速定位到叶子节点，特别是当 B+ 树比较深的时 候，通过自适应 Hash 索引可以明显提高数据的检索效率。 我们可以通过 innodb_adaptive_hash_index 变量来查看是否开启了自适应 Hash，比如： mysql\u003e show variables like '%adaptive_hash_index'; ","date":"2022-09-24","objectID":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/:6:2","tags":[],"title":"MySQL索引及调优篇","uri":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/"},{"categories":["mysql"],"content":"6.3 二叉搜索树 如果我们利用二叉树作为索引结构，那么磁盘的IO次数和索引树的高度是相关的。 1. 二叉搜索树的特点 一个节点只能有两个子节点，也就是一个节点度不能超过2 左子节点 \u003c 本节点; 右子节点 \u003e= 本节点，比我大的向右，比我小的向左 2. 查找规则 但是特殊情况，就是有时候二叉树的深度非常大，比如： 为了提高查询效率，就需要 减少磁盘IO数 。为了减少磁盘IO的次数，就需要尽量 降低树的高度 ，需要把 原来“瘦高”的树结构变的“矮胖”，树的每层的分叉越多越好。 ","date":"2022-09-24","objectID":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/:6:3","tags":[],"title":"MySQL索引及调优篇","uri":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/"},{"categories":["mysql"],"content":"6.4 AVL树 `每访问一次节点就需要进行一次磁盘 I/O 操作，对于上面的树来说，我们需要进行 5次 I/O 操作。虽然平衡二叉树的效率高，但是树的深度也同样高，这就意味着磁盘 I/O 操作次数多，会影响整体数据查询的效率。 针对同样的数据，如果我们把二叉树改成 M 叉树 （M\u003e2）呢？当 M=3 时，同样的 31 个节点可以由下面 的三叉树来进行存储： 你能看到此时树的高度降低了，当数据量 N 大的时候，以及树的分叉树 M 大的时候，M叉树的高度会远小于二叉树的高度 (M \u003e 2)。所以，我们需要把 `树从“瘦高” 变 “矮胖”。 ","date":"2022-09-24","objectID":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/:6:4","tags":[],"title":"MySQL索引及调优篇","uri":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/"},{"categories":["mysql"],"content":"6.5 B-Tree B 树的英文是 Balance Tree，也就是 多路平衡查找树。简写为 B-Tree。它的高度远小于平衡二叉树的高度。 B 树的结构如下图所示： 一个 M 阶的 B 树（M\u003e2）有以下的特性： 根节点的儿子数的范围是 [2,M]。 每个中间节点包含 k-1 个关键字和 k 个孩子，孩子的数量 = 关键字的数量 +1，k 的取值范围为 [ceil(M/2), M]。 叶子节点包括 k-1 个关键字（叶子节点没有孩子），k 的取值范围为 [ceil(M/2), M]。 假设中间节点节点的关键字为：Key[1], Key[2], …, Key[k-1]，且关键字按照升序排序，即 Key[i]\u003cKey[i+1]。此时 k-1 个关键字相当于划分了 k 个范围，也就是对应着 k 个指针，即为：P[1], P[2], …, P[k]，其中 P[1] 指向关键字小于 Key[1] 的子树，P[i] 指向关键字属于 (Key[i-1], Key[i]) 的子树，P[k] 指向关键字大于 Key[k-1] 的子树。 所有叶子节点位于同一层。 上面那张图所表示的 B 树就是一棵 3 阶的 B 树。我们可以看下磁盘块 2，里面的关键字为（8，12），它 有 3 个孩子 (3，5)，(9，10) 和 (13，15)，你能看到 (3，5) 小于 8，(9，10) 在 8 和 12 之间，而 (13，15) 大于 12，刚好符合刚才我们给出的特征。 然后我们来看下如何用 B 树进行查找。假设我们想要 查找的关键字是 9 ，那么步骤可以分为以下几步： 我们与根节点的关键字 (17，35）进行比较，9 小于 17 那么得到指针 P1； 按照指针 P1 找到磁盘块 2，关键字为（8，12），因为 9 在 8 和 12 之间，所以我们得到指针 P2； 按照指针 P2 找到磁盘块 6，关键字为（9，10），然后我们找到了关键字 9。 你能看出来在 B 树的搜索过程中，我们比较的次数并不少，但如果把数据读取出来然后在内存中进行比 较，这个时间就是可以忽略不计的。而读取磁盘块本身需要进行 I/O 操作，消耗的时间比在内存中进行 比较所需要的时间要多，是数据查找用时的重要因素。 B 树相比于平衡二叉树来说磁盘 I/O 操作要少 ， 在数据查询中比平衡二叉树效率要高。所以 只要树的高度足够低，IO次数足够少，就可以提高查询性能 。 再举例1： ","date":"2022-09-24","objectID":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/:6:5","tags":[],"title":"MySQL索引及调优篇","uri":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/"},{"categories":["mysql"],"content":"6.6 B+Tree MySQL官网说明： B+ 树和 B 树的差异在于以下几点： 有 k 个孩子的节点就有 k 个关键字。也就是孩子数量 = 关键字数，而 B 树中，孩子数量 = 关键字数 +1。 非叶子节点的关键字也会同时存在在子节点中，并且是在子节点中所有关键字的最大（或最 小）。 非叶子节点仅用于索引，不保存数据记录，跟记录有关的信息都放在叶子节点中。而 B 树中， 非 叶子节点既保存索引，也保存数据记录 。 所有关键字都在叶子节点出现，叶子节点构成一个有序链表，而且叶子节点本身按照关键字的大 小从小到大顺序链接。 B 树和 B+ 树都可以作为索引的数据结构，在 MySQL 中采用的是 B+ 树。 但B树和B+树各有自己的应用场景，不能说B+树完全比B树好，反之亦然。 思考题：为了减少IO，索引树会一次性加载吗？ 思考题：B+树的存储能力如何？为何说一般查找行记录，最多只需1~3次磁盘IO 思考题：为什么说B+树比B-树更适合实际应用中操作系统的文件索引和数据库索引？ 思考题：Hash 索引与 B+ 树索引的区别 思考题：Hash 索引与 B+ 树索引是在建索引的时候手动指定的吗？ ","date":"2022-09-24","objectID":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/:6:6","tags":[],"title":"MySQL索引及调优篇","uri":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/"},{"categories":["mysql"],"content":"6.7 R树 R-Tree在MySQL很少使用，仅支持 geometry数据类型 ，支持该类型的存储引擎只有myisam、bdb、 innodb、ndb、archive几种。举个R树在现实领域中能够解决的例子：查找20英里以内所有的餐厅。如果 没有R树你会怎么解决？一般情况下我们会把餐厅的坐标(x,y)分为两个字段存放在数据库中，一个字段记 录经度，另一个字段记录纬度。这样的话我们就需要遍历所有的餐厅获取其位置信息，然后计算是否满 足要求。如果一个地区有100家餐厅的话，我们就要进行100次位置计算操作了，如果应用到谷歌、百度 地图这种超大数据库中，这种方法便必定不可行了。R树就很好的 解决了这种高维空间搜索问题 。它把B 树的思想很好的扩展到了多维空间，采用了B树分割空间的思想，并在添加、删除操作时采用合并、分解 结点的方法，保证树的平衡性。因此，R树就是一棵用来 存储高维数据的平衡树 。相对于B-Tree，R-Tree 的优势在于范围查找。 索引 / 存储引擎 MyISAM InnoDB Memory R-Tree索引 支持 支持 不支持 ","date":"2022-09-24","objectID":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/:6:7","tags":[],"title":"MySQL索引及调优篇","uri":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/"},{"categories":["mysql"],"content":"6.8 小结 ","date":"2022-09-24","objectID":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/:6:8","tags":[],"title":"MySQL索引及调优篇","uri":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/"},{"categories":["mysql"],"content":"附录：算法的时间复杂度 同一问题可用不同算法解决，而一个算法的质量优劣将影响到算法乃至程序的效率。算法分析的目的在 于选择合适算法和改进算法。 第7章_InnoDB数据存储结构 ","date":"2022-09-24","objectID":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/:6:9","tags":[],"title":"MySQL索引及调优篇","uri":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/"},{"categories":["mysql"],"content":"1. 数据库的存储结构：页 ","date":"2022-09-24","objectID":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/:7:0","tags":[],"title":"MySQL索引及调优篇","uri":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/"},{"categories":["mysql"],"content":"1.1 磁盘与内存交互基本单位：页 ","date":"2022-09-24","objectID":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/:7:1","tags":[],"title":"MySQL索引及调优篇","uri":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/"},{"categories":["mysql"],"content":"1.2 页结构概述 ","date":"2022-09-24","objectID":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/:7:2","tags":[],"title":"MySQL索引及调优篇","uri":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/"},{"categories":["mysql"],"content":"1.3 页的大小 不同的数据库管理系统（简称DBMS）的页大小不同。比如在 MySQL 的 InnoDB 存储引擎中，默认页的大小是 16KB，我们可以通过下面的命令来进行查看： show variables like '%innodb_page_size%'; SQL Server 中页的大小为 8KB，而在 Oracle 中我们用术语 “块” （Block）来表示 “页”，Oracle 支持的快大小为2KB, 4KB, 8KB, 16KB, 32KB 和 64KB。 ","date":"2022-09-24","objectID":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/:7:3","tags":[],"title":"MySQL索引及调优篇","uri":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/"},{"categories":["mysql"],"content":"1.4 页的上层结构 另外在数据库中，还存在着区（Extent）、段（Segment）和表空间（Tablespace）的概念。行、页、区、段、表空间的关系如下图所示： ","date":"2022-09-24","objectID":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/:7:4","tags":[],"title":"MySQL索引及调优篇","uri":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/"},{"categories":["mysql"],"content":"2. 页的内部结构 页如果按类型划分的话，常见的有 数据页（保存B+树节点）、系统表、Undo 页 和 事物数据页 等。数据页是我们最常使用的页。 数据页的 16KB 大小的存储空间被划分为七个部分，分别是文件头（File Header）、页头（Page Header）、最大最小记录（Infimum + supremum）、用户记录（User Records）、空闲空间（Free Space）、页目录（Page Directory）和文件尾（File Tailer）。 页结构的示意图如下所示： 如下表所示： 我们可以把这7个结构分为3个部分。 ","date":"2022-09-24","objectID":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/:8:0","tags":[],"title":"MySQL索引及调优篇","uri":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/"},{"categories":["mysql"],"content":"第一部分：File Header (文件头部) 和 File Trailer (文件尾部) 见文件InnoDB数据库存储结构.mmap ","date":"2022-09-24","objectID":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/:8:1","tags":[],"title":"MySQL索引及调优篇","uri":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/"},{"categories":["mysql"],"content":"第二部分：User Records (用户记录)、最大最小记录、Free Space (空闲空间) 见文件InnoDB数据库存储结构.mmap ","date":"2022-09-24","objectID":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/:8:2","tags":[],"title":"MySQL索引及调优篇","uri":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/"},{"categories":["mysql"],"content":"第三部分：Page Directory (页目录) 和 Page Header (页面头部) 见文件InnoDB数据库存储结构.mmap ","date":"2022-09-24","objectID":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/:8:3","tags":[],"title":"MySQL索引及调优篇","uri":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/"},{"categories":["mysql"],"content":"2.3 从数据库页的角度看B+树如何查询 一颗B+树按照字节类型可以分为两部分： 叶子节点，B+ 树最底层的节点，节点的高度为0，存储行记录。 非叶子节点，节点的高度大于0，存储索引键和页面指针，并不存储行记录本身。(索引键肯定包括主键，聚簇索引只有主键，辅助索引这些一般是主键+索引列形成新的索引结构) 当我们从页结构来理解 B+ 树的结构的时候，可以帮我们理解一些通过索引进行检索的原理： 当然，一般只要普通索引字段的重复值不要太多，保持在同一个数据页，这样就和唯一索引差别不大，毕竟每次处理查询都是以页为单位把数据从磁盘加载到内存里。这样普通索引临近的重复数据都能快速找到，但是如果重复数据太多，导致多页的重复数据，那么就会导致要多一次甚至多次磁盘io才能查询到数据。当然，也可能普通索引重复数据不多，但是刚好被安排在两页的尾首，这样也会耽误运行的效率。综合来说，一般唯一索引肯定要优于或等于普通索引的检索效率。 ","date":"2022-09-24","objectID":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/:8:4","tags":[],"title":"MySQL索引及调优篇","uri":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/"},{"categories":["mysql"],"content":"3. InnoDB行格式 (或记录格式) 见文件InnoDB数据库存储结构.mmap下 ","date":"2022-09-24","objectID":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/:9:0","tags":[],"title":"MySQL索引及调优篇","uri":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/"},{"categories":["mysql"],"content":"4. 区、段与碎片区 ","date":"2022-09-24","objectID":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/:10:0","tags":[],"title":"MySQL索引及调优篇","uri":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/"},{"categories":["mysql"],"content":"4.1 为什么要有区？ ","date":"2022-09-24","objectID":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/:10:1","tags":[],"title":"MySQL索引及调优篇","uri":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/"},{"categories":["mysql"],"content":"4.2 为什么要有段？ ","date":"2022-09-24","objectID":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/:10:2","tags":[],"title":"MySQL索引及调优篇","uri":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/"},{"categories":["mysql"],"content":"4.3 为什么要有碎片区？ ","date":"2022-09-24","objectID":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/:10:3","tags":[],"title":"MySQL索引及调优篇","uri":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/"},{"categories":["mysql"],"content":"4.4 区的分类 区大体上可以分为4种类型： 空闲的区 (FREE) : 现在还没有用到这个区中的任何页面。 有剩余空间的碎片区 (FREE_FRAG)：表示碎片区中还有可用的页面。 没有剩余空间的碎片区 (FULL_FRAG)：表示碎片区中的所有页面都被使用，没有空闲页面。 附属于某个段的区 (FSEG)：每一个索引都可以分为叶子节点段和非叶子节点段。 处于FREE、FREE_FRAG 以及 FULL_FRAG 这三种状态的区都是独立的，直属于表空间。而处于 FSEG 状态的区是附属于某个段的。 如果把表空间比作是一个集团军，段就相当于师，区就相当于团。一般的团都是隶属于某个师的，就像是处于 FSEG 的区全部隶属于某个段，而处于 FREE、FREE_FRAG 以及 FULL_FRAG 这三种状态的区却直接隶属于表空间，就像独立团直接听命于军部一样。 ","date":"2022-09-24","objectID":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/:10:4","tags":[],"title":"MySQL索引及调优篇","uri":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/"},{"categories":["mysql"],"content":"5. 表空间 ","date":"2022-09-24","objectID":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/:11:0","tags":[],"title":"MySQL索引及调优篇","uri":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/"},{"categories":["mysql"],"content":"5.1 独立表空间 独立表空间，即每张表有一个独立的表空间，也就是数据和索引信息都会保存在自己的表空间中。独立的表空间 (即：单表) 可以在不同的数据库之间进行 迁移。 空间可以回收 (DROP TABLE 操作可自动回收表空间；其他情况，表空间不能自己回收) 。如果对于统计分析或是日志表，删除大量数据后可以通过：alter table TableName engine=innodb; 回收不用的空间。对于使用独立表空间的表，不管怎么删除，表空间的碎片不会太严重的影响性能，而且还有机会处理。 独立表空间结构 独立表空间由段、区、页组成。 真实表空间对应的文件大小 我们到数据目录里看，会发现一个新建的表对应的 .ibd 文件只占用了 96K，才6个页面大小 (MySQL5.7中)，这是因为一开始表空间占用的空间很小，因为表里边都没有数据。不过别忘了这些 .ibd 文件是自扩展的，随着表中数据的增多，表空间对应的文件也逐渐增大。 查看 InnoDB 的表空间类型： show variables like 'innodb_file_per_table' 你能看到 innodb_file_per_table=ON, 这就意味着每张表都会单词保存一个 .ibd 文件。 ","date":"2022-09-24","objectID":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/:11:1","tags":[],"title":"MySQL索引及调优篇","uri":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/"},{"categories":["mysql"],"content":"5.2 系统表空间 系统表空间的结构和独立表空间基本类似，只不过由于整个MySQL进程只有一个系统表空间，在系统表空间中会额外记录一些有关整个系统信息的页面，这部分是独立表空间中没有的。 InnoDB数据字典 删除这些数据并不是我们使用 INSERT 语句插入的用户数据，实际上是为了更好的管理我们这些用户数据而不得以引入的一些额外数据，这些数据页称为 元数据。InnoDB 存储引擎特意定义了一些列的 内部系统表 (internal system table) 来记录这些元数据： 这些系统表也称为 数据字典，它们都是以 B+ 树的形式保存在系统表空间的某个页面中。其中 SYS_TABLES、SYS_COLUMNS、SYS_INDEXES、SYS_FIELDS 这四个表尤其重要，称之为基本系统表 (basic system tables) ，我们先看看这4个表的结构： 注意：用户不能直接访问 InnoDB 的这些内部系统表，除非你直接去解析系统表空间对应文件系统上的文件。不过考虑到查看这些表的内容可能有助于大家分析问题，所以在系统数据库 information_schema 中提供了一些以 innodb_sys 开头的表: USE information_schema; SHOW TABLES LIKE 'innodb_sys%'; 在 information_scheme 数据库中的这些以 INNODB_SYS 开头的表并不是真正的内部系统表 (内部系统表就是我们上边以 SYS 开头的那些表)，而是在存储引擎启动时读取这些以 SYS 开头的系统表，然后填充到这些以 INNODB_SYS 开头的表中。以 INNODB_SYS 开头的表和以 SYS 开头的表中的字段并不完全一样，但仅供大家参考已经足矣。 ","date":"2022-09-24","objectID":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/:11:2","tags":[],"title":"MySQL索引及调优篇","uri":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/"},{"categories":["mysql"],"content":"附录：数据页加载的三种方式 InnoDB从磁盘中读取数据 最小单位 是数据页。而你想得到的 id = xxx 的数据，就是这个数据页众多行中的一行。 对于MySQL存放的数据，逻辑概念上我们称之为表，在磁盘等物理层面而言是按 数据页 形式进行存放的，当其加载到 MySQL 中我们称之为 缓存页。 如果缓冲池没有该页数据，那么缓冲池有以下三种读取数据的方式，每种方式的读取速率是不同的： 1. 内存读取 如果该数据存在于内存中，基本上执行时间在 1ms 左右，效率还是很高的。 2. 随机读取 3. 顺序读取 第8章_索引的创建与设计原则 ","date":"2022-09-24","objectID":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/:12:0","tags":[],"title":"MySQL索引及调优篇","uri":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/"},{"categories":["mysql"],"content":"1. 索引的声明与使用 ","date":"2022-09-24","objectID":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/:13:0","tags":[],"title":"MySQL索引及调优篇","uri":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/"},{"categories":["mysql"],"content":"1.1 索引的分类 MySQL的索引包括普通索引、唯一性索引、全文索引、单列索引、多列索引和空间索引等。 从 功能逻辑 上说，索引主要有 4 种，分别是普通索引、唯一索引、主键索引、全文索引。 按照 物理实现方式 ，索引可以分为 2 种：聚簇索引和非聚簇索引。 按照 作用字段个数 进行划分，分成单列索引和联合索引。 1. 普通索引 2. 唯一性索引 3. 主键索引 4. 单列索引 5. 多列 (组合、联合) 索引 6. 全文检索 7. 补充：空间索引 **小结：不同的存储引擎支持的索引类型也不一样 ** InnoDB ：支持 B-tree、Full-text 等索引，不支持 Hash 索引； MyISAM ： 支持 B-tree、Full-text 等索引，不支持 Hash 索引； Memory ：支持 B-tree、Hash 等 索引，不支持 Full-text 索引； NDB ：支持 Hash 索引，不支持 B-tree、Full-text 等索引； Archive ：不支 持 B-tree、Hash、Full-text 等索引； ","date":"2022-09-24","objectID":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/:13:1","tags":[],"title":"MySQL索引及调优篇","uri":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/"},{"categories":["mysql"],"content":"1.2 创建索引 MySQL支持多种方法在单个或多个列上创建索引：在创建表的定义语句 CREATE TABLE 中指定索引列，使用 ALTER TABLE 语句在存在的表上创建索引，或者使用 CREATE INDEX 语句在已存在的表上添加索引。 1. 创建表的时候创建索引 使用CREATE TABLE创建表时，除了可以定义列的数据类型外，还可以定义主键约束、外键约束或者唯一性约束，而不论创建哪种约束，在定义约束的同时相当于在指定列上创建了一个索引。 举例： CREATE TABLE dept( dept_id INT PRIMARY KEY AUTO_INCREMENT, dept_name VARCHAR(20) ); CREATE TABLE emp( emp_id INT PRIMARY KEY AUTO_INCREMENT, emp_name VARCHAR(20) UNIQUE, dept_id INT, CONSTRAINT emp_dept_id_fk FOREIGN KEY(dept_id) REFERENCES dept(dept_id) ) 但是，如果显式创建表时创建索引的话，基本语法格式如下： CREATE TABLE table_name [col_name data_type] [UNIQUE | FULLTEXT | SPATIAL] [INDEX | KEY] [index_name] (col_name [length]) [ASC | DESC] UNIQUE 、 FULLTEXT 和 SPATIAL 为可选参数，分别表示唯一索引、全文索引和空间索引； INDEX 与 KEY 为同义词，两者的作用相同，用来指定创建索引； index_name 指定索引的名称，为可选参数，如果不指定，那么MySQL默认col_name为索引名； col_name 为需要创建索引的字段列，该列必须从数据表中定义的多个列中选择； length 为可选参数，表示索引的长度，只有字符串类型的字段才能指定索引长度； ASC 或 DESC 指定升序或者降序的索引值存储。 1. 创建普通索引 在book表中的year_publication字段上建立普通索引，SQL语句如下： CREATE TABLE book( book_id INT , book_name VARCHAR(100), `authors` VARCHAR(100), info VARCHAR(100) , `comment` VARCHAR(100), year_publication YEAR, INDEX(year_publication) ); 2. 创建唯一索引 CREATE TABLE test1( id INT NOT NULL, name varchar(30) NOT NULL, UNIQUE INDEX uk_idx_id(id) ); 该语句执行完毕之后，使用SHOW CREATE TABLE查看表结构： SHOW INDEX FROM test1 \\G 3. 主键索引 设定为主键后数据库会自动建立索引，innodb为聚簇索引，语法： 随表一起建索引： CREATE TABLE student ( id INT(10) UNSIGNED AUTO_INCREMENT , student_no VARCHAR(200), student_name VARCHAR(200), PRIMARY KEY(id) ); 删除主键索引： ALTER TABLE student drop PRIMARY KEY; 修改主键索引：必须先删除掉(drop)原索引，再新建(add)索引 4. 创建单列索引 引举: CREATE TABLE test2( id INT NOT NULL, name CHAR(50) NULL, INDEX single_idx_name(name(20)) ); 该语句执行完毕之后，使用SHOW CREATE TABLE查看表结构： SHOW INDEX FROM test2 \\G 5. 创建组合索引 举例：创建表test3，在表中的id、name和age字段上建立组合索引，SQL语句如下： CREATE TABLE test3( id INT(11) NOT NULL, name CHAR(30) NOT NULL, age INT(11) NOT NULL, info VARCHAR(255), INDEX multi_idx(id,name,age) ); 该语句执行完毕之后，使用SHOW INDEX 查看： SHOW INDEX FROM test3 \\G 在test3表中，查询id和name字段，使用EXPLAIN语句查看索引的使用情况： EXPLAIN SELECT * FROM test3 WHERE id=1 AND name='songhongkang' \\G 可以看到，查询id和name字段时，使用了名称为MultiIdx的索引，如果查询 (name, age) 组合或者单独查询name和age字段，会发现结果中possible_keys和key值为NULL, 并没有使用在t3表中创建的索引进行查询。虽然查询条件为(id,name)字段时是可以使用到索引的，因为创建索引时的字段是(id,name,age),也就是说，B+tree是先按照id递增，id相同时根据name字段，name又相同时根据age字段进行排序的，所以，从整个B+tree来看，查询条件为(name,age)是无法使用到索引的，因为在联合索引(id,name,age)构成的B+树整棵树来看name和age都是无序的，只能说局部有序 6. 创建全文索引 FULLTEXT全文索引可以用于全文检索，并且只为 CHAR 、VARCHAR 和 TEXT 列创建索引。索引总是对整个列进行，不支持局部 (前缀) 索引。 举例1：创建表test4，在表中的info字段上建立全文索引，SQL语句如下： CREATE TABLE test4( id INT NOT NULL, name CHAR(30) NOT NULL, age INT NOT NULL, info VARCHAR(255), FULLTEXT INDEX futxt_idx_info(info) ) ENGINE=MyISAM; 在MySQL5.7及之后版本中可以不指定最后的ENGINE了，因为在此版本中InnoDB支持全文索引。 语句执行完毕之后，使用SHOW CREATE TABLE查看表结构： SHOW INDEX FROM test4 \\G 由结果可以看到，info字段上已经成功建立了一个名为futxt_idx_info的FULLTEXT索引。 举例2： CREATE TABLE articles ( id INT UNSIGNED AUTO_INCREMENT PRIMARY KEY, title VARCHAR (200), body TEXT, FULLTEXT index (title, body) ) ENGINE = INNODB; 创建了一个给title和body字段添加全文索引的表。 举例3： CREATE TABLE `papers` ( `id` int(10) unsigned NOT NULL AUTO_INCREMENT, `title` varchar(200) DEFAULT NULL, `content` text, PRIMARY KEY (`id`), FULLTEXT KEY `title` (`title`,`content`) ) ENGINE=MyISAM DEFAULT CHARSET=utf8; 不同于like方式的的查询： SELECT * FROM papers WHERE content LIKE ‘%查询字符串%’; 全文索引用match+against方式查询： SELECT * FROM papers WHERE MATCH(title,content) AGAINST (‘查询字符串’); 明显的提高查询效率。 注意点 使用全文索引前，搞清楚版本支持情况； 全文索引比 like + % 快 N 倍，但是可能存在精度问题； 如果需要全文索引的是大量数据，建议先添加数据，再创建索引。 7. 创建空间索引 空间索引创建中，要求空间类型的字段必须为 非空 。 举例：创建表test5，在空间类型为GEOMETRY的字段上创建空间索引，SQL语句如下： CREATE TABLE test5( geo GEOMETRY NOT NULL, SPATIAL INDEX spa_idx_geo(geo) ) ENGINE=MyISAM; 该语句执行完毕之后，使用SHOW CREATE TABLE查看表结构： SHOW INDEX FROM test5 \\G 可以看到，test5表的geo字段上创建了名称为spa_idx_geo的空间索引。注意创建时指定空间类型字段值的非空约束，并且表的存储引擎为MyISAM。 2. 在已经存在的表上创建索引 在已经存在的表中创建索引可以使用ALTER TABLE语句或者CREATE INDEX语句。 1. 使用ALTER TABLE语句创建索引 ALTER TABLE语句创建索引的基本语法如下： ALTER TABLE table_name ADD [UNIQUE | FULLTEXT | SPATIAL] [INDEX |","date":"2022-09-24","objectID":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/:13:2","tags":[],"title":"MySQL索引及调优篇","uri":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/"},{"categories":["mysql"],"content":"1.3 删除索引 1. 使用ALTER TABLE删除索引 ALTER TABLE删除索引的基本语法格式如下： ALTER TABLE table_name DROP INDEX index_name; 2. 使用DROP INDEX语句删除索引 DROP INDEX删除索引的基本语法格式如下： DROP INDEX index_name ON table_name; 提示: 删除表中的列时，如果要删除的列为索引的组成部分，则该列也会从索引中删除。如果组成索引的所有列都被删除，则整个索引将被删除。 ","date":"2022-09-24","objectID":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/:13:3","tags":[],"title":"MySQL索引及调优篇","uri":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/"},{"categories":["mysql"],"content":"2. MySQL8.0索引新特性 ","date":"2022-09-24","objectID":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/:14:0","tags":[],"title":"MySQL索引及调优篇","uri":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/"},{"categories":["mysql"],"content":"2.1 支持降序索引 降序索引以降序存储键值。虽然在语法上，从MySQL 4版本开始就已经支持降序索引的语法了，但实际上DESC定义是被忽略的，直到MySQL 8.x版本才开始真正支持降序索引 (仅限于InnoDBc存储引擎)。 MySQL在8.0版本之前创建的仍然是升序索引，使用时进行反向扫描，这大大降低了数据库的效率。在某些场景下，降序索引意义重大。例如，如果一个查询，需要对多个列进行排序，且顺序要求不一致，那么使用降序索引将会避免数据库使用额外的文件排序操作，从而提高性能。 举例：分别在MySQL 5.7版本和MySQL 8.0版本中创建数据表ts1，结果如下： CREATE TABLE ts1(a int,b int,index idx_a_b(a,b desc)); 在MySQL 5.7版本中查看数据表ts1的结构，结果如下: 从结果可以看出，索引仍然是默认的升序 在MySQL 8.0版本中查看数据表ts1的结构，结果如下： 从结果可以看出，索引已经是降序了。下面继续测试降序索引在执行计划中的表现。 分别在MySQL 5.7版本和MySQL 8.0版本的数据表ts1中插入800条随机数据，执行语句如下： DELIMITER // CREATE PROCEDURE ts_insert() BEGIN DECLARE i INT DEFAULT 1; WHILE i \u003c 800 DO insert into ts1 select rand()*80000, rand()*80000; SET i = i+1; END WHILE; commit; END // DELIMITER; # 调用 CALL ts_insert(); 在MySQL 5.7版本中查看数据表ts1的执行计划，结果如下: EXPLAIN SELECT * FROM ts1 ORDER BY a, b DESC LIMIT 5; 在MySQL 8.0版本中查看数据表 ts1 的执行计划。 从结果可以看出，修改后MySQL 5.7 的执行计划要明显好于MySQL 8.0。 ","date":"2022-09-24","objectID":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/:14:1","tags":[],"title":"MySQL索引及调优篇","uri":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/"},{"categories":["mysql"],"content":"2.2 隐藏索引 在MySQL 5.7版本及之前，只能通过显式的方式删除索引。此时，如果发展删除索引后出现错误，又只能通过显式创建索引的方式将删除的索引创建回来。如果数据表中的数据量非常大，或者数据表本身比较 大，这种操作就会消耗系统过多的资源，操作成本非常高。 从MySQL 8.x开始支持 隐藏索引（invisible indexes） ，只需要将待删除的索引设置为隐藏索引，使 查询优化器不再使用这个索引（即使使用force index（强制使用索引），优化器也不会使用该索引）， 确认将索引设置为隐藏索引后系统不受任何响应，就可以彻底删除索引。 这种通过先将索引设置为隐藏索 引，再删除索引的方式就是软删除。 同时，如果你想验证某个索引删除之后的 查询性能影响，就可以暂时先隐藏该索引。 注意： 主键不能被设置为隐藏索引。当表中没有显式主键时，表中第一个唯一非空索引会成为隐式主键，也不能设置为隐藏索引。 索引默认是可见的，在使用CREATE TABLE, CREATE INDEX 或者 ALTER TABLE 等语句时可以通过 VISIBLE 或者 INVISIBLE 关键词设置索引的可见性。 1. 创建表时直接创建 在MySQL中创建隐藏索引通过SQL语句INVISIBLE来实现，其语法形式如下： CREATE TABLE tablename( propname1 type1[CONSTRAINT1], propname2 type2[CONSTRAINT2], …… propnamen typen, INDEX [indexname](propname1 [(length)]) INVISIBLE ); 上述语句比普通索引多了一个关键字INVISIBLE，用来标记索引为不可见索引。 2. 在已经存在的表上创建 可以为已经存在的表设置隐藏索引，其语法形式如下： CREATE INDEX indexname ON tablename(propname[(length)]) INVISIBLE; 3. 通过ALTER TABLE语句创建 语法形式如下： ALTER TABLE tablename ADD INDEX indexname (propname [(length)]) INVISIBLE; 4. 切换索引可见状态 已存在的索引可通过如下语句切换可见状态： ALTER TABLE tablename ALTER INDEX index_name INVISIBLE; #切换成隐藏索引 ALTER TABLE tablename ALTER INDEX index_name VISIBLE; #切换成非隐藏索引 如果将index_cname索引切换成可见状态，通过explain查看执行计划，发现优化器选择了index_cname索引。 注意 当索引被隐藏时，它的内容仍然是和正常索引一样实时更新的。如果一个索引需要长期被隐藏，那么可以将其删除，因为索引的存在会影响插入、更新和删除的性能。 通过设置隐藏索引的可见性可以查看索引对调优的帮助。 5. 使隐藏索引对查询优化器可见 在MySQL 8.x版本中，为索引提供了一种新的测试方式，可以通过查询优化器的一个开关 (use_invisible_indexes) 来打开某个设置，使隐藏索引对查询优化器可见。如果use_invisible_indexes 设置为off (默认)，优化器会忽略隐藏索引。如果设置为on，即使隐藏索引不可见，优化器在生成执行计 划时仍会考虑使用隐藏索引。 （1）在MySQL命令行执行如下命令查看查询优化器的开关设置。 mysql\u003e select @@optimizer_switch \\G 在输出的结果信息中找到如下属性配置。 use_invisible_indexes=off 此属性配置值为off，说明隐藏索引默认对查询优化器不可见。 （2）使隐藏索引对查询优化器可见，需要在MySQL命令行执行如下命令： mysql\u003e set session optimizer_switch=\"use_invisible_indexes=on\"; Query OK, 0 rows affected (0.00 sec) SQL语句执行成功，再次查看查询优化器的开关设置。 mysql\u003e select @@optimizer_switch \\G *************************** 1. row *************************** @@optimizer_switch: index_merge=on,index_merge_union=on,index_merge_sort_union=on,index_merge_ intersection=on,engine_condition_pushdown=on,index_condition_pushdown=on,mrr=on,mrr_co st_based=on,block_nested_loop=on,batched_key_access=off,materialization=on,semijoin=on ,loosescan=on,firstmatch=on,duplicateweedout=on,subquery_materialization_cost_based=on ,use_index_extensions=on,condition_fanout_filter=on,derived_merge=on,use_invisible_ind exes=on,skip_scan=on,hash_join=on 1 row in set (0.00 sec) 此时，在输出结果中可以看到如下属性配置。 use_invisible_indexes=on use_invisible_indexes属性的值为on，说明此时隐藏索引对查询优化器可见。 （3）使用EXPLAIN查看以字段invisible_column作为查询条件时的索引使用情况。 explain select * from classes where cname = '高一2班'; 查询优化器会使用隐藏索引来查询数据。 （4）如果需要使隐藏索引对查询优化器不可见，则只需要执行如下命令即可。 mysql\u003e set session optimizer_switch=\"use_invisible_indexes=off\"; Query OK, 0 rows affected (0.00 sec) 再次查看查询优化器的开关设置。 mysql\u003e select @@optimizer_switch \\G 此时，use_invisible_indexes属性的值已经被设置为“off”。 ","date":"2022-09-24","objectID":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/:14:2","tags":[],"title":"MySQL索引及调优篇","uri":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/"},{"categories":["mysql"],"content":"3. 索引的设计原则 为了使索引的使用效率更高，在创建索引时，必须考虑在哪些字段上创建索引和创建什么类型的索引。**索引设计不合理或者缺少索引都会对数据库和应用程序的性能造成障碍。**高效的索引对于获得良好的性能非常重要。设计索引时，应该考虑相应准则。 ","date":"2022-09-24","objectID":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/:15:0","tags":[],"title":"MySQL索引及调优篇","uri":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/"},{"categories":["mysql"],"content":"3.1 数据准备 第1步：创建数据库、创建表 CREATE DATABASE atguigudb1; USE atguigudb1; #1.创建学生表和课程表 CREATE TABLE `student_info` ( `id` INT(11) NOT NULL AUTO_INCREMENT, `student_id` INT NOT NULL , `name` VARCHAR(20) DEFAULT NULL, `course_id` INT NOT NULL , `class_id` INT(11) DEFAULT NULL, `create_time` DATETIME DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP, PRIMARY KEY (`id`) ) ENGINE=INNODB AUTO_INCREMENT=1 DEFAULT CHARSET=utf8; CREATE TABLE `course` ( `id` INT(11) NOT NULL AUTO_INCREMENT, `course_id` INT NOT NULL , `course_name` VARCHAR(40) DEFAULT NULL, PRIMARY KEY (`id`) ) ENGINE=INNODB AUTO_INCREMENT=1 DEFAULT CHARSET=utf8; 第2步：创建模拟数据必需的存储函数 #函数1：创建随机产生字符串函数 DELIMITER // CREATE FUNCTION rand_string(n INT) RETURNS VARCHAR(255) #该函数会返回一个字符串 BEGIN DECLARE chars_str VARCHAR(100) DEFAULT 'abcdefghijklmnopqrstuvwxyzABCDEFJHIJKLMNOPQRSTUVWXYZ'; DECLARE return_str VARCHAR(255) DEFAULT ''; DECLARE i INT DEFAULT 0; WHILE i \u003c n DO SET return_str =CONCAT(return_str,SUBSTRING(chars_str,FLOOR(1+RAND()*52),1)); SET i = i + 1; END WHILE; RETURN return_str; END // DELIMITER ; #函数2：创建随机数函数 DELIMITER // CREATE FUNCTION rand_num (from_num INT ,to_num INT) RETURNS INT(11) BEGIN DECLARE i INT DEFAULT 0; SET i = FLOOR(from_num +RAND()*(to_num - from_num+1)) ; RETURN i; END // DELIMITER ; 创建函数，假如报错： This function has none of DETERMINISTIC...... 由于开启过慢查询日志bin-log, 我们就必须为我们的function指定一个参数。 主从复制，主机会将写操作记录在bin-log日志中。从机读取bin-log日志，执行语句来同步数据。如果使用函数来操作数据，会导致从机和主机操作时间不一致。所以，默认情况下，mysql不开启创建函数设置。 查看mysql是否允许创建函数： show variables like 'log_bin_trust_function_creators'; 命令开启：允许创建函数设置： set global log_bin_trust_function_creators=1; # 不加global只是当前窗口有效。 mysqld重启，上述参数又会消失。永久方法： windows下：my.ini[mysqld]加上： log_bin_trust_function_creators=1 linux下：/etc/my.cnf下my.cnf[mysqld]加上： log_bin_trust_function_creators=1 第3步：创建插入模拟数据的存储过程 # 存储过程1：创建插入课程表存储过程 DELIMITER // CREATE PROCEDURE insert_course( max_num INT ) BEGIN DECLARE i INT DEFAULT 0; SET autocommit = 0; #设置手动提交事务 REPEAT #循环 SET i = i + 1; #赋值 INSERT INTO course (course_id, course_name ) VALUES (rand_num(10000,10100),rand_string(6)); UNTIL i = max_num END REPEAT; COMMIT; #提交事务 END // DELIMITER ; # 存储过程2：创建插入学生信息表存储过程 DELIMITER // CREATE PROCEDURE insert_stu( max_num INT ) BEGIN DECLARE i INT DEFAULT 0; SET autocommit = 0; #设置手动提交事务 REPEAT #循环 SET i = i + 1; #赋值 INSERT INTO student_info (course_id, class_id ,student_id ,NAME ) VALUES (rand_num(10000,10100),rand_num(10000,10200),rand_num(1,200000),rand_string(6)); UNTIL i = max_num END REPEAT; COMMIT; #提交事务 END // DELIMITER ; 第4步：调用存储过程 CALL insert_course(100); CALL insert_stu(1000000); ","date":"2022-09-24","objectID":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/:15:1","tags":[],"title":"MySQL索引及调优篇","uri":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/"},{"categories":["mysql"],"content":"3.2 哪些情况适合创建索引 1. 字段的数值有唯一性的限制 业务上具有唯一特性的字段，即使是组合字段，也必须建成唯一索引。（来源：Alibaba） 说明：不要以为唯一索引影响了 insert 速度，这个速度损耗可以忽略，但提高查找速度是明显的。 2. 频繁作为 WHERE 查询条件的字段 某个字段在SELECT语句的 WHERE 条件中经常被使用到，那么就需要给这个字段创建索引了。尤其是在数据量大的情况下，创建普通索引就可以大幅提升数据查询的效率。 比如 student_info 数据表（含100万条数据），假设我们想要查询 student_id=123110 的用户信息。 3. 经常 GROUP BY 和 ORDER BY 的列 索引就是让数据按照某种顺序进行存储或检索，因此当我们使用 GROUP BY 对数据进行分组查询，或者使用 ORDER BY 对数据进行排序的时候，就需要对分组或者排序的字段进行索引。如果待排序的列有多个，那么可以在这些列上建立组合索引。 4. UPDATE、DELETE 的 WHERE 条件列 对数据按照某个条件进行查询后再进行 UPDATE 或 DELETE 的操作，如果对 WHERE 字段创建了索引，就能大幅提升效率。原理是因为我们需要先根据 WHERE 条件列检索出来这条记录，然后再对它进行更新或删除。如果进行更新的时候，更新的字段是非索引字段，提升的效率会更明显，这是因为非索引字段更新不需要对索引进行维护。所以，频繁地更新删除的字段，也要考虑索引是否建立的问题。 5.DISTINCT 字段需要创建索引 有时候我们需要对某个字段进行去重，使用 DISTINCT，那么对这个字段创建索引，也会提升查询效率。 比如，我们想要查询课程表中不同的 student_id 都有哪些，如果我们没有对 student_id 创建索引，执行 SQL 语句： SELECT DISTINCT(student_id) FROM `student_info`; 运行结果（600637 条记录，运行时间 0.683s ） 如果我们对 student_id 创建索引，再执行 SQL 语句： SELECT DISTINCT(student_id) FROM `student_info`; 运行结果（600637 条记录，运行时间 0.010s ） 你能看到 SQL 查询效率有了提升，同时显示出来的 student_id 还是按照递增的顺序进行展示的。这是因为索引会对数据按照某种顺序进行排序，所以在去重的时候也会快很多。 6. 多表 JOIN 连接操作时，创建索引注意事项 首先， 连接表的数量尽量不要超过 3 张 ，因为每增加一张表就相当于增加了一次嵌套的循环，数量级增长会非常快，严重影响查询的效率。 其次， 对 WHERE 条件创建索引 ，因为 WHERE 才是对数据条件的过滤。如果在数据量非常大的情况下， 没有 WHERE 条件过滤是非常可怕的。 最后， 对用于连接的字段创建索引 ，并且该字段在多张表中的类型必须一致 。比如 course_id 在 student_info 表和 course 表中都为 int(11) 类型，而不能一个为 int 另一个为 varchar 类型。因为，类型不一致时，mysql会使用函数来做一个隐式地类型转换，一旦使用函数，索引就会失效，失效就使用不到索引了，变成全表扫描。 举个例子，如果我们只对 student_id 创建索引，执行 SQL 语句： SELECT s.course_id, name, s.student_id, c.course_name FROM student_info s JOIN course c ON s.course_id = c.course_id WHERE name = '462eed7ac6e791292a79'; 运行结果（1 条数据，运行时间 0.189s ） 这里我们对 name 创建索引，再执行上面的 SQL 语句，运行时间为 0.002s 。 7. 使用列的类型小的创建索引 8. 使用字符串前缀创建索引 创建一张商户表，因为地址字段比较长，在地址字段上建立前缀索引 create table shop(address varchar(120) not null); alter table shop add index(address(12)); 问题是，截取多少呢？截取得少了，达不到节省索引存储空间的目的；截取得多了，重复内容太多，字的散列度(选择性)会降低。怎么计算不同的长度的选择性呢？ 先看一下字段在全部数据中的选择度： select count(distinct address) / count(*) from shop 通过不同长度去计算，与全表的选择性对比： 公式： count(distinct left(列名, 索引长度))/count(*) 例如： select count(distinct left(address,10)) / count(*) as sub10, -- 截取前10个字符的选择度 count(distinct left(address,15)) / count(*) as sub11, -- 截取前15个字符的选择度 count(distinct left(address,20)) / count(*) as sub12, -- 截取前20个字符的选择度 count(distinct left(address,25)) / count(*) as sub13 -- 截取前25个字符的选择度 from shop; 越接近于1越好，说明越有区分度 引申另一个问题：索引列前缀对排序的影响 如果使用了索引列前缀，比方说前边只把address列的 前12个字符 放到了二级索引中，下边这个查询可能就有点尴尬了： SELECT * FROM shop ORDER BY address LIMIT 12; 因为二级索引中不包含完整的address列信息，所以无法对前12个字符相同，后边的字符不同的记录进行排序，也就是使用索引列前缀的方式 无法支持使用索引排序 ，只能使用文件排序。 拓展：Alibaba《Java开发手册》 【 强制 】在 varchar 字段上建立索引时，必须指定索引长度，没必要对全字段建立索引，根据实际文本区分度决定索引长度。 说明：索引的长度与区分度是一对矛盾体，一般对字符串类型数据，长度为 20 的索引，区分度会高达 90% 以上 ，可以使用 count(distinct left(列名, 索引长度))/count(*)的区分度来确定。 9. 区分度高(散列性高)的列适合作为索引 列的基数 指的是某一列中不重复数据的个数，比方说某个列包含值 2, 5, 8, 2, 5, 8, 2, 5, 8，虽然有9条记录，但该列的基数却是3。也就是说**在记录行数一定的情况下，列的基数越大，该列中的值越分散；列的基数越小，该列中的值越集中。**这个列的基数指标非常重要，直接影响我们是否能有效的利用索引。最好为列的基数大的列简历索引，为基数太小的列的简历索引效果可能不好。 可以使用公式select count(distinct a) / count(*) from t1 计算区分度，越接近1越好，一般超过33%就算比较高效的索引了。 扩展：联合索引把区分度搞(散列性高)的列放在前面。 10. 使用最频繁的列放到联合索引的左侧 这样也可以较少的建立一些索引。同时，由于\"最左前缀原则\"，可以增加联合索引的使用率。不满足最左前缀原则，可能会导致无法使用到联合索引，只能文件排序 11. 在多个字段都要创建索引的情况下，联合索引优于单值索引 ","date":"2022-09-24","objectID":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/:15:2","tags":[],"title":"MySQL索引及调优篇","uri":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/"},{"categories":["mysql"],"content":"3.3 限制索引的数目 ","date":"2022-09-24","objectID":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/:15:3","tags":[],"title":"MySQL索引及调优篇","uri":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/"},{"categories":["mysql"],"content":"3.4 哪些情况不适合创建索引 1. 在where中使用不到的字段，不要设置索引 WHERE条件 (包括 GROUP BY、ORDER BY) 里用不到的字段不需要创建索引，索引的价值是快速定位，如果起不到定位的字段通常是不需要创建索引的。因为索引在update、insert、delete数据时，数据库会维护索引的结构，造成性能损耗。 举个例子： SELECT course_id, student_id, create_time FROM student_info WHERE student_id = 41251; 因为我们是按照 student_id 来进行检索的，所以不需要对其他字段创建索引，即使这些字段出现在SELECT字段中。 2. 数据量小的表最好不要使用索引 如果表记录太少，比如少于1000个，那么是不需要创建索引的。表记录太少，是否创建索引 对查询效率的影响并不大。甚至说，查询花费的时间可能比遍历索引的时间还要短，索引可能不会产生优化效果。 举例：创建表1： CREATE TABLE t_without_index( a INT PRIMARY KEY AUTO_INCREMENT, b INT ); 提供存储过程1： #创建存储过程 DELIMITER // CREATE PROCEDURE t_wout_insert() BEGIN DECLARE i INT DEFAULT 1; WHILE i \u003c= 900 DO INSERT INTO t_without_index(b) SELECT RAND()*10000; SET i = i + 1; END WHILE; COMMIT; END // DELIMITER ; #调用 CALL t_wout_insert() 创建表2： CREATE TABLE t_with_index( a INT PRIMARY KEY AUTO_INCREMENT, b INT, INDEX idx_b(b) ); 创建存储过程2： #创建存储过程 DELIMITER // CREATE PROCEDURE t_with_insert() BEGIN DECLARE i INT DEFAULT 1; WHILE i \u003c= 900 DO INSERT INTO t_with_index(b) SELECT RAND()*10000; SET i = i + 1; END WHILE; COMMIT; END // DELIMITER ; #调用 CALL t_with_insert(); 查询对比： mysql\u003e select * from t_without_index where b = 9879; +------+------+ | a | b | +------+------+ | 1242 | 9879 | +------+------+ 1 row in set (0.00 sec) mysql\u003e select * from t_with_index where b = 9879; +-----+------+ | a | b | +-----+------+ | 112 | 9879 | +-----+------+ 1 row in set (0.00 sec) 你能看到运行结果相同，但是在数据量不大的情况下，索引就发挥不出作用了。 结论：在数据表中的数据行数比较少的情况下，比如不到 1000 行，是不需要创建索引的。 3. 有大量重复数据的列上不要建立索引 在条件表达式中经常用到的不同值较多的列上建立索引，但字段中如果有大量重复数据，也不用创建索引。比如在学生表的\"性别\"字段上只有“男”与“女”两个不同值，因此无须建立索引。如果建立索引，不但不会提高查询效率，反而会严重降低数据更新速度。 举例1：要在 100 万行数据中查找其中的 50 万行（比如性别为男的数据），一旦创建了索引，你需要先 访问 50 万次索引，然后再访问 50 万次数据表，这样加起来的开销比不使用索引可能还要大。 举例2：假设有一个学生表，学生总数为 100 万人，男性只有 10 个人，也就是占总人口的 10 万分之 1。 学生表 student_gender 结构如下。其中数据表中的 student_gender 字段取值为 0 或 1，0 代表女性，1 代表男性。 CREATE TABLE student_gender( student_id INT(11) NOT NULL, student_name VARCHAR(50) NOT NULL, student_gender TINYINT(1) NOT NULL, PRIMARY KEY(student_id) )ENGINE = INNODB; 如果我们要筛选出这个学生表中的男性，可以使用： SELECT * FROM student_gender WHERE student_gender = 1; 结论：当数据重复度大，比如 高于 10% 的时候，也不需要对这个字段使用索引。 4. 避免对经常更新的表创建过多的索引 第一层含义：频繁更新的字段不一定要创建索引。因为更新数据的时候，也需要更新索引，如果索引太多，在更新索引的时候也会造成负担，从而影响效率。 第二层含义：避免对经常更新的表创建过多的索引，并且索引中的列尽可能少。此时，虽然提高了查询速度，同时却降低更新表的速度。 5. 不建议用无序的值作为索引 例如身份证、UUID(在索引比较时需要转为ASCII，并且插入时可能造成页分裂)、MD5、HASH、无序长字 符串等。 6. 删除不再使用或者很少使用的索引 表中的数据被大量更新，或者数据的使用方式被改变后，原有的一些索引可能不再需要。数据库管理员应当定期找出这些索引，将它们删除，从而减少索引对更新操作的影响。 7. 不要定义夯余或重复的索引 ① 冗余索引 举例：建表语句如下 CREATE TABLE person_info( id INT UNSIGNED NOT NULL AUTO_INCREMENT, name VARCHAR(100) NOT NULL, birthday DATE NOT NULL, phone_number CHAR(11) NOT NULL, country varchar(100) NOT NULL, PRIMARY KEY (id), KEY idx_name_birthday_phone_number (name(10), birthday, phone_number), KEY idx_name (name(10)) ); 我们知道，通过 idx_name_birthday_phone_number 索引就可以对 name 列进行快速搜索，再创建一 个专门针对 name 列的索引就算是一个 冗余索引 ，维护这个索引只会增加维护的成本，并不会对搜索有 什么好处。 ② 重复索引 另一种情况，我们可能会对某个列 重复建立索引 ，比方说这样： CREATE TABLE repeat_index_demo ( col1 INT PRIMARY KEY, col2 INT, UNIQUE uk_idx_c1 (col1), INDEX idx_c1 (col1) ); 我们看到，col1 既是主键、又给它定义为一个唯一索引，还给它定义了一个普通索引，可是主键本身会生成聚簇索引，所以定义的唯一索引和普通索引是重复的，这种情况要避免。 第09章_性能分析工具的使用 在数据库调优中，我们的目标是 响应时间更快, 吞吐量更大 。利用宏观的监控工具和微观的日志分析可以帮我们快速找到调优的思路和方式。 ","date":"2022-09-24","objectID":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/:15:4","tags":[],"title":"MySQL索引及调优篇","uri":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/"},{"categories":["mysql"],"content":"1. 数据库服务器的优化步骤 当我们遇到数据库调优问题的时候，该如何思考呢？这里把思考的流程整理成下面这张图。 整个流程划分成了 观察（Show status） 和 行动（Action） 两个部分。字母 S 的部分代表观察（会使 用相应的分析工具），字母 A 代表的部分是行动（对应分析可以采取的行动）。 我们可以通过观察了解数据库整体的运行状态，通过性能分析工具可以让我们了解执行慢的SQL都有哪些，查看具体的SQL执行计划，甚至是SQL执行中的每一步的成本代价，这样才能定位问题所在，找到了问题，再采取相应的行动。 详细解释一下这张图： ","date":"2022-09-24","objectID":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/:16:0","tags":[],"title":"MySQL索引及调优篇","uri":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/"},{"categories":["mysql"],"content":"2. 查看系统性能参数 在MySQL中，可以使用 SHOW STATUS 语句查询一些MySQL数据库服务器的性能参数、执行频率。 SHOW STATUS语句语法如下： SHOW [GLOBAL|SESSION] STATUS LIKE '参数'; 一些常用的性能参数如下： Connections：连接MySQL服务器的次数。 Uptime：MySQL服务器的上线时间。 Slow_queries：慢查询的次数。 Innodb_rows_read：Select查询返回的行数 Innodb_rows_inserted：执行INSERT操作插入的行数 Innodb_rows_updated：执行UPDATE操作更新的 行数 Innodb_rows_deleted：执行DELETE操作删除的行数 Com_select：查询操作的次数。 Com_insert：插入操作的次数。对于批量插入的 INSERT 操作，只累加一次。 Com_update：更新操作的次数。 Com_delete：删除操作的次数。 若查询MySQL服务器的连接次数，则可以执行如下语句: SHOW STATUS LIKE 'Connections'; 若查询服务器工作时间，则可以执行如下语句: SHOW STATUS LIKE 'Uptime'; 若查询MySQL服务器的慢查询次数，则可以执行如下语句: SHOW STATUS LIKE 'Slow_queries'; 慢查询次数参数可以结合慢查询日志找出慢查询语句，然后针对慢查询语句进行表结构优化或者查询语句优化。 再比如，如下的指令可以查看相关的指令情况： SHOW STATUS LIKE 'Innodb_rows_%'; ","date":"2022-09-24","objectID":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/:17:0","tags":[],"title":"MySQL索引及调优篇","uri":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/"},{"categories":["mysql"],"content":"3. 统计SQL的查询成本: last_query_cost 一条SQL查询语句在执行前需要查询执行计划，如果存在多种执行计划的话，MySQL会计算每个执行计划所需要的成本，从中选择成本最小的一个作为最终执行的执行计划。 如果我们想要查看某条SQL语句的查询成本，可以在执行完这条SQL语句之后，通过查看当前会话中的last_query_cost变量值来得到当前查询的成本。它通常也是我们评价一个查询的执行效率的一个常用指标。这个查询成本对应的是SQL 语句所需要读取的读页的数量。 我们依然使用第8章的 student_info 表为例： CREATE TABLE `student_info` ( `id` INT(11) NOT NULL AUTO_INCREMENT, `student_id` INT NOT NULL , `name` VARCHAR(20) DEFAULT NULL, `course_id` INT NOT NULL , `class_id` INT(11) DEFAULT NULL, `create_time` DATETIME DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP, PRIMARY KEY (`id`) ) ENGINE=INNODB AUTO_INCREMENT=1 DEFAULT CHARSET=utf8; 如果我们想要查询 id=900001 的记录，然后看下查询成本，我们可以直接在聚簇索引上进行查找： SELECT student_id, class_id, NAME, create_time FROM student_info WHERE id = 900001; 运行结果（1 条记录，运行时间为 0.042s ） 然后再看下查询优化器的成本，实际上我们只需要检索一个页即可： mysql\u003e SHOW STATUS LIKE 'last_query_cost'; +-----------------+----------+ | Variable_name | Value | +-----------------+----------+ | Last_query_cost | 1.000000 | +-----------------+----------+ 如果我们想要查询 id 在 900001 到 9000100 之间的学生记录呢？ SELECT student_id, class_id, NAME, create_time FROM student_info WHERE id BETWEEN 900001 AND 900100; 运行结果（100 条记录，运行时间为 0.046s ）： 然后再看下查询优化器的成本，这时我们大概需要进行 20 个页的查询。 mysql\u003e SHOW STATUS LIKE 'last_query_cost'; +-----------------+-----------+ | Variable_name | Value | +-----------------+-----------+ | Last_query_cost | 21.134453 | +-----------------+-----------+ 你能看到页的数量是刚才的 20 倍，但是查询的效率并没有明显的变化，实际上这两个 SQL 查询的时间基本上一样，就是因为采用了顺序读取的方式将页面一次性加载到缓冲池中，然后再进行查找。虽然页数量（last_query_cost）增加了不少，但是通过缓冲池的机制，并没有增加多少查询时间 。 **使用场景：**它对于比较开销是非常有用的，特别是我们有好几种查询方式可选的时候。 SQL查询时一个动态的过程，从页加载的角度来看，我们可以得到以下两点结论： 位置决定效率。如果页就在数据库 缓冲池 中，那么效率是最高的，否则还需要从 内存 或者 磁盘 中进行读取，当然针对单个页的读取来说，如果页存在于内存中，会比在磁盘中读取效率高很多。 批量决定效率。如果我们从磁盘中对单一页进行随机读，那么效率是很低的(差不多10ms)，而采用顺序读取的方式，批量对页进行读取，平均一页的读取效率就会提升很多，甚至要快于单个页面在内存中的随机读取。 所以说，遇到I/O并不用担心，方法找对了，效率还是很高的。我们首先要考虑数据存放的位置，如果是进程使用的数据就要尽量放到缓冲池中，其次我们可以充分利用磁盘的吞吐能力，一次性批量读取数据，这样单个页的读取效率也就得到了提升。 ","date":"2022-09-24","objectID":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/:18:0","tags":[],"title":"MySQL索引及调优篇","uri":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/"},{"categories":["mysql"],"content":"4. 定位执行慢的 SQL：慢查询日志 ","date":"2022-09-24","objectID":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/:19:0","tags":[],"title":"MySQL索引及调优篇","uri":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/"},{"categories":["mysql"],"content":"4.1 开启慢查询日志参数 1. 开启 slow_query_log 在使用前，我们需要先查下慢查询是否已经开启，使用下面这条命令即可： mysql \u003e show variables like '%slow_query_log'; 我们可以看到 slow_query_log=OFF，我们可以把慢查询日志打开，注意设置变量值的时候需要使用 global，否则会报错： mysql \u003e set global slow_query_log='ON'; 然后我们再来查看下慢查询日志是否开启，以及慢查询日志文件的位置： 你能看到这时慢查询分析已经开启，同时文件保存在 /var/lib/mysql/atguigu02-slow.log 文件 中。 2. 修改 long_query_time 阈值 接下来我们来看下慢查询的时间阈值设置，使用如下命令： mysql \u003e show variables like '%long_query_time%'; 这里如果我们想把时间缩短，比如设置为 1 秒，可以这样设置： #测试发现：设置global的方式对当前session的long_query_time失效。对新连接的客户端有效。所以可以一并 执行下述语句 mysql \u003e set global long_query_time = 1; mysql\u003e show global variables like '%long_query_time%'; mysql\u003e set long_query_time=1; mysql\u003e show variables like '%long_query_time%'; 补充：配置文件中一并设置参数 如下的方式相较于前面的命令行方式，可以看做是永久设置的方式。 修改 my.cnf 文件，[mysqld] 下增加或修改参数 long_query_time、slow_query_log 和 slow_query_log_file 后，然后重启 MySQL 服务器。 [mysqld] slow_query_log=ON # 开启慢查询日志开关 slow_query_log_file=/var/lib/mysql/atguigu-low.log # 慢查询日志的目录和文件名信息 long_query_time=3 # 设置慢查询的阈值为3秒，超出此设定值的SQL即被记录到慢查询日志 log_output=FILE 如果不指定存储路径，慢查询日志默认存储到MySQL数据库的数据文件夹下。如果不指定文件名，默认文件名为hostname_slow.log。 ","date":"2022-09-24","objectID":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/:19:1","tags":[],"title":"MySQL索引及调优篇","uri":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/"},{"categories":["mysql"],"content":"4.2 查看慢查询数目 查询当前系统中有多少条慢查询记录 SHOW GLOBAL STATUS LIKE '%Slow_queries%'; ","date":"2022-09-24","objectID":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/:19:2","tags":[],"title":"MySQL索引及调优篇","uri":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/"},{"categories":["mysql"],"content":"4.3 案例演示 步骤1. 建表 CREATE TABLE `student` ( `id` INT(11) NOT NULL AUTO_INCREMENT, `stuno` INT NOT NULL , `name` VARCHAR(20) DEFAULT NULL, `age` INT(3) DEFAULT NULL, `classId` INT(11) DEFAULT NULL, PRIMARY KEY (`id`) ) ENGINE=INNODB AUTO_INCREMENT=1 DEFAULT CHARSET=utf8; 步骤2：设置参数 log_bin_trust_function_creators 创建函数，假如报错： This function has none of DETERMINISTIC...... 命令开启：允许创建函数设置： set global log_bin_trust_function_creators=1; # 不加global只是当前窗口有效。 步骤3：创建函数 随机产生字符串：（同上一章） DELIMITER // CREATE FUNCTION rand_string(n INT) RETURNS VARCHAR(255) #该函数会返回一个字符串 BEGIN DECLARE chars_str VARCHAR(100) DEFAULT 'abcdefghijklmnopqrstuvwxyzABCDEFJHIJKLMNOPQRSTUVWXYZ'; DECLARE return_str VARCHAR(255) DEFAULT ''; DECLARE i INT DEFAULT 0; WHILE i \u003c n DO SET return_str =CONCAT(return_str,SUBSTRING(chars_str,FLOOR(1+RAND()*52),1)); SET i = i + 1; END WHILE; RETURN return_str; END // DELIMITER ; # 测试 SELECT rand_string(10); 产生随机数值：（同上一章） DELIMITER // CREATE FUNCTION rand_num (from_num INT ,to_num INT) RETURNS INT(11) BEGIN DECLARE i INT DEFAULT 0; SET i = FLOOR(from_num +RAND()*(to_num - from_num+1)) ; RETURN i; END // DELIMITER ; #测试： SELECT rand_num(10,100); 步骤4：创建存储过程 DELIMITER // CREATE PROCEDURE insert_stu1( START INT , max_num INT ) BEGIN DECLARE i INT DEFAULT 0; SET autocommit = 0; #设置手动提交事务 REPEAT #循环 SET i = i + 1; #赋值 INSERT INTO student (stuno, NAME ,age ,classId ) VALUES ((START+i),rand_string(6),rand_num(10,100),rand_num(10,1000)); UNTIL i = max_num END REPEAT; COMMIT; #提交事务 END // DELIMITER ; 步骤5：调用存储过程 #调用刚刚写好的函数, 4000000条记录,从100001号开始 CALL insert_stu1(100001,4000000); ","date":"2022-09-24","objectID":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/:19:3","tags":[],"title":"MySQL索引及调优篇","uri":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/"},{"categories":["mysql"],"content":"4.4 测试及分析 1. 测试 mysql\u003e SELECT * FROM student WHERE stuno = 3455655; +---------+---------+--------+------+---------+ | id | stuno | name | age | classId | +---------+---------+--------+------+---------+ | 3523633 | 3455655 | oQmLUr | 19 | 39 | +---------+---------+--------+------+---------+ 1 row in set (2.09 sec) mysql\u003e SELECT * FROM student WHERE name = 'oQmLUr'; +---------+---------+--------+------+---------+ | id | stuno | name | age | classId | +---------+---------+--------+------+---------+ | 1154002 | 1243200 | OQMlUR | 266 | 28 | | 1405708 | 1437740 | OQMlUR | 245 | 439 | | 1748070 | 1680092 | OQMlUR | 240 | 414 | | 2119892 | 2051914 | oQmLUr | 17 | 32 | | 2893154 | 2825176 | OQMlUR | 245 | 435 | | 3523633 | 3455655 | oQmLUr | 19 | 39 | +---------+---------+--------+------+---------+ 6 rows in set (2.39 sec) 从上面的结果可以看出来，查询学生编号为“3455655”的学生信息花费时间为2.09秒。查询学生姓名为 “oQmLUr”的学生信息花费时间为2.39秒。已经达到了秒的数量级，说明目前查询效率是比较低的，下面的小节我们分析一下原因。 2. 分析 show status like 'slow_queries'; ","date":"2022-09-24","objectID":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/:19:4","tags":[],"title":"MySQL索引及调优篇","uri":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/"},{"categories":["mysql"],"content":"4.5 慢查询日志分析工具：mysqldumpslow 在生产环境中，如果要手工分析日志，查找、分析SQL，显然是个体力活，MySQL提供了日志分析工具 mysqldumpslow 。 查看 mysqldumpslow 的帮助信息 mysqldumpslow --help mysqldumpslow 命令的具体参数如下： -a: 不将数字抽象成N，字符串抽象成S -s: 是表示按照何种方式排序： c: 访问次数 l: 锁定时间 r: 返回记录 t: 查询时间 al:平均锁定时间 ar:平均返回记录数 at:平均查询时间 （默认方式） ac:平均查询次数 -t: 即为返回前面多少条的数据； -g: 后边搭配一个正则匹配模式，大小写不敏感的； 举例：我们想要按照查询时间排序，查看前五条 SQL 语句，这样写即可： mysqldumpslow -s t -t 5 /var/lib/mysql/atguigu01-slow.log [root@bogon ~] mysqldumpslow -s t -t 5 /var/lib/mysql/atguigu01-slow.log Reading mysql slow query log from /var/lib/mysql/atguigu01-slow.log Count: 1 Time=2.39s (2s) Lock=0.00s (0s) Rows=13.0 (13), root[root]@localhost SELECT * FROM student WHERE name = 'S' Count: 1 Time=2.09s (2s) Lock=0.00s (0s) Rows=2.0 (2), root[root]@localhost SELECT * FROM student WHERE stuno = N Died at /usr/bin/mysqldumpslow line 162, \u003c\u003e chunk 2. 工作常用参考： #得到返回记录集最多的10个SQL mysqldumpslow -s r -t 10 /var/lib/mysql/atguigu-slow.log #得到访问次数最多的10个SQL mysqldumpslow -s c -t 10 /var/lib/mysql/atguigu-slow.log #得到按照时间排序的前10条里面含有左连接的查询语句 mysqldumpslow -s t -t 10 -g \"left join\" /var/lib/mysql/atguigu-slow.log #另外建议在使用这些命令时结合 | 和more 使用 ，否则有可能出现爆屏情况 mysqldumpslow -s r -t 10 /var/lib/mysql/atguigu-slow.log | more ","date":"2022-09-24","objectID":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/:19:5","tags":[],"title":"MySQL索引及调优篇","uri":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/"},{"categories":["mysql"],"content":"4.6 关闭慢查询日志 MySQL服务器停止慢查询日志功能有两种方法： 方式1：永久性方式 [mysqld] slow_query_log=OFF 或者，把slow_query_log一项注释掉 或 删除 [mysqld] #slow_query_log =OFF 重启MySQL服务，执行如下语句查询慢日志功能。 SHOW VARIABLES LIKE '%slow%'; #查询慢查询日志所在目录 SHOW VARIABLES LIKE '%long_query_time%'; #查询超时时长 方式2：临时性方式 使用SET语句来设置。 （1）停止MySQL慢查询日志功能，具体SQL语句如下。 SET GLOBAL slow_query_log=off; （2）重启MySQL服务，使用SHOW语句查询慢查询日志功能信息，具体SQL语句如下。 SHOW VARIABLES LIKE '%slow%'; #以及 SHOW VARIABLES LIKE '%long_query_time%'; ","date":"2022-09-24","objectID":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/:19:6","tags":[],"title":"MySQL索引及调优篇","uri":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/"},{"categories":["mysql"],"content":"4.7 删除慢查询日志 使用SHOW语句显示慢查询日志信息，具体SQL语句如下。 SHOW VARIABLES LIKE `slow_query_log%`; 从执行结果可以看出，慢查询日志的目录默认为MySQL的数据目录，在该目录下手动删除慢查询日志文件即可。 使用命令 mysqladmin flush-logs 来重新生成查询日志文件，具体命令如下，执行完毕会在数据目录下重新生成慢查询日志文件。 mysqladmin -uroot -p flush-logs slow 提示 慢查询日志都是使用mysqladmin flush-logs命令来删除重建的。使用时一定要注意，一旦执行了这个命令，慢查询日志都只存在新的日志文件中，如果需要旧的查询日志，就必须事先备份。 ","date":"2022-09-24","objectID":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/:19:7","tags":[],"title":"MySQL索引及调优篇","uri":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/"},{"categories":["mysql"],"content":"5. 查看 SQL 执行成本：SHOW PROFILE show profile 在《逻辑架构》章节中讲过，这里作为复习。 show profile 是 MySQL 提供的可以用来分析当前会话中 SQL 都做了什么、执行的资源消耗工具的情况，可用于 sql 调优的测量。默认情况下处于关闭状态，并保存最近15次的运行结果。 show profile 命令可以显示一条sql的每个过程里花费的时间以及各种资源开销，如果是 executing 时间过长，表示sql执行时间太长，需要使用explain来分析sql效率慢的原因；如果是其他问题，可能是网络延时，锁等情况 我们可以在会话级别开启这个功能。 mysql \u003e show variables like 'profiling'; 通过设置 profiling=‘ON’ 来开启 show profile: mysql \u003e set profiling = 'ON'; 然后执行相关的查询语句。接着看下当前会话都有哪些 profiles，使用下面这条命令： mysql \u003e show profiles; 你能看到当前会话一共有 2 个查询。如果我们想要查看最近一次查询的开销，可以使用： mysql \u003e show profile; mysql\u003e show profile cpu,block io for query 2 **show profile的常用查询参数： ** ① ALL：显示所有的开销信息。 ② BLOCK IO：显示块IO开销。 ③ CONTEXT SWITCHES：上下文切换开销。 ④ CPU：显示CPU开销信息。 ⑤ IPC：显示发送和接收开销信息。 ⑥ MEMORY：显示内存开销信息。 ⑦ PAGE FAULTS：显示页面错误开销信息。 ⑧ SOURCE：显示和Source_function，Source_file， Source_line相关的开销信息。 ⑨ SWAPS：显示交换次数开销信息。 日常开发需注意的结论： ① converting HEAP to MyISAM: 查询结果太大，内存不够，数据往磁盘上搬了。 ② Creating tmp table：创建临时表。先拷贝数据到临时表，用完后再删除临时表。 ③ Copying to tmp table on disk：把内存中临时表复制到磁盘上，警惕！ ④ locked。 如果在show profile诊断结果中出现了以上4条结果中的任何一条，则sql语句需要优化。 注意：不过SHOW PROFILE命令将被启用，我们可以从 information_schema 中的 profiling 数据表进行查看。 ","date":"2022-09-24","objectID":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/:20:0","tags":[],"title":"MySQL索引及调优篇","uri":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/"},{"categories":["mysql"],"content":"6. 分析查询语句：EXPLAIN 先了解在join连接时哪个表是驱动表，哪个表是被驱动表： 当使用join时，mysql会选择数据量比较小的表作为驱动表，大表作为被驱动表 驱动表的含义: MySQL 表关联的算法是 Nest Loop Join，是通过驱动表的结果集作为循环基础数据，然后一条一条地通过该结果集中的数据作为过滤条件到下一个表中查询数据，然后合并结果。如果还有第三个参与Join，则再通过前两个表的Join结果集作为循环基础数据，再一次通过循环查询条件到第三个表中查询数据，如此往复。 例如： 小表驱动大表：for(140条){for(20万条){}} 大表驱动小表：for(20万条){for(140条){}} 大表驱动小表，要通过20万次的连接 小表驱动小表，只需要通过140多次的连接就可以了. 所以也可以得出结论: 如果A表，B表数据量差不多大的时候，那么选择谁作为驱动表也是无所谓了; 以小表作为驱动表，大表作为被驱动表的方式查询速度更快 ","date":"2022-09-24","objectID":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/:21:0","tags":[],"title":"MySQL索引及调优篇","uri":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/"},{"categories":["mysql"],"content":"6.1 概述 1. 能做什么？ 表的读取顺序 数据读取操作的操作类型 哪些索引可以使用 哪些索引被实际使用 表之间的引用 每张表有多少行被优化器查询 2. 官网介绍 https://dev.mysql.com/doc/refman/5.7/en/explain-output.html https://dev.mysql.com/doc/refman/8.0/en/explain-output.html 3. 版本情况 MySQL 5.6.3以前只能 EXPLAIN SELECT ；MYSQL 5.6.3以后就可以 EXPLAIN SELECT，UPDATE， DELETE 在5.7以前的版本中，想要显示 partitions 需要使用 explain partitions 命令；想要显示 filtered 需要使用 explain extended 命令。在5.7版本后，默认explain直接显示partitions和 filtered中的信息。 ","date":"2022-09-24","objectID":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/:21:1","tags":[],"title":"MySQL索引及调优篇","uri":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/"},{"categories":["mysql"],"content":"6.2 基本语法 EXPLAIN 或 DESCRIBE语句的语法形式如下： EXPLAIN SELECT select_options 或者 DESCRIBE SELECT select_options 如果我们想看看某个查询的执行计划的话，可以在具体的查询语句前边加一个 EXPLAIN ，就像这样： mysql\u003e EXPLAIN SELECT 1; EXPLAIN 语句输出的各个列的作用如下： 在这里把它们都列出来知识为了描述一个轮廓，让大家有一个大致的印象。 ","date":"2022-09-24","objectID":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/:21:2","tags":[],"title":"MySQL索引及调优篇","uri":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/"},{"categories":["mysql"],"content":"6.3 数据准备 1. 建表 CREATE TABLE s1 ( id INT AUTO_INCREMENT, key1 VARCHAR(100), key2 INT, key3 VARCHAR(100), key_part1 VARCHAR(100), key_part2 VARCHAR(100), key_part3 VARCHAR(100), common_field VARCHAR(100), PRIMARY KEY (id), INDEX idx_key1 (key1), UNIQUE INDEX idx_key2 (key2), INDEX idx_key3 (key3), INDEX idx_key_part(key_part1, key_part2, key_part3) ) ENGINE=INNODB CHARSET=utf8; CREATE TABLE s2 ( id INT AUTO_INCREMENT, key1 VARCHAR(100), key2 INT, key3 VARCHAR(100), key_part1 VARCHAR(100), key_part2 VARCHAR(100), key_part3 VARCHAR(100), common_field VARCHAR(100), PRIMARY KEY (id), INDEX idx_key1 (key1), UNIQUE INDEX idx_key2 (key2), INDEX idx_key3 (key3), INDEX idx_key_part(key_part1, key_part2, key_part3) ) ENGINE=INNODB CHARSET=utf8; 2. 设置参数 log_bin_trust_function_creators 创建函数，假如报错，需开启如下命令：允许创建函数设置： set global log_bin_trust_function_creators=1; # 不加global只是当前窗口有效。 3. 创建函数 DELIMITER // CREATE FUNCTION rand_string1(n INT) RETURNS VARCHAR(255) #该函数会返回一个字符串 BEGIN DECLARE chars_str VARCHAR(100) DEFAULT 'abcdefghijklmnopqrstuvwxyzABCDEFJHIJKLMNOPQRSTUVWXYZ'; DECLARE return_str VARCHAR(255) DEFAULT ''; DECLARE i INT DEFAULT 0; WHILE i \u003c n DO SET return_str =CONCAT(return_str,SUBSTRING(chars_str,FLOOR(1+RAND()*52),1)); SET i = i + 1; END WHILE; RETURN return_str; END // DELIMITER ; 4. 创建存储过程 创建往s1表中插入数据的存储过程： DELIMITER // CREATE PROCEDURE insert_s1 (IN min_num INT (10),IN max_num INT (10)) BEGIN DECLARE i INT DEFAULT 0; SET autocommit = 0; REPEAT SET i = i + 1; INSERT INTO s1 VALUES( (min_num + i), rand_string1(6), (min_num + 30 * i + 5), rand_string1(6), rand_string1(10), rand_string1(5), rand_string1(10), rand_string1(10)); UNTIL i = max_num END REPEAT; COMMIT; END // DELIMITER ; 创建往s2表中插入数据的存储过程： DELIMITER // CREATE PROCEDURE insert_s2 (IN min_num INT (10),IN max_num INT (10)) BEGIN DECLARE i INT DEFAULT 0; SET autocommit = 0; REPEAT SET i = i + 1; INSERT INTO s2 VALUES( (min_num + i), rand_string1(6), (min_num + 30 * i + 5), rand_string1(6), rand_string1(10), rand_string1(5), rand_string1(10), rand_string1(10)); UNTIL i = max_num END REPEAT; COMMIT; END // DELIMITER ; 5. 调用存储过程 s1表数据的添加：加入1万条记录： CALL insert_s1(10001,10000); s2表数据的添加：加入1万条记录： CALL insert_s2(10001,10000); ","date":"2022-09-24","objectID":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/:21:3","tags":[],"title":"MySQL索引及调优篇","uri":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/"},{"categories":["mysql"],"content":"6.4 EXPLAIN各列作用 为了让大家有比较好的体验，我们调整了下 EXPLAIN 输出列的顺序。 1. table 不论我们的查询语句有多复杂，里边儿包含了多少个表 ，到最后也是需要对每个表进行单表访问的，所以MySQL规定EXPLAIN语句输出的每条记录都对应着某个单表的访问方法，该条记录的table列代表着该表的表名（有时不是真实的表名字，可能是简称）。 mysql \u003e EXPLAIN SELECT * FROM s1; 这个查询语句只涉及对s1表的单表查询，所以 EXPLAIN 输出中只有一条记录，其中的table列的值为s1，表明这条记录是用来说明对s1表的单表访问方法的。 边我们看一个连接查询的执行计划 mysql \u003e EXPLAIN SELECT * FROM s1 INNER JOIN s2; 可以看出这个连接查询的执行计划中有两条记录，这两条记录的table列分别是s1和s2，这两条记录用来分别说明对s1表和s2表的访问方法是什么。 2. id 我们写的查询语句一般都以 SELECT 关键字开头，比较简单的查询语句里只有一个 SELECT 关键字，比如下边这个查询语句： SELECT * FROM s1 WHERE key1 = 'a'; 稍微复杂一点的连接查询中也只有一个 SELECT 关键字，比如： SELECT * FROM s1 INNER JOIN s2 ON s1.key1 = s2.key1 WHERE s1.common_field = 'a'; 但是下边两种情况下在一条查询语句中会出现多个SELECT关键字： mysql \u003e EXPLAIN SELECT * FROM s1 WHERE key1 = 'a'; 对于连接查询来说，一个SELECT关键字后边的FROM字句中可以跟随多个表，所以在连接查询的执行计划中，每个表都会对应一条记录，但是这些记录的id值都是相同的，比如： mysql\u003e EXPLAIN SELECT * FROM s1 INNER JOIN s2; 可以看到，上述连接查询中参与连接的s1和s2表分别对应一条记录，但是这两条记录对应的id都是1。这里需要大家记住的是，在连接查询的执行计划中，每个表都会对应一条记录，这些记录的id列的值是相同的，出现在前边的表表示驱动表，出现在后面的表表示被驱动表。所以从上边的EXPLAIN输出中我们可以看到，查询优化器准备让s1表作为驱动表，让s2表作为被驱动表来执行查询。 对于包含子查询的查询语句来说，就可能涉及多个SELECT关键字，所以在包含子查询的查询语句的执行计划中，每个SELECT关键字都会对应一个唯一的id值，比如这样： mysql\u003e EXPLAIN SELECT * FROM s1 WHERE key1 IN (SELECT key1 FROM s2) OR key3 = 'a'; # 查询优化器可能对涉及子查询的查询语句进行重写，转变为多表查询的操作。 mysql\u003e EXPLAIN SELECT * FROM s1 WHERE key1 IN (SELECT key2 FROM s2 WHERE common_field = 'a'); 可以看到，虽然我们的查询语句是一个子查询，但是执行计划中s1和s2表对应的记录的id值全部是1，这就表明查询优化器将子查询转换为了连接查询。 对于包含UNION子句的查询语句来说，每个SELECT关键字对应一个id值也是没错的，不过还是有点儿特别的东西，比方说下边的查询： # Union去重 mysql\u003e EXPLAIN SELECT * FROM s1 UNION SELECT * FROM s2; mysql\u003e EXPLAIN SELECT * FROM s1 UNION ALL SELECT * FROM s2; 小结: id如果相同，可以认为是一组，从上往下顺序执行 在所有组中，id值越大，优先级越高，越先执行 关注点：id号每个号码，表示一趟独立的查询, 一个sql的查询趟数越少越好 3. select_type 具体分析如下： SIMPLE 查询语句中不包含UNION或者子查询的查询都算作是SIMPLE类型，比方说下边这个单表查询select_type的值就是SIMPLE: mysql\u003e EXPLAIN SELECT * FROM s1; 当然，连接查询也算是 SIMPLE 类型，比如： mysql\u003e EXPLAIN SELECT * FROM s1 INNER JOIN s2; PRIMARY 对于包含UNION、UNION ALL或者子查询的大查询来说，它是由几个小查询组成的，其中最左边的那个查询的select_type的值就是PRIMARY,比方说： mysql\u003e EXPLAIN SELECT * FROM s1 UNION SELECT * FROM s2; 从结果中可以看到，最左边的小查询SELECT * FROM s1对应的是执行计划中的第一条记录，它的select_type的值就是PRIMARY。 UNION 对于包含UNION或者UNION ALL的大查询来说，它是由几个小查询组成的，其中除了最左边的那个小查询意外，其余的小查询的select_type值就是UNION，可以对比上一个例子的效果。 UNION RESULT MySQL 选择使用临时表来完成UNION查询的去重工作，针对该临时表的查询的select_type就是UNION RESULT, 例子上边有。 SUBQUERY 如果包含子查询的查询语句不能够转为对应的semi-join的形式，并且该子查询是不相关子查询，并且查询优化器决定采用将该子查询物化的方案来执行该子查询时，该子查询的第一个SELECT关键字代表的那个查询的select_type就是SUBQUERY，比如下边这个查询： mysql\u003e EXPLAIN SELECT * FROM s1 WHERE key1 IN (SELECT key1 FROM s2) OR key3 = 'a'; DEPENDENT SUBQUERY mysql\u003e EXPLAIN SELECT * FROM s1 WHERE key1 IN (SELECT key1 FROM s2 WHERE s1.key2 = s2.key2) OR key3 = 'a'; DEPENDENT UNION mysql\u003e EXPLAIN SELECT * FROM s1 WHERE key1 IN (SELECT key1 FROM s2 WHERE key1 = 'a' UNION SELECT key1 FROM s1 WHERE key1 = 'b'); DERIVED mysql\u003e EXPLAIN SELECT * FROM (SELECT key1, count(*) as c FROM s1 GROUP BY key1) AS derived_s1 where c \u003e 1; 从执行计划中可以看出，id为2的记录就代表子查询的执行方式，它的select_type是DERIVED, 说明该子查询是以物化的方式执行的。id为1的记录代表外层查询，大家注意看它的table列显示的是derived2，表示该查询时针对将派生表物化之后的表进行查询的。 MATERIALIZED 当查询优化器在执行包含子查询的语句时，选择将子查询物化之后的外层查询进行连接查询时，该子查询对应的select_type属性就是DERIVED，比如下边这个查询： mysql\u003e EXPLAIN SELECT * FROM s1 WHERE key1 IN (SELECT key1 FROM s2); UNCACHEABLE SUBQUERY 不常用，就不多说了。 UNCACHEABLE UNION 不常用，就不多说了。 4. partitions (可略) 代表分区表中的命中情况，非分区表，该项为NULL。一般情况下我们的查询语句的执行计划的partitions列的值为NULL。 https://dev.mysql.com/doc/refman/5.7/en/alter-table-partition-operations.html 如果想详细了解，可以如下方式测试。创建分区表： -- 创建分区表， -- 按照id分区，id\u003c100 p0分区，其他p1分区 CREATE TABLE user_partitions (id INT auto_increment, NAME VARCHAR(12),PRIMARY KEY(id)) PARTITION BY RANGE(id)( PARTITION p0 VALUES less than(100), PARTITION p1 VALUES less than MAXVALUE ); DESC SELECT * FROM user_partitions WHERE id\u003e200; 查询id大于200（200\u003e100，p1分区）的记录，查看执行计划，partitions是p1，符合我们的分区规则 5. type ☆ 执行计划的一条记录就代表着MySQL对某个表的 执行查询时的访问方法 , 又称“访问类型”，其中的 type 列就表明了这个访问方法是啥，是较为重要的一个指标。比如，看到type列的值是ref，表明MySQL即将使用ref访问方法来执行对s1表的查询。 完整的访问方法如下： system ， const ， eq_ref ， ref ， fulltext ， ref_or_nu","date":"2022-09-24","objectID":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/:21:4","tags":[],"title":"MySQL索引及调优篇","uri":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/"},{"categories":["mysql"],"content":"7. EXPLAIN的进一步使用 ","date":"2022-09-24","objectID":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/:22:0","tags":[],"title":"MySQL索引及调优篇","uri":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/"},{"categories":["mysql"],"content":"7.1 EXPLAIN四种输出格式 这里谈谈EXPLAIN的输出格式。EXPLAIN可以输出四种格式： 传统格式 ，JSON格式 ， TREE格式 以及 可视化输出 。用户可以根据需要选择适用于自己的格式。 1. 传统格式 传统格式简单明了，输出是一个表格形式，概要说明查询计划。 mysql\u003e EXPLAIN SELECT s1.key1, s2.key1 FROM s1 LEFT JOIN s2 ON s1.key1 = s2.key1 WHERE s2.common_field IS NOT NULL; 2. JSON格式 第1种格式中介绍的EXPLAIN语句输出中缺少了一个衡量执行好坏的重要属性 —— 成本。而JSON格式是四种格式里面输出信息最详尽的格式，里面包含了执行的成本信息。 JSON格式：在EXPLAIN单词和真正的查询语句中间加上 FORMAT=JSON 。 EXPLAIN FORMAT=JSON SELECT .... EXPLAIN的Column与JSON的对应关系：(来源于MySQL 5.7文档) 这样我们就可以得到一个json格式的执行计划，里面包含该计划花费的成本。比如这样： mysql\u003e EXPLAIN FORMAT=JSON SELECT * FROM s1 INNER JOIN s2 ON s1.key1 = s2.key2 WHERE s1.common_field = 'a'\\G 我们使用 # 后边跟随注释的形式为大家解释了 EXPLAIN FORMAT=JSON 语句的输出内容，但是大家可能 有疑问 “cost_info” 里边的成本看着怪怪的，它们是怎么计算出来的？先看 s1 表的 “cost_info” 部 分： \"cost_info\": { \"read_cost\": \"1840.84\", \"eval_cost\": \"193.76\", \"prefix_cost\": \"2034.60\", \"data_read_per_join\": \"1M\" } read_cost 是由下边这两部分组成的： IO 成本 检测 rows × (1 - filter) 条记录的 CPU 成本 小贴士： rows和filter都是我们前边介绍执行计划的输出列，在JSON格式的执行计划中，rows 相当于rows_examined_per_scan，filtered名称不变。 eval_cost 是这样计算的： 检测 rows × filter 条记录的成本。 prefix_cost 就是单独查询 s1 表的成本，也就是： read_cost + eval_cost data_read_per_join 表示在此次查询中需要读取的数据量。 对于 s2 表的 “cost_info” 部分是这样的： \"cost_info\": { \"read_cost\": \"968.80\", \"eval_cost\": \"193.76\", \"prefix_cost\": \"3197.16\", \"data_read_per_join\": \"1M\" } 由于 s2 表是被驱动表，所以可能被读取多次，这里的read_cost 和 eval_cost 是访问多次 s2 表后累加起来的值，大家主要关注里边儿的 prefix_cost 的值代表的是整个连接查询预计的成本，也就是单次查询 s1 表和多次查询 s2 表后的成本的和，也就是： 968.80 + 193.76 + 2034.60 = 3197.16 3. TREE格式 TREE格式是8.0.16版本之后引入的新格式，主要根据查询的 各个部分之间的关系 和 各部分的执行顺序 来描述如何查询。 mysql\u003e EXPLAIN FORMAT=tree SELECT * FROM s1 INNER JOIN s2 ON s1.key1 = s2.key2 WHERE s1.common_field = 'a'\\G *************************** 1. row *************************** EXPLAIN: -\u003e Nested loop inner join (cost=1360.08 rows=990) -\u003e Filter: ((s1.common_field = 'a') and (s1.key1 is not null)) (cost=1013.75 rows=990) -\u003e Table scan on s1 (cost=1013.75 rows=9895) -\u003e Single-row index lookup on s2 using idx_key2 (key2=s1.key1), with index condition: (cast(s1.key1 as double) = cast(s2.key2 as double)) (cost=0.25 rows=1) 1 row in set, 1 warning (0.00 sec) 4. 可视化输出 可视化输出，可以通过MySQL Workbench可视化查看MySQL的执行计划。通过点击Workbench的放大镜图标，即可生成可视化的查询计划。 上图按从左到右的连接顺序显示表。红色框表示 全表扫描 ，而绿色框表示使用 索引查找 。对于每个表， 显示使用的索引。还要注意的是，每个表格的框上方是每个表访问所发现的行数的估计值以及访问该表的成本。 ","date":"2022-09-24","objectID":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/:22:1","tags":[],"title":"MySQL索引及调优篇","uri":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/"},{"categories":["mysql"],"content":"7.2 SHOW WARNINGS的使用 在我们使用EXPLAIN语句查看了某个查询的执行计划后，紧接着还可以使用SHOW WARNINGS语句查看与这个查询的执行计划有关的一些扩展信息，比如这样： mysql\u003e EXPLAIN SELECT s1.key1, s2.key1 FROM s1 LEFT JOIN s2 ON s1.key1 = s2.key1 WHERE s2.common_field IS NOT NULL; mysql\u003e SHOW WARNINGS\\G *************************** 1. row *************************** Level: Note Code: 1003 Message: /* select#1 */ select `atguigu`.`s1`.`key1` AS `key1`,`atguigu`.`s2`.`key1` AS `key1` from `atguigu`.`s1` join `atguigu`.`s2` where ((`atguigu`.`s1`.`key1` = `atguigu`.`s2`.`key1`) and (`atguigu`.`s2`.`common_field` is not null)) 1 row in set (0.00 sec) 大家可以看到SHOW WARNINGS展示出来的信息有三个字段，分别是Level、Code、Message。我们最常见的就是Code为1003的信息，当Code值为1003时，Message字段展示的信息类似于查询优化器将我们的查询语句重写后的语句。比如我们上边的查询本来是一个左(外)连接查询，但是有一个s2.common_field IS NOT NULL的条件，这就会导致查询优化器把左(外)连接查询优化为内连接查询，从SHOW WARNINGS的Message字段也可以看出来，原本的LEFE JOIN已经变成了JOIN。 但是大家一定要注意，我们说Message字段展示的信息类似于查询优化器将我们的查询语句重写后的语句，并不是等价于，也就是说Message字段展示的信息并不是标准的查询语句，在很多情况下并不能直接拿到黑框框中运行，它只能作为帮助我们理解MySQL将如何执行查询语句的一个参考依据而已。 ","date":"2022-09-24","objectID":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/:22:2","tags":[],"title":"MySQL索引及调优篇","uri":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/"},{"categories":["mysql"],"content":"8. 分析优化器执行计划：trace SET optimizer_trace=\"enabled=on\",end_markers_in_json=on; set optimizer_trace_max_mem_size=1000000; 开启后，可分析如下语句： SELECT INSERT REPLACE UPDATE DELETE EXPLAIN SET DECLARE CASE IF RETURN CALL 测试：执行如下SQL语句 select * from student where id \u003c 10; 最后， 查询 information_schema.optimizer_trace 就可以知道MySQL是如何执行SQL的 ： select * from information_schema.optimizer_trace\\G *************************** 1. row *************************** //第1部分：查询语句 QUERY: select * from student where id \u003c 10 //第2部分：QUERY字段对应语句的跟踪信息 TRACE: { \"steps\": [ { \"join_preparation\": { //预备工作 \"select#\": 1, \"steps\": [ { \"expanded_query\": \"/* select#1 */ select `student`.`id` AS `id`,`student`.`stuno` AS `stuno`,`student`.`name` AS `name`,`student`.`age` AS `age`,`student`.`classId` AS `classId` from `student` where (`student`.`id` \u003c 10)\" } ] /* steps */ } /* join_preparation */ }, { \"join_optimization\": { //进行优化 \"select#\": 1, \"steps\": [ { \"condition_processing\": { //条件处理 \"condition\": \"WHERE\", \"original_condition\": \"(`student`.`id` \u003c 10)\", \"steps\": [ { \"transformation\": \"equality_propagation\", \"resulting_condition\": \"(`student`.`id` \u003c 10)\" }, { \"transformation\": \"constant_propagation\", \"resulting_condition\": \"(`student`.`id` \u003c 10)\" }, { \"transformation\": \"trivial_condition_removal\", \"resulting_condition\": \"(`student`.`id` \u003c 10)\" } ] /* steps */ } /* condition_processing */ }, { \"substitute_generated_columns\": { //替换生成的列 } /* substitute_generated_columns */ }, { \"table_dependencies\": [ //表的依赖关系 { \"table\": \"`student`\", \"row_may_be_null\": false, \"map_bit\": 0, \"depends_on_map_bits\": [ ] /* depends_on_map_bits */ } ] /* table_dependencies */ }, { \"ref_optimizer_key_uses\": [ //使用键 ] /* ref_optimizer_key_uses */ }, { \"rows_estimation\": [ //行判断 { \"table\": \"`student`\", \"range_analysis\": { \"table_scan\": { \"rows\": 3973767, \"cost\": 408558 } /* table_scan */, //扫描表 \"potential_range_indexes\": [ //潜在的范围索引 { \"index\": \"PRIMARY\", \"usable\": true, \"key_parts\": [ \"id\" ] /* key_parts */ } ] /* potential_range_indexes */, \"setup_range_conditions\": [ //设置范围条件 ] /* setup_range_conditions */, \"group_index_range\": { \"chosen\": false, \"cause\": \"not_group_by_or_distinct\" } /* group_index_range */, \"skip_scan_range\": { \"potential_skip_scan_indexes\": [ { \"index\": \"PRIMARY\", \"usable\": false, \"cause\": \"query_references_nonkey_column\" } ] /* potential_skip_scan_indexes */ } /* skip_scan_range */, \"analyzing_range_alternatives\": { //分析范围选项 \"range_scan_alternatives\": [ { \"index\": \"PRIMARY\", \"ranges\": [ \"id \u003c 10\" ] /* ranges */, \"index_dives_for_eq_ranges\": true, \"rowid_ordered\": true, \"using_mrr\": false, \"index_only\": false, \"rows\": 9, \"cost\": 1.91986, \"chosen\": true } ] /* range_scan_alternatives */, \"analyzing_roworder_intersect\": { \"usable\": false, \"cause\": \"too_few_roworder_scans\" } /* analyzing_roworder_intersect */ } /* analyzing_range_alternatives */, \"chosen_range_access_summary\": { //选择范围访问摘要 \"range_access_plan\": { \"type\": \"range_scan\", \"index\": \"PRIMARY\", \"rows\": 9, \"ranges\": [ \"id \u003c 10\" ] /* ranges */ } /* range_access_plan */, \"rows_for_plan\": 9, \"cost_for_plan\": 1.91986, \"chosen\": true } /* chosen_range_access_summary */ } /* range_analysis */ } ] /* rows_estimation */ }, { \"considered_execution_plans\": [ //考虑执行计划 { \"plan_prefix\": [ ] /* plan_prefix */, \"table\": \"`student`\", \"best_access_path\": { //最佳访问路径 \"considered_access_paths\": [ { \"rows_to_scan\": 9, \"access_type\": \"range\", \"range_details\": { \"used_index\": \"PRIMARY\" } /* range_details */, \"resulting_rows\": 9, \"cost\": 2.81986, \"chosen\": true } ] /* considered_access_paths */ } /* best_access_path */, \"condition_filtering_pct\": 100, //行过滤百分比 \"rows_for_plan\": 9, \"cost_for_plan\": 2.81986, \"chosen\": true } ] /* considered_execution_plans */ }, { \"attaching_conditions_to_tables\": { //将条件附加到表上 \"original_condition\": \"(`student`.`id` \u003c 10)\", \"attached_conditions_computation\": [ ] /* attached_conditions_computation */, \"attached_conditions_summary\": [ //附加条件概要 { \"table\": \"`student`\", \"attached\": \"(`student`.`id` \u003c 10)\" } ] /* attached_conditi","date":"2022-09-24","objectID":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/:23:0","tags":[],"title":"MySQL索引及调优篇","uri":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/"},{"categories":["mysql"],"content":"9. MySQL监控分析视图-sys schema ","date":"2022-09-24","objectID":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/:24:0","tags":[],"title":"MySQL索引及调优篇","uri":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/"},{"categories":["mysql"],"content":"9.1 Sys schema视图摘要 主机相关：以host_summary开头，主要汇总了IO延迟的信息。 Innodb相关：以innodb开头，汇总了innodb buffer信息和事务等待innodb锁的信息。 I/o相关：以io开头，汇总了等待I/O、I/O使用量情况。 内存使用情况：以memory开头，从主机、线程、事件等角度展示内存的使用情况 连接与会话信息：processlist和session相关视图，总结了会话相关信息。 表相关：以schema_table开头的视图，展示了表的统计信息。 索引信息：统计了索引的使用情况，包含冗余索引和未使用的索引情况。 语句相关：以statement开头，包含执行全表扫描、使用临时表、排序等的语句信息。 用户相关：以user开头的视图，统计了用户使用的文件I/O、执行语句统计信息。 等待事件相关信息：以wait开头，展示等待事件的延迟情况。 ","date":"2022-09-24","objectID":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/:24:1","tags":[],"title":"MySQL索引及调优篇","uri":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/"},{"categories":["mysql"],"content":"9.2 Sys schema视图使用场景 索引情况 #1. 查询冗余索引 select * from sys.schema_redundant_indexes; #2. 查询未使用过的索引 select * from sys.schema_unused_indexes; #3. 查询索引的使用情况 select index_name,rows_selected,rows_inserted,rows_updated,rows_deleted from sys.schema_index_statistics where table_schema='dbname'; 表相关 # 1. 查询表的访问量 select table_schema,table_name,sum(io_read_requests+io_write_requests) as io from sys.schema_table_statistics group by table_schema,table_name order by io desc; # 2. 查询占用bufferpool较多的表 select object_schema,object_name,allocated,data from sys.innodb_buffer_stats_by_table order by allocated limit 10; # 3. 查看表的全表扫描情况 select * from sys.statements_with_full_table_scans where db='dbname'; 语句相关 #1. 监控SQL执行的频率 select db,exec_count,query from sys.statement_analysis order by exec_count desc; #2. 监控使用了排序的SQL select db,exec_count,first_seen,last_seen,query from sys.statements_with_sorting limit 1; #3. 监控使用了临时表或者磁盘临时表的SQL select db,exec_count,tmp_tables,tmp_disk_tables,query from sys.statement_analysis where tmp_tables\u003e0 or tmp_disk_tables \u003e0 order by (tmp_tables+tmp_disk_tables) desc; IO相关 #1. 查看消耗磁盘IO的文件 select file,avg_read,avg_write,avg_read+avg_write as avg_io from sys.io_global_by_file_by_bytes order by avg_read limit 10; Innodb 相关 #1. 行锁阻塞情况 select * from sys.innodb_lock_waits; ","date":"2022-09-24","objectID":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/:24:2","tags":[],"title":"MySQL索引及调优篇","uri":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/"},{"categories":["mysql"],"content":"10. 小结 查询是数据库中最频繁的操作，提高查询速度可以有效地提高MySQL数据库的性能。通过对查询语句的分析可以了解查询语句的执行情况，找出查询语句执行的瓶颈，从而优化查询语句。 第10章_索引优化与查询优化 都有哪些维度可以进行数据库调优？简言之： 索引失效、没有充分利用到索引——建立索引 关联查询太多JOIN（设计缺陷或不得已的需求）——SQL优化 服务器调优及各个参数设置（缓冲、线程数等）——调整my.cnf 数据过多——分库分表 关于数据库调优的知识非常分散。不同的DBMS，不同的公司，不同的职位，不同的项目遇到的问题都不尽相同。这里我们分为三个章节进行细致讲解。 虽然SQL查询优化的技术有很多，但是大方向上完全可以分成物理查询优化和逻辑查询优化两大块。 物理查询优化是通过索引和表连接方式等技术来进行优化，这里重点需要掌握索引的使用。 逻辑查询优化就是通过SQL等价变换提升查询效率，直白一点就是说，换一种查询写法效率可能更高。 ","date":"2022-09-24","objectID":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/:25:0","tags":[],"title":"MySQL索引及调优篇","uri":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/"},{"categories":["mysql"],"content":"1. 数据准备 学员表 插 50万 条， 班级表 插 1万 条。 CREATE DATABASE atguigudb2; USE atguigudb2; 步骤1：建表 CREATE TABLE `class` ( `id` INT(11) NOT NULL AUTO_INCREMENT, `className` VARCHAR(30) DEFAULT NULL, `address` VARCHAR(40) DEFAULT NULL, `monitor` INT NULL , PRIMARY KEY (`id`) ) ENGINE=INNODB AUTO_INCREMENT=1 DEFAULT CHARSET=utf8; CREATE TABLE `student` ( `id` INT(11) NOT NULL AUTO_INCREMENT, `stuno` INT NOT NULL , `name` VARCHAR(20) DEFAULT NULL, `age` INT(3) DEFAULT NULL, `classId` INT(11) DEFAULT NULL, PRIMARY KEY (`id`) #CONSTRAINT `fk_class_id` FOREIGN KEY (`classId`) REFERENCES `t_class` (`id`) ) ENGINE=INNODB AUTO_INCREMENT=1 DEFAULT CHARSET=utf8; 步骤2：设置参数 命令开启：允许创建函数设置： set global log_bin_trust_function_creators=1; # 不加global只是当前窗口有效。 步骤3：创建函数 保证每条数据都不同。 #随机产生字符串 DELIMITER // CREATE FUNCTION rand_string(n INT) RETURNS VARCHAR(255) BEGIN DECLARE chars_str VARCHAR(100) DEFAULT 'abcdefghijklmnopqrstuvwxyzABCDEFJHIJKLMNOPQRSTUVWXYZ'; DECLARE return_str VARCHAR(255) DEFAULT ''; DECLARE i INT DEFAULT 0; WHILE i \u003c n DO SET return_str =CONCAT(return_str,SUBSTRING(chars_str,FLOOR(1+RAND()*52),1)); SET i = i + 1; END WHILE; RETURN return_str; END // DELIMITER ; #假如要删除 #drop function rand_string; 随机产生班级编号 #用于随机产生多少到多少的编号 DELIMITER // CREATE FUNCTION rand_num (from_num INT ,to_num INT) RETURNS INT(11) BEGIN DECLARE i INT DEFAULT 0; SET i = FLOOR(from_num +RAND()*(to_num - from_num+1)) ; RETURN i; END // DELIMITER ; #假如要删除 #drop function rand_num; 步骤4：创建存储过程 #创建往stu表中插入数据的存储过程 DELIMITER // CREATE PROCEDURE insert_stu( START INT , max_num INT ) BEGIN DECLARE i INT DEFAULT 0; SET autocommit = 0; #设置手动提交事务 REPEAT #循环 SET i = i + 1; #赋值 INSERT INTO student (stuno, name ,age ,classId ) VALUES ((START+i),rand_string(6),rand_num(1,50),rand_num(1,1000)); UNTIL i = max_num END REPEAT; COMMIT; #提交事务 END // DELIMITER ; #假如要删除 #drop PROCEDURE insert_stu; 创建往class表中插入数据的存储过程 #执行存储过程，往class表添加随机数据 DELIMITER // CREATE PROCEDURE `insert_class`( max_num INT ) BEGIN DECLARE i INT DEFAULT 0; SET autocommit = 0; REPEAT SET i = i + 1; INSERT INTO class ( classname,address,monitor ) VALUES (rand_string(8),rand_string(10),rand_num(1,100000)); UNTIL i = max_num END REPEAT; COMMIT; END // DELIMITER ; #假如要删除 #drop PROCEDURE insert_class; 步骤5：调用存储过程 class #执行存储过程，往class表添加1万条数据 CALL insert_class(10000); stu #执行存储过程，往stu表添加50万条数据 CALL insert_stu(100000,500000); 步骤6：删除某表上的索引 创建存储过程 DELIMITER // CREATE PROCEDURE `proc_drop_index`(dbname VARCHAR(200),tablename VARCHAR(200)) BEGIN DECLARE done INT DEFAULT 0; DECLARE ct INT DEFAULT 0; DECLARE _index VARCHAR(200) DEFAULT ''; DECLARE _cur CURSOR FOR SELECT index_name FROM information_schema.STATISTICS WHERE table_schema=dbname AND table_name=tablename AND seq_in_index=1 AND index_name \u003c\u003e'PRIMARY' ; #每个游标必须使用不同的declare continue handler for not found set done=1来控制游标的结束 DECLARE CONTINUE HANDLER FOR NOT FOUND set done=2 ; #若没有数据返回,程序继续,并将变量done设为2 OPEN _cur; FETCH _cur INTO _index; WHILE _index\u003c\u003e'' DO SET @str = CONCAT(\"drop index \" , _index , \" on \" , tablename ); PREPARE sql_str FROM @str ; EXECUTE sql_str; DEALLOCATE PREPARE sql_str; SET _index=''; FETCH _cur INTO _index; END WHILE; CLOSE _cur; END // DELIMITER ; 执行存储过程 CALL proc_drop_index(\"dbname\",\"tablename\"); ","date":"2022-09-24","objectID":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/:26:0","tags":[],"title":"MySQL索引及调优篇","uri":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/"},{"categories":["mysql"],"content":"2. 索引失效案例 ","date":"2022-09-24","objectID":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/:27:0","tags":[],"title":"MySQL索引及调优篇","uri":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/"},{"categories":["mysql"],"content":"2.1 全值匹配我最爱 系统中经常出现的sql语句如下： EXPLAIN SELECT SQL_NO_CACHE * FROM student WHERE age=30; EXPLAIN SELECT SQL_NO_CACHE * FROM student WHERE age=30 AND classId=4; EXPLAIN SELECT SQL_NO_CACHE * FROM student WHERE age=30 AND classId=4 AND name = 'abcd'; 建立索引前执行：（关注执行时间） mysql\u003e SELECT SQL_NO_CACHE * FROM student WHERE age=30 AND classId=4 AND name = 'abcd'; Empty set, 1 warning (0.28 sec) 建立索引 CREATE INDEX idx_age ON student(age); CREATE INDEX idx_age_classid ON student(age,classId); CREATE INDEX idx_age_classid_name ON student(age,classId,name); 建立索引后执行： mysql\u003e SELECT SQL_NO_CACHE * FROM student WHERE age=30 AND classId=4 AND name = 'abcd'; Empty set, 1 warning (0.01 sec) ","date":"2022-09-24","objectID":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/:27:1","tags":[],"title":"MySQL索引及调优篇","uri":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/"},{"categories":["mysql"],"content":"2.2 最佳左前缀法则 在MySQL建立联合索引时会遵守最佳左前缀原则，即最左优先，在检索数据时从联合索引的最左边开始匹配。 举例1： EXPLAIN SELECT SQL_NO_CACHE * FROM student WHERE student.age=30 AND student.name = 'abcd'; 举例2： EXPLAIN SELECT SQL_NO_CACHE * FROM student WHERE student.classId=1 AND student.name = 'abcd'; 举例3：索引idx_age_classid_name还能否正常使用？ EXPLAIN SELECT SQL_NO_CACHE * FROM student WHERE student.classId=4 AND student.age=30 AND student.name = 'abcd'; 如果索引了多列，要遵守最左前缀法则。指的是查询从索引的最左前列开始并且不跳过索引中的列。 mysql\u003e EXPLAIN SELECT SQL_NO_CACHE * FROM student WHERE student.age=30 AND student.name = 'abcd'; 虽然可以正常使用，但是只有部分被使用到了。 mysql\u003e EXPLAIN SELECT SQL_NO_CACHE * FROM student WHERE student.classId=1 AND student.name = 'abcd'; 完全没有使用上索引。 结论：MySQL可以为多个字段创建索引，一个索引可以包含16个字段。对于多列索引，过滤条件要使用索引必须按照索引建立时的顺序，依次满足，一旦跳过某个字段，索引后面的字段都无法被使用。如果查询条件中没有用这些字段中第一个字段时，多列（或联合）索引不会被使用。 原因：假设我们现在给student表建立一个多列索引 idx_age_classid_name(CREATE INDEX idx_age_classid_name ON student(age,classId,name);)。mysql 就会先根据 age 字段排序，age 相同的时候呢，再根据 classid 排序，classid 又相同的时候就根据 name 字段排序。如此就很清晰：如果不满足最左前缀匹配原则，也就是说，要在局部有序，全局无序的索引列（如classid、name字段）进行全局的查找，显然没有使用到索引，索引失效 注：只有在查询范围内，有序，才能使用n分叉快速定位 拓展：Alibaba《Java开发手册》 索引文件具有 B-Tree 的最左前缀匹配特性，如果左边的值未确定，那么无法使用此索引。 ","date":"2022-09-24","objectID":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/:27:2","tags":[],"title":"MySQL索引及调优篇","uri":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/"},{"categories":["mysql"],"content":"2.3 主键插入顺序 如果此时再插入一条主键值为 9 的记录，那它插入的位置就如下图： 可这个数据页已经满了，再插进来咋办呢？我们需要把当前 页面分裂 成两个页面，把本页中的一些记录移动到新创建的这个页中。页面分裂和记录移位意味着什么？意味着：性能损耗！所以如果我们想尽量避免这样无谓的性能损耗，最好让插入的记录的 主键值依次递增 ，这样就不会发生这样的性能损耗了。所以我们建议：让主键具有AUTO_INCREMENT，让存储引擎自己为表生成主键，而不是我们手动插入，比如：person_info表： CREATE TABLE person_info( id INT UNSIGNED NOT NULL AUTO_INCREMENT, name VARCHAR(100) NOT NULL, birthday DATE NOT NULL, phone_number CHAR(11) NOT NULL, country varchar(100) NOT NULL, PRIMARY KEY (id), KEY idx_name_birthday_phone_number (name(10), birthday, phone_number) ); 我们自定义的主键列 id 拥有 AUTO_INCREMENT 属性，在插入记录时存储引擎会自动为我们填入自增的主键值。这样的主键占用空间小，顺序写入，减少页分裂。 ","date":"2022-09-24","objectID":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/:27:3","tags":[],"title":"MySQL索引及调优篇","uri":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/"},{"categories":["mysql"],"content":"2.4 计算、函数、类型转换(自动或手动)导致索引失效 这两条sql哪种写法更好 EXPLAIN SELECT SQL_NO_CACHE * FROM student WHERE student.name LIKE 'abc%'; EXPLAIN SELECT SQL_NO_CACHE * FROM student WHERE LEFT(student.name,3) = 'abc'; 创建索引 CREATE INDEX idx_name ON student(NAME); 第一种：索引优化生效 mysql\u003e EXPLAIN SELECT SQL_NO_CACHE * FROM student WHERE student.name LIKE 'abc%'; mysql\u003e SELECT SQL_NO_CACHE * FROM student WHERE student.name LIKE 'abc%'; +---------+---------+--------+------+---------+ | id | stuno | name | age | classId | +---------+---------+--------+------+---------+ | 5301379 | 1233401 | AbCHEa | 164 | 259 | | 7170042 | 3102064 | ABcHeB | 199 | 161 | | 1901614 | 1833636 | ABcHeC | 226 | 275 | | 5195021 | 1127043 | abchEC | 486 | 72 | | 4047089 | 3810031 | AbCHFd | 268 | 210 | | 4917074 | 849096 | ABcHfD | 264 | 442 | | 1540859 | 141979 | abchFF | 119 | 140 | | 5121801 | 1053823 | AbCHFg | 412 | 327 | | 2441254 | 2373276 | abchFJ | 170 | 362 | | 7039146 | 2971168 | ABcHgI | 502 | 465 | | 1636826 | 1580286 | ABcHgK | 71 | 262 | | 374344 | 474345 | abchHL | 367 | 212 | | 1596534 | 169191 | AbCHHl | 102 | 146 | ... | 5266837 | 1198859 | abclXe | 292 | 298 | | 8126968 | 4058990 | aBClxE | 316 | 150 | | 4298305 | 399962 | AbCLXF | 72 | 423 | | 5813628 | 1745650 | aBClxF | 356 | 323 | | 6980448 | 2912470 | AbCLXF | 107 | 78 | | 7881979 | 3814001 | AbCLXF | 89 | 497 | | 4955576 | 887598 | ABcLxg | 121 | 385 | | 3653460 | 3585482 | AbCLXJ | 130 | 174 | | 1231990 | 1283439 | AbCLYH | 189 | 429 | | 6110615 | 2042637 | ABcLyh | 157 | 40 | +---------+---------+--------+------+---------+ 401 rows in set, 1 warning (0.01 sec) 第二种：索引优化失效 mysql\u003e EXPLAIN SELECT SQL_NO_CACHE * FROM student WHERE LEFT(student.name,3) = 'abc'; mysql\u003e SELECT SQL_NO_CACHE * FROM student WHERE LEFT(student.name,3) = 'abc'; +---------+---------+--------+------+---------+ | id | stuno | name | age | classId | +---------+---------+--------+------+---------+ | 5301379 | 1233401 | AbCHEa | 164 | 259 | | 7170042 | 3102064 | ABcHeB | 199 | 161 | | 1901614 | 1833636 | ABcHeC | 226 | 275 | | 5195021 | 1127043 | abchEC | 486 | 72 | | 4047089 | 3810031 | AbCHFd | 268 | 210 | | 4917074 | 849096 | ABcHfD | 264 | 442 | | 1540859 | 141979 | abchFF | 119 | 140 | | 5121801 | 1053823 | AbCHFg | 412 | 327 | | 2441254 | 2373276 | abchFJ | 170 | 362 | | 7039146 | 2971168 | ABcHgI | 502 | 465 | | 1636826 | 1580286 | ABcHgK | 71 | 262 | | 374344 | 474345 | abchHL | 367 | 212 | | 1596534 | 169191 | AbCHHl | 102 | 146 | ... | 5266837 | 1198859 | abclXe | 292 | 298 | | 8126968 | 4058990 | aBClxE | 316 | 150 | | 4298305 | 399962 | AbCLXF | 72 | 423 | | 5813628 | 1745650 | aBClxF | 356 | 323 | | 6980448 | 2912470 | AbCLXF | 107 | 78 | | 7881979 | 3814001 | AbCLXF | 89 | 497 | | 4955576 | 887598 | ABcLxg | 121 | 385 | | 3653460 | 3585482 | AbCLXJ | 130 | 174 | | 1231990 | 1283439 | AbCLYH | 189 | 429 | | 6110615 | 2042637 | ABcLyh | 157 | 40 | +---------+---------+--------+------+---------+ 401 rows in set, 1 warning (3.62 sec) type为“ALL”，表示没有使用到索引，查询时间为 3.62 秒，查询效率较之前低很多。 再举例： student表的字段stuno上设置有索引 CREATE INDEX idx_sno ON student(stuno); 索引优化失效：（假设：student表的字段stuno上设置有索引） EXPLAIN SELECT SQL_NO_CACHE id, stuno, NAME FROM student WHERE stuno+1 = 900001; 运行结果： 索引优化生效： EXPLAIN SELECT SQL_NO_CACHE id, stuno, NAME FROM student WHERE stuno = 900000; 再举例： student表的字段name上设置有索引 CREATE INDEX idx_name ON student(NAME); EXPLAIN SELECT id, stuno, name FROM student WHERE SUBSTRING(name, 1,3)='abc'; 索引优化生效 EXPLAIN SELECT id, stuno, NAME FROM student WHERE NAME LIKE 'abc%'; ","date":"2022-09-24","objectID":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/:27:4","tags":[],"title":"MySQL索引及调优篇","uri":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/"},{"categories":["mysql"],"content":"2.5 类型转换导致索引失效 下列哪个sql语句可以用到索引。（假设name字段上设置有索引） # 未使用到索引 EXPLAIN SELECT SQL_NO_CACHE * FROM student WHERE name=123; # 使用到索引 EXPLAIN SELECT SQL_NO_CACHE * FROM student WHERE name='123'; name=123发生类型转换，索引失效。 ","date":"2022-09-24","objectID":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/:27:5","tags":[],"title":"MySQL索引及调优篇","uri":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/"},{"categories":["mysql"],"content":"2.6 范围条件右边的列索引失效 系统经常出现的sql如下： ALTER TABLE student DROP INDEX idx_name; ALTER TABLE student DROP INDEX idx_age; ALTER TABLE student DROP INDEX idx_age_classid; # 只剩下 idx_age_classid_name 索引 EXPLAIN SELECT SQL_NO_CACHE * FROM student WHERE student.age=30 AND student.name = 'abc' AND student.classId\u003e20; 那么索引 idx_age_classId_name 这个索引还能正常使用么？ 注：where字段条件的顺序，mysql优化器会根据索引调整的 不能，范围右边的列不能使用。比如：(\u003c) (\u003c=) (\u003e) (\u003e=) 和 between 等。因为，范围查询时，会有多条记录，每条记录对应的右索引列，局部上是有序的，但是多条记录整体上看又变成无序的了，所以范围条件右边的列索引也会失效 如果这种sql出现较多，应该建立： create index idx_age_name_classId on student(age,name,classId); 将范围查询条件放置语句最后： EXPLAIN SELECT SQL_NO_CACHE * FROM student WHERE student.age=30 AND student.classId\u003e20 AND student.name = 'abc'; 应用开发中范围查询，例如：金额查询，日期查询往往都是范围查询。应将查询条件放置where语句最后。（创建的联合索引中，务必把范围涉及到的字段写在最后） 效果 ","date":"2022-09-24","objectID":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/:27:6","tags":[],"title":"MySQL索引及调优篇","uri":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/"},{"categories":["mysql"],"content":"2.7 不等于(!= 或者\u003c\u003e)索引失效 为什么呢？其实，不等于 操作运算符也是范围查询的一种，他会挨个儿比较不同，只有 等于 才能使用到n分叉定位查找 为name字段创建索引 CREATE INDEX idx_name ON student(NAME); 查看索引是否失效 EXPLAIN SELECT SQL_NO_CACHE * FROM student WHERE student.name \u003c\u003e 'abc'; 或者 EXPLAIN SELECT SQL_NO_CACHE * FROM student WHERE student.name != 'abc'; 场景举例：用户提出需求，将财务数据，产品利润金额不等于0的都统计出来。 ","date":"2022-09-24","objectID":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/:27:7","tags":[],"title":"MySQL索引及调优篇","uri":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/"},{"categories":["mysql"],"content":"2.8 is null可以使用索引，is not null无法使用索引 IS NULL: 可以触发索引 EXPLAIN SELECT SQL_NO_CACHE * FROM student WHERE age IS NULL; IS NOT NULL: 无法触发索引 EXPLAIN SELECT SQL_NO_CACHE * FROM student WHERE age IS NOT NULL; 结论：最好在设计数据库的时候就将字段设置为 NOT NULL 约束，比如你可以将 INT 类型的字段，默认值设置为0。将字符类型的默认值设置为空字符串(’’)。 扩展：同理，在查询中使用not like也无法使用索引，导致全表扫描。 ","date":"2022-09-24","objectID":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/:27:8","tags":[],"title":"MySQL索引及调优篇","uri":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/"},{"categories":["mysql"],"content":"2.9 like以通配符%开头索引失效 在使用LIKE关键字进行查询的查询语句中，如果匹配字符串的第一个字符为’%’，索引就不会起作用。只有’%‘不在第一个位置，索引才会起作用。 为什么呢？因为，如果第一个字符是%，说明前缀是无法确定的，可以匹配任意值，那么就无法定位索引了 使用到索引 EXPLAIN SELECT SQL_NO_CACHE * FROM student WHERE name LIKE 'ab%'; 未使用到索引 EXPLAIN SELECT SQL_NO_CACHE * FROM student WHERE name LIKE '%ab%'; 拓展：Alibaba《Java开发手册》 【强制】页面搜索严禁左模糊或者全模糊，如果需要请走搜索引擎来解决。 ","date":"2022-09-24","objectID":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/:27:9","tags":[],"title":"MySQL索引及调优篇","uri":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/"},{"categories":["mysql"],"content":"2.10 OR 前后存在非索引的列，索引失效 在WHERE子句中，如果在OR前的条件列进行了索引，而在OR后的条件列没有进行索引，那么索引会失效。也就是说，OR前后的两个条件中的列都是索引时，查询中才使用索引。 因为OR的含义就是两个只要满足一个即可，因此只有一个条件列进行了索引是没有意义的，只要有条件列没有进行索引，就会进行全表扫描，因此所以的条件列也会失效。 查询语句使用OR关键字的情况： # 未使用到索引 EXPLAIN SELECT SQL_NO_CACHE * FROM student WHERE age = 10 OR classid = 100; 因为classId字段上没有索引，所以上述查询语句没有使用索引。 # 使用到索引 EXPLAIN SELECT SQL_NO_CACHE * FROM student WHERE age = 10 OR name = 'Abel'; 因为age字段和name字段上都有索引，所以查询中使用了索引。你能看到这里使用到了index_merge，简单来说index_merge就是对age和name分别进行了扫描，然后将这两个结果集进行了合并。这样做的好处就是避免了全表扫描。 ","date":"2022-09-24","objectID":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/:27:10","tags":[],"title":"MySQL索引及调优篇","uri":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/"},{"categories":["mysql"],"content":"2.11 数据库和表的字符集统一使用utf8mb4 统一使用utf8mb4( 5.5.3版本以上支持)兼容性更好，统一字符集可以避免由于字符集转换产生的乱码。不同的字符集进行比较前需要进行 转换 会造成索引失效。 ","date":"2022-09-24","objectID":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/:27:11","tags":[],"title":"MySQL索引及调优篇","uri":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/"},{"categories":["mysql"],"content":"2.12 练习及一般性建议 **练习：**假设：index(a,b,c) 一般性建议 对于单列索引，尽量选择针对当前query过滤性更好的索引 在选择组合索引的时候，当前query中过滤性最好的字段在索引字段顺序中，位置越靠前越好。 在选择组合索引的时候，尽量选择能够当前query中where子句中更多的索引。 在选择组合索引的时候，如果某个字段可能出现范围查询时，尽量把这个字段放在索引次序的最后面。 总之，书写SQL语句时，尽量避免造成索引失效的情况 ","date":"2022-09-24","objectID":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/:27:12","tags":[],"title":"MySQL索引及调优篇","uri":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/"},{"categories":["mysql"],"content":"3. 关联查询优化 ","date":"2022-09-24","objectID":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/:28:0","tags":[],"title":"MySQL索引及调优篇","uri":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/"},{"categories":["mysql"],"content":"3.1 数据准备 # 分类 CREATE TABLE IF NOT EXISTS `type` ( `id` INT(10) UNSIGNED NOT NULL AUTO_INCREMENT, `card` INT(10) UNSIGNED NOT NULL, PRIMARY KEY (`id`) ); # 图书 CREATE TABLE IF NOT EXISTS `book` ( `bookid` INT(10) UNSIGNED NOT NULL AUTO_INCREMENT, `card` INT(10) UNSIGNED NOT NULL, PRIMARY KEY (`bookid`) ); # 向分类表中添加20条记录 INSERT INTO `type`(card) VALUES(FLOOR(1 + (RAND() * 20))); INSERT INTO `type`(card) VALUES(FLOOR(1 + (RAND() * 20))); INSERT INTO `type`(card) VALUES(FLOOR(1 + (RAND() * 20))); INSERT INTO `type`(card) VALUES(FLOOR(1 + (RAND() * 20))); INSERT INTO `type`(card) VALUES(FLOOR(1 + (RAND() * 20))); INSERT INTO `type`(card) VALUES(FLOOR(1 + (RAND() * 20))); INSERT INTO `type`(card) VALUES(FLOOR(1 + (RAND() * 20))); INSERT INTO `type`(card) VALUES(FLOOR(1 + (RAND() * 20))); INSERT INTO `type`(card) VALUES(FLOOR(1 + (RAND() * 20))); INSERT INTO `type`(card) VALUES(FLOOR(1 + (RAND() * 20))); INSERT INTO `type`(card) VALUES(FLOOR(1 + (RAND() * 20))); INSERT INTO `type`(card) VALUES(FLOOR(1 + (RAND() * 20))); INSERT INTO `type`(card) VALUES(FLOOR(1 + (RAND() * 20))); INSERT INTO `type`(card) VALUES(FLOOR(1 + (RAND() * 20))); INSERT INTO `type`(card) VALUES(FLOOR(1 + (RAND() * 20))); INSERT INTO `type`(card) VALUES(FLOOR(1 + (RAND() * 20))); INSERT INTO `type`(card) VALUES(FLOOR(1 + (RAND() * 20))); INSERT INTO `type`(card) VALUES(FLOOR(1 + (RAND() * 20))); INSERT INTO `type`(card) VALUES(FLOOR(1 + (RAND() * 20))); INSERT INTO `type`(card) VALUES(FLOOR(1 + (RAND() * 20))); # 向图书表中添加20条记录 INSERT INTO book(card) VALUES(FLOOR(1 + (RAND() * 20))); INSERT INTO book(card) VALUES(FLOOR(1 + (RAND() * 20))); INSERT INTO book(card) VALUES(FLOOR(1 + (RAND() * 20))); INSERT INTO book(card) VALUES(FLOOR(1 + (RAND() * 20))); INSERT INTO book(card) VALUES(FLOOR(1 + (RAND() * 20))); INSERT INTO book(card) VALUES(FLOOR(1 + (RAND() * 20))); INSERT INTO book(card) VALUES(FLOOR(1 + (RAND() * 20))); INSERT INTO book(card) VALUES(FLOOR(1 + (RAND() * 20))); INSERT INTO book(card) VALUES(FLOOR(1 + (RAND() * 20))); INSERT INTO book(card) VALUES(FLOOR(1 + (RAND() * 20))); INSERT INTO book(card) VALUES(FLOOR(1 + (RAND() * 20))); INSERT INTO book(card) VALUES(FLOOR(1 + (RAND() * 20))); INSERT INTO book(card) VALUES(FLOOR(1 + (RAND() * 20))); INSERT INTO book(card) VALUES(FLOOR(1 + (RAND() * 20))); INSERT INTO book(card) VALUES(FLOOR(1 + (RAND() * 20))); INSERT INTO book(card) VALUES(FLOOR(1 + (RAND() * 20))); INSERT INTO book(card) VALUES(FLOOR(1 + (RAND() * 20))); INSERT INTO book(card) VALUES(FLOOR(1 + (RAND() * 20))); INSERT INTO book(card) VALUES(FLOOR(1 + (RAND() * 20))); INSERT INTO book(card) VALUES(FLOOR(1 + (RAND() * 20))); ","date":"2022-09-24","objectID":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/:28:1","tags":[],"title":"MySQL索引及调优篇","uri":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/"},{"categories":["mysql"],"content":"3.2 采用左外连接 下面开始 EXPLAIN 分析 EXPLAIN SELECT SQL_NO_CACHE * FROM `type` LEFT JOIN book ON type.card = book.card; 结论：type 有All 添加索引优化 ALTER TABLE book ADD INDEX Y (card); #【被驱动表】，可以避免全表扫描 EXPLAIN SELECT SQL_NO_CACHE * FROM `type` LEFT JOIN book ON type.card = book.card; # 注意，驱动表type是all，无可厚非，本来连接查询本来就是将驱动表的结果根据连接条件逐行与被驱动表里的所有行进行比较筛选，只要能够让被驱动表使用到索引，从而避免【依据驱动表的结果在被驱动表全表扫描的情况】发生，这样就如同下图，可以达到优化的目的 可以看到第二行的 type 变为了 ref，rows 也变成了优化比较明显。这是由左连接特性决定的。LEFT JOIN 条件用于确定如何从右表搜索行，左边一定都有，所以 右边是我们的关键点,一定需要建立索引 。 ALTER TABLE `type` ADD INDEX X (card); #【驱动表】，无法避免全表扫描 EXPLAIN SELECT SQL_NO_CACHE * FROM `type` LEFT JOIN book ON type.card = book.card; 接着： DROP INDEX Y ON book; EXPLAIN SELECT SQL_NO_CACHE * FROM `type` LEFT JOIN book ON type.card = book.card; ","date":"2022-09-24","objectID":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/:28:2","tags":[],"title":"MySQL索引及调优篇","uri":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/"},{"categories":["mysql"],"content":"3.3 采用内连接 drop index X on type; drop index Y on book; #（如果已经删除了可以不用再执行该操作） 换成 inner join（MySQL自动选择驱动表，一般优化器会选择小的表作为驱动表，大的表作为被驱动表） EXPLAIN SELECT SQL_NO_CACHE * FROM type INNER JOIN book ON type.card=book.card; 添加索引优化 ALTER TABLE book ADD INDEX Y (card); EXPLAIN SELECT SQL_NO_CACHE * FROM type INNER JOIN book ON type.card=book.card; ALTER TABLE type ADD INDEX X (card); EXPLAIN SELECT SQL_NO_CACHE * FROM type INNER JOIN book ON type.card=book.card; 对于内连接来说，查询优化器可以决定谁作为驱动表，谁作为被驱动表出现的 接着： DROP INDEX X ON `type`; EXPLAIN SELECT SQL_NO_CACHE * FROM TYPE INNER JOIN book ON type.card=book.card; 接着： ALTER TABLE `type` ADD INDEX X (card); EXPLAIN SELECT SQL_NO_CACHE * FROM `type` INNER JOIN book ON type.card=book.card; 接着： # 向图书表中添加20条记录 INSERT INTO book(card) VALUES(FLOOR(1 + (RAND() * 20))); INSERT INTO book(card) VALUES(FLOOR(1 + (RAND() * 20))); INSERT INTO book(card) VALUES(FLOOR(1 + (RAND() * 20))); INSERT INTO book(card) VALUES(FLOOR(1 + (RAND() * 20))); INSERT INTO book(card) VALUES(FLOOR(1 + (RAND() * 20))); INSERT INTO book(card) VALUES(FLOOR(1 + (RAND() * 20))); INSERT INTO book(card) VALUES(FLOOR(1 + (RAND() * 20))); INSERT INTO book(card) VALUES(FLOOR(1 + (RAND() * 20))); INSERT INTO book(card) VALUES(FLOOR(1 + (RAND() * 20))); INSERT INTO book(card) VALUES(FLOOR(1 + (RAND() * 20))); INSERT INTO book(card) VALUES(FLOOR(1 + (RAND() * 20))); INSERT INTO book(card) VALUES(FLOOR(1 + (RAND() * 20))); INSERT INTO book(card) VALUES(FLOOR(1 + (RAND() * 20))); INSERT INTO book(card) VALUES(FLOOR(1 + (RAND() * 20))); INSERT INTO book(card) VALUES(FLOOR(1 + (RAND() * 20))); INSERT INTO book(card) VALUES(FLOOR(1 + (RAND() * 20))); INSERT INTO book(card) VALUES(FLOOR(1 + (RAND() * 20))); INSERT INTO book(card) VALUES(FLOOR(1 + (RAND() * 20))); INSERT INTO book(card) VALUES(FLOOR(1 + (RAND() * 20))); INSERT INTO book(card) VALUES(FLOOR(1 + (RAND() * 20))); ALTER TABLE book ADD INDEX Y (card); EXPLAIN SELECT SQL_NO_CACHE * FROM `type` INNER JOIN book ON `type`.card = book.card; 图中发现，由于type表数据大于book表数据，MySQL选择将type作为被驱动表。–\u003e 小表驱动大表 ","date":"2022-09-24","objectID":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/:28:3","tags":[],"title":"MySQL索引及调优篇","uri":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/"},{"categories":["mysql"],"content":"3.4 join语句原理 join方式连接多个表，本质就是各个表之间数据的循环匹配。MySQL5.5版本之前，MySQL只支持一种表间关联方式，就是嵌套循环(Nested Loop Join)。如果关联表的数据量很大，则join关联的执行时间会很长。在MySQL5.5以后的版本中，MySQL通过引入BNLJ算法来优化嵌套执行。 1. 驱动表和被驱动表 驱动表就是主表，被驱动表就是从表、非驱动表。 对于内连接来说： SELECT * FROM A JOIN B ON ... A一定是驱动表吗？不一定，优化器会根据你查询语句做优化，决定先查哪张表。先查询的那张表就是驱动表，反之就是被驱动表。通过explain关键字可以查看。一般来说，小表驱动大表。 对于外连接来说： SELECT * FROM A LEFT JOIN B ON ... # 或 SELECT * FROM B RIGHT JOIN A ON ... **通常，大家会认为A就是驱动表，B就是被驱动表。但也未必。**测试如下： CREATE TABLE a(f1 INT, f2 INT, INDEX(f1)) ENGINE=INNODB; CREATE TABLE b(f1 INT, f2 INT) ENGINE=INNODB; INSERT INTO a VALUES(1,1),(2,2),(3,3),(4,4),(5,5),(6,6); INSERT INTO b VALUES(3,3),(4,4),(5,5),(6,6),(7,7),(8,8); SELECT * FROM b; # 测试1 EXPLAIN SELECT * FROM a LEFT JOIN b ON(a.f1=b.f1) WHERE (a.f2=b.f2); # 测试2 EXPLAIN SELECT * FROM a LEFT JOIN b ON(a.f1=b.f1) AND (a.f2=b.f2); 当连接查询没有where条件时，左连接查询时，前面的表是驱动表，后面的表是被驱动表，右连接查询时相反，内连接查询时，哪张表的数据较少，哪张表就是驱动表 当连接查询有where条件时，不好说了 2. Simple Nested-Loop Join (简单嵌套循环连接) 算法相当简单，从表A中取出一条数据1，遍历表B，将匹配到的数据放到result.. 以此类推，驱动表A中的每一条记录与被驱动表B的记录进行判断： 可以看到这种方式效率是非常低的，以上述表A数据100条，表B数据1000条计算，则A*B=10万次。开销统计如下: 当然mysql肯定不会这么粗暴的去进行表的连接，所以就出现了后面的两种对Nested-Loop Join优化算法。 可以看到，A越小，内表扫描次数就越少，性能越好，因此一般优化器小表驱动大表 3. Index Nested-Loop Join （索引嵌套循环连接） Index Nested-Loop Join 其优化的思路主要是为了减少被驱动表数据的匹配次数，所以要求被驱动表上必须有索引才行。通过驱动表匹配条件直接与被驱动表索引进行匹配，避免和被驱动表的每条记录去进行比较，这样极大的减少了对内存表的匹配次数。 驱动表中的每条记录通过被驱动表的索引进行访问，因为索引查询的成本是比较固定的，故mysql优化器都倾向于使用记录数少的表作为驱动表（外表）。 如果被驱动表加索引，效率是非常高的，但如果索引不是主键索引，所以还得进行一次回表查询。相比，被驱动表的索引是主键索引，效率会更高。 4. Block Nested-Loop Join（块嵌套循环连接） 注意： 这里缓存的不只是关联表的列，select后面的列也会缓存起来。 在一个有N个join关联的sql中会分配N-1个join buffer。所以查询的时候尽量减少不必要的字段，可以让join buffer中可以存放更多的列。 参数设置： block_nested_loop 通过show variables like '%optimizer_switch% 查看 block_nested_loop状态。默认是开启的。 join_buffer_size 驱动表能不能一次加载完，要看join buffer能不能存储所有的数据，默认情况下join_buffer_size=256k。 mysql\u003e show variables like '%join_buffer%'; join_buffer_size的最大值在32位操作系统可以申请4G，而在64位操作系统下可以申请大于4G的Join Buffer空间（64位Windows除外，其大值会被截断为4GB并发出警告）。 5. Join小结 1、整体效率比较：INLJ \u003e BNLJ \u003e SNLJ 2、永远用小结果集驱动大结果集（其本质就是减少外层循环的数据数量）（小的度量单位指的是表行数 * 每行大小） select t1.b,t2.* from t1 straight_join t2 on (t1.b=t2.b) where t2.id\u003c=100; # 推荐 select t1.b,t2.* from t2 straight_join t1 on (t1.b=t2.b) where t2.id\u003c=100; # 不推荐 3、为被驱动表匹配的条件增加索引(减少内层表的循环匹配次数) 4、增大join buffer size的大小（一次索引的数据越多，那么内层表的扫描次数就越少） 5、减少驱动表不必要的字段查询（字段越少，join buffer所缓存的数据就越多） 6. Hash Join 从MySQL的8.0.20版本开始将废弃BNLJ，因为从MySQL8.0.18版本开始就加入了hash join，默认都会使用hash join Nested Loop: 对于被连接的数据子集较小的情况，Nested Loop是个较好的选择。 Hash Join是做大数据集连接时的常用方式，优化器使用两个表中较小（相对较小）的表利用Join Key在内存中建立散列表，然后扫描较大的表并探测散列表，找出与Hash表匹配的行。 这种方式适合于较小的表完全可以放于内存中的情况，这样总成本就是访问两个表的成本之和。 在表很大的情况下并不能完全放入内存，这时优化器会将它分割成若干不同的分区，不能放入内存的部分就把该分区写入磁盘的临时段，此时要求有较大的临时段从而尽量提高I/O的性能。 它能够很好的工作于没有索引的大表和并行查询的环境中，并提供最好的性能。大多数人都说它是Join的重型升降机。Hash Join只能应用于等值连接（如WHERE A.COL1 = B.COL2），这是由Hash的特点决定的。 ","date":"2022-09-24","objectID":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/:28:4","tags":[],"title":"MySQL索引及调优篇","uri":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/"},{"categories":["mysql"],"content":"3.5 小结 保证被驱动表的JOIN字段已经创建了索引 需要 JOIN 的字段，数据类型保持绝对一致，避免类型隐式或显示转换，使索引失效。 LEFT JOIN 时，选择小表作为驱动表，大表作为被驱动表。减少外层循环的次数。 INNER JOIN 时，MySQL会自动将小结果集的表选为驱动表 。选择相信MySQL优化策略。 能够直接多表关联的尽量直接关联，不用子查询。(减少查询的趟数) 不建议使用子查询，建议将子查询SQL拆开结合程序多次查询，或使用 JOIN 来代替子查询。 衍生表建不了索引 ","date":"2022-09-24","objectID":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/:28:5","tags":[],"title":"MySQL索引及调优篇","uri":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/"},{"categories":["mysql"],"content":"4. 子查询优化 MySQL从4.1版本开始支持子查询，使用子查询可以进行SELECT语句的嵌套查询，即一个SELECT查询的结 果作为另一个SELECT语句的条件。 子查询可以一次性完成很多逻辑上需要多个步骤才能完成的SQL操作 。 子查询是 MySQL 的一项重要的功能，可以帮助我们通过一个 SQL 语句实现比较复杂的查询。但是，子查询的执行效率不高。 原因： ① 执行子查询时，MySQL需要为内层查询语句的查询结果建立一个临时表，然后外层查询语句从临时表中查询记录。查询完毕后，再撤销这些临时表。这样会消耗过多的CPU和IO资源，产生大量的慢查询。 ② 子查询的结果集存储的临时表，不论是内存临时表还是磁盘临时表都不会存在索引，所以查询性能会受到一定的影响。 ③ 对于返回结果集比较大的子查询，其对查询性能的影响也就越大。 在MySQL中，可以使用连接（JOIN）查询来替代子查询。 连接查询不需要建立临时表，其速度比子查询要快，如果查询中使用索引的话，性能就会更好。 举例1：查询学生表中是班长的学生信息 使用子查询 # 创建班级表中班长的索引 CREATE INDEX idx_monitor ON class(monitor); EXPLAIN SELECT * FROM student stu1 WHERE stu1.`stuno` IN ( SELECT monitor FROM class c WHERE monitor IS NOT NULL # is not null 使用不到索引 ) 推荐使用多表查询 EXPLAIN SELECT stu1.* FROM student stu1 JOIN class c ON stu1.`stuno` = c.`monitor` WHERE c.`monitor` is NOT NULL; 举例2：取所有不为班长的同学 不推荐 EXPLAIN SELECT SQL_NO_CACHE a.* FROM student a WHERE a.stuno NOT IN ( SELECT monitor FROM class b WHERE monitor IS NOT NULL ); SELECT a.* FROM student a JOIN class b ON a.stuno != b.monitor; # 不等于符号，容易让索引失效，giao 执行结果如下： 推荐： EXPLAIN SELECT SQL_NO_CACHE a.* FROM student a LEFT OUTER JOIN class b ON a.stuno = b.monitor WHERE b.monitor IS NULL; # 首先，保留student表所有记录，连接class表，使用等号 结论：尽量不要使用NOT IN或者NOT EXISTS，用LEFT JOIN xxx ON xx WHERE xx IS NULL替代 ","date":"2022-09-24","objectID":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/:29:0","tags":[],"title":"MySQL索引及调优篇","uri":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/"},{"categories":["mysql"],"content":"5. 排序优化 ","date":"2022-09-24","objectID":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/:30:0","tags":[],"title":"MySQL索引及调优篇","uri":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/"},{"categories":["mysql"],"content":"5.1 排序优化 问题：在 WHERE 条件字段上加索引，但是为什么在 ORDER BY 字段上还要加索引呢？ 回答： 在MySQL中，支持两种排序方式，分别是 FileSort 和 Index 排序。 Index 排序中，索引可以保证数据的有序性，不需要再进行排序，效率更高。 FileSort 排序则一般在 内存中 进行排序，占用CPU较多。如果待排结果较大，会产生临时文件 I/O 到磁盘进行排序的情况，效率较低。 优化建议： SQL 中，可以在 WHERE 子句和 ORDER BY 子句中使用索引，目的是在 WHERE 子句中 避免全表扫描 ，在 ORDER BY 子句 避免使用 FileSort 排序 。当然，某些情况下全表扫描，或者 FileSort 排序不一定比索引慢。但总的来说，我们还是要避免，以提高查询效率。 尽量使用 Index 完成 ORDER BY 排序。如果 WHERE 和 ORDER BY 后面是相同的列就使用单索引列； 如果不同就使用联合索引。 无法使用 Index 时，需要对 FileSort 方式进行调优。 ","date":"2022-09-24","objectID":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/:30:1","tags":[],"title":"MySQL索引及调优篇","uri":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/"},{"categories":["mysql"],"content":"5.2 测试 删除student表和class表中已创建的索引。 # 方式1 DROP INDEX idx_monitor ON class; DROP INDEX idx_cid ON student; DROP INDEX idx_age ON student; DROP INDEX idx_name ON student; DROP INDEX idx_age_name_classId ON student; DROP INDEX idx_age_classId_name ON student; # 方式2 call proc_drop_index('atguigudb2','student';) 以下是否能使用到索引，能否去掉using filesort 过程一： 过程二： order by 时不limit,索引失效 过程三：order by 时顺序错误，索引失效 过程四：order by 时规则不一致，索引失效（顺序错，不索引；方向反，不索引） 结论：ORDER BY 子句，尽量使用 Index 方式排序，避免使用 FileSort 方式排序 过程五：无过滤，不索引 小结 INDEX a_b_c(a,b,c) order by 能使用索引最左前缀 - ORDER BY a - ORDER BY a,b - ORDER BY a,b,c - ORDER BY a DESC,b DESC,c DESC 如果 WHERE 使用索引的最左前缀定义为常量，则 order by 能使用索引 - WHERE a = const ORDER BY b,c - WHERE a = const AND b = const ORDER BY c - WHERE a = const ORDER BY b,c - WHERE a = const AND b \u003e const ORDER BY b,c 不能使用索引进行排序 - ORDER BY a ASC,b DESC,c DESC /* 排序不一致 */ - WHERE g = const ORDER BY b,c /*丢失a索引*/ - WHERE a = const ORDER BY c /*丢失b索引*/ - WHERE a = const ORDER BY a,d /*d不是索引的一部分*/ - WHERE a in (...) ORDER BY b,c /*对于排序来说，多个相等条件也是范围查询*/ ","date":"2022-09-24","objectID":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/:30:2","tags":[],"title":"MySQL索引及调优篇","uri":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/"},{"categories":["mysql"],"content":"5.3 案例实战 ORDER BY子句，尽量使用Index方式排序，避免使用FileSort方式排序。 执行案例前先清除student上的索引，只留主键： DROP INDEX idx_age ON student; DROP INDEX idx_age_classid_stuno ON student; DROP INDEX idx_age_classid_name ON student; #或者 call proc_drop_index('atguigudb2','student'); 场景:查询年龄为30岁的，且学生编号小于101000的学生，按用户名称排序 EXPLAIN SELECT SQL_NO_CACHE * FROM student WHERE age = 30 AND stuno \u003c101000 ORDER BY NAME; 查询结果如下： mysql\u003e SELECT SQL_NO_CACHE * FROM student WHERE age = 30 AND stuno \u003c101000 ORDER BY NAME; +---------+--------+--------+------+---------+ | id | stuno | name | age | classId | +---------+--------+--------+------+---------+ | 922 | 100923 | elTLXD | 30 | 249 | | 3723263 | 100412 | hKcjLb | 30 | 59 | | 3724152 | 100827 | iHLJmh | 30 | 387 | | 3724030 | 100776 | LgxWoD | 30 | 253 | | 30 | 100031 | LZMOIa | 30 | 97 | | 3722887 | 100237 | QzbJdx | 30 | 440 | | 609 | 100610 | vbRimN | 30 | 481 | | 139 | 100140 | ZqFbuR | 30 | 351 | +---------+--------+--------+------+---------+ 8 rows in set, 1 warning (3.16 sec) 结论：type 是 ALL，即最坏的情况。Extra 里还出现了 Using filesort,也是最坏的情况。优化是必须的。 方案一: 为了去掉filesort我们可以把索引建成 #创建新索引 CREATE INDEX idx_age_name ON student(age,NAME); EXPLAIN SELECT SQL_NO_CACHE * FROM student WHERE age = 30 AND stuno \u003c101000 ORDER BY NAME; 这样我们优化掉了 using filesort 查询结果如下： 方案二：尽量让where的过滤条件和排序使用上索引 建一个三个字段的组合索引： DROP INDEX idx_age_name ON student; CREATE INDEX idx_age_stuno_name ON student (age,stuno,NAME); EXPLAIN SELECT SQL_NO_CACHE * FROM student WHERE age = 30 AND stuno \u003c101000 ORDER BY NAME; # 范围查询，右边的 name 列索引失效，排序为filesort，但是由于where条件过滤了很多数据，剩余数据量不大，所以是否filesort对处理的效率影响不大 我们发现using filesort依然存在，所以name并没有用到索引，而且type还是range光看名字其实并不美好。原因是，因为stuno是一个范围过滤，所以索引后面的字段不会在使用索引了。 结果如下： mysql\u003e SELECT SQL_NO_CACHE * FROM student -\u003e WHERE age = 30 AND stuno \u003c101000 ORDER BY NAME; +-----+--------+--------+------+---------+ | id | stuno | name | age | classId | +-----+--------+--------+------+---------+ | 167 | 100168 | AClxEF | 30 | 319 | | 323 | 100324 | bwbTpQ | 30 | 654 | | 651 | 100652 | DRwIac | 30 | 997 | | 517 | 100518 | HNSYqJ | 30 | 256 | | 344 | 100345 | JuepiX | 30 | 329 | | 905 | 100906 | JuWALd | 30 | 892 | | 574 | 100575 | kbyqjX | 30 | 260 | | 703 | 100704 | KJbprS | 30 | 594 | | 723 | 100724 | OTdJkY | 30 | 236 | | 656 | 100657 | Pfgqmj | 30 | 600 | | 982 | 100983 | qywLqw | 30 | 837 | | 468 | 100469 | sLEKQW | 30 | 346 | | 988 | 100989 | UBYqJl | 30 | 457 | | 173 | 100174 | UltkTN | 30 | 830 | | 332 | 100333 | YjWiZw | 30 | 824 | +-----+--------+--------+------+---------+ 15 rows in set, 1 warning (0.00 sec) 结果竟然有 filesort 的 sql 运行速度， 超过了已经优化掉 filesort 的 sql ，而且快了很多，几乎一瞬间就出现了结果。 原因： 结论： 两个索引同时存在，mysql自动选择最优的方案。（对于这个例子，mysql选择 idx_age_stuno_name）。但是，随着数据量的变化，选择的索引也会随之变化的 。 当【范围条件】和【group by 或者 order by】的字段出现二选一时，优先观察条件字段的过滤数量，如果过滤的数据足够多，而需要排序的数据并不多时，优先把索引放在范围字段上。反之，亦然。 思考：这里我们使用如下索引，是否可行？ DROP INDEX idx_age_stuno_name ON student; CREATE INDEX idx_age_stuno ON student(age,stuno); 当然可以。 ","date":"2022-09-24","objectID":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/:30:3","tags":[],"title":"MySQL索引及调优篇","uri":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/"},{"categories":["mysql"],"content":"5.4 filesort算法：双路排序和单路排序 排序的字段若不在索引列上，则filesort会有两种算法：双路排序和单路排序 双路排序 （慢） MySQL 4.1之前是使用双路排序 ，字面意思就是两次扫描磁盘，最终得到数据，读取行指针和 order by 列 ，对他们进行排序，然后扫描已经排序好的列表，按照列表中的值重新从列表中读取对应的数据输出 从磁盘取排序字段，在buffer进行排序，再从磁盘取其他字段。 取一批数据，要对磁盘进行两次扫描，众所周知，IO是很耗时的，所以在mysql4.1之后，出现了第二种改进的算法，就是单路排序。 单路排序 （快） 从磁盘读取查询需要的所有列 ，按照order by列在buffer对它们进行排序，然后扫描排序后的列表进行输出，它的效率更快一些，避免了第二次读取数据。并且把随机IO变成了顺序IO，但是它会使用更多的空间，因为它把每一行都保存在内存中了。 结论及引申出的问题 由于单路是后出的，总体而言好过双路 但是用单路有问题 在sort_buffer中，单路要比多路多占用很多空间，因为单路是把所有字段都取出，所以有可能取出的数据的总大小超出了sort_buffer的容量，导致每次只能取sort_buffer容量大小的数据，进行排序（创建tmp文件，多路合并），排完再取sort_buffer容量大小，再排…从而多次I/O。 单路本来想省一次I/O操作，反而导致了大量的I/O操作，反而得不偿失。 优化策略 1. 尝试提高 sort_buffer_size 2. 尝试提高 max_length_for_sort_data 3. Order by 时select * 是一个大忌。最好只Query需要的字段。 ","date":"2022-09-24","objectID":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/:30:4","tags":[],"title":"MySQL索引及调优篇","uri":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/"},{"categories":["mysql"],"content":"6. GROUP BY优化 group by 使用索引的原则几乎跟order by一致 ，group by 即使没有过滤条件用到索引，也可以直接使用索引。 group by 先排序再分组，遵照索引建的最佳左前缀法则 当无法使用索引列，增大 max_length_for_sort_data 和 sort_buffer_size 参数的设置 where效率高于having，能写在where限定的条件就不要写在having中了（having里可以使用聚合函数，where不可以 减少使用order by，和业务沟通能不排序就不排序，或将排序放到程序端去做。Order by、group by、distinct这些语句较为耗费CPU，数据库的CPU资源是极其宝贵的。 包含了order by、group by、distinct 这些查询的语句，where条件过滤出来的结果集请保持在1000行以内，否则SQL会很慢。 ","date":"2022-09-24","objectID":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/:31:0","tags":[],"title":"MySQL索引及调优篇","uri":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/"},{"categories":["mysql"],"content":"7. 优化分页查询 limit offset,row分页常用语句。当offset比较小的时候，影响不大；但是比较大的时候，也就是一些用户翻到了后面的数据页了。 如limit 2000000,10，意味着需要先扫描2000010这么多行数据，但是只会丢弃前面2000000而取后面十条数据，这样分页效率低下。 可以考虑使用子查询分页、join连接分页或者limit查询转化为某个位置的查询（当然这种情况还是不太适用） 优化思路一 在索引上完成排序分页操作，最后根据主键关联回原表查询所需要的其他列内容。 EXPLAIN SELECT * FROM student t,(SELECT id FROM student ORDER BY id LIMIT 2000000,10) a WHERE t.id = a.id; 优化思路二 该方案适用于主键自增的表，可以把Limit 查询转换成某个位置的查询。 EXPLAIN SELECT * FROM student WHERE id \u003e 2000000 LIMIT 10; ","date":"2022-09-24","objectID":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/:32:0","tags":[],"title":"MySQL索引及调优篇","uri":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/"},{"categories":["mysql"],"content":"8. 优先考虑覆盖索引 ","date":"2022-09-24","objectID":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/:33:0","tags":[],"title":"MySQL索引及调优篇","uri":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/"},{"categories":["mysql"],"content":"8.1 什么是覆盖索引？ 理解方式一：索引是高效找到行的一个方法，但是一般数据库也能使用索引找到一个列的数据，因此它不必读取整个行。毕竟索引叶子节点存储了它们索引的数据；当能通过读取索引就可以得到想要的数据，那就不需要读取行了。一个索引包含了满足查询结果的数据就叫做覆盖索引（省掉了回表过程）。 理解方式二：非聚簇复合索引的一种形式，它包括在查询里的SELECT、JOIN和WHERE子句用到的所有列 （即建索引的字段正好是覆盖查询条件中所涉及的字段）。 简单说就是，索引列+主键 包含 SELECT 到 FROM之间查询的列 。 举例一： # 删除之前的索引 DROP INDEX idx_age_stuno ON student; CREATE INDEX idx_age_name ON student(age, NAME); EXPLAIN SELECT * FROM student WHERE age \u003c\u003e 20; 举例二： EXPLAIN SELECT * FROM student WHERE NAME LIKE '%abc'; CREATE INDEX idx_age_name ON student(age, NAME); EXPLAIN SELECT id,age,NAME FROM student WHERE NAME = 'abc'; 上述都使用到了声明的索引，下面的情况则不然，查询列依然多了classId,结果是未使用到索引： EXPLAIN SELECT id,age,NAME,classId FROM student WHERE NAME LIKE '%abc'; ","date":"2022-09-24","objectID":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/:33:1","tags":[],"title":"MySQL索引及调优篇","uri":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/"},{"categories":["mysql"],"content":"8.2 覆盖索引的利弊 ","date":"2022-09-24","objectID":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/:33:2","tags":[],"title":"MySQL索引及调优篇","uri":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/"},{"categories":["mysql"],"content":"9. 如何给字符串添加索引 有一张教师表，表定义如下： create table teacher( ID bigint unsigned primary key, email varchar(64), ... )engine=innodb; 讲师要使用邮箱登录，所以业务代码中一定会出现类似于这样的语句： mysql\u003e select col1, col2 from teacher where email='xxx'; 如果email这个字段上没有索引，那么这个语句就只能做 全表扫描 。 ","date":"2022-09-24","objectID":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/:34:0","tags":[],"title":"MySQL索引及调优篇","uri":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/"},{"categories":["mysql"],"content":"9.1 前缀索引 MySQL是支持前缀索引的。默认地，如果你创建索引的语句不指定前缀长度，那么索引就会包含整个字符串。 mysql\u003e alter table teacher add index index1(email); #或 mysql\u003e alter table teacher add index index2(email(6)); 这两种不同的定义在数据结构和存储上有什么区别呢？下图就是这两个索引的示意图。 以及 如果使用的是index1（即email整个字符串的索引结构），执行顺序是这样的： 从index1索引树找到满足索引值是zhangssxyz@xxx.com的这条记录，取得ID2的值； 到主键上查到主键值是ID2的行，判断email的值是正确的，将这行记录加入结果集； 取index1索引树上刚刚查到的位置的下一条记录，发现已经不满足email=zhangssxyz@xxx.com的 条件了，循环结束。 这个过程中，只需要回主键索引取一次数据，所以系统认为只扫描了一行。 如果使用的是index2（即email(6)索引结构），执行顺序是这样的： 从index2索引树找到满足索引值是zhangs的记录，找到的第一个是ID1； 到主键上查到主键值是ID1的行，判断出email的值不是zhangssxyz@xxx.com，这行记录丢弃； 取index2上刚刚查到的位置的下一条记录，发现仍然是zhangs，取出ID2，再到ID索引上取整行然后判断，这次值对了，将这行记录加入结果集； 重复上一步，直到在idxe2上取到的值不是zhangs时，循环结束。 也就是说**使用前缀索引，定义好长度，就可以做到既节省空间，又不用额外增加太多的查询成本。**前面已经讲过区分度，区分度越高越好。因为区分度越高，意味着重复的键值越少。 ","date":"2022-09-24","objectID":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/:34:1","tags":[],"title":"MySQL索引及调优篇","uri":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/"},{"categories":["mysql"],"content":"9.2 前缀索引对覆盖索引的影响 结论： 使用前缀索引就用不上覆盖索引对查询性能的优化了，这也是你在选择是否使用前缀索引时需要考虑的一个因素。 ","date":"2022-09-24","objectID":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/:34:2","tags":[],"title":"MySQL索引及调优篇","uri":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/"},{"categories":["mysql"],"content":"10. 索引下推 ","date":"2022-09-24","objectID":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/:35:0","tags":[],"title":"MySQL索引及调优篇","uri":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/"},{"categories":["mysql"],"content":"10.1 使用前后对比 Index Condition Pushdown(ICP) 是MySQL 5.6中新特性，是一种在存储引擎层使用索引过滤数据的一种优化方式。 ","date":"2022-09-24","objectID":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/:35:1","tags":[],"title":"MySQL索引及调优篇","uri":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/"},{"categories":["mysql"],"content":"10.2 ICP的开启/关闭 默认情况下启动索引条件下推。可以通过设置系统变量optimizer_switch控制：index_condition_pushdown # 打开索引下推 SET optimizer_switch = 'index_condition_pushdown=on'; # 关闭索引下推 SET optimizer_switch = 'index_condition_pushdown=off'; 当使用索引条件下推是，EXPLAIN语句输出结果中Extra列内容显示为Using index condition。 ","date":"2022-09-24","objectID":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/:35:2","tags":[],"title":"MySQL索引及调优篇","uri":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/"},{"categories":["mysql"],"content":"10.3 ICP使用案例 主键索引 (简图) 二级索引zip_last_first (简图，这里省略了数据页等信息) ","date":"2022-09-24","objectID":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/:35:3","tags":[],"title":"MySQL索引及调优篇","uri":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/"},{"categories":["mysql"],"content":"10.4 开启和关闭ICP性能对比 ","date":"2022-09-24","objectID":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/:35:4","tags":[],"title":"MySQL索引及调优篇","uri":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/"},{"categories":["mysql"],"content":"10.5 ICP的使用条件 如果表的访问类型为 range 、 ref 、 eq_ref 或者 ref_or_null 可以使用ICP。 ICP可以使用InnDB和MyISAM表，包括分区表InnoDB和MyISAM表 对于InnoDB表，ICP仅用于二级索引。ICP的目标是减少全行读取次数，从而减少I/O操作。 当SQL使用覆盖索引时，不支持ICP优化方法。因为这种情况下使用ICP不会减少I/O。 相关子查询的条件不能使用ICP ","date":"2022-09-24","objectID":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/:35:5","tags":[],"title":"MySQL索引及调优篇","uri":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/"},{"categories":["mysql"],"content":"11. 普通索引 vs 唯一索引 从性能的角度考虑，你选择唯一索引还是普通索引呢？选择的依据是什么呢？ 假设，我们有一个主键列为ID的表，表中有字段k，并且在k上有索引，假设字段 k 上的值都不重复。 这个表的建表语句是： mysql\u003e create table test( id int primary key, k int not null, name varchar(16), index (k) )engine=InnoDB; 表中R1~R5的(ID,k)值分别为(100,1)、(200,2)、(300,3)、(500,5)和(600,6)。 ","date":"2022-09-24","objectID":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/:36:0","tags":[],"title":"MySQL索引及调优篇","uri":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/"},{"categories":["mysql"],"content":"11.1 查询过程 假设，执行查询的语句是 select id from test where k=5。 对于普通索引来说，查找到满足条件的第一个记录(5,500)后，需要查找下一个记录，直到碰到第一个不满足k=5条件的记录。 对于唯一索引来说，由于索引定义了唯一性，查找到第一个满足条件的记录后，就会停止继续检索。 那么，这个不同带来的性能差距会有多少呢？答案是，微乎其微。 ","date":"2022-09-24","objectID":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/:36:1","tags":[],"title":"MySQL索引及调优篇","uri":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/"},{"categories":["mysql"],"content":"11.2 更新过程 为了说明普通索引和唯一索引对更新语句性能的影响这个问题，介绍一下change buffer。 当需要更新一个数据页时，如果数据页在内存中,就直接更新，而如果这个数据页还没有在内存中的话，在不影响数据一致性的前提下， InooDB会将这些更新操作缓存在change buffer中 ，这样就不需要从磁盘中读入这个数据页了。在下次查询需要访问这个数据页的时候，将数据页读入内存，然后执行change buffer中与这个页有关的操作。通过这种方式就能保证这个数据逻辑的正确性。 将change buffer中的操作应用到原数据页，得到最新结果的过程称为 merge 。除了 访问这个数据页 会触发merge外，系统有 后台线程会定期 merge。在 数据库正常关闭（shutdown） 的过程中，也会执行merge操作。 如果能够将更新操作先记录在change buffer， 减少读磁盘 ，语句的执行速度会得到明显的提升。而且， 数据读入内存是需要占用 buffer pool 的，所以这种方式还能够 避免占用内存 ，提高内存利用率。 唯一索引的更新就不能使用change buffer ，实际上也只有普通索引可以使用。 如果要在这张表中插入一个新记录(4,400)的话，InnoDB的处理流程是怎样的？ ","date":"2022-09-24","objectID":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/:36:2","tags":[],"title":"MySQL索引及调优篇","uri":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/"},{"categories":["mysql"],"content":"11.3 change buffer的使用场景 普通索引和唯一索引应该怎么选择？其实，这两类索引在查询能力上是没差别的，主要考虑的是对更新性能的影响。所以，建议你尽量选择普通索引 。 在实际使用中会发现，普通索引和 change buffer 的配合使用，对于数据量大的表的更新优化还是很明显的。 如果所有的更新后面，都马上伴随着对这个记录的查询 ，那么你应该 关闭change buffer 。而在 其他情况下，change buffer都能提升更新性能。 由于唯一索引用不上change buffer的优化机制，因此如果业务可以接受，从性能角度出发建议优先考虑非唯一索引。但是如果\"业务可能无法确保\"的情况下，怎么处理呢？ 首先，业务正确性优先。我们的前提是“业务代码已经保证不会写入重复数据”的情况下，讨论性能问题。如果业务不能保证，或者业务就是要求数据库来做约束，那么没得选，必须创建唯一索引。这种情况下，本节的意义在于，如果碰上了大量插入数据慢、内存命中率低的时候，给你多提供一个排查思路。 然后，在一些“ 归档库 ”的场景，你是可以考虑使用唯一索引的。比如，线上数据只需要保留半年， 然后历史数据保存在归档库。这时候，归档数据已经是确保没有唯一键冲突了。要提高归档效率，可以考虑把表里面的唯一索引改成普通索引。 ","date":"2022-09-24","objectID":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/:36:3","tags":[],"title":"MySQL索引及调优篇","uri":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/"},{"categories":["mysql"],"content":"12. 其它查询优化策略 ","date":"2022-09-24","objectID":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/:37:0","tags":[],"title":"MySQL索引及调优篇","uri":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/"},{"categories":["mysql"],"content":"12.1 EXISTS 和 IN 的区分 问题： 不太理解哪种情况下应该使用 EXISTS，哪种情况应该用 IN。选择的标准是看能否使用表的索引吗？ 回答： ","date":"2022-09-24","objectID":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/:37:1","tags":[],"title":"MySQL索引及调优篇","uri":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/"},{"categories":["mysql"],"content":"12.2 COUNT(*)与COUNT(具体字段)效率 问：在 MySQL 中统计数据表的行数，可以使用三种方式： SELECT COUNT(*) 、 SELECT COUNT(1) 和 SELECT COUNT(具体字段) ，使用这三者之间的查询效率是怎样的？ 答： ","date":"2022-09-24","objectID":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/:37:2","tags":[],"title":"MySQL索引及调优篇","uri":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/"},{"categories":["mysql"],"content":"12.3 关于SELECT(*) 在表查询中，建议明确字段，不要使用 * 作为查询的字段列表，推荐使用SELECT \u003c字段列表\u003e 查询。原因： ① MySQL 在解析的过程中，会通过查询数据字典 将\"*“按序转换成所有列名，这会大大的耗费资源和时间。 ② 无法使用覆盖索引 ","date":"2022-09-24","objectID":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/:37:3","tags":[],"title":"MySQL索引及调优篇","uri":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/"},{"categories":["mysql"],"content":"12.4 LIMIT 1 对优化的影响 针对的是会扫描全表的 SQL 语句，如果你可以确定结果集只有一条，那么加上 LIMIT 1 的时候，当找到一条结果的时候就不会继续扫描了，这样会加快查询速度。 如果数据表已经对字段建立了唯一索引，那么可以通过索引进行查询，不会全表扫描的话，就不需要加上 LIMIT 1 了。 ","date":"2022-09-24","objectID":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/:37:4","tags":[],"title":"MySQL索引及调优篇","uri":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/"},{"categories":["mysql"],"content":"12.5 多使用COMMIT 只要有可能，在程序中尽量多使用 COMMIT，这样程序的性能得到提高，需求也会因为 COMMIT 所释放 的资源而减少。 COMMIT 所释放的资源： 回滚段上用于恢复数据的信息 被程序语句获得的锁 redo / undo log buffer 中的空间 管理上述 3 种资源中的内部花费 ","date":"2022-09-24","objectID":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/:37:5","tags":[],"title":"MySQL索引及调优篇","uri":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/"},{"categories":["mysql"],"content":"13. 淘宝数据库，主键如何设计的？ 聊一个实际问题：淘宝的数据库，主键是如何设计的？ 某些错的离谱的答案还在网上年复一年的流传着，甚至还成为了所谓的MySQL军规。其中，一个最明显的错误就是关于MySQL的主键设计。 大部分人的回答如此自信：用8字节的 BIGINT 做主键，而不要用INT。 错 ！ 这样的回答，只站在了数据库这一层，而没有 从业务的角度 思考主键。主键就是一个自增ID吗？站在 2022年的新年档口，用自增做主键，架构设计上可能 连及格都拿不到 。 ","date":"2022-09-24","objectID":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/:38:0","tags":[],"title":"MySQL索引及调优篇","uri":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/"},{"categories":["mysql"],"content":"13.1 自增ID的问题 自增ID做主键，简单易懂，几乎所有数据库都支持自增类型，只是实现上各自有所不同而已。自增ID除了简单，其他都是缺点，总体来看存在以下几方面的问题： 可靠性不高 存在自增ID回溯的问题，这个问题直到最新版本的MySQL 8.0才修复。 **安全性不高 ** 对外暴露的接口可以非常容易猜测对应的信息。比如：/User/1/这样的接口，可以非常容易猜测用户ID的 值为多少，总用户数量有多少，也可以非常容易地通过接口进行数据的爬取。 性能差 自增ID的性能较差，需要在数据库服务器端生成。 交互多 业务还需要额外执行一次类似 last_insert_id() 的函数才能知道刚才插入的自增值，这需要多一次的 网络交互。在海量并发的系统中，多1条SQL，就多一次性能上的开销。 **局部唯一性 ** 最重要的一点，自增ID是局部唯一，只在当前数据库实例中唯一，而不是全局唯一，在任意服务器间都是唯一的。对于目前分布式系统来说，这简直就是噩梦。 ","date":"2022-09-24","objectID":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/:38:1","tags":[],"title":"MySQL索引及调优篇","uri":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/"},{"categories":["mysql"],"content":"13.2 业务字段做主键 为了能够唯一地标识一个会员的信息，需要为会员信息表设置一个主键。那么，怎么为这个表设置主键，才能达到我们理想的目标呢？这里我们考虑业务字段做主键。 表数据如下： 在这个表里，哪个字段比较合适呢？ 选择卡号（cardno） 会员卡号（cardno）看起来比较合适，因为会员卡号不能为空，而且有唯一性，可以用来标识一条会员记录。 mysql\u003e CREATE TABLE demo.membermaster -\u003e ( -\u003e cardno CHAR(8) PRIMARY KEY, -- 会员卡号为主键 -\u003e membername TEXT, -\u003e memberphone TEXT, -\u003e memberpid TEXT, -\u003e memberaddress TEXT, -\u003e sex TEXT, -\u003e birthday DATETIME -\u003e ); Query OK, 0 rows affected (0.06 sec) 不同的会员卡号对应不同的会员，字段“cardno”唯一地标识某一个会员。如果都是这样，会员卡号与会员一一对应，系统是可以正常运行的。 但实际情况是，会员卡号可能存在重复使用的情况。比如，张三因为工作变动搬离了原来的地址，不再到商家的门店消费了 （退还了会员卡），于是张三就不再是这个商家门店的会员了。但是，商家不想让这个会员卡空着，就把卡号是“10000001”的会员卡发给了王五。 从系统设计的角度看，这个变化只是修改了会员信息表中的卡号是“10000001”这个会员信息，并不会影响到数据一致性。也就是说，修改会员卡号是“10000001”的会员信息，系统的各个模块，都会获取到修改后的会员信息，不会出现“有的模块获取到修改之前的会员信息，有的模块获取到修改后的会员信息，而导致系统内部数据不一致”的情况。因此，从 信息系统层面 上看是没问题的。 但是从使用 系统的业务层面 来看，就有很大的问题 了，会对商家造成影响。 比如，我们有一个销售流水表（trans），记录了所有的销售流水明细。2020 年 12 月 01 日，张三在门店 购买了一本书，消费了 89 元。那么，系统中就有了张三买书的流水记录，如下所示： 接着，我们查询一下 2020 年 12 月 01 日的会员销售记录： mysql\u003e SELECT b.membername,c.goodsname,a.quantity,a.salesvalue,a.transdate -\u003e FROM demo.trans AS a -\u003e JOIN demo.membermaster AS b -\u003e JOIN demo.goodsmaster AS c -\u003e ON (a.cardno = b.cardno AND a.itemnumber=c.itemnumber); +------------+-----------+----------+------------+---------------------+ | membername | goodsname | quantity | salesvalue | transdate | +------------+-----------+----------+------------+---------------------+ | 张三 | 书 | 1.000 | 89.00 | 2020-12-01 00:00:00 | +------------+-----------+----------+------------+---------------------+ 1 row in set (0.00 sec) 如果会员卡“10000001”又发给了王五，我们会更改会员信息表。导致查询时： mysql\u003e SELECT b.membername,c.goodsname,a.quantity,a.salesvalue,a.transdate -\u003e FROM demo.trans AS a -\u003e JOIN demo.membermaster AS b -\u003e JOIN demo.goodsmaster AS c -\u003e ON (a.cardno = b.cardno AND a.itemnumber=c.itemnumber); +------------+-----------+----------+------------+---------------------+ | membername | goodsname | quantity | salesvalue | transdate | +------------+-----------+----------+------------+---------------------+ | 王五 | 书 | 1.000 | 89.00 | 2020-12-01 00:00:00 | +------------+-----------+----------+------------+---------------------+ 1 row in set (0.01 sec) 这次得到的结果是：王五在 2020 年 12 月 01 日，买了一本书，消费 89 元。显然是错误的！结论：千万不能把会员卡号当做主键。 选择会员电话 或 身份证号 会员电话可以做主键吗？不行的。在实际操作中，手机号也存在被运营商收回 ，重新发给别人用的情况。 那身份证号行不行呢？好像可以。因为身份证决不会重复，身份证号与一个人存在一一对 应的关系。可问题是，身份证号属于个人隐私 ，顾客不一定愿意给你。要是强制要求会员必须登记身份证号，会把很多客人赶跑的。其实，客户电话也有这个问题，这也是我们在设计会员信息表的时候，允许身份证号和 电话都为空的原因。 所以，建议尽量不要用跟业务有关的字段做主键。毕竟，作为项目设计的技术人员，我们谁也无法预测 在项目的整个生命周期中，哪个业务字段会因为项目的业务需求而有重复，或者重用之类的情况出现。 经验： 刚开始使用 MySQL 时，很多人都很容易犯的错误是喜欢用业务字段做主键，想当然地认为了解业务需求，但实际情况往往出乎意料，而更改主键设置的成本非常高。 ","date":"2022-09-24","objectID":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/:38:2","tags":[],"title":"MySQL索引及调优篇","uri":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/"},{"categories":["mysql"],"content":"13.3 淘宝的主键设计 在淘宝的电商业务中，订单服务是一个核心业务。请问， 订单表的主键 淘宝是如何设计的呢？是自增ID 吗？ 打开淘宝，看一下订单信息： 从上图可以发现，订单号不是自增ID！我们详细看下上述4个订单号： 1550672064762308113 1481195847180308113 1431156171142308113 1431146631521308113 订单号是19位的长度，且订单的最后5位都是一样的，都是08113。且订单号的前面14位部分是单调递增的。 大胆猜测，淘宝的订单ID设计应该是： 订单ID = 时间 + 去重字段 + 用户ID后6位尾号 这样的设计能做到全局唯一，且对分布式系统查询及其友好。 ","date":"2022-09-24","objectID":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/:38:3","tags":[],"title":"MySQL索引及调优篇","uri":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/"},{"categories":["mysql"],"content":"13.4 推荐的主键设计 非核心业务 ：对应表的主键自增ID，如告警、日志、监控等信息。 核心业务 ：主键设计至少应该是全局唯一且是单调递增。全局唯一保证在各系统之间都是唯一的，单调递增是希望插入时不影响数据库性能（避免聚簇索引里的数据页的分裂）。 这里推荐最简单的一种主键设计：UUID。 UUID的特点： 全局唯一，占用36字节，数据无序，插入性能差。 认识UUID： 为什么UUID是全局唯一的？ 为什么UUID占用36个字节？ 为什么UUID是无序的？ MySQL数据库的UUID组成如下所示： UUID = 时间+UUID版本（16字节）- 时钟序列（4字节） - MAC地址（12字节） 我们以UUID值e0ea12d4-6473-11eb-943c-00155dbaa39d举例： 为什么UUID是全局唯一的？ 在UUID中时间部分占用60位，存储的类似TIMESTAMP的时间戳，但表示的是从1582-10-15 00：00：00.00 到现在的100ns的计数。可以看到UUID存储的时间精度比TIMESTAMPE更高，时间维度发生重复的概率降 低到1/100ns。 时钟序列是为了避免时钟被回拨导致产生时间重复的可能性。MAC地址用于全局唯一。 为什么UUID占用36个字节？ UUID根据字符串进行存储，设计时还带有无用”-“字符串，因此总共需要36个字节。 为什么UUID是随机无序的呢？ 因为UUID的设计中，将时间低位放在最前面，而这部分的数据是一直在变化的，并且是无序。 改造UUID 若将时间高低位互换，则时间就是单调递增的了，也就变得单调递增了。MySQL 8.0可以更换时间低位和时间高位的存储方式，这样UUID就是有序的UUID了。 MySQL 8.0还解决了UUID存在的空间占用的问题，除去了UUID字符串中无意义的”-“字符串，并且将字符串用二进制类型保存，这样存储空间降低为了16字节。 可以通过MySQL8.0提供的uuid_to_bin函数实现上述功能，同样的，MySQL也提供了bin_to_uuid函数进行转化： SET @uuid = UUID(); SELECT @uuid,uuid_to_bin(@uuid),uuid_to_bin(@uuid,TRUE); 通过函数uuid_to_bin(@uuid,true)将UUID转化为有序UUID了。全局唯一 + 单调递增，这不就是我们想要的主键！ 有序UUID性能测试 16字节的有序UUID，相比之前8字节的自增ID，性能和存储空间对比究竟如何呢？ 我们来做一个测试，插入1亿条数据，每条数据占用500字节，含有3个二级索引，最终的结果如下所示： 从上图可以看到插入1亿条数据有序UUID是最快的，而且在实际业务使用中有序UUID在 业务端就可以生成 。还可以进一步减少SQL的交互次数。 另外，虽然有序UUID相比自增ID多了8个字节，但实际只增大了3G的存储空间，还可以接受。 在当今的互联网环境中，非常不推荐自增ID作为主键的数据库设计。更推荐类似有序UUID的全局唯一的实现。 另外在真实的业务系统中，主键还可以加入业务和系统属性，如用户的尾号，机房的信息等。这样的主键设计就更为考验架构师的水平了。 如果不是MySQL8.0 肿么办？ 手动赋值字段做主键！ 比如，设计各个分店的会员表的主键，因为如果每台机器各自产生的数据需要合并，就可能会出现主键重复的问题。 可以在总部 MySQL 数据库中，有一个管理信息表，在这个表中添加一个字段，专门用来记录当前会员编号的最大值。 门店在添加会员的时候，先到总部 MySQL 数据库中获取这个最大值，在这个基础上加 1，然后用这个值 作为新会员的“id”，同时，更新总部 MySQL 数据库管理信息表中的当前会员编号的最大值。 这样一来，各个门店添加会员的时候，都对同一个总部 MySQL 数据库中的数据表字段进行操作，就解决了各门店添加会员时会员编号冲突的问题。 第11章_数据库的设计规范 ","date":"2022-09-24","objectID":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/:38:4","tags":[],"title":"MySQL索引及调优篇","uri":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/"},{"categories":["mysql"],"content":"1. 为什么需要数据库设计 ","date":"2022-09-24","objectID":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/:39:0","tags":[],"title":"MySQL索引及调优篇","uri":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/"},{"categories":["mysql"],"content":"2. 范 式 ","date":"2022-09-24","objectID":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/:40:0","tags":[],"title":"MySQL索引及调优篇","uri":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/"},{"categories":["mysql"],"content":"2.1 范式简介 在关系型数据库中，关于数据表设计的基本原则、规则就称为范式。可以理解为，一张数据表的设计结构需要满足的某种设计标准的级别。要想设计一个结构合理的关系型数据库，必须满足一定的范式。 ","date":"2022-09-24","objectID":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/:40:1","tags":[],"title":"MySQL索引及调优篇","uri":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/"},{"categories":["mysql"],"content":"2.2 范式都包括哪些 目前关系型数据库有六种常见范式，按照范式级别，从低到高分别是：第一范式（1NF）、第二范式 （2NF）、第三范式（3NF）、巴斯-科德范式（BCNF）、第四范式(4NF）和第五范式（5NF，又称完美范式）。 数据库的范式设计越高阶，夯余度就越低，同时高阶的范式一定符合低阶范式的要求，满足最低要求的范式是第一范式（1NF）。在第一范式的基础上进一步满足更多规范的要求称为第二范式（2NF），其余范式以此类推。 一般来说，在关系型数据库设计中，最高也就遵循到BCNF, 普遍还是3NF。但也不绝对，有时候为了提高某些查询性能，我们还需要破坏范式规则，也就是反规范化。 ","date":"2022-09-24","objectID":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/:40:2","tags":[],"title":"MySQL索引及调优篇","uri":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/"},{"categories":["mysql"],"content":"2.3 键和相关属性的概念 举例: 这里有两个表： 球员表(player) ：球员编号 | 姓名 | 身份证号 | 年龄 | 球队编号 球队表(team) ：球队编号 | 主教练 | 球队所在地 超键 ：对于球员表来说，超键就是包括 球员编号 或者 身份证号 的任意组合，比如（球员编号）、（球员编号，姓名）、（身份证号，年龄）等。 候选键 ：就是最小的超键，对于球员表来说，候选键就是（球员编号）或者（身份证号）。 主键 ：我们自己选定，也就是从候选键中选择一个，比如（球员编号），那么另一个候选键就不是主键了。 外键 ：球员表中的球队编号（球队表的主键）。 主属性 、 非主属性 ：在球员表中，主属性是（球员编号）（身份证号），其他的属性（姓名） （年龄）（球队编号）都是非主属性。 ","date":"2022-09-24","objectID":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/:40:3","tags":[],"title":"MySQL索引及调优篇","uri":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/"},{"categories":["mysql"],"content":"2.4 第一范式(1st NF) 第一范式主要确保数据库中每个字段的值必须具有原子性，也就是说数据表中每个字段的值为不可再次拆分的最小数据单元。 我们在设计某个字段的时候，对于字段X来说，不能把字段X拆分成字段X-1和字段X-2。事实上，任何的DBMS都会满足第一范式的要求，不会将字段进行拆分。 举例1： 假设一家公司要存储员工的姓名和联系方式。它创建一个如下表： 该表不符合 1NF ，因为规则说“表的每个属性必须具有原子（单个）值”，lisi 和 zhaoliu 员工的 emp_mobile 值违反了该规则。为了使表符合 1NF ，我们应该有如下表数据： 举例2： user 表的设计不符合第一范式 其中，user_info 字段为用户信息，可以进一步拆分成更小粒度的字段，不符合数据库设计对第一范式的 要求。将 user_info 拆分后如下： 举例3： **属性的原子性是主观的。**例如，Employees关系中雇员姓名应当使用1个（fullname）、2个（firstname 和lastname）还是3个（firstname、middlename和lastname）属性表示呢？**答案取决于应用程序。**如果应用程序需要分别处理雇员的姓名部分（如：用于搜索目的），则有必要把它们分开。否则，不需要。 表1： 表2： ","date":"2022-09-24","objectID":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/:40:4","tags":[],"title":"MySQL索引及调优篇","uri":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/"},{"categories":["mysql"],"content":"2.5 第二范式(2nd NF) 第二范式要求，在满足第一范式的基础上，还要满足数据库里的每一条数据记录，都是可唯一标识的。而且所有非主键字段，都必须完全依赖主键，不能只依赖主键的一部分。如果知道主键的所有属性的值，就可以检索到任何元组（行）的任何属性的任何值。（要求中的主键，其实可以扩展替换为候选键）。 举例1： 成绩表（学号，课程号，成绩）关系中，（学号，课程号）可以决定成绩，但是学号不能决定成绩，课程号也不能决定成绩，所以“（学号，课程号）→成绩”就是完全依赖关系。 举例2： 比赛表 player_game，里面包含 球员编号、姓名、年龄、比赛编号、比赛时间和比赛场地 等属性，这里候选键和主键都为（球员编号，比赛编号），我们可以通过候选键（或主键）来决定如下的关系： (球员编号, 比赛编号) → (姓名, 年龄, 比赛时间, 比赛场地，得分) 但是这个数据表不满足第二范式，因为数据表中的字段之间还存在着如下的对应关系： (球员编号) → (姓名，年龄) (比赛编号) → (比赛时间, 比赛场地) 对于非主属性来说，并非完全依赖候选键。这样会产生怎样的问题呢？ 数据冗余 ：如果一个球员可以参加 m 场比赛，那么球员的姓名和年龄就重复了 m-1 次。一个比赛 也可能会有 n 个球员参加，比赛的时间和地点就重复了 n-1 次。 插入异常 ：如果我们想要添加一场新的比赛，但是这时还没有确定参加的球员都有谁，那么就没法插入。 删除异常 ：如果我要删除某个球员编号，如果没有单独保存比赛表的话，就会同时把比赛信息删 除掉。 更新异常 ：如果我们调整了某个比赛的时间，那么数据表中所有这个比赛的时间都需要进行调 整，否则就会出现一场比赛时间不同的情况。 为了避免出现上述的情况，我们可以把球员比赛表设计为下面的三张表。 这样的话，每张数据表都符合第二范式，也就避免了异常情况的发生。 1NF 告诉我们字段属性需要是原子性的，而 2NF 告诉我们一张表就是一个独立的对象，一张表只表达一个意思。 举例3： 定义了一个名为 Orders 的关系，表示订单和订单行的信息： 违反了第二范式，因为有非主键属性仅依赖于候选键（或主键）的一部分。例如，可以仅通过orderid找 到订单的 orderdate，以及 customerid 和 companyname，而没有必要再去使用productid。 修改： Orders表和OrderDetails表如下，此时符合第二范式。 小结：第二范式（2NF）要求实体的属性完全依赖主关键字。如果存在不完全依赖，那么这个属性和主关键字的这一部分应该分离出来形成一个新的实体，新实体与元实体之间是一对多的关系。 ","date":"2022-09-24","objectID":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/:40:5","tags":[],"title":"MySQL索引及调优篇","uri":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/"},{"categories":["mysql"],"content":"2.6 第三范式(3rd NF) 第三范式是在第二范式的基础上，确保数据表中的每一个非主键字段都和主键字段直接相关，也就是说，要求数据表中的所有非主键字段不能依赖于其他非主键字段。（即，不能存在非主属性A依赖于非主属性B，非主属性B依赖于主键C的情况，即存在“A-\u003eB-\u003eC\"的决定关系）通俗地讲，该规则的意思是所有非主键属性之间不能由依赖关系，必须相互独立。 这里的主键可以扩展为候选键。 举例1： 部门信息表 ：每个部门有部门编号（dept_id）、部门名称、部门简介等信息。 员工信息表 ：每个员工有员工编号、姓名、部门编号。列出部门编号后就不能再将部门名称、部门简介 等与部门有关的信息再加入员工信息表中。 如果不存在部门信息表，则根据第三范式（3NF）也应该构建它，否则就会有大量的数据冗余。 举例2： 商品类别名称依赖于商品类别编号，不符合第三范式。 修改： 表1：符合第三范式的 商品类别表 的设计 表2：符合第三范式的 商品表 的设计 商品表goods通过商品类别id字段（category_id）与商品类别表goods_category进行关联。 举例3： 球员player表 ：球员编号、姓名、球队名称和球队主教练。现在，我们把属性之间的依赖关系画出来，如下图所示: 你能看到球员编号决定了球队名称，同时球队名称决定了球队主教练，非主属性球队主教练就会传递依 赖于球员编号，因此不符合 3NF 的要求。 如果要达到 3NF 的要求，需要把数据表拆成下面这样： 举例4： 修改第二范式中的举例3。 此时的Orders关系包含 orderid、orderdate、customerid 和 companyname 属性，主键定义为 orderid。 customerid 和companyname均依赖于主键——orderid。例如，你需要通过orderid主键来查找代表订单中 客户的customerid，同样，你需要通过 orderid 主键查找订单中客户的公司名称（companyname）。然 而， customerid和companyname也是互相依靠的。为满足第三范式，可以改写如下： 符合3NF后的数据模型通俗地讲，2NF和3NF通常以这句话概括：“每个非键属性依赖于键，依赖于 整个键，并且除了键别无他物”。 ","date":"2022-09-24","objectID":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/:40:6","tags":[],"title":"MySQL索引及调优篇","uri":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/"},{"categories":["mysql"],"content":"2.7 小结 ","date":"2022-09-24","objectID":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/:40:7","tags":[],"title":"MySQL索引及调优篇","uri":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/"},{"categories":["mysql"],"content":"3. 反范式化 ","date":"2022-09-24","objectID":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/:41:0","tags":[],"title":"MySQL索引及调优篇","uri":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/"},{"categories":["mysql"],"content":"3.1 概述 规范化 vs 性能 为满足某种商业目标 , 数据库性能比规范化数据库更重要 在数据规范化的同时 , 要综合考虑数据库的性能 通过在给定的表中添加额外的字段，以大量减少需要从中搜索信息所需的时间 通过在给定的表中插入计算列，以方便查询 ","date":"2022-09-24","objectID":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/:41:1","tags":[],"title":"MySQL索引及调优篇","uri":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/"},{"categories":["mysql"],"content":"3.2 应用举例 举例1： 员工的信息存储在 employees 表 中，部门信息存储在 departments 表 中。通过 employees 表中的 department_id字段与 departments 表建立关联关系。如果要查询一个员工所在部门的名称： select employee_id,department_name from employees e join departments d on e.department_id = d.department_id; 如果经常需要进行这个操作，连接查询就会浪费很多时间。可以在 employees 表中增加一个冗余字段 department_name，这样就不用每次都进行连接操作了。 举例2： 反范式化的 goods商品信息表 设计如下： 举例3： 我们有 2 个表，分别是 商品流水表（atguigu.trans ）和 商品信息表 （atguigu.goodsinfo） 。商品流水表里有 400 万条流水记录，商品信息表里有 2000 条商品记录。 商品流水表： 商品信息表： 新的商品流水表如下所示： 举例4： 课程评论表 class_comment ，对应的字段名称及含义如下： 学生表 student ，对应的字段名称及含义如下： 在实际应用中，我们在显示课程评论的时候，通常会显示这个学生的昵称，而不是学生 ID，因此当我们 想要查询某个课程的前 1000 条评论时，需要关联 class_comment 和 student这两张表来进行查询。 实验数据：模拟两张百万量级的数据表 为了更好地进行 SQL 优化实验，我们需要给学生表和课程评论表随机模拟出百万量级的数据。我们可以 通过存储过程来实现模拟数据。 反范式优化实验对比 如果我们想要查询课程 ID 为 10001 的前 1000 条评论，需要写成下面这样： SELECT p.comment_text, p.comment_time, stu.stu_name FROM class_comment AS p LEFT JOIN student AS stu ON p.stu_id = stu.stu_id WHERE p.class_id = 10001 ORDER BY p.comment_id DESC LIMIT 1000; 运行结果（1000 条数据行）： 运行时长为 0.395 秒，对于网站的响应来说，这已经很慢了，用户体验会非常差。 如果我们想要提升查询的效率，可以允许适当的数据冗余，也就是在商品评论表中增加用户昵称字段， 在 class_comment 数据表的基础上增加 stu_name 字段，就得到了 class_comment2 数据表。 这样一来，只需单表查询就可以得到数据集结果： SELECT comment_text, comment_time, stu_name FROM class_comment2 WHERE class_id = 10001 ORDER BY class_id DESC LIMIT 1000; 运行结果（1000 条数据）： 优化之后只需要扫描一次聚集索引即可，运行时间为 0.039 秒，查询时间是之前的 1/10。 你能看到， 在数据量大的情况下，查询效率会有显著的提升。 ","date":"2022-09-24","objectID":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/:41:2","tags":[],"title":"MySQL索引及调优篇","uri":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/"},{"categories":["mysql"],"content":"3.3 反范式的新问题 存储空间变大了 一个表中字段做了修改，另一个表中冗余的字段也需要做同步修改，否则数据不一致 若采用存储过程来支持数据的更新、删除等额外操作，如果更新频繁，会非常消耗系统资源 在 数据量小 的情况下，反范式不能体现性能的优势，可能还会让数据库的设计更加复杂 ","date":"2022-09-24","objectID":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/:41:3","tags":[],"title":"MySQL索引及调优篇","uri":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/"},{"categories":["mysql"],"content":"3.4 反范式的适用场景 当冗余信息有价值或者能 大幅度提高查询效率 的时候，我们才会采取反范式的优化。 1. 增加冗余字段的建议 增加冗余字段一定要符合如下两个条件。只要满足这两个条件，才可以考虑增加夯余字段。 1）这个冗余字段不需要经常进行修改。 2）这个冗余字段查询的时候不可或缺。 2. 历史快照、历史数据的需要 在现实生活中，我们经常需要一些冗余信息，比如订单中的收货人信息，包括姓名、电话和地址等。每次发生的 订单收货信息 都属于 历史快照 ，需要进行保存，但用户可以随时修改自己的信息，这时保存这些冗余信息是非常有必要的。 反范式优化也常用在 数据仓库 的设计中，因为数据仓库通常存储历史数据 ，对增删改的实时性要求不强，对历史数据的分析需求强。这时适当允许数据的冗余度，更方便进行数据分析。 我简单总结下数据仓库和数据库在使用上的区别： 数据库设计的目的在于捕捉数据，而数据仓库设计的目的在于分析数据。 数据库对数据的增删改实时性要求强，需要存储在线的用户数据，而数据仓库存储的一般是历史数据。 数据库设计需要尽量避免冗余，但为了提高查询效率也允许一定的冗余度，而数据仓库在设计上更偏向采用反范式设计， ","date":"2022-09-24","objectID":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/:41:4","tags":[],"title":"MySQL索引及调优篇","uri":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/"},{"categories":["mysql"],"content":"4. BCNF(巴斯范式) 人们在3NF的基础上进行了改进，提出了巴斯范式（BCNF），页脚巴斯 - 科德范式（Boyce - Codd Normal Form）。BCNF被认为没有新的设计规范加入，只是对第三范式中设计规范要求更强，使得数据库冗余度更小。所以，称为是修正的第三范式，或扩充的第三范式，BCNF不被称为第四范式。 若一个关系达到了第三范式，并且它只有一个候选键，或者它的每个候选键都是单属性，则该关系自然达到BC范式。 一般来说，一个数据库设符合3NF或者BCNF就可以了。 1. 案例 我们分析如下表的范式情况： 在这个表中，一个仓库只有一个管理员，同时一个管理员也只管理一个仓库。我们先来梳理下这些属性之间的依赖关系。 仓库名决定了管理员，管理员也决定了仓库名，同时（仓库名，物品名）的属性集合可以决定数量这个属性。这样，我们就可以找到数据表的候选键。 候选键 ：是（管理员，物品名）和（仓库名，物品名），然后我们从候选键中选择一个作为主键 ，比 如（仓库名，物品名）。 主属性 ：包含在任一候选键中的属性，也就是仓库名，管理员和物品名。 非主属性 ：数量这个属性。 2. 是否符合三范式 如何判断一张表的范式呢？我们需要根据范式的等级，从低到高来进行判断。 首先，数据表每个属性都是原子性的，符合 1NF 的要求； 其次，数据表中非主属性”数量“都与候选键全部依赖，（仓库名，物品名）决定数量，（管理员，物品名）决定数量。因此，数据表符合 2NF 的要求； 最后，数据表中的非主属性，不传递依赖于候选键。因此符合 3NF 的要求。 3. 存在的问题 既然数据表已经符合了 3NF 的要求，是不是就不存在问题了呢？我们来看下面的情况： 增加一个仓库，但是还没有存放任何物品。根据数据表实体完整性的要求，主键不能有空值，因此会出现插入异常； 如果仓库更换了管理员，我们就可能会修改数据表中的多条记录 ； 如果仓库里的商品都卖空了，那么此时仓库名称和相应的管理员名称也会随之被删除。 你能看到，即便数据表符合 3NF 的要求，同样可能存在插入，更新和删除数据的异常情况。 4. 问题解决 首先我们需要确认造成异常的原因：主属性仓库名对于候选键（管理员，物品名）是部分依赖的关系， 这样就有可能导致上面的异常情况。因此引入BCNF，它在 3NF 的基础上消除了主属性对候选键的部分依赖或者传递依赖关系。 如果在关系R中，U为主键，A属性是主键的一个属性，若存在A-\u003eY，Y为主属性，则该关系不属于 BCNF。 根据 BCNF 的要求，我们需要把仓库管理关系 warehouse_keeper 表拆分成下面这样： 仓库表 ：（仓库名，管理员） 库存表 ：（仓库名，物品名，数量） 这样就不存在主属性对于候选键的部分依赖或传递依赖，上面数据表的设计就符合 BCNF。 再举例： 有一个 学生导师表 ，其中包含字段：学生ID，专业，导师，专业GPA，这其中学生ID和专业是联合主键。 这个表的设计满足三范式，但是这里存在另一个依赖关系，“专业”依赖于“导师”，也就是说每个导师只做一个专业方面的导师，只要知道了是哪个导师，我们自然就知道是哪个专业的了。 所以这个表的部分主键Major依赖于非主键属性Advisor，那么我们可以进行以下的调整，拆分成2个表： 学生导师表： 导师表： ","date":"2022-09-24","objectID":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/:42:0","tags":[],"title":"MySQL索引及调优篇","uri":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/"},{"categories":["mysql"],"content":"5. 第四范式 多值依赖的概念： 多值依赖即属性之间的一对多关系，记为K—\u003e—\u003eA。 函数依赖事实上是单值依赖，所以不能表达属性值之间的一对多关系。 平凡的多值依赖：全集U=K+A，一个K可以对应于多个A，即K—\u003e—\u003eA。此时整个表就是一组一对多关系。 非平凡的多值依赖：全集U=K+A+B，一个K可以对应于多个A，也可以对应于多个B，A与B相互独立，即K—\u003e—\u003eA，K—\u003e—\u003eB。整个表有多组一对多关系，且有：“一\"部分是相同的属性集合，“多”部分是相互独立的属性集合。 第四范式即在满足巴斯 - 科德范式（BCNF）的基础上，消除非平凡且非函数依赖的多值依赖（即把同一表的多对多关系删除）。 **举例1：**职工表(职工编号，职工孩子姓名，职工选修课程)。 在这个表中，同一个职工可能会有多个职工孩子姓名。同样，同一个职工也可能会有多个职工选修课程，即这里存在着多值事实，不符合第四范式。 如果要符合第四范式，只需要将上表分为两个表，使它们只有一个多值事实，例如： 职工表一 (职工编 号，职工孩子姓名)， 职工表二(职工编号，职工选修课程)，两个表都只有一个多值事实，所以符合第四范式。 举例2： 比如我们建立课程、教师、教材的模型。我们规定，每门课程有对应的一组教师，每门课程也有对应的一组教材，一门课程使用的教材和教师没有关系。我们建立的关系表如下： 课程ID，教师ID，教材ID；这三列作为联合主键。 为了表述方便，我们用Name代替ID，这样更容易看懂： 这个表除了主键，就没有其他字段了，所以肯定满足BC范式，但是却存在 多值依赖 导致的异常。 假如我们下学期想采用一本新的英版高数教材，但是还没确定具体哪个老师来教，那么我们就无法在这 个表中维护Course高数和Book英版高数教材的的关系。 解决办法是我们把这个多值依赖的表拆解成2个表，分别建立关系。这是我们拆分后的表： 以及 ","date":"2022-09-24","objectID":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/:43:0","tags":[],"title":"MySQL索引及调优篇","uri":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/"},{"categories":["mysql"],"content":"6. 第五范式、域键范式 除了第四范式外，我们还有更高级的第五范式（又称完美范式）和域键范式（DKNF）。 在满足第四范式（4NF）的基础上，消除不是由候选键所蕴含的连接依赖。如果关系模式R中的每一个连 接依赖均由R的候选键所隐含，则称此关系模式符合第五范式。 函数依赖是多值依赖的一种特殊的情况，而多值依赖实际上是连接依赖的一种特殊情况。但连接依赖不 像函数依赖和多值依赖可以由 语义直接导出 ，而是在 关系连接运算 时才反映出来。存在连接依赖的关系 模式仍可能遇到数据冗余及插入、修改、删除异常等问题。 第五范式处理的是 无损连接问题 ，这个范式基本 没有实际意义 ，因为无损连接很少出现，而且难以察觉。而域键范式试图定义一个 终极范式 ，该范式考虑所有的依赖和约束类型，但是实用价值也是最小的，只存在理论研究中。 ","date":"2022-09-24","objectID":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/:44:0","tags":[],"title":"MySQL索引及调优篇","uri":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/"},{"categories":["mysql"],"content":"7. 实战案例 商超进货系统中的进货单表进行剖析： 进货单表： 这个表中的字段很多，表里的数据量也很惊人。大量重复导致表变得庞大，效率极低。如何改造？ 在实际工作场景中，这种由于数据表结构设计不合理，而导致的数据重复的现象并不少见。往往是系统虽然能够运行，承载能力却很差，稍微有点流量，就会出现内存不足、CPU使用率飙升的情况，甚至会导致整个项目失败。 ","date":"2022-09-24","objectID":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/:45:0","tags":[],"title":"MySQL索引及调优篇","uri":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/"},{"categories":["mysql"],"content":"7.1 迭代1次：考虑1NF 第一范式要求：所有的字段都是基本数据类型，不可进行拆分。这里需要确认，所有的列中，每个字段只包含一种数据。 这张表里，我们把“property\"这一字段，拆分成”specification (规格)” 和 “unit (单位)\"，这两个字段如下： ","date":"2022-09-24","objectID":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/:45:1","tags":[],"title":"MySQL索引及调优篇","uri":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/"},{"categories":["mysql"],"content":"7.2 迭代2次：考虑2NF 第二范式要求，在满足第一范式的基础上，还要满足数据表里的每一条数据记录，都是可唯一标识的。而且所有字段，都必须完全依赖主键，不能只依赖主键的一部分。 第1步，就是要确定这个表的主键。通过观察发现，字段“listnumber（单号）\"+“barcode（条码）“可以唯一标识每一条记录，可以作为主键。 第2步，确定好了主键以后，判断哪些字段完全依赖主键，哪些字段只依赖于主键的一部分。把只依赖于主键一部分的字段拆出去，形成新的数据表。 首先，进货单明细表里面的\"goodsname(名称)““specification(规格)““unit(单位)“这些信息是商品的属性，只依赖于\"batcode(条码)\"，不完全依赖主键，可以拆分出去。我们把这3个字段加上它们所依赖的字段\"barcode(条码)\"，拆分形成新的数据表\"商品信息表”。 这样一来，原来的数据表就被拆分成了两个表。 商品信息表： 进货单表： 此外，字段\"supplierid(供应商编号)““suppliername(供应商名称)““stock(仓库)“只依赖于\"listnumber(单号)\"，不完全依赖于主键，所以，我们可以把\"supplierid\"“suppliername\"“stock\"这3个字段拆出去，再加上它们依赖的字段\"listnumber(单号)\"，就形成了一个新的表\"进货单头表”。剩下的字段，会组成新的表，我们叫它\"进货单明细表”。 原来的数据表就拆分成了3个表。 进货单头表： 进货单明细表： 商品信息表： 现在，我们再来分析一下拆分后的3个表，保证这3个表都满足第二范式的要求。 第3步，在“商品信息表”中，字段“barcode\"是有可能存在重复的，比如，用户门店可能有散装称重商品和自产商品，会存在条码共用的情况。所以，所有的字段都不能唯一标识表里的记录。这个时候，我们必须给这个表加上一个主键，比如说是自增字段\"itemnumber\"。 ","date":"2022-09-24","objectID":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/:45:2","tags":[],"title":"MySQL索引及调优篇","uri":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/"},{"categories":["mysql"],"content":"7.3 迭代3次：考虑3NF 我们的进货单头表，还有数据冗余的可能。因为\"suppliername\"依赖\"supplierid”，那么就可以按照第三范式的原则进行拆分了。我们就进一步拆分进货单头表，把它拆解陈供货商表和进货单头表。 供货商表： 进货单头表： 这2个表都满足第三范式的要求了。 ","date":"2022-09-24","objectID":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/:45:3","tags":[],"title":"MySQL索引及调优篇","uri":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/"},{"categories":["mysql"],"content":"7.4 反范式化：业务优先的原则 因此，最后我们可以把进货单表拆分成下面的4个表： 供货商表： 进货单头表： 进货单明细表： 商品信息表： 这样一来，我们就避免了冗余数据，而且还能够满足业务的需求，这样的数据库设计，才是合格的设计。 ","date":"2022-09-24","objectID":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/:45:4","tags":[],"title":"MySQL索引及调优篇","uri":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/"},{"categories":["mysql"],"content":"8. ER模型 ","date":"2022-09-24","objectID":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/:46:0","tags":[],"title":"MySQL索引及调优篇","uri":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/"},{"categories":["mysql"],"content":"8.1 ER模型包括哪些要素？ ER 模型中有三个要素，分别是实体、属性和关系。 实体 ，可以看做是数据对象，往往对应于现实生活中的真实存在的个体。在 ER 模型中，用 矩形 来表 示。实体分为两类，分别是 强实体 和 弱实体 。强实体是指不依赖于其他实体的实体；弱实体是指对另一个实体有很强的依赖关系的实体。 属性 ，则是指实体的特性。比如超市的地址、联系电话、员工数等。在 ER 模型中用 椭圆形 来表示。 关系 ，则是指实体之间的联系。比如超市把商品卖给顾客，就是一种超市与顾客之间的联系。在 ER 模 型中用 菱形 来表示。 注意：实体和属性不容易区分。这里提供一个原则：我们从系统整体的角度出发去看，可以独立存在的是实体，不可再分的是属性。也就是说，属性不能包含其他属性。 ","date":"2022-09-24","objectID":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/:46:1","tags":[],"title":"MySQL索引及调优篇","uri":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/"},{"categories":["mysql"],"content":"8.2 关系的类型 在 ER 模型的 3 个要素中，关系又可以分为 3 种类型，分别是 一对一、一对多、多对多。 一对一 ：指实体之间的关系是一一对应的，比如个人与身份证信息之间的关系就是一对一的关系。一个人只能有一个身份证信息，一个身份证信息也只属于一个人。 一对多 ：指一边的实体通过关系，可以对应多个另外一边的实体。相反，另外一边的实体通过这个关系，则只能对应唯一的一边的实体。比如说，我们新建一个班级表，而每个班级都有多个学生，每个学 生则对应一个班级，班级对学生就是一对多的关系。 多对多 ：指关系两边的实体都可以通过关系对应多个对方的实体。比如在进货模块中，供货商与超市之 间的关系就是多对多的关系，一个供货商可以给多个超市供货，一个超市也可以从多个供货商那里采购 商品。再比如一个选课表，有许多科目，每个科目有很多学生选，而每个学生又可以选择多个科目，这 就是多对多的关系。 ","date":"2022-09-24","objectID":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/:46:2","tags":[],"title":"MySQL索引及调优篇","uri":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/"},{"categories":["mysql"],"content":"8.3 建模分析 ER 模型看起来比较麻烦，但是对我们把控项目整体非常重要。如果你只是开发一个小应用，或许简单设 计几个表够用了，一旦要设计有一定规模的应用，在项目的初始阶段，建立完整的 ER 模型就非常关键了。开发应用项目的实质，其实就是 建模 。 我们设计的案例是 电商业务 ，由于电商业务太过庞大且复杂，所以我们做了业务简化，比如针对 SKU（StockKeepingUnit，库存量单位）和SPU（Standard Product Unit，标准化产品单元）的含义上，我 们直接使用了SKU，并没有提及SPU的概念。本次电商业务设计总共有8个实体，如下所示。 地址实体 用户实体 购物车实体 评论实体 商品实体 商品分类实体 订单实体 订单详情实体 其中， 用户 和 商品分类 是强实体，因为它们不需要依赖其他任何实体。而其他属于弱实体，因为它们 虽然都可以独立存在，但是它们都依赖用户这个实体，因此都是弱实体。知道了这些要素，我们就可以 给电商业务创建 ER 模型了，如图： 在这个图中，地址和用户之间的添加关系，是一对多的关系，而商品和商品详情示一对一的关系，商品和订单是多对多的关系。这个 ER 模型，包括了 8 个实体之间的 8 种关系。 （1）用户可以在电商平台添加多个地址； （2）用户只能拥有一个购物车； （3）用户可以生成多个订单； （4）用户可以发表多条评论； （5）一件商品可以有多条评论； （6）每一个商品分类包含多种商品； （7）一个订单可以包含多个商品，一个商品可以在多个订单里。 （8）订单中又包含多个订单详情，因为一个订单中可能包含不同种类的商品 ","date":"2022-09-24","objectID":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/:46:3","tags":[],"title":"MySQL索引及调优篇","uri":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/"},{"categories":["mysql"],"content":"8.4 ER 模型的细化 有了这个 ER 模型，我们就可以从整体上 理解 电商的业务了。刚刚的 ER 模型展示了电商业务的框架， 但是只包括了订单，地址，用户，购物车，评论，商品，商品分类和订单详情这八个实体，以及它们之间的关系，还不能对应到具体的表，以及表与表之间的关联。我们需要把属性加上 ，用 椭圆 来表示， 这样我们得到的 ER 模型就更加完整了。 因此，我们需要进一步去设计一下这个 ER 模型的各个局部，也就是细化下电商的具体业务流程，然后把它们综合到一起，形成一个完整的 ER 模型。这样可以帮助我们理清数据库的设计思路。 接下来，我们再分析一下各个实体都有哪些属性，如下所示。 （1） 地址实体 包括用户编号、省、市、地区、收件人、联系电话、是否是默认地址。 （2） 用户实体 包括用户编号、用户名称、昵称、用户密码、手机号、邮箱、头像、用户级别。 （3） 购物车实体 包括购物车编号、用户编号、商品编号、商品数量、图片文件url。 （4） 订单实体 包括订单编号、收货人、收件人电话、总金额、用户编号、付款方式、送货地址、下单时间。 （5） 订单详情实体 包括订单详情编号、订单编号、商品名称、商品编号、商品数量。 （6） 商品实体 包括商品编号、价格、商品名称、分类编号、是否销售，规格、颜色。 （7） 评论实体 包括评论id、评论内容、评论时间、用户编号、商品编号 （8） 商品分类实体 包括类别编号、类别名称、父类别编号 这样细分之后，我们就可以重新设计电商业务了，ER 模型如图： ","date":"2022-09-24","objectID":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/:46:4","tags":[],"title":"MySQL索引及调优篇","uri":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/"},{"categories":["mysql"],"content":"8.5 ER 模型图转换成数据表 通过绘制 ER 模型，我们已经理清了业务逻辑，现在，我们就要进行非常重要的一步了：把绘制好的 ER 模型，转换成具体的数据表，下面介绍下转换的原则： （1）一个实体通常转换成一个数据表； （2）一个多对多的关系，通常也转换成一个数据表； 显然添加冗余字段就无法满足了。必须添加一个新的数据表用于描述这种多对多的关系。如果强行添加冗余字段来支持这种多对多的关系的话，会破坏第二范式里的“数据表里的每一行数据都是可唯一标识的”这一句话。 常见的用户表和角色表来举例，一个用户可以对应有多个角色，同时一个角色也对应着多个用户。如果建表的时候，按照在用户表里面添加一个冗余字段 角色ID，我们会看到这样一种情况。 用户表user（主键为id,user_id） id user_id user_name role_id 1 001 张三 01 2 002 李四 02 3 003 王五 03 4 001 张三 02 5 002 李四 03 角色表role(主键为id,role_id) id role_id role_name 1 01 管理员 2 02 测试 3 03 运维 显然，上面表的设计违反了第二范式。 改进后如下： 用户表user id user_id user_name 1 001 张三 2 002 李四 3 003 王五 角色表role id role_id role_name 1 01 管理员 2 02 测试 3 03 运维 用户角色关系表user_role id user_id role_id 1 001 01 2 001 02 3 002 02 4 002 03 5 003 03 这样增加了一个关系表，我们的每张表可读性与易维护性都变高了。关系表是两张实体表建立关系，关系独立出来就是关系表。 （3）一个1对1，或者1对多的关系，往往通过表的 外键 来表达，而不是设计一个新的数据表； 针对一对多、一对一的实体关系，往往采用添加冗余字段来维持关系，而不是新建一个关系表来表述两个实体之间的关系，虽然这样做行得通，但是成本更高 下面正反举例解释： 坏情况： 用户和地址这两个实体的关系就是一对多的关系。如果设计一个新的数据表来刻画两个表的关系———数据表（主键id，用户表的用户编号，地址表的地址编号）。 如果有一个业务：我要查询某一个用户的所有地址信息。针对这样的一个业务场景，如果使用建表的方式来刻画这种关系的话，会这样来完成业务：首先，根据用户表的唯一属性（id或编号）查到**关系表**上的对应的**所有满足要求的地址编号**；之后，再根据查到的地址编号，再去地址表里查询所有满足要求具体地址信息。 好情况： 针对一对多的这样实体关系，不使用建立数据表描述关系，而是使用针对“多”表附属一个“一”表里唯一标识字段，即在地址表里附加一个用户表的唯一标识字段即可。那么针对这样一个业务：“我要查询某一个用户的所有地址信息。”，就会这样做：直接就查询地址表里的对应用户的所有地址信息了。 好坏对比： 坏情况需要维护一张额外的表，使得内存更多地被占用、数据库还得维护又一个聚簇索引、并且为了增加这个关系的查询效率避免全表扫描，需要为用户编号添加唯一索引时，又多了索引维护还有回表查询的过程。 好情况就优势突出了，只是在原来实体，模型上增加了一个冗余字段即可满足业务需求。成本的话，主要集中在可能的地址表里用户编号普通索引维护即回表查询，相较坏情况而言，省去了中间表的所有缺点。 当然一对一和一对多的关系往往都被在ER图分析的时候基本都会被刻画成属性了 （4）属性转换成表的字段。 下面结合前面的ER模型，具体讲解一下怎么运用这些转换的原则，把 ER 模型转换成具体的数据表，从而把抽象出来的数据模型，落实到具体的数据库设计当中。 1. 一个实体转换成一个数据库 先来看一下强实体转换成数据表: 用户实体转换成用户表(user_info)的代码如下所示。 上面这张表的设计谬误很常见：所以设计表的时候，一定要考虑数据表是否符合第三范式。 下面我们再把弱实体转换成数据表： 2. 一个多对多的关系转换成一个数据表 3. 通过外键来表达一对多的关系 4. 把属性转换成表的字段 ","date":"2022-09-24","objectID":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/:46:5","tags":[],"title":"MySQL索引及调优篇","uri":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/"},{"categories":["mysql"],"content":"9. 数据表的设计原则 综合以上内容，总结出数据表设计的一般原则：“三少一多” 1. 数据表的个数越少越好 2. 数据表中的字段个数越少越好 3. 数据表中联合主键的字段个数越少越好 4. 使用主键和外键越多越好 ","date":"2022-09-24","objectID":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/:47:0","tags":[],"title":"MySQL索引及调优篇","uri":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/"},{"categories":["mysql"],"content":"10. 数据库对象编写建议 ","date":"2022-09-24","objectID":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/:48:0","tags":[],"title":"MySQL索引及调优篇","uri":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/"},{"categories":["mysql"],"content":"10.1 关于库 【强制】库的名称必须控制在32个字符以内，只能使用英文字母、数字和下划线，建议以英文字母开头。 【强制】库名中英文一律小写 ，不同单词采用 下划线 分割。须见名知意。 【强制】库的名称格式：业务系统名称_子系统名。 【强制】库名禁止使用关键字（如type,order等）。 【强制】创建数据库时必须显式指定字符集 ，并且字符集只能是utf8或者utf8mb4。 创建数据库SQL举例：CREATE DATABASE crm_fund DEFAULT CHARACTER SET ‘utf8’ ; 【建议】对于程序连接数据库账号，遵循权限最小原则 使用数据库账号只能在一个DB下使用，不准跨库。程序使用的账号 原则上不准有drop权限 。 【建议】临时库以 tmp_ 为前缀，并以日期为后缀； 备份库以 bak_ 为前缀，并以日期为后缀。 ","date":"2022-09-24","objectID":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/:48:1","tags":[],"title":"MySQL索引及调优篇","uri":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/"},{"categories":["mysql"],"content":"10.2 关于表、列 【强制】表和列的名称必须控制在32个字符以内，表名只能使用英文字母、数字和下划线，建议以英文字母开头 。 【强制】表名、列名一律小写，不同单词采用下划线分割。须见名知意。 【强制】表名要求有模块名强相关，同一模块的表名尽量使用 统一前缀 。比如：crm_fund_item 【强制】创建表时必须显式指定字符集 为utf8或utf8mb4。 【强制】表名、列名禁止使用关键字（如type,order等）。 【强制】创建表时必须显式指定表存储引擎类型。如无特殊需求，一律为InnoDB。 【强制】建表必须有comment。 【强制】字段命名应尽可能使用表达实际含义的英文单词或缩写 。如：公司 ID，不要使用 corporation_id, 而用corp_id 即可。 【强制】布尔值类型的字段命名为is_描述 。如member表上表示是否为enabled的会员的字段命 名为 is_enabled。 【强制】禁止在数据库中存储图片、文件等大的二进制数据通常文件很大，短时间内造成数据量快速增长，数据库进行数据库读取时，通常会进行大量的随机IO操作，文件很大时，IO操作很耗时。通常存储于文件服务器，数据库只存储文件地址信息。 【建议】建表时关于主键： 表必须有主键 (1)强制要求主键为id，类型为int或bigint，且为 auto_increment 建议使用unsigned无符号型。 (2)标识表里每一行主体的字段不要设为主键，建议设为其他字段如user_id，order_id等，并建立unique key索引。因为如果设为主键且主键值为随机插入，则会导致innodb内部页分裂和大量随机I/O，性能下降。 【建议】核心表（如用户表）必须有行数据的 创建时间字段（create_time）和 最后更新时间字段（update_time），便于查问题。 【建议】表中所有字段尽量都是 NOT NULL 属性，业务可以根据需要定义 DEFAULT值。因为使用NULL值会存在每一行都会占用额外存储空间、数据迁移容易出错、聚合函数计算结果偏差等问题。 【建议】所有存储相同数据的列名和列类型必须一致（一般作为关联列，如果查询时关联列类型不一致会自动进行数据类型隐式转换，会造成列上的索引失效，导致查询效率降低）。 【建议】中间表（或临时表）用于保留中间结果集，名称以 tmp_ 开头。 备份表用于备份或抓取源表快照，名称以 bak_ 开头。中间表和备份表定期清理。 【示范】一个较为规范的建表语句： CREATE TABLE user_info ( `id` int unsigned NOT NULL AUTO_INCREMENT COMMENT '自增主键', `user_id` bigint(11) NOT NULL COMMENT '用户id', `username` varchar(45) NOT NULL COMMENT '真实姓名', `email` varchar(30) NOT NULL COMMENT '用户邮箱', `nickname` varchar(45) NOT NULL COMMENT '昵称', `birthday` date NOT NULL COMMENT '生日', `sex` tinyint(4) DEFAULT '0' COMMENT '性别', `short_introduce` varchar(150) DEFAULT NULL COMMENT '一句话介绍自己，最多50个汉字', `user_resume` varchar(300) NOT NULL COMMENT '用户提交的简历存放地址', `user_register_ip` int NOT NULL COMMENT '用户注册时的源ip', `create_time` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP COMMENT '创建时间', `update_time` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP COMMENT '修改时间', `user_review_status` tinyint NOT NULL COMMENT '用户资料审核状态，1为通过，2为审核中，3为未 通过，4为还未提交审核', PRIMARY KEY (`id`), UNIQUE KEY `uniq_user_id` (`user_id`), KEY `idx_username`(`username`), KEY `idx_create_time_status`(`create_time`,`user_review_status`) ) ENGINE=InnoDB DEFAULT CHARSET=utf8 COMMENT='网站用户基本信息 【建议】创建表时，可以使用可视化工具。这样可以确保表、字段相关的约定都能设置上。 实际上，我们通常很少自己写 DDL 语句，可以使用一些可视化工具来创建和操作数据库和数据表。 可视化工具除了方便，还能直接帮我们将数据库的结构定义转化成 SQL 语言，方便数据库和数据表结构的导出和导入。 ","date":"2022-09-24","objectID":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/:48:2","tags":[],"title":"MySQL索引及调优篇","uri":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/"},{"categories":["mysql"],"content":"10.3 关于索引 【强制】InnoDB表必须主键为id int/bigint auto_increment，且主键值禁止被更新 。 【强制】InnoDB和MyISAM存储引擎表，索引类型必须为 BTREE 。 【建议】主键的名称以 pk_ 开头，唯一键以 uni_ 或 uk_ 开头，普通索引以 idx_ 开头，一律使用小写格式，以字段的名称或缩写作为后缀。 【建议】多单词组成的columnname，取前几个单词首字母，加末单词组成column_name。如: sample 表 member_id 上的索引：idx_sample_mid。 【建议】单个表上的索引个数不能超过6个 。 【建议】在建立索引时，多考虑建立联合索引，并把区分度最高的字段放在最前面。 【建议】在多表 JOIN 的SQL里，保证被驱动表的连接列上有索引，这样JOIN 执行效率最高。 【建议】建表或加索引时，保证表里互相不存在冗余索引。比如：如果表里已经存在key(a,b)，则key(a)为冗余索引，需要删除。 ","date":"2022-09-24","objectID":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/:48:3","tags":[],"title":"MySQL索引及调优篇","uri":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/"},{"categories":["mysql"],"content":"10.4 SQL编写 【强制】程序端SELECT语句必须指定具体字段名称，禁止写成 *。 【建议】程序端insert语句指定具体字段名称，不要写成INSERT INTO t1 VALUES(…)。 【建议】除静态表或小表（100行以内），DML语句必须有WHERE条件，且使用索引查找。 【建议】INSERT INTO…VALUES(XX),(XX),(XX).. 这里XX的值不要超过5000个。 值过多虽然上线很 快，但会引起主从同步延迟。 【建议】SELECT语句不要使用UNION，推荐使用UNION ALL，并且UNION子句个数限制在5个以 内。 【建议】线上环境，多表 JOIN 不要超过5个表。 【建议】减少使用ORDER BY，和业务沟通能不排序就不排序，或将排序放到程序端去做。ORDER BY、GROUP BY、DISTINCT 这些语句较为耗费CPU，数据库的CPU资源是极其宝贵的。 【建议】包含了ORDER BY、GROUP BY、DISTINCT 这些查询的语句，WHERE 条件过滤出来的结果集请保持在1000行以内，否则SQL会很慢。 【建议】对单表的多次alter操作必须合并为一次，对于超过100W行的大表进行 alter table ，必须经过DBA审核，并在业务低峰期执行，多个alter需整合在一起。 因为alter table会产生表锁，期间阻塞对于该表的所有写入，对于业务可能会产生极大影响。 【建议】批量操作数据时，需要控制事务处理间隔时间，进行必要的sleep。 【建议】事务里包含SQL不超过5个。因为过长的事务会导致锁数据较久，MySQL内部缓存、连接消耗过多等问题。 【建议】事务里更新语句尽量基于主键或UNIQUE KEY，如UPDATE… WHERE id=XX; 否则会产生间隙锁，内部扩大锁定范围，导致系统性能下降，产生死锁。 ","date":"2022-09-24","objectID":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/:48:4","tags":[],"title":"MySQL索引及调优篇","uri":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/"},{"categories":["mysql"],"content":"11. PowerDesigner的使用 PowerDesigner是一款开发人员常用的数据库建模工具，用户利用该软件可以方便地制作 数据流程图 、 概念数据模型 、 物理数据模型 ，它几乎包括了数据库模型设计的全过程，是Sybase公司为企业建模和设 计提供的一套完整的集成化企业级建模解决方案。 ","date":"2022-09-24","objectID":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/:49:0","tags":[],"title":"MySQL索引及调优篇","uri":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/"},{"categories":["mysql"],"content":"11.1 开始界面 当前使用的PowerDesigner版本是16.5的。打开软件即是此页面，可选择Create Model,也可以选择Do Not Show page Again,自行在打开软件后创建也可以！完全看个人的喜好，在此我在后面的学习中不在显示此页面。 “Create Model”的作用类似于普通的一个文件，该文件可以单独存放也可以归类存放。 “Create Project”的作用类似于文件夹，负责把有关联关系的文件集中归类存放。 ","date":"2022-09-24","objectID":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/:49:1","tags":[],"title":"MySQL索引及调优篇","uri":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/"},{"categories":["mysql"],"content":"11.2 概念数据模型 常用的模型有4种，分别是 概念模型(CDM Conceptual Data Model) ， 物理模型（PDM,Physical Data Model） ， 面向对象的模型（OOM Objcet Oriented Model） 和 业务模型（BPM Business Process Model） ，我们先创建概念数据模型。 点击上面的ok，即可出现下图左边的概念模型1，可以自定义概念模型的名字，在概念模型中使用最多的 就是如图所示的Entity(实体),Relationship(关系) Entity实体 选中右边框中Entity这个功能，即可出现下面这个方框，需要注意的是书写name的时候，code自行补全，name可以是英文的也可以是中文的，但是code必须是英文的。 填充实体字段 General中的name和code填好后，就可以点击Attributes（属性）来设置name（名字），code(在数据库中 的字段名)，Data Type(数据类型) ，length(数据类型的长度) Name: 实体名字一般为中文，如论坛用户 Code: 实体代号，一般用英文，如XXXUser Comment:注释，对此实体详细说明 Code属性：代号，一般用英文UID DataType Domain域，表示属性取值范围如可以创建10个字符的地址域 M:Mandatory强制属性，表示该属性必填。不能为空 P:Primary Identifer是否是主标识符，表示实体唯一标识符 D:Displayed显示出来，默认全部勾选 在此上图说明name和code的起名方法 设置主标识符 如果不希望系统自动生成标识符而是手动设置的话，那么切换到Identifiers选项卡，添加一行Identifier， 然后单击左上角的“属性”按钮，然后弹出的标识属性设置对话框中单击“添加行”按钮，选择该标识中使用的属性。例如将学号设置为学生实体的标识。 放大模型 创建好概念数据模型如图所示，但是创建好的字体很小，读者可以按着ctrl键同时滑动鼠标的可滑动按钮 即可放大缩写字体，同时也可以看到主标识符有一个*号的标志，同时也显示出来了，name,Data type和 length这些可见的属性 实体关系 同理创建一个班级的实体（需要特别注意的是，点击完右边功能的按钮后需要点击鼠标指针状态的按钮 或者右击鼠标即可，不然很容易乱操作，这点注意一下就可以了），然后使用Relationship（关系）这个 按钮可以连接学生和班级之间的关系，发生一对多（班级对学生）或者多对一（学生对班级）的关系。 如图所示 需要注意的是点击Relationship这个按钮，就把班级和学生联系起来了，就是一条线，然后双击这条线进 行编辑，在General这块起name和code 上面的name和code起好后就可以在Cardinalities这块查看班级和学生的关系，可以看到班级的一端是一 条线，学生的一端是三条，代表班级对学生是一对多的关系即one对many的关系，点击应用，然后确定 即可 一对多和多对一练习完还有多对多的练习，如下图操作所示，老师实体和上面介绍的一样，自己将 name，data type等等修改成自己需要的即可，满足项目开发需求即可。（comment是解释说明，自己可以写相关的介绍和说明） 多对多需要注意的是自己可以手动点击按钮将关系调整称为多对多的关系many对many的关系，然后点击应用和确定即可 综上即可完成最简单的学生，班级，教师这种概念数据模型的设计，需要考虑数据的类型和主标识码， 是否为空。关系是一对一还是一对多还是多对多的关系，自己需要先规划好再设计，然后就ok了。 ","date":"2022-09-24","objectID":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/:49:2","tags":[],"title":"MySQL索引及调优篇","uri":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/"},{"categories":["mysql"],"content":"11.3 物理数据模型 上面是概念数据模型，下面介绍一下物理数据模型，以后经常使用 的就是物理数据模型。打开 PowerDesigner，然后点击File–\u003eNew Model然后选择如下图所示的物理数据模型，物理数据模型的名字自己起，然后选择自己所使用的数据库即可。 创建好主页面如图所示，但是右边的按钮和概念模型略有差别，物理模型最常用的三个是 table(表) ， view(视图)， reference(关系) ； 鼠标先点击右边table这个按钮然后在新建的物理模型点一下，即可新建一个表，然后双击新建如下图所示，在General的name和code填上自己需要的，点击应用即可），如下图： 然后点击Columns,如下图设置，非常简单，需要注意的就是P（primary主键） , F （foreign key外键） , M（mandatory强制性的，代表不可为空） 这三个。 在此设置学号的自增（MYSQL里面的自增是这个AUTO_INCREMENT），班级编号同理，不多赘述！ 在下面的这个点上对号即可，就设置好了自增 全部完成后如下图所示。 班级物理模型同理如下图所示创建即可 完成后如下图所示 上面的设置好如上图所示，然后下面是关键的地方，点击右边按钮Reference这个按钮，因为是班级对学生是一对多的，所以鼠标从学生拉到班级如下图所示，学生表将发生变化，学生表里面增加了一行，这行是班级表的主键作为学生表的外键，将班级表和学生表联系起来。（仔细观察即可看到区别。） 做完上面的操作，就可以双击中间的一条线，显示如下图，修改name和code即可 但是需要注意的是，修改完毕后显示的结果却如下图所示，并没有办法直接像概念模型那样，修改过后 显示在中间的那条线上面，自己明白即可。 学习了多对一或者一对多的关系，接下来学习多对对的关系，同理自己建好老师表，这里不在叙述，记得老师编号自增，建好如下图所示 下面是多对多关系的关键，由于物理模型多对多的关系需要一个中间表来连接，如下图，只设置一个字段，主键，自增 点击应用，然后设置Columns，只添加一个字段 这是设置字段递增，前面已经叙述过好几次 设置好后如下图所示，需要注意的是有箭头的一方是一，无箭头的一方是多，即一对多的多对一的关系 需要搞清楚，学生也可以有很多老师，老师也可以有很多学生，所以学生和老师都可以是主体； 可以看到添加关系以后学生和教师的关系表前后发生的变化 ","date":"2022-09-24","objectID":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/:49:3","tags":[],"title":"MySQL索引及调优篇","uri":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/"},{"categories":["mysql"],"content":"11.4 概念模型转为物理模型 1：如下图所示先打开概念模型图，然后点击Tool,如下图所示 点开的页面如下所示，name和code已经从概念模型1改成物理模型1了 完成后如下图所示，将自行打开修改的物理模型，需要注意的是这些表的数据类型已经自行改变了，而 且中间表出现两个主键，即双主键 ","date":"2022-09-24","objectID":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/:49:4","tags":[],"title":"MySQL索引及调优篇","uri":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/"},{"categories":["mysql"],"content":"11.5 物理模型转为概念模型 上面介绍了概念模型转物理模型，下面介绍一下物理模型转概念模型（如下图点击操作即可） 然后出现如下图所示界面，然后将物理修改为概念 ，点击应用确认即可 点击确认后将自行打开如下图所示的页面，自己观察有何变化，如果转换为oracle的，数据类型会发生变 化，比如Varchar2等等）； ","date":"2022-09-24","objectID":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/:49:5","tags":[],"title":"MySQL索引及调优篇","uri":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/"},{"categories":["mysql"],"content":"11.6 物理模型导出SQL语句 打开之后如图所示，修改好存在sql语句的位置和生成文件的名称即可 在Selection中选择需要导出的表，然后点击应用和确认即可 完成以后出现如下图所示，可以点击Edit或者close按钮 自此，就完成了导出sql语句，就可以到自己指定的位置查看导出的sql语句了；PowerDesigner在以后在 项目开发过程中用来做需求分析和数据库的设计非常的方便和快捷。 第12章_数据库其它调优策略 ","date":"2022-09-24","objectID":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/:49:6","tags":[],"title":"MySQL索引及调优篇","uri":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/"},{"categories":["mysql"],"content":"1. 数据库调优的措施 ","date":"2022-09-24","objectID":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/:50:0","tags":[],"title":"MySQL索引及调优篇","uri":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/"},{"categories":["mysql"],"content":"1.1 调优的目标 尽可能节省系统资源 ，以便系统可以提供更大负荷的服务。（吞吐量更大） 合理的结构设计和参数调整，以提高用户操作响应的速度。（响应速度更快） 减少系统的瓶颈，提高MySQL数据库整体的性能。 ","date":"2022-09-24","objectID":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/:50:1","tags":[],"title":"MySQL索引及调优篇","uri":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/"},{"categories":["mysql"],"content":"1.2 如何定位调优问题 如何确定呢？一般情况下，有如下几种方式： ","date":"2022-09-24","objectID":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/:50:2","tags":[],"title":"MySQL索引及调优篇","uri":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/"},{"categories":["mysql"],"content":"1.3 调优的维度和步骤 我们需要调优的对象是整个数据库管理系统，它不仅包括 SQL 查询，还包括数据库的部署配置、架构等。从这个角度来说，我们思考的维度就不仅仅局限在 SQL 优化上了。通过如下的步骤我们进行梳理： 第1步：选择适合的 DBMS 第2步：优化表设计 第3步：优化逻辑查询 第4步：优化物理查询 物理查询优化是在确定了逻辑查询优化之后，采用物理优化技术（比如索引等），通过计算代价模型对 各种可能的访问路径进行估算，从而找到执行方式中代价最小的作为执行计划。在这个部分中，我们需要掌握的重点是对索引的创建和使用。 第5步：使用 Redis 或 Memcached 作为缓存 除了可以对 SQL 本身进行优化以外，我们还可以请外援提升查询的效率。 因为数据都是存放到数据库中，我们需要从数据库层中取出数据放到内存中进行业务逻辑的操作，当用户量增大的时候，如果频繁地进行数据查询，会消耗数据库的很多资源。如果我们将常用的数据直接放到内存中，就会大幅提升查询的效率。 键值存储数据库可以帮我们解决这个问题。 常用的键值存储数据库有 Redis 和 Memcached，它们都可以将数据存放到内存中。 第6步：库级优化 但需要注意的是，分拆在提升数据库性能的同时，也会增加维护和使用成本。 ","date":"2022-09-24","objectID":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/:50:3","tags":[],"title":"MySQL索引及调优篇","uri":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/"},{"categories":["mysql"],"content":"2. 优化MySQL服务器 ","date":"2022-09-24","objectID":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/:51:0","tags":[],"title":"MySQL索引及调优篇","uri":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/"},{"categories":["mysql"],"content":"2.1 优化服务器硬件 服务器的硬件性能直接决定着MySQL数据库的性能。硬件的性能瓶颈直接决定MySQL数据库的运行速度 和效率。针对性能瓶颈提高硬件配置，可以提高MySQL数据库查询、更新的速度。 （1） 配置较大的内存 。足够大的显存是提高MySQL数据库性能的方法之一。内存的速度比磁盘I/O快得多，可以通过增加系统的缓冲区容量使数据在内存中停留的时间更长，以减少磁盘I/O。 （2） 配置高速磁盘系统 ，以减少读盘的等待时间，提高响应速度。磁盘的I/O能力，也就是它的寻道能力，目前的SCSI高速旋转的是7200转/分钟，这样的速度，一旦访问的用户量上去，磁盘的压力就会过大，如果是每天的网站pv (page view) 在150w，这样的一般的配置就无法满足这样的需求了。现在SSD盛行，在SSD上随机访问和顺序访问性能差不多，使用SSD可以减少随机IO带来的性能损耗。 （3） 合理分布磁盘I/O，把磁盘I/O分散在多个设备，以减少资源竞争，提高冰箱操作能力。 （4） 配置多处理器, MySQL是多线程的数据库，多处理器可同时执行多个线程。 ","date":"2022-09-24","objectID":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/:51:1","tags":[],"title":"MySQL索引及调优篇","uri":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/"},{"categories":["mysql"],"content":"2.2 优化MySQL的参数 innodb_buffer_pool_size ：这个参数是Mysql数据库最重要的参数之一，表示InnoDB类型的 表 和索引的最大缓存 。它不仅仅缓存 索引数据 ，还会缓存 表的数据 。这个值越大，查询的速度就会越 快。但是这个值太大会影响操作系统的性能。 key_buffer_size ：表示 索引缓冲区的大小 。索引缓冲区是所有的 线程共享 。增加索引缓冲区可 以得到更好处理的索引（对所有读和多重写）。当然，这个值不是越大越好，它的大小取决于内存 的大小。如果这个值太大，就会导致操作系统频繁换页，也会降低系统性能。对于内存在 4GB 左右 的服务器该参数可设置为 256M 或 384M 。 table_cache ：表示 同时打开的表的个数 。这个值越大，能够同时打开的表的个数越多。物理内 存越大，设置就越大。默认为2402，调到512-1024最佳。这个值不是越大越好，因为同时打开的表 太多会影响操作系统的性能。 query_cache_size ：表示 查询缓冲区的大小 。可以通过在MySQL控制台观察，如果 Qcache_lowmem_prunes的值非常大，则表明经常出现缓冲不够的情况，就要增加Query_cache_size 的值；如果Qcache_hits的值非常大，则表明查询缓冲使用非常频繁，如果该值较小反而会影响效 率，那么可以考虑不用查询缓存；Qcache_free_blocks，如果该值非常大，则表明缓冲区中碎片很 多。MySQL8.0之后失效。该参数需要和query_cache_type配合使用。 query_cache_type 的值是0时，所有的查询都不使用查询缓存区。但是query_cache_type=0并不 会导致MySQL释放query_cache_size所配置的缓存区内存。 当query_cache_type=1时，所有的查询都将使用查询缓存区，除非在查询语句中指定 SQL_NO_CACHE ，如SELECT SQL_NO_CACHE * FROM tbl_name。 当query_cache_type=2时，只有在查询语句中使用 SQL_CACHE 关键字，查询才会使用查询缓 存区。使用查询缓存区可以提高查询的速度，这种方式只适用于修改操作少且经常执行相同的 查询操作的情况。 sort_buffer_size：表示每个需要进行排序的线程分配的缓冲区的大小。增加这个参数的值可以提高 ORDER BY 或 GROUP BY 操作的速度。默认数值是2097144字节（约2MB）。对于内存在4GB 左右的服务器推荐设置为6-8M，如果有100个连接，那么实际分配的总共排序缓冲区大小为 100 × 6 ＝ 600MB 。 join_buffer_size = 8M：表示联合查询操作所能使用的缓冲区大小，和sort_buffer_size一样， 该参数对应的分配内存也是每个连接独享。 read_buffer_size：表示 每个线程连续扫描时为扫描的每个表分配的缓冲区的大小（字节）。当线程从表中连续读取记录时需要用到这个缓冲区。SET SESSION read_buffer_size=n可以临时设置该参数的值。默认为64K，可以设置为4M。 innodb_flush_log_at_trx_commit：表示何时将缓冲区的数据写入日志文件，并且将日志文件写入磁盘中。该参数对于innoDB引擎非常重要。该参数有3个值，分别为0、1和2。该参数的默认值为1。 值为 0 时，表示 每秒1次 的频率将数据写入日志文件并将日志文件写入磁盘。每个事务的 commit 并不会触发前面的任何操作。该模式速度最快，但不太安全，mysqld进程的崩溃会导致上一秒钟所有事务数据的丢失。 值为 1 时，表示 每次提交事务时将数据写入日志文件并将日志文件写入磁盘进行同步。该模式是最安全的，但也是最慢的一种方式。因为每次事务提交或事务外的指令都需要把日志写入（flush）硬盘。 值为 2 时，表示 每次提交事务时将数据写入日志文件，每隔1秒将日志文件写入磁盘。该模式速度较快，也比0安全，只有在操作系统崩溃或者系统断电的情况下，上一秒钟所有事务数据才可能丢失。 innodb_log_buffer_size：这是 InnoDB 存储引擎的 事务日志所使用的缓冲区 。为了提高性能， 也是先将信息写入 Innodb Log Buffer 中，当满足 innodb_flush_log_trx_commit 参数所设置的相应条 件（或者日志缓冲区写满）之后，才会将日志写到文件（或者同步到磁盘）中。 max_connections ：表示 允许连接到MySQL数据库的最大数量 ，默认值是 151 。如果状态变量 connection_errors_max_connections 不为零，并且一直增长，则说明不断有连接请求因数据库连接数已达到允许最大值而失败，这是可以考虑增大 max_connections 的值。在Linux 平台下，性能好的服务器，支持 500-1000 个连接不是难事，需要根据服务器性能进行评估设定。这个连接数不是越大越好 ，因为这些连接会浪费内存的资源。过多的连接可能会导致MySQL服务器僵死。 back_log：用于控制MySQL监听TCP端口时设置的积压请求栈大小 。如果MySql的连接数达到 max_connections时，新来的请求将会被存在堆栈中，以等待某一连接释放资源，该堆栈的数量即 back_log，如果等待连接的数量超过back_log，将不被授予连接资源，将会报错。5.6.6 版本之前默 认值为 50 ， 之后的版本默认为 50 +（max_connections / 5）， 对于Linux系统推荐设置为小于512 的整数，但最大不超过900。 如果需要数据库在较短的时间内处理大量连接请求， 可以考虑适当增大back_log 的值。 thread_cache_size ： 线程池缓存线程数量的大小 ，当客户端断开连接后将当前线程缓存起来，当在接到新的连接请求时快速响应无需创建新的线程。这尤其对那些使用短连接的应用程序来说可以极大的提高创建连接的效率。那么为了提高性能可以增大该参数的值。默认为60，可以设置为 120。 可以通过如下几个MySQL状态值来适当调整线程池的大小： mysql\u003e show global status like 'Thread%'; +-------------------+-------+ | Variable_name | Value | +-------------------+-------+ | Threads_cached | 2 | | Threads_connected | 1 | | Threads_created | 3 | | Threads_running | 2 | +-------------------+-------+ 4 rows in set (0.01 sec) 当 Threads_cached 越来越少，但 Threads_connected 始终不降，且 Threads_created 持续升高，可适当增加 thread_cache_size 的大小。 wait_timeout ：指定一个请求的最大连接时间 ，对于4GB左右内存的服务器可以设置为5-10。 interactive_timeout ：表示服务器在关闭连接前等待行动的秒数。 这里给出一份my.cnf的参考配置： mysqld] port = 3306 serverid = 1 socket = /tmp/mysql.sock skip-locking #避免MySQL的外部锁定，减少出错几率增强稳定性。 skip-name-resolve #禁止MySQL对外部连接进行DNS解析，使用这一选项可以消除MySQL进行DNS解析的时间。但需要注意，如果开启该选项，则所有远程主机连接授权都要使用IP地址方式，否则MySQL将无法正常处理连接请求！ back_log = 384 key_buffer_size = 256M max_allowed_packet = 4M thread_stack = 256K table_cache = 128K sort_buffer_size = 6M read_buffer_size = 4M read_rnd_buffer_size=16M join_buffer_size = 8M myisam_sort_buffer_size =64M table_cache = 512 thread_cache_size = 64 query_cache_size = 64M tmp_table_size = 256M max_connections = 768 max_connect_errors = 10000000 wait_timeout = 10 thread_concurrency = 8 #该参数取值为服务器逻辑CPU数量*2，在本例中，服务器有2颗物理CPU，而每颗物理CPU又支持H.T超线程，所以实际取值为4*2=8 skip-networking #开启该选项可以彻底关闭MySQL的TCP/IP连接方式，如果WEB服务器是以远程连接的方式访问MySQL数据库服务器则不要开启该选项！否则将无法正常连接！ table_cache=1024 innodb_additional_mem_pool_size=4M #默认为2","date":"2022-09-24","objectID":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/:51:2","tags":[],"title":"MySQL索引及调优篇","uri":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/"},{"categories":["mysql"],"content":"3. 优化数据库结构 ","date":"2022-09-24","objectID":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/:52:0","tags":[],"title":"MySQL索引及调优篇","uri":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/"},{"categories":["mysql"],"content":"3.1 拆分表：冷热数据分离 举例1： 会员members表 存储会员登录认证信息，该表中有很多字段，如id、姓名、密码、地址、电话、个人描述字段。其中地址、电话、个人描述等字段并不常用，可以将这些不常用的字段分解出另一个表。将这个表取名叫members_detail，表中有member_id、address、telephone、description等字段。这样就把会员表分成了两个表，分别为 members表 和 members_detail表 。 创建这两个表的SQL语句如下： CREATE TABLE members ( id int(11) NOT NULL AUTO_INCREMENT, username varchar(50) DEFAULT NULL, password varchar(50) DEFAULT NULL, last_login_time datetime DEFAULT NULL, last_login_ip varchar(100) DEFAULT NULL, PRIMARY KEY(Id) ); CREATE TABLE members_detail ( Member_id int(11) NOT NULL DEFAULT 0, address varchar(255) DEFAULT NULL, telephone varchar(255) DEFAULT NULL, description text ); 如果需要查询会员的基本信息或详细信息，那么可以用会员的id来查询。如果需要将会员的基本信息和详细信息同时显示，那么可以将members表和members_detail表进行联合查询，查询语句如下： SELECT * FROM members LEFT JOIN members_detail on members.id = members_detail.member_id; 通过这种分解可以提高表的查询效率。对于字段很多且有些字段使用不频繁的表，可以通过这种分解的方式来优化数据库的性能。 ","date":"2022-09-24","objectID":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/:52:1","tags":[],"title":"MySQL索引及调优篇","uri":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/"},{"categories":["mysql"],"content":"3.2 增加中间表 举例1： 学生信息表 和 班级表 的SQL语句如下： CREATE TABLE `class` ( `id` INT(11) NOT NULL AUTO_INCREMENT, `className` VARCHAR(30) DEFAULT NULL, `address` VARCHAR(40) DEFAULT NULL, `monitor` INT NULL, PRIMARY KEY (`id`) ) ENGINE=INNODB AUTO_INCREMENT=1 DEFAULT CHARSET=utf8; CREATE TABLE `student` ( `id` INT(11) NOT NULL AUTO_INCREMENT, `stuno` INT NOT NULL, `name` VARCHAR(20) DEFAULT NULL, `age` INT(3) DEFAULT NULL, `classId` INT(11) DEFAULT NULL, PRIMARY KEY (`id`) ) ENGINE=INNODB AUTO_INCREMENT=1 DEFAULT CHARSET=utf8; 现在有一个模块需要经常查询带有学生名称（name）、学生所在班级名称（className）、学生班级班长（monitor）的学生信息。根据这种情况可以创建一个 temp_student 表。temp_student表中存储学生名称（stu_name）、学生所在班级名称（className）和学生班级班长（monitor）信息。创建表的语句如下： CREATE TABLE `temp_student` ( `id` INT(11) NOT NULL AUTO_INCREMENT, `stu_name` INT NOT NULL , `className` VARCHAR(20) DEFAULT NULL, `monitor` INT(3) DEFAULT NULL, PRIMARY KEY (`id`) ) ENGINE=INNODB AUTO_INCREMENT=1 DEFAULT CHARSET=utf8; 接下来，从学生信息表和班级表中查询相关信息存储到临时表中： insert into temp_student(stu_name,className,monitor) select s.name,c.className,c.monitor from student as s,class as c where s.classId = c.id 以后，可以直接从temp_student表中查询学生名称、班级名称和班级班长，而不用每次都进行联合查询。这样可以提高数据库的查询速度。 ","date":"2022-09-24","objectID":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/:52:2","tags":[],"title":"MySQL索引及调优篇","uri":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/"},{"categories":["mysql"],"content":"3.3 增加冗余字段 设计数据库表时应尽量遵循范式理论的规约，尽可能减少冗余字段，让数据库设计看起来精致、优雅。 但是，合理地加入冗余字段可以提高查询速度。 表的规范化程度越高，表与表之间的关系就越多，需要连接查询的情况也就越多。尤其在数据量大，而 且需要频繁进行连接的时候，为了提升效率，我们也可以考虑增加冗余字段来减少连接。 这部分内容在《第11章_数据库的设计规范》章节中 反范式化小节中具体展开讲解了。这里省略。 ","date":"2022-09-24","objectID":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/:52:3","tags":[],"title":"MySQL索引及调优篇","uri":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/"},{"categories":["mysql"],"content":"3.4 优化数据类型 情况1：对整数类型数据进行优化。 遇到整数类型的字段可以用 INT 型 。这样做的理由是，INT 型数据有足够大的取值范围，不用担心数据超出取值范围的问题。刚开始做项目的时候，首先要保证系统的稳定性，这样设计字段类型是可以的。但在数据量很大的时候，数据类型的定义，在很大程度上会影响到系统整体的执行效率。 对于 非负型 的数据（如自增ID、整型IP）来说，要优先使用无符号整型 UNSIGNED 来存储。因为无符号相对于有符号，同样的字节数，存储的数值范围更大。如tinyint有符号为-128-127，无符号为0-255，多出一倍的存储空间。 情况2：既可以使用文本类型也可以使用整数类型的字段，要选择使用整数类型。 跟文本类型数据相比，大整数往往占用更少的存储空间 ，因此，在存取和比对的时候，可以占用更少的 内存空间。所以，在二者皆可用的情况下，尽量使用整数类型，这样可以提高查询的效率。如：将IP地址转换成整型数据。 情况3：避免使用TEXT、BLOB数据类型 情况4：避免使用ENUM类型 修改ENUM值需要使用ALTER语句。 ENUM类型的ORDER BY 操作效率低，需要额外操作。使用TINYINT来代替ENUM类型。 情况5：使用TIMESTAMP存储时间 TIMESTAMP存储的时间范围1970-01-01 00:00:01 ~ 2038-01_19-03:14:07。TIMESTAMP使用4字节，DATETIME使用8个字节，同时TIMESTAMP具有自动赋值以及自动更新的特性。 情况6：用DECIMAL代替FLOAT和DOUBLE存储精确浮点数 非精准浮点： float, double 精准浮点：decimal Decimal类型为精准浮点数，在计算时不会丢失精度，尤其是财务相关的金融类数据。占用空间由定义的宽度决定，每4个字节可以存储9位数字，并且小数点要占用一个字节。可用于存储比bigint更大的整型数据。 总之，遇到数据量大的项目时，一定要在充分了解业务需求的前提下，合理优化数据类型，这样才能充分发挥资源的效率，使系统达到最优。 ","date":"2022-09-24","objectID":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/:52:4","tags":[],"title":"MySQL索引及调优篇","uri":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/"},{"categories":["mysql"],"content":"3.5 优化插入记录的速度 插入记录时，影响插入速度的主要是索引、唯一性校验、一次插入记录条数等。根据这些情况可以分别进行优化。这里我们分为MyISAM引擎和InnoDB引擎来讲。 1. MyISAM引擎的表： ① 禁用索引 ② 禁用唯一性检查 ③ 使用批量插入 插入多条记录时，可以使用一条INSERT语句插入一条数据，也可以使用一条INSERT语句插入多条数据。插入一条记录的INSERT语句情形如下： insert into student values(1,'zhangsan',18,1); insert into student values(2,'lisi',17,1); insert into student values(3,'wangwu',17,1); insert into student values(4,'zhaoliu',19,1); 使用一条INSERT语句插入多条记录的情形如下： insert into student values (1,'zhangsan',18,1), (2,'lisi',17,1), (3,'wangwu',17,1), (4,'zhaoliu',19,1); 第2种情形的插入速度要比第1种情形快。 ④ 使用LOAD DATA INFILE 批量导入 当需要批量导入数据时，如果能用LOAD DATA INFILE语句，就尽量使用。因为LOAD DATA INFILE语句导入数据的速度比INSERT语句块。 2. InnoDB引擎的表： ① 禁用唯一性检查 插入数据之前执行set unique_checks=0来禁止对唯一索引的检查，数据导入完成之后再运行set unique_check=1。这个和MyISAM引擎的使用方法一样。 ② 禁用外键检查 ③ 禁止自动提交 ","date":"2022-09-24","objectID":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/:52:5","tags":[],"title":"MySQL索引及调优篇","uri":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/"},{"categories":["mysql"],"content":"3.6 使用非空约束 ","date":"2022-09-24","objectID":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/:52:6","tags":[],"title":"MySQL索引及调优篇","uri":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/"},{"categories":["mysql"],"content":"3.7 分析表、检查表与优化表 MySQL提供了分析表、检查表和优化表的语句。分析表主要是分析关键字的分布，检查表主要是检查表是否存在错误，优化表主要是消除删除或者更新造成的空间浪费。 1. 分析表 MySQL中提供了ANALYZE TABLE语句分析表，ANALYZE TABLE语句的基本语法如下： ANALYZE [LOCAL | NO_WRITE_TO_BINLOG] TABLE tbl_name[,tbl_name]… 默认的，MySQL服务会将 ANALYZE TABLE语句写到binlog中，以便在主从架构中，从服务能够同步数据。 可以添加参数LOCAL 或者 NO_WRITE_TO_BINLOG 取消将语句写到binlog中。 使用 ANALYZE TABLE 分析表的过程中，数据库系统会自动对表加一个 只读锁 。在分析期间，只能读取表中的记录，不能更新和插入记录。ANALYZE TABLE语句能够分析InnoDB和MyISAM类型的表，但是不能作用于视图。 ANALYZE TABLE分析后的统计结果会反应到 cardinality 的值，该值统计了表中某一键所在的列不重复 的值的个数。该值越接近表中的总行数，则在表连接查询或者索引查询时，就越优先被优化器选择使用。也就是索引列的cardinality的值与表中数据的总条数差距越大，即使查询的时候使用了该索引作为查询条件，存储引擎实际查询的时候使用的概率就越小。下面通过例子来验证下。cardinality可以通过 SHOW INDEX FROM 表名查看。 mysql\u003e ANALYZE TABLE user; +--------------+---------+----------+---------+ | Table | Op | Msg_type |Msg_text | +--------------+---------+----------+---------+ | atguigu.user | analyze | status | Ok | +--------------+----------+---------+---------+ 上面结果显示的信息说明如下： Table: 表示分析的表的名称。 Op: 表示执行的操作。analyze表示进行分析操作。 Msg_type: 表示信息类型，其值通常是状态 (status) 、信息 (info) 、注意 (note) 、警告 (warning) 和 错误 (error) 之一。 Msg_text: 显示信息。 2. 检查表 MySQL中可以使用 CHECK TABLE 语句来检查表。CHECK TABLE语句能够检查InnoDB和MyISAM类型的表 是否存在错误。CHECK TABLE语句在执行过程中也会给表加上 只读锁 。 对于MyISAM类型的表，CHECK TABLE语句还会更新关键字统计数据。而且，CHECK TABLE也可以检查视图是否有错误，比如在视图定义中被引用的表已不存在。该语句的基本语法如下： CHECK TABLE tbl_name [, tbl_name] ... [option] ... option = {QUICK | FAST | MEDIUM | EXTENDED | CHANGED} 其中，tbl_name是表名；option参数有5个取值，分别是QUICK、FAST、MEDIUM、EXTENDED和 CHANGED。各个选项的意义分别是： QUICK ：不扫描行，不检查错误的连接。 FAST ：只检查没有被正确关闭的表。 CHANGED ：只检查上次检查后被更改的表和没有被正确关闭的表。 MEDIUM ：扫描行，以验证被删除的连接是有效的。也可以计算各行的关键字校验和，并使用计算出的校验和验证这一点。 EXTENDED ：对每行的所有关键字进行一个全面的关键字查找。这可以确保表是100%一致的，但 是花的时间较长。 option只对MyISAM类型的表有效，对InnoDB类型的表无效。比如： 该语句对于检查的表可能会产生多行信息。最后一行有一个状态的 Msg_type 值，Msg_text 通常为 OK。 如果得到的不是 OK，通常要对其进行修复；是 OK 说明表已经是最新的了。表已经是最新的，意味着存 储引擎对这张表不必进行检查。 3. 优化表 方式1：OPTIMIZE TABLE MySQL中使用 OPTIMIZE TABLE 语句来优化表。但是，OPTILMIZE TABLE语句只能优化表中的 VARCHAR 、 BLOB 或 TEXT 类型的字段。一个表使用了这些字段的数据类型，若已经 删除 了表的一大部分数据，或者已经对含有可变长度行的表（含有VARCHAR、BLOB或TEXT列的表）进行了很多 更新 ，则应使用OPTIMIZE TABLE来重新利用未使用的空间，并整理数据文件的 碎片 。 OPTIMIZE TABLE 语句对InnoDB和MyISAM类型的表都有效。该语句在执行过程中也会给表加上 只读锁 。 OPTILMIZE TABLE语句的基本语法如下： OPTIMIZE [LOCAL | NO_WRITE_TO_BINLOG] TABLE tbl_name [, tbl_name] ... LOCAL | NO_WRITE_TO_BINLOG关键字的意义和分析表相同，都是指定不写入二进制日志。 执行完毕，Msg_text显示 ‘numysql.SYS_APP_USER’, ‘optimize’, ‘note’, ‘Table does not support optimize, doing recreate + analyze instead’ 原因是我服务器上的MySQL是InnoDB存储引擎。 到底优化了没有呢？看官网！ MySQL :: MySQL 8.0 Reference Manual :: 13.7.3.4 OPTIMIZE TABLE Statement 在MyISAM中，是先分析这张表，然后会整理相关的MySQL datafile，之后回收未使用的空间；在InnoDB 中，回收空间是简单通过Alter table进行整理空间。在优化期间，MySQL会创建一个临时表，优化完成之 后会删除原始表，然后会将临时表rename成为原始表。 说明： 在多数的设置中，根本不需要运行OPTIMIZE TABLE。即使对可变长度的行进行了大量的更新，也不需要经常运行， 每周一次 或 每月一次 即可，并且只需要对 特定的表 运行。 方式二：使用mysqlcheck命令 3.8 小结 上述这些方法都是有利有弊的。比如： 修改数据类型，节省存储空间的同时，你要考虑到数据不能超过取值范围； 增加冗余字段的时候，不要忘了确保数据一致性； 把大表拆分，也意味着你的查询会增加新的连接，从而增加额外的开销和运维的成本。 因此，你一定要结合实际的业务需求进行权衡。 ","date":"2022-09-24","objectID":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/:52:7","tags":[],"title":"MySQL索引及调优篇","uri":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/"},{"categories":["mysql"],"content":"4. 大表优化 当MySQL单表记录数过大时，数据库的CRUD性能会明显下降，一些常见的优化措施如下： ","date":"2022-09-24","objectID":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/:53:0","tags":[],"title":"MySQL索引及调优篇","uri":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/"},{"categories":["mysql"],"content":"4.1 限定查询的范围 禁止不带任何限制数据范围条件的查询语句。比如：我们当用户在查询订单历史的时候，我们可以控制在一个月的范围内； ","date":"2022-09-24","objectID":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/:53:1","tags":[],"title":"MySQL索引及调优篇","uri":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/"},{"categories":["mysql"],"content":"4.2 读/写分离 经典的数据库拆分方案，主库负责写，从库负责读。 一主一从模式： 双主双从模式： ","date":"2022-09-24","objectID":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/:53:2","tags":[],"title":"MySQL索引及调优篇","uri":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/"},{"categories":["mysql"],"content":"4.3 垂直拆分 当数据量级达到 千万级 以上时，有时候我们需要把一个数据库切成多份，放到不同的数据库服务器上， 减少对单一数据库服务器的访问压力。 如果数据库的数据表过多，可以采用垂直分库的方式，将关联的数据库部署在同一个数据库上。 如果数据库中的列过多，可以采用垂直分表的方式，将一张数据表分拆成多张数据表，把经常一起使用的列放在同一张表里。 垂直拆分的优点： 可以使得列数据变小，在查询时减少读取的Block数，减少I/O次数。此外，垂直分区可以简化表的结构，易于维护。 垂直拆分的缺点： 主键会出现冗余，需要管理冗余列，并会引起 JOIN 操作。此外，垂直拆分会让事务变得更加复杂。 ","date":"2022-09-24","objectID":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/:53:3","tags":[],"title":"MySQL索引及调优篇","uri":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/"},{"categories":["mysql"],"content":"4.4 水平拆分 下面补充一下数据库分片的两种常见方案： 客户端代理： 分片逻辑在应用端，封装在jar包中，通过修改或者封装JDBC层来实现。 当当网的 Sharding-JDBC 、阿里的TDDL是两种比较常用的实现。 **中间件代理： 在应用和数据中间加了一个代理层。分片逻辑统一维护在中间件服务中。**我们现在谈的 Mycat 、360的Atlas、网易的DDB等等都是这种架构的实现。 ","date":"2022-09-24","objectID":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/:53:4","tags":[],"title":"MySQL索引及调优篇","uri":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/"},{"categories":["mysql"],"content":"5. 其它调优策略 ","date":"2022-09-24","objectID":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/:54:0","tags":[],"title":"MySQL索引及调优篇","uri":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/"},{"categories":["mysql"],"content":"5.1 服务器语句超时处理 在MySQL 8.0中可以设置服务器语句超时的限制，单位可以达到毫秒级别。当中断的执行语句超过设置的毫秒数后，服务器将终止查询影响不大的事务或连接，然后将错误报给客户端。 设置服务器语句超时的限制，可以通过设置系统变量 MAX_EXECUTION_TIME 来实现。默认情况下，MAX_EXECUTION_TIME的值为0，代表没有时间限制。例如： SET GLOBAL MAX_EXECUTION_TIME=2000; SET SESSION MAX_EXECUTION_TIME=2000; #指定该会话中SELECT语句的超时时间 ","date":"2022-09-24","objectID":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/:54:1","tags":[],"title":"MySQL索引及调优篇","uri":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/"},{"categories":["mysql"],"content":"5.2 创建全局通用表空间 ","date":"2022-09-24","objectID":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/:54:2","tags":[],"title":"MySQL索引及调优篇","uri":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/"},{"categories":["mysql"],"content":"5.3 MySQL 8.0新特性：隐藏索引对调优的帮助 ","date":"2022-09-24","objectID":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/:54:3","tags":[],"title":"MySQL索引及调优篇","uri":"/mysql%E7%B4%A2%E5%BC%95%E5%8F%8A%E8%B0%83%E4%BC%98%E7%AF%87/"},{"categories":["mysql"],"content":"[toc] 第04章_逻辑架构 ","date":"2022-09-05","objectID":"/mysql%E6%9E%B6%E6%9E%84%E7%AF%87/:0:0","tags":[],"title":"MySQL架构篇","uri":"/mysql%E6%9E%B6%E6%9E%84%E7%AF%87/"},{"categories":["mysql"],"content":"1. 逻辑架构剖析 ","date":"2022-09-05","objectID":"/mysql%E6%9E%B6%E6%9E%84%E7%AF%87/:1:0","tags":[],"title":"MySQL架构篇","uri":"/mysql%E6%9E%B6%E6%9E%84%E7%AF%87/"},{"categories":["mysql"],"content":"1.1 服务器处理客户端请求 首先MySQL是典型的C/S架构，即Client/Server 架构，服务端程序使用的mysqld。 不论客户端进程和服务器进程是采用哪种方式进行通信，最后实现的效果是：客户端进程向服务器进程发送一段文本（SQL语句），服务器进程处理后再向客户端进程发送一段文本（处理结果）。 那服务器进程对客户端进程发送的请求做了什么处理，才能产生最后的处理结果呢？这里以查询请求为 例展示： 下面具体展开如下： ","date":"2022-09-05","objectID":"/mysql%E6%9E%B6%E6%9E%84%E7%AF%87/:1:1","tags":[],"title":"MySQL架构篇","uri":"/mysql%E6%9E%B6%E6%9E%84%E7%AF%87/"},{"categories":["mysql"],"content":"1.2 Connectors Connectors, 指的是不同语言中与SQL的交互。MySQL首先是一个网络程序，在TCP之上定义了自己的应用层协议。所以要使用MySQL，我们可以编写代码，跟MySQL Server建立TCP连接，之后按照其定义好的协议进行交互。或者比较方便的方法是调用SDK，比如Native C API、JDBC、PHP等各语言MySQL Connectors,或者通过ODBC。但通过SDK来访问MySQL，本质上还是在TCP连接上通过MySQL协议跟MySQL进行交互 接下来的MySQL Server结构可以分为如下三层： ","date":"2022-09-05","objectID":"/mysql%E6%9E%B6%E6%9E%84%E7%AF%87/:1:2","tags":[],"title":"MySQL架构篇","uri":"/mysql%E6%9E%B6%E6%9E%84%E7%AF%87/"},{"categories":["mysql"],"content":"1.3 第一层：连接层 系统（客户端）访问 MySQL 服务器前，做的第一件事就是建立 TCP 连接。经过三次握手建立连接成功后， MySQL 服务器对 TCP 传输过来的账号密码做身份认证、权限获取。 用户名或密码不对，会收到一个Access denied for user错误，客户端程序结束执行 用户名密码认证通过，会从权限表查出账号拥有的权限与连接关联，之后的权限判断逻辑，都将依赖于此时读到的权限 TCP 连接收到请求后，必须要分配给一个线程专门与这个客户端的交互。所以还会有个线程池，去走后面的流程。每一个连接从线程池中获取线程，省去了创建和销毁线程的开销。 所以连接管理的职责是负责认证、管理连接、获取权限信息。 ","date":"2022-09-05","objectID":"/mysql%E6%9E%B6%E6%9E%84%E7%AF%87/:1:3","tags":[],"title":"MySQL架构篇","uri":"/mysql%E6%9E%B6%E6%9E%84%E7%AF%87/"},{"categories":["mysql"],"content":"1.4 第二层：服务层 第二层架构主要完成大多数的核心服务功能，如SQL接口，并完成缓存的查询，SQL的分析和优化及部分内置函数的执行。所有跨存储引擎的功能也在这一层实现，如过程、函数等。 在该层，服务器会解析查询并创建相应的内部解析树，并对其完成相应的优化：如确定查询表的顺序，是否利用索引等，最后生成相应的执行操作。 如果是SELECT语句，服务器还会查询内部的缓存。如果缓存空间足够大，这样在解决大量读操作的环境中能够很好的提升系统的性能。 SQL Interface: SQL接口 接收用户的SQL命令，并且返回用户需要查询的结果。比如SELECT ... FROM就是调用SQL Interface MySQL支持DML（数据操作语言）、DDL（数据定义语言）、存储过程、视图、触发器、自定义函数等多种SQL语言接口 Parser: 解析器 在解析器中对 SQL 语句进行语法分析、语义分析。将SQL语句分解成数据结构，并将这个结构传递到后续步骤，以后SQL语句的传递和处理就是基于这个结构的。如果在分解构成中遇到错误，那么就说明这个SQL语句是不合理的。 在SQL命令传递到解析器的时候会被解析器验证和解析，并为其创建语法树 ，并根据数据字典丰富查询语法树，会验证该客户端是否具有执行该查询的权限。创建好语法树后，MySQL还会对SQl查询进行语法上的优化，进行查询重写。 ps: 所以在连接数据库的时候权限一般不要给的太高，一般只是应用数据的话，只需要赋予用户不能对系统数据库进行操作的权限 Optimizer: 查询优化器 SQL语句在语法解析之后、查询之前会使用查询优化器确定 SQL 语句的执行路径，生成一个执行计划 。 这个执行计划表明应该使用哪些索引进行查询（全表检索还是使用索引检索），表之间的连接顺序如何，最后会按照执行计划中的步骤调用存储引擎提供的方法来真正的执行查询，并将查询结果返回给用户。 它使用“ 选取-投影-连接 ”策略进行查询。例如： SELECT id,name FROM student WHERE gender = '女'; 这个SELECT查询先根据WHERE语句进行选取 ，而不是将表全部查询出来以后再进行gender过滤。 这个SELECT查询先根据id和name进行属性投影，而不是将属性全部取出以后再进行过滤，将这两个查询条件连接起来生成最终查询结果。 ps: 编写查询语句时，应尽可能避免全表查询和表级锁，写SQL语句应该尽可能地书写性能更好的SQL语句 Caches \u0026 Buffers： 查询缓存组件 MySQL内部维持着一些Cache和Buffer，比如Query Cache用来缓存一条SELECT语句的执行结果，如果能够在其中找到对应的查询结果，那么就不必再进行查询解析、优化和执行的整个过程了，直接将结果反馈给客户端。 这个缓存机制是由一系列小缓存组成的。比如表缓存，记录缓存，key缓存，权限缓存等。 这个查询缓存可以在不同客户端之间共享。 MySQL 8.0+ 不支持查询缓存，并且鼓励用户升级以使用服务器端查询重写或ProxySQL作为中间人缓存。 ","date":"2022-09-05","objectID":"/mysql%E6%9E%B6%E6%9E%84%E7%AF%87/:1:4","tags":[],"title":"MySQL架构篇","uri":"/mysql%E6%9E%B6%E6%9E%84%E7%AF%87/"},{"categories":["mysql"],"content":"1.5 第三层：引擎层 插件式存储引擎层（Storage Engines），真正的负责了MySQL中数据的存储和提取，对物理服务器级别维护的底层数据执行操作，服务器通过API与存储引擎进行通信。不同的存储引擎具有的功能不同，这样 我们可以根据自己的实际需要进行选取。 MySQL 8.0.25默认支持的存储引擎如下： ","date":"2022-09-05","objectID":"/mysql%E6%9E%B6%E6%9E%84%E7%AF%87/:1:5","tags":[],"title":"MySQL架构篇","uri":"/mysql%E6%9E%B6%E6%9E%84%E7%AF%87/"},{"categories":["mysql"],"content":"1.6 存储层 所有的数据，数据库、表的定义，表的每一行的内容，索引，都是存在文件系统上，以文件的方式存在的，并完成与存储引擎的交互。当然有些存储引擎比如InnoDB，也支持不使用文件系统直接管理裸设备，但现代文件系统的实现使得这样做没有必要了。在文件系统之下，可以使用本地磁盘，可以使用 DAS、NAS、SAN等各种存储系统。 ","date":"2022-09-05","objectID":"/mysql%E6%9E%B6%E6%9E%84%E7%AF%87/:1:6","tags":[],"title":"MySQL架构篇","uri":"/mysql%E6%9E%B6%E6%9E%84%E7%AF%87/"},{"categories":["mysql"],"content":"1.7 小结 MySQL架构图本节开篇所示。下面为了熟悉SQL执行流程方便，我们可以简化如下： 简化为三层结构： 连接层：客户端和服务器端建立连接，客户端发送 SQL 至服务器端； SQL 层（服务层）：对 SQL 语句进行查询处理；与数据库文件的存储方式无关； 存储引擎层：与数据库文件打交道，负责数据的存储和读取。 ","date":"2022-09-05","objectID":"/mysql%E6%9E%B6%E6%9E%84%E7%AF%87/:1:7","tags":[],"title":"MySQL架构篇","uri":"/mysql%E6%9E%B6%E6%9E%84%E7%AF%87/"},{"categories":["mysql"],"content":"2. SQL执行流程 ","date":"2022-09-05","objectID":"/mysql%E6%9E%B6%E6%9E%84%E7%AF%87/:2:0","tags":[],"title":"MySQL架构篇","uri":"/mysql%E6%9E%B6%E6%9E%84%E7%AF%87/"},{"categories":["mysql"],"content":"2.1 MySQL中的SQL执行流程 MySQL的查询流程： 查询缓存：Server 如果在查询缓存中发现了这条 SQL 语句，就会直接将结果返回给客户端；如果没有，就进入到解析器阶段。需要说明的是，因为查询缓存往往效率不高，所以在 MySQL8.0 之后就抛弃了这个功能。 总之，因为查询缓存往往弊大于利，查询缓存的失效非常频繁。缓存本质就是将查询语句的字符串作为key，查询结果作为value进行缓存，瓶颈在于更新数据之后，再使用缓存取得数据虽然快但是显然不是新的数据，而且查询的语法发生改变、多个空格等都会导致缓存命中失败 一般建议大家在静态表里使用查询缓存，什么叫静态表呢？就是一般我们极少更新的表。比如，一个系统配置表、字典表，这张表上的查询才适合使用查询缓存。好在MySQL也提供了这种“按需使用”的方式。你可以将 my.cnf 参数 query_cache_type 设置成 DEMAND，代表当 sql 语句中有 SQL_CACHE关键字时才缓存。比如： # query_cache_type 有3个值。 0代表关闭查询缓存OFF，1代表开启ON，2代表(DEMAND) query_cache_type = 2 这样对于默认的SQL语句都不使用查询缓存。而对于你确定要使用查询缓存的语句，可以供SQL_CACHE显示指定，像下面这个语句一样： SELECT SQl_CACHE * FROM test WHERE ID=5; 查看当前 mysql 实例是否开启缓存机制 # MySQL5.7中： show global variables like \"%query_cache_type%\"; 监控查询缓存的命中率： show status like '%Qcache%'; 运行结果解析： Qcache_free_blocks: 表示查询缓存中海油多少剩余的blocks，如果该值显示较大，则说明查询缓存中的内部碎片过多了，可能在一定的时间进行整理。 Qcache_free_memory: 查询缓存的内存大小，通过这个参数可以很清晰的知道当前系统的查询内存是否够用，DBA可以根据实际情况做出调整。 Qcache_hits: 表示有多少次命中缓存。我们主要可以通过该值来验证我们的查询缓存的效果。数字越大，缓存效果越理想。 Qcache_inserts: 表示多少次未命中然后插入，意思是新来的SQL请求在缓存中未找到，不得不执行查询处理，执行查询处理后把结果insert到查询缓存中。这样的情况的次数越多，表示查询缓存应用到的比较少，效果也就不理想。当然系统刚启动后，查询缓存是空的，这也正常。 Qcache_lowmem_prunes: 该参数记录有多少条查询因为内存不足而被移除出查询缓存。通过这个值，用户可以适当的调整缓存大小。 Qcache_not_cached: 表示因为query_cache_type的设置而没有被缓存的查询数量。 Qcache_queries_in_cache: 当前缓存中缓存的查询数量。 Qcache_total_blocks: 当前缓存的block数量。 解析器：在解析器中对 SQL 语句进行语法分析、语义分析。 如果没有命中查询缓存，就要开始真正执行语句了。首先，MySQL需要知道你要做什么，因此需要对SQL语句做解析。SQL语句的分析分为词法分析与语法分析。 分析器先做词法分析。你输入的是由多个字符串和空格组成的一条 SQL 语句，MySQL 需要识别出里面的字符串分别是什么，代表什么。 MySQL 从你输入的select这个关键字识别出来，这是一个查询语句。它也要把字符串T识别成表名T，把字符串ID识别成列ID。 接着，要做语法分析。根据词法分析的结果，语法分析器（比如：Bison）会根据语法规则，判断你输入的这个 SQL 语句是否满足 MySQL 语法 。 select department_id,job_id, avg(salary) from employees group by department_id; 如果SQL语句正确，则会生成一个这样的语法树： 下图是SQL分词分析的过程步骤: 至此解析器的工作任务也基本圆满了。 优化器：在优化器中会确定 SQL 语句的执行路径，比如是根据 全表检索 ，还是根据 索引检索 等。 经过解释器，MySQL就知道你要做什么了。在开始执行之前，还要先经过优化器的处理。一条查询可以有很多种执行方式，最后都返回相同的结果。优化器的作用就是找到这其中最好的执行计划。 比如：优化器是在表里面有多个索引的时候，决定使用哪个索引；或者在一个语句有多表关联 (join) 的时候，决定各个表的连接顺序，还有表达式简化、子查询转为连接、外连接转为内连接等。 举例：如下语句是执行两个表的 join： select * from test1 join test2 using(ID) where test1.name='zhangwei' and test2.name='mysql高级课程'; 方案1：可以先从表 test1 里面取出 name='zhangwei'的记录的 ID 值，再根据 ID 值关联到表 test2，再判 断 test2 里面 name的值是否等于 'mysql高级课程'。 方案2：可以先从表 test2 里面取出 name='mysql高级课程' 的记录的 ID 值，再根据 ID 值关联到 test1， 再判断 test1 里面 name的值是否等于 zhangwei。 这两种执行方法的逻辑结果是一样的，但是执行的效率会有不同，而优化器的作用就是决定选择使用哪一个方案。优化 器阶段完成后，这个语句的执行方案就确定下来了，然后进入执行器阶段。 如果你还有一些疑问，比如优化器是怎么选择索引的，有没有可能选择错等。后面讲到索引我们再谈。 在查询优化器中，可以分为 逻辑查询 优化阶段和 物理查询 优化阶段。 逻辑查询优化就是通过改变SQL语句的内容来使得SQL查询更高效，同时为物理查询优化提供更多的候选执行计划。通常采用的方式是对SQL语句进行等价变换，对查询进行重写，而查询重写的数学基础就是关系代数。对条件表达式进行等价谓词重写、条件简化，对视图进行重写，对子查询进行优化，对连接语义进行了外连接消除、嵌套连接消除等。 物理查询优化是基于关系代数进行的查询重写，而关系代数的每一步都对应着物理计算，这些物理计算往往存在多种算法，因此需要计算各种物理路径的代价，从中选择代价最小的作为执行计划。在这个阶段里，对于单表和多表连接的操作，需要高效地使用索引，提升查询效率。 执行器： 截止到现在，还没有真正去读写真实的表，仅仅只是产出了一个执行计划。于是就进入了执行器阶段 。 在执行之前需要判断该用户是否 具备权限 。如果没有，就会返回权限错误。如果具备权限，就执行 SQL 查询并返回结果。在 MySQL8.0 以下的版本，如果设置了查询缓存，这时会将查询结果进行缓存。 select * from test where id=1; 比如：表 test 中，ID 字段没有索引，那么执行器的执行流程是这样的： 调用 InnoDB 引擎接口取这个表的第一行，判断 ID 值是不是1，如果不是则跳过，如果是则将这行存在结果集中； 调用引擎接口取“下一行”，重复相同的判断逻辑，直到取到这个表的最后一行。 执行器将上述遍历过程中所有满足条件的行组成的记录集作为结果集返回给客户端。 至此，这个语句就执行完成了。对于有索引的表，执行的逻辑也差不多。 SQL 语句在 MySQL 中的流程是： SQL语句→查询缓存→解析器→优化器→执行器 。 ","date":"2022-09-05","objectID":"/mysql%E6%9E%B6%E6%9E%84%E7%AF%87/:2:1","tags":[],"title":"MySQL架构篇","uri":"/mysql%E6%9E%B6%E6%9E%84%E7%AF%87/"},{"categories":["mysql"],"content":"2.2 MySQL8中SQL执行原理 1) 确认profiling是否开启 了解查询语句底层执行的过程：select @profiling 或者 show variables like '%profiling' 查看是否开启计划。开启它可以让MySQL收集在SQL 执行时所使用的资源情况，命令如下： mysql\u003e select @@profiling; mysql\u003e show variables like 'profiling'; profiling=0 代表关闭，我们需要把 profiling 打开，即设置为 1： mysql\u003e set profiling=1; 2) 多次执行相同SQL查询 然后我们执行一个 SQL 查询（你可以执行任何一个 SQL 查询）： mysql\u003e select * from employees; 3) 查看profiles 查看当前会话所产生的所有 profiles： mysql\u003e show profiles; # 显示最近的几次查询 4) 查看profile 显示执行计划，查看程序的执行步骤： mysql\u003e show profile; 当然你也可以查询指定的 Query ID，比如： mysql\u003e show profile for query 7; 查询 SQL 的执行时间结果和上面是一样的。 此外，还可以查询更丰富的内容： mysql\u003e show profile cpu,block io for query 6; 继续： mysql\u003e show profile cpu,block io for query 7; 1、除了查看cpu、io阻塞等参数情况，还可以查询下列参数的利用情况。 Syntax: SHOW PROFILE [type [, type] ... ] [FOR QUERY n] [LIMIT row_count [OFFSET offset]] type: { | ALL -- 显示所有参数的开销信息 | BLOCK IO -- 显示IO的相关开销 | CONTEXT SWITCHES -- 上下文切换相关开销 | CPU -- 显示CPU相关开销信息 | IPC -- 显示发送和接收相关开销信息 | MEMORY -- 显示内存相关开销信息 | PAGE FAULTS -- 显示页面错误相关开销信息 | SOURCE -- 显示和Source_function,Source_file,Source_line 相关的开销信息 | SWAPS -- 显示交换次数相关的开销信息 } 2、发现两次查询当前情况都一致，说明没有缓存。 在 8.0 版本之后，MySQL 不再支持缓存的查询。一旦数据表有更新，缓存都将清空，因此只有数据表是静态的时候，或者数据表很少发生变化时，使用缓存查询才有价值，否则如果数据表经常更新，反而增加了 SQL 的查询时间。 ","date":"2022-09-05","objectID":"/mysql%E6%9E%B6%E6%9E%84%E7%AF%87/:2:2","tags":[],"title":"MySQL架构篇","uri":"/mysql%E6%9E%B6%E6%9E%84%E7%AF%87/"},{"categories":["mysql"],"content":"2.3 MySQL5.7中SQL执行原理 上述操作在MySQL5.7中测试，发现前后两次相同的sql语句，执行的查询过程仍然是相同的。不是会使用 缓存吗？这里我们需要 显式开启查询缓存模式 。在MySQL5.7中如下设置： 1) 配置文件中开启查询缓存 在 /etc/my.cnf 中新增一行： query_cache_type=1 2) 重启mysql服务 systemctl restart mysqld 3) 开启查询执行计划 由于重启过服务，需要重新执行如下指令，开启profiling。 mysql\u003e set profiling=1; 4) 执行语句两次： mysql\u003e select * from locations; 5) 查看profiles 6) 查看profile 显示执行计划，查看程序的执行步骤： mysql\u003e show profile for query 1; mysql\u003e show profile for query 2; 结论不言而喻。执行编号2时，比执行编号1时少了很多信息，从截图中可以看出查询语句直接从缓存中 获取数据。 ","date":"2022-09-05","objectID":"/mysql%E6%9E%B6%E6%9E%84%E7%AF%87/:2:3","tags":[],"title":"MySQL架构篇","uri":"/mysql%E6%9E%B6%E6%9E%84%E7%AF%87/"},{"categories":["mysql"],"content":"2.4 SQL语法顺序 随着Mysql版本的更新换代，其优化器也在不断的升级，优化器会分析不同执行顺序产生的性能消耗不同 而动态调整执行顺序。 ","date":"2022-09-05","objectID":"/mysql%E6%9E%B6%E6%9E%84%E7%AF%87/:2:4","tags":[],"title":"MySQL架构篇","uri":"/mysql%E6%9E%B6%E6%9E%84%E7%AF%87/"},{"categories":["mysql"],"content":"3. 数据库缓冲池（buffer pool） InnoDB 存储引擎是以页为单位来管理存储空间的，我们进行的增删改查操作其实本质上都是在访问页面（包括读页面、写页面、创建新页面等操作）。而磁盘 I/O 需要消耗的时间很多，而在内存中进行操作，效率则会高很多，为了能让数据表或者索引中的数据随时被我们所用，DBMS会申请占用内存来作为数据缓冲池，在真正访问页面之前，需要把在磁盘上的页缓存到内存中的 Buffer Pool 之后才可以访问。 这样做的好处是可以让磁盘活动最小化，从而减少与磁盘直接进行 I/O 的时间。要知道，这种策略对提升 SQL 语句的查询性能来说至关重要。如果索引的数据在缓冲池里，那么访问的成本就会降低很多。 ","date":"2022-09-05","objectID":"/mysql%E6%9E%B6%E6%9E%84%E7%AF%87/:3:0","tags":[],"title":"MySQL架构篇","uri":"/mysql%E6%9E%B6%E6%9E%84%E7%AF%87/"},{"categories":["mysql"],"content":"3.1 缓冲池 vs 查询缓存 缓冲池和查询缓存是一个东西吗？不是。 1) 缓冲池（Buffer Pool） 首先我们需要了解在 InnoDB 存储引擎中，缓冲池都包括了哪些。 在 InnoDB 存储引擎中有一部分数据会放到内存中，缓冲池则占了这部分内存的大部分，它用来存储各种数据的缓存，如下图所示： 从图中，你能看到 InnoDB 缓冲池包括了数据页、索引页、插入缓冲、锁信息、自适应 Hash 和数据字典信息等。 缓存池的重要性： 缓存原则： 位置 * 频次，这个原则可以帮我们对 I/O 访问效率进行优化。 首先，位置决定效率，提供缓冲池就是为了在内存中可以直接访问数据。 其次，频次决定优先级顺序。因为缓冲池的大小是有限的，比如磁盘有200G，但是内存只有16G，缓冲池大小只有1G，就无法将所有数据都加载到缓冲池里，这时就涉及到优先级顺序，会优先对使用频次高的热数据进行加载。 缓冲池的预读特性: 缓冲池的作用就是提升 I/O 效率，而我们进行读取数据的时候存在一个“局部性原理”，也就是说我们使用了一些数据，大概率还会使用它周围的一些数据，因此采用“预读”的机制提前加载，可以减少未来可能的磁盘 I/O 操作。 2) 查询缓存 那么什么是查询缓存呢？ 查询缓存是提前把查询结果缓存起来，这样下次不需要执行就可以直接拿到结果。需要说明的是，在 MySQL 中的查询缓存，不是缓存查询计划，而是查询对应的结果。因为命中条件苛刻，而且只要数据表发生变化，查询缓存就会失效，因此命中率低。 ","date":"2022-09-05","objectID":"/mysql%E6%9E%B6%E6%9E%84%E7%AF%87/:3:1","tags":[],"title":"MySQL架构篇","uri":"/mysql%E6%9E%B6%E6%9E%84%E7%AF%87/"},{"categories":["mysql"],"content":"3.2 缓冲池如何读取数据 缓冲池管理器会尽量将经常使用的数据保存起来，在数据库进行页面读操作的时候，首先会判断该页面是否在缓冲池中，如果存在就直接读取，如果不存在，就会通过内存或磁盘将页面存放到缓冲池中再进行读取。 缓存在数据库中的结构和作用如下图所示： 如果我们执行 SQL 语句的时候更新了缓存池中的数据，那么这些数据会马上同步到磁盘上吗？ 实际上，当我们对数据库中的记录进行修改的时候，首先会修改缓冲池中页里面的记录信息，然后数据库会以一定的频率刷新到磁盘中。注意并不是每次发生更新操作，都会立即进行磁盘回写。缓冲池会采用一种叫做 checkpoint 的机制 将数据回写到磁盘上，这样做的好处就是提升了数据库的整体性能。 比如，当缓冲池不够用时，需要释放掉一些不常用的页，此时就可以强行采用checkpoint的方式，将不常用的脏页回写到磁盘上，然后再从缓存池中将这些页释放掉。这里的脏页 (dirty page) 指的是缓冲池中被修改过的页，与磁盘上的数据页不一致。 ","date":"2022-09-05","objectID":"/mysql%E6%9E%B6%E6%9E%84%E7%AF%87/:3:2","tags":[],"title":"MySQL架构篇","uri":"/mysql%E6%9E%B6%E6%9E%84%E7%AF%87/"},{"categories":["mysql"],"content":"3.3 查看/设置缓冲池的大小 如果你使用的是 MySQL MyISAM 存储引擎，它只缓存索引，不缓存数据，对应的键缓存参数为key_buffer_size，你可以用它进行查看。 如果你使用的是 InnoDB 存储引擎，可以通过查看 innodb_buffer_pool_size 变量来查看缓冲池的大小。命令如下： show variables like 'innodb_buffer_pool_size'; 你能看到此时 InnoDB 的缓冲池大小只有 134217728/1024/1024=128MB。我们可以修改缓冲池大小，比如改为256MB，方法如下： set global innodb_buffer_pool_size = 268435456; 或者： [server] innodb_buffer_pool_size = 268435456 ","date":"2022-09-05","objectID":"/mysql%E6%9E%B6%E6%9E%84%E7%AF%87/:3:3","tags":[],"title":"MySQL架构篇","uri":"/mysql%E6%9E%B6%E6%9E%84%E7%AF%87/"},{"categories":["mysql"],"content":"3.4 多个Buffer Pool实例 [server] innodb_buffer_pool_instances = 2 这样就表明我们要创建2个 Buffer Pool 实例。 我们看下如何查看缓冲池的个数，使用命令： show variables like 'innodb_buffer_pool_instances'; 那每个 Buffer Pool 实例实际占多少内存空间呢？其实使用这个公式算出来的： innodb_buffer_pool_size/innodb_buffer_pool_instances 也就是总共的大小除以实例的个数，结果就是每个 Buffer Pool 实例占用的大小。 不过也不是说 Buffer Pool 实例创建的越多越好，分别管理各个 Buffer Pool 也是需要性能开销的，InnDB规定：当innodb_buffer_pool_size的值小于1G的时候设置多个实例是无效的，InnoDB会默认把innodb_buffer_pool_instances的值修改为1。而我们鼓励在 Buffer Pool 大于等于 1G 的时候设置多个 Buffer Pool 实例。 ","date":"2022-09-05","objectID":"/mysql%E6%9E%B6%E6%9E%84%E7%AF%87/:3:4","tags":[],"title":"MySQL架构篇","uri":"/mysql%E6%9E%B6%E6%9E%84%E7%AF%87/"},{"categories":["mysql"],"content":"3.5 引申问题 Buffer Pool是MySQL内存结构中十分核心的一个组成，你可以先把它想象成一个黑盒子。 黑盒下的更新数据流程 当我们查询数据的时候，会先去 Buffer Pool 中查询。如果 Buffer Pool 中不存在，存储引擎会先将数据从磁盘加载到 Buffer Pool 中，然后将数据返回给客户端；同理，当我们更新某个数据的时候，如果这个数据不存在于 Buffer Pool，同样会先数据加载进来，然后修改内存的数据。被修改的数据会在之后统一刷入磁盘。 我更新到一半突然发生错误了，想要回滚到更新之前的版本，该怎么办？连数据持久化的保证、事务回滚都做不到还谈什么崩溃恢复？ 答案：Redo Log \u0026 Undo Log 第05章_存储引擎 ","date":"2022-09-05","objectID":"/mysql%E6%9E%B6%E6%9E%84%E7%AF%87/:3:5","tags":[],"title":"MySQL架构篇","uri":"/mysql%E6%9E%B6%E6%9E%84%E7%AF%87/"},{"categories":["mysql"],"content":"1. 查看存储引擎 查看mysql提供什么存储引擎 show engines; ","date":"2022-09-05","objectID":"/mysql%E6%9E%B6%E6%9E%84%E7%AF%87/:4:0","tags":[],"title":"MySQL架构篇","uri":"/mysql%E6%9E%B6%E6%9E%84%E7%AF%87/"},{"categories":["mysql"],"content":"2. 设置系统默认的存储引擎 查看默认的存储引擎 show variables like '%storage_engine%'; #或 SELECT @@default_storage_engine; 修改默认的存储引擎 如果在创建表的语句中没有显式指定表的存储引擎的话，那就会默认使用 InnoDB 作为表的存储引擎。 如果我们想改变表的默认存储引擎的话，可以这样写启动服务器的命令行： SET DEFAULT_STORAGE_ENGINE=MyISAM; 或者修改 my.cnf 文件： default-storage-engine=MyISAM # 重启服务 systemctl restart mysqld.service ","date":"2022-09-05","objectID":"/mysql%E6%9E%B6%E6%9E%84%E7%AF%87/:5:0","tags":[],"title":"MySQL架构篇","uri":"/mysql%E6%9E%B6%E6%9E%84%E7%AF%87/"},{"categories":["mysql"],"content":"3. 设置表的存储引擎 存储引擎是负责对表中的数据进行提取和写入工作的，我们可以为 不同的表设置不同的存储引擎 ，也就是 说不同的表可以有不同的物理存储结构，不同的提取和写入方式。 ","date":"2022-09-05","objectID":"/mysql%E6%9E%B6%E6%9E%84%E7%AF%87/:6:0","tags":[],"title":"MySQL架构篇","uri":"/mysql%E6%9E%B6%E6%9E%84%E7%AF%87/"},{"categories":["mysql"],"content":"3.1 创建表时指定存储引擎 我们之前创建表的语句都没有指定表的存储引擎，那就会使用默认的存储引擎 InnoDB 。如果我们想显 式的指定一下表的存储引擎，那可以这么写： CREATE TABLE 表名( 建表语句; ) ENGINE = 存储引擎名称; ","date":"2022-09-05","objectID":"/mysql%E6%9E%B6%E6%9E%84%E7%AF%87/:6:1","tags":[],"title":"MySQL架构篇","uri":"/mysql%E6%9E%B6%E6%9E%84%E7%AF%87/"},{"categories":["mysql"],"content":"3.2 修改表的存储引擎 如果表已经建好了，我们也可以使用下边这个语句来修改表的存储引擎： ALTER TABLE 表名 ENGINE = 存储引擎名称; 比如我们修改一下 engine_demo_table 表的存储引擎： mysql\u003e ALTER TABLE engine_demo_table ENGINE = InnoDB; 这时我们再查看一下 engine_demo_table 的表结构： mysql\u003e SHOW CREATE TABLE engine_demo_table\\G *************************** 1. row *************************** Table: engine_demo_table Create Table: CREATE TABLE `engine_demo_table` ( `i` int(11) DEFAULT NULL ) ENGINE=InnoDB DEFAULT CHARSET=utf8 1 row in set (0.01 sec) ","date":"2022-09-05","objectID":"/mysql%E6%9E%B6%E6%9E%84%E7%AF%87/:6:2","tags":[],"title":"MySQL架构篇","uri":"/mysql%E6%9E%B6%E6%9E%84%E7%AF%87/"},{"categories":["mysql"],"content":"4. 引擎介绍 ","date":"2022-09-05","objectID":"/mysql%E6%9E%B6%E6%9E%84%E7%AF%87/:7:0","tags":[],"title":"MySQL架构篇","uri":"/mysql%E6%9E%B6%E6%9E%84%E7%AF%87/"},{"categories":["mysql"],"content":"4.1 InnoDB 引擎：具备外键支持功能的事务存储引擎 MySQL从3.23.34a开始就包含InnoDB存储引擎。 大于等于5.5之后，默认采用InnoDB引擎 。 InnoDB是MySQL的 默认事务型引擎 ，它被设计用来处理大量的短期(short-lived)事务。可以确保事务的完整提交(Commit)和回滚(Rollback)。 除了增加和查询外，还需要更新、删除操作，那么，应优先选择InnoDB存储引擎。 除非有非常特别的原因需要使用其他的存储引擎，否则应该优先考虑InnoDB引擎。 数据文件结构：（在《第02章_MySQL数据目录》章节已讲） 表名.frm 存储表结构（MySQL8.0时，合并在表名.ibd中） 表名.ibd 存储数据和索引 InnoDB是 为处理巨大数据量的最大性能设计 。 在以前的版本中，字典数据以元数据文件、非事务表等来存储。现在这些元数据文件被删除 了。比如： .frm ， .par ， .trn ， .isl ， .db.opt 等都在MySQL8.0中不存在了。 对比MyISAM的存储引擎，InnoDB写的处理效率差一些，并且会占用更多的磁盘空间以保存数据和索引。 MyISAM只缓存索引，不缓存真实数据；InnoDB不仅缓存索引还要缓存真实数据，对内存要求较高 ，而且内存大小对性能有决定性的影响。 ","date":"2022-09-05","objectID":"/mysql%E6%9E%B6%E6%9E%84%E7%AF%87/:7:1","tags":[],"title":"MySQL架构篇","uri":"/mysql%E6%9E%B6%E6%9E%84%E7%AF%87/"},{"categories":["mysql"],"content":"4.2 MyISAM 引擎：主要的非事务处理存储引擎 MyISAM提供了大量的特性，包括全文索引、压缩、空间函数(GIS)等，但MyISAM不支持事务、行级 锁、外键 ，有一个毫无疑问的缺陷就是崩溃后无法安全恢复 。 5.5之前默认的存储引擎 优势是访问的速度快 ，对事务完整性没有要求或者以SELECT、INSERT为主的应用 针对数据统计有额外的常数存储。故而 count(*) 的查询效率很高 数据文件结构：（在《第02章_MySQL数据目录》章节已讲） 表名.frm 存储表结构 表名.MYD 存储数据 (MYData) 表名.MYI 存储索引 (MYIndex) 应用场景：只读应用或者以读为主的业务 ","date":"2022-09-05","objectID":"/mysql%E6%9E%B6%E6%9E%84%E7%AF%87/:7:2","tags":[],"title":"MySQL架构篇","uri":"/mysql%E6%9E%B6%E6%9E%84%E7%AF%87/"},{"categories":["mysql"],"content":"4.3 Archive 引擎：用于数据存档 下表展示了ARCHIVE 存储引擎功能 ","date":"2022-09-05","objectID":"/mysql%E6%9E%B6%E6%9E%84%E7%AF%87/:7:3","tags":[],"title":"MySQL架构篇","uri":"/mysql%E6%9E%B6%E6%9E%84%E7%AF%87/"},{"categories":["mysql"],"content":"4.4 Blackhole 引擎：丢弃写操作，读操作会返回空内容 没有实现任何存储机制，会丢弃所有插入的数据，不做任何保存 但服务器会记录该引擎对应表的日志，所以可以用于赋值数据到备库，或者简单地记录到日志，但这种应用方式会碰到很多问题，因此不推荐 ","date":"2022-09-05","objectID":"/mysql%E6%9E%B6%E6%9E%84%E7%AF%87/:7:4","tags":[],"title":"MySQL架构篇","uri":"/mysql%E6%9E%B6%E6%9E%84%E7%AF%87/"},{"categories":["mysql"],"content":"4.5 CSV 引擎：存储数据时，以逗号分隔各个数据项 使用案例如下 mysql\u003e CREATE TABLE test (i INT NOT NULL, c CHAR(10) NOT NULL) ENGINE = CSV; Query OK, 0 rows affected (0.06 sec) mysql\u003e INSERT INTO test VALUES(1,'record one'),(2,'record two'); Query OK, 2 rows affected (0.05 sec) Records: 2 Duplicates: 0 Warnings: 0 mysql\u003e SELECT * FROM test; +---+------------+ | i | c | +---+------------+ | 1 | record one | | 2 | record two | +---+------------+ 2 rows in set (0.00 sec) 创建CSV表还会创建相应的元文件 ，用于 存储表的状态 和 表中存在的行数 。此文件的名称与表的名称相同，后缀为 CSM 。如图所示 如果检查 test.CSV 通过执行上述语句创建的数据库目录中的文件，其内容使用Notepad++打开如下： \"1\",\"record one\" \"2\",\"record two\" 这种格式可以被 Microsoft Excel 等电子表格应用程序读取，甚至写入。使用Microsoft Excel打开如图所示 ","date":"2022-09-05","objectID":"/mysql%E6%9E%B6%E6%9E%84%E7%AF%87/:7:5","tags":[],"title":"MySQL架构篇","uri":"/mysql%E6%9E%B6%E6%9E%84%E7%AF%87/"},{"categories":["mysql"],"content":"4.6 Memory 引擎：置于内存的表 概述： Memory采用的逻辑介质是内存，响应速度很快 ，但是当mysqld守护进程崩溃的时候数据会丢失 。另外，要求存储的数据是数据长度不变的格式，比如，Blob和Text类型的数据不可用(长度不固定的)。 主要特征： Memory同时 支持哈希（HASH）索引 和 B+树索引 。 Memory表至少比MyISAM表要快一个数量级 。 MEMORY 表的大小是受到限制 的。表的大小主要取决于两个参数，分别是 max_rows 和 max_heap_table_size 。其中，max_rows可以在创建表时指定；max_heap_table_size的大小默 认为16MB，可以按需要进行扩大。 数据文件与索引文件分开存储。 缺点：其数据易丢失，生命周期短。基于这个缺陷，选择MEMORY存储引擎时需要特别小心。 使用Memory存储引擎的场景： 目标数据比较小 ，而且非常频繁的进行访问 ，在内存中存放数据，如果太大的数据会造成内存溢出 。可以通过参数 max_heap_table_size 控制Memory表的大小，限制Memory表的最大的大小。 如果数据是临时的 ，而且必须立即可用得到，那么就可以放在内存中。 存储在Memory表中的数据如果突然间丢失的话也没有太大的关系。 ","date":"2022-09-05","objectID":"/mysql%E6%9E%B6%E6%9E%84%E7%AF%87/:7:6","tags":[],"title":"MySQL架构篇","uri":"/mysql%E6%9E%B6%E6%9E%84%E7%AF%87/"},{"categories":["mysql"],"content":"4.7 Federated 引擎：访问远程表 Federated引擎是访问其他MySQL服务器的一个 代理 ，尽管该引擎看起来提供了一种很好的 跨服务 器的灵活性 ，但也经常带来问题，因此 默认是禁用的 。 ","date":"2022-09-05","objectID":"/mysql%E6%9E%B6%E6%9E%84%E7%AF%87/:7:7","tags":[],"title":"MySQL架构篇","uri":"/mysql%E6%9E%B6%E6%9E%84%E7%AF%87/"},{"categories":["mysql"],"content":"4.8 Merge引擎：管理多个MyISAM表构成的表集合 ","date":"2022-09-05","objectID":"/mysql%E6%9E%B6%E6%9E%84%E7%AF%87/:7:8","tags":[],"title":"MySQL架构篇","uri":"/mysql%E6%9E%B6%E6%9E%84%E7%AF%87/"},{"categories":["mysql"],"content":"4.9 NDB引擎：MySQL集群专用存储引擎 也叫做 NDB Cluster 存储引擎，主要用于 MySQL Cluster 分布式集群 环境，类似于 Oracle 的 RAC 集 群。 ","date":"2022-09-05","objectID":"/mysql%E6%9E%B6%E6%9E%84%E7%AF%87/:7:9","tags":[],"title":"MySQL架构篇","uri":"/mysql%E6%9E%B6%E6%9E%84%E7%AF%87/"},{"categories":["mysql"],"content":"4.10 引擎对比 MySQL中同一个数据库，不同的表可以选择不同的存储引擎。如下表对常用存储引擎做出了对比。 其实这些东西大家没必要立即就给记住，列出来的目的就是想让大家明白不同的存储引擎支持不同的功能。 其实我们最常用的就是 InnoDB 和 MyISAM ，有时会提一下 Memory 。其中 InnoDB 是 MySQL 默认的存储引擎。 ","date":"2022-09-05","objectID":"/mysql%E6%9E%B6%E6%9E%84%E7%AF%87/:7:10","tags":[],"title":"MySQL架构篇","uri":"/mysql%E6%9E%B6%E6%9E%84%E7%AF%87/"},{"categories":["mysql"],"content":"5. MyISAM和InnoDB 很多人对 InnoDB 和 MyISAM 的取舍存在疑问，到底选择哪个比较好呢？ MySQL5.5之前的默认存储引擎是MyISAM，5.5之后改为了InnoDB。 ","date":"2022-09-05","objectID":"/mysql%E6%9E%B6%E6%9E%84%E7%AF%87/:8:0","tags":[],"title":"MySQL架构篇","uri":"/mysql%E6%9E%B6%E6%9E%84%E7%AF%87/"},{"categories":["golang"],"content":"Go语言reflect包的使用 ","date":"2022-09-02","objectID":"/go%E8%AF%AD%E8%A8%80reflect%E5%8C%85%E7%9A%84%E4%BD%BF%E7%94%A8/:0:0","tags":["go编程技巧"],"title":"Go语言reflect包的使用","uri":"/go%E8%AF%AD%E8%A8%80reflect%E5%8C%85%E7%9A%84%E4%BD%BF%E7%94%A8/"},{"categories":["golang"],"content":"反射包使用 map and slice func MapAndSlice() { stringSlice := make([]string,0) stringMap := make(map[string]string) sliceType := reflect.TypeOf(stringSlice) mapType := reflect.TypeOf(stringMap) rMap := reflect.MakeMap(mapType) rSlice := reflect.MakeSlice(sliceType,0,0) k := \"first\" rMap.SetMapIndex(reflect.ValueOf(k),reflect.ValueOf(\"test\")) i := rMap.Interface().(map[string]string) fmt.Println(i) reflect.AppendSlice(rSlice,reflect.ValueOf(\"test slice\")) strings := rSlice.Interface().([]string) fmt.Println(strings) } function func MakeFun()interface{} { f := timeMe vf := reflect.ValueOf(f) return reflect.MakeFunc(reflect.TypeOf(f),func(in []reflect.Value) []reflect.Value { start := time.Now() out := vf.Call(in) end := time.Now() fmt.Printf(\"calling %s took %v\\n\", runtime.FuncForPC(vf.Pointer()).Name(), end.Sub(start)) return out }).Interface() } struct func MakeStruct(vals ...interface{}) interface{} { var sfs []reflect.StructField for k, v := range vals { t := reflect.TypeOf(v) sf := reflect.StructField{ Name: fmt.Sprintf(\"F%d\", (k + 1)), Type: t, } sfs = append(sfs, sf) } st := reflect.StructOf(sfs) so := reflect.New(st) return so.Interface() } 获取type typeOf valueOf //type和value m := MyStruct{\"test\",10} t := reflect.TypeOf(m) fmt.Println(t) v := reflect.ValueOf(\u0026m) fmt.Println(v) //读取 for i := 0; i \u003c t.NumField() ;i++ { fmt.Printf(\"name:%s,json_tag:%s\",t.Field(i).Name,t.Field(i).Tag.Get(\"json\")) fmt.Println() } //设置 v.Elem().Field(0).SetString(\"test1\") v.Elem().Field(1).SetInt(31) //读取 for i := 0; i \u003c t.NumField() ;i++ { fmt.Printf(\"name:%v\",v.Elem().Field(i)) fmt.Println() } 反射调用struct方法 //带参数调用方式 setNameMethod := v.MethodByName( \"AddAge\" ) args := []reflect.Value{ reflect.ValueOf(10) } //构造一个类型为reflect.Value的切片 setNameMethod.Call(args) //返回Value类型 fmt.Println(\"User.Age = \",m.Age) 还有很多反射api就不逐一介绍了，翻翻源码就行 ","date":"2022-09-02","objectID":"/go%E8%AF%AD%E8%A8%80reflect%E5%8C%85%E7%9A%84%E4%BD%BF%E7%94%A8/:1:0","tags":["go编程技巧"],"title":"Go语言reflect包的使用","uri":"/go%E8%AF%AD%E8%A8%80reflect%E5%8C%85%E7%9A%84%E4%BD%BF%E7%94%A8/"},{"categories":["golang"],"content":"三大法则 运行时反射是程序在运行期间检查其自身结构的一种方式。反射带来的灵活性是一把双刃剑，反射作为一种元编程方式可以减少重复代码，但是过量的使用反射会使我们的程序逻辑变得难以理解并且运行缓慢。我们在这一节中会介绍 Go 语言反射的三大法则，其中包括： 从 interface{} 变量可以反射出反射对象； 从反射对象可以获取 interface{} 变量； 要修改反射对象，其值必须可设置； ","date":"2022-09-02","objectID":"/go%E8%AF%AD%E8%A8%80reflect%E5%8C%85%E7%9A%84%E4%BD%BF%E7%94%A8/:2:0","tags":["go编程技巧"],"title":"Go语言reflect包的使用","uri":"/go%E8%AF%AD%E8%A8%80reflect%E5%8C%85%E7%9A%84%E4%BD%BF%E7%94%A8/"},{"categories":["golang"],"content":"第一法则 反射的第一法则是我们能将 Go 语言的 interface{} 变量转换成反射对象。很多读者可能会对这以法则产生困惑 — 为什么是从 interface{} 变量到反射对象？当我们执行 reflect.ValueOf(1) 时，虽然看起来是获取了基本类型 int 对应的反射类型，但是由于 reflect.TypeOf、reflect.ValueOf 两个方法的入参都是 interface{} 类型，所以在方法执行的过程中发生了类型转换。 因为Go 语言的函数调用都是值传递的，所以变量会在函数调用时进行类型转换。基本类型 int 会转换成 interface{} 类型，这也就是为什么第一条法则是从接口到反射对象。 上面提到的 reflect.TypeOf 和 reflect.ValueOf 函数就能完成这里的转换，如果我们认为 Go 语言的类型和反射类型处于两个不同的世界，那么这两个函数就是连接这两个世界的桥梁。 图 接口到反射对象 我们可以通过以下例子简单介绍它们的作用，reflect.TypeOf 获取了变量 author 的类型，reflect.ValueOf 获取了变量的值 draven。如果我们知道了一个变量的类型和值，那么就意味着我们知道了这个变量的全部信息。 package main import ( \"fmt\" \"reflect\" ) func main() { author := \"draven\" fmt.Println(\"TypeOf author:\", reflect.TypeOf(author)) fmt.Println(\"ValueOf author:\", reflect.ValueOf(author)) } $ go run main.go TypeOf author: string ValueOf author: draven 有了变量的类型之后，我们可以通过 Method 方法获得类型实现的方法，通过 Field 获取类型包含的全部字段。对于不同的类型，我们也可以调用不同的方法获取相关信息： 结构体：获取字段的数量并通过下标和字段名获取字段 StructField； 哈希表：获取哈希表的 Key 类型； 函数或方法：获取入参和返回值的类型； …… 总而言之，使用 reflect.TypeOf 和 reflect.ValueOf 能够获取 Go 语言中的变量对应的反射对象。一旦获取了反射对象，我们就能得到跟当前类型相关数据和操作，并可以使用这些运行时获取的结构执行方法。 ","date":"2022-09-02","objectID":"/go%E8%AF%AD%E8%A8%80reflect%E5%8C%85%E7%9A%84%E4%BD%BF%E7%94%A8/:2:1","tags":["go编程技巧"],"title":"Go语言reflect包的使用","uri":"/go%E8%AF%AD%E8%A8%80reflect%E5%8C%85%E7%9A%84%E4%BD%BF%E7%94%A8/"},{"categories":["golang"],"content":"第二法则 反射的第二法则是我们可以从反射对象可以获取 interface{} 变量。既然能够将接口类型的变量转换成反射对象，那么一定需要其他方法将反射对象还原成接口类型的变量，reflect 中的 reflect.Value.Interface 就能完成这项工作： 图 反射对象到接口 不过调用 reflect.Value.Interface 方法只能获得 interface{} 类型的变量，如果想要将其还原成最原始的状态还需要经过如下所示的显式类型转换： v := reflect.ValueOf(1) v.Interface().(int) 从反射对象到接口值的过程是从接口值到反射对象的镜面过程，两个过程都需要经历两次转换 从接口值到反射对象 从基本类型到接口类型的类型转换 从接口类型到反射对象的转换 从反射对象到接口值 反射对象转换成接口类型 通过显式类型转换变成原始类型 图 接口和反射对象的双向转换 当然不是所有的变量都需要类型转换这一过程。如果变量本身就是 interface{} 类型的，那么它不需要类型转换，因为类型转换这一过程一般都是隐式的，所以我不太需要关心它，只有在我们需要将反射对象转换回基本类型时才需要显式的转换操作。 ","date":"2022-09-02","objectID":"/go%E8%AF%AD%E8%A8%80reflect%E5%8C%85%E7%9A%84%E4%BD%BF%E7%94%A8/:2:2","tags":["go编程技巧"],"title":"Go语言reflect包的使用","uri":"/go%E8%AF%AD%E8%A8%80reflect%E5%8C%85%E7%9A%84%E4%BD%BF%E7%94%A8/"},{"categories":["golang"],"content":"第三法则 Go 语言反射的最后一条法则是与值是否可以被更改有关，如果我们想要更新一个 reflect.Value，那么它持有的值一定是可以被更新的，假设我们有以下代码： func main() { i := 1 v := reflect.ValueOf(i) v.SetInt(10) fmt.Println(i) } $ go run reflect.go panic: reflect: reflect.flag.mustBeAssignable using unaddressable value goroutine 1 [running]: reflect.flag.mustBeAssignableSlow(0x82, 0x1014c0) /usr/local/go/src/reflect/value.go:247 +0x180 reflect.flag.mustBeAssignable(...) /usr/local/go/src/reflect/value.go:234 reflect.Value.SetInt(0x100dc0, 0x414020, 0x82, 0x1840, 0xa, 0x0) /usr/local/go/src/reflect/value.go:1606 +0x40 main.main() /tmp/sandbox590309925/prog.go:11 +0xe0 运行上述代码会导致程序崩溃并报出 “reflect: reflect.flag.mustBeAssignable using unaddressable value” 错误，仔细思考一下就能够发现出错的原因：由于 Go 语言的函数调用都是传值的，所以我们得到的反射对象跟最开始的变量没有任何关系，那么直接修改反射对象无法改变原始变量，程序为了防止错误就会崩溃。 想要修改原变量只能使用如下的方法： func main() { i := 1 v := reflect.ValueOf(\u0026i) v.Elem().SetInt(10) fmt.Println(i) } $ go run reflect.go 10 调用 reflect.ValueOf 获取变量指针； 调用 reflect.Value.Elem 获取指针指向的变量； 调用 reflect.Value.SetInt 更新变量的值： 由于 Go 语言的函数调用都是值传递的，所以我们只能只能用迂回的方式改变原变量：先获取指针对应的 reflect.Value，再通过 reflect.Value.Elem 方法得到可以被设置的变量，我们可以通过下面的代码理解这个过程： func main() { i := 1 v := \u0026i *v = 10 } 如果不能直接操作 i 变量修改其持有的值，我们就只能获取 i 变量所在地址并使用 *v 修改所在地址中存储的整数。 ","date":"2022-09-02","objectID":"/go%E8%AF%AD%E8%A8%80reflect%E5%8C%85%E7%9A%84%E4%BD%BF%E7%94%A8/:2:3","tags":["go编程技巧"],"title":"Go语言reflect包的使用","uri":"/go%E8%AF%AD%E8%A8%80reflect%E5%8C%85%E7%9A%84%E4%BD%BF%E7%94%A8/"},{"categories":["golang"],"content":"反射底层原理刨析 虽然在大多数的应用和服务中并不常见，但是很多框架都依赖 Go 语言的反射机制简化代码。因为 Go 语言的语法元素很少、设计简单，所以它没有特别强的表达能力，但是 Go 语言的 reflect 包能够弥补它在语法上reflect.Type的一些劣势。 reflect 实现了运行时的反射能力，能够让程序操作不同类型的对象。反射包中有两对非常重要的函数和类型，两个函数分别是： reflect.TypeOf 能获取类型信息； reflect.ValueOf 能获取数据的运行时表示； 两个类型是 reflect.Type 和 reflect.Value，它们与函数是一一对应的关系： 类型 reflect.Type 是反射包定义的一个接口，我们可以使用 reflect.TypeOf 函数获取任意变量的类型，reflect.Type 接口中定义了一些有趣的方法，MethodByName 可以获取当前类型对应方法的引用、Implements 可以判断当前类型是否实现了某个接口： type Type interface { Align() int FieldAlign() int Method(int) Method MethodByName(string) (Method, bool) NumMethod() int ... Implements(u Type) bool ... } 反射包中 reflect.Value 的类型与 reflect.Type 不同，它被声明成了结构体。这个结构体没有对外暴露的字段，但是提供了获取或者写入数据的方法： type Value struct { // 包含过滤的或者未导出的字段 } func (v Value) Addr() Value func (v Value) Bool() bool func (v Value) Bytes() []byte ... 反射包中的所有方法基本都是围绕着 reflect.Type 和 reflect.Value 两个类型设计的。我们通过 reflect.TypeOf、reflect.ValueOf 可以将一个普通的变量转换成反射包中提供的 reflect.Type 和 reflect.Value，随后就可以使用反射包中的方法对它们进行复杂的操作。 ","date":"2022-09-02","objectID":"/go%E8%AF%AD%E8%A8%80reflect%E5%8C%85%E7%9A%84%E4%BD%BF%E7%94%A8/:3:0","tags":["go编程技巧"],"title":"Go语言reflect包的使用","uri":"/go%E8%AF%AD%E8%A8%80reflect%E5%8C%85%E7%9A%84%E4%BD%BF%E7%94%A8/"},{"categories":["golang"],"content":"类型和值 Go 语言的 interface{} 类型在语言内部是通过 reflect.emptyInterface 结体表示的，其中的 rtype 字段用于表示变量的类型，另一个 word 字段指向内部封装的数据： type emptyInterface struct { typ *rtype word unsafe.Pointer } 用于获取变量类型的 reflect.TypeOf 函数将传入的变量隐式转换成 reflect.emptyInterface 类型并获取其中存储的类型信息 reflect.rtype： func TypeOf(i interface{}) Type { eface := *(*emptyInterface)(unsafe.Pointer(\u0026i)) return toType(eface.typ) } func toType(t *rtype) Type { if t == nil { return nil } return t } reflect.rtype 是一个实现了 reflect.Type 接口的结构体，该结构体实现的 reflect.rtype.String 方法可以帮助我们获取当前类型的名称： func (t *rtype) String() string { s := t.nameOff(t.str).name() if t.tflag\u0026tflagExtraStar != 0 { return s[1:] } return s } reflect.TypeOf 的实现原理其实并不复杂，它只是将一个 interface{} 变量转换成了内部的 reflect.emptyInterface 表示，然后从中获取相应的类型信息。 用于获取接口值 reflect.Value 的函数 reflect.ValueOf 实现也非常简单，在该函数中我们先调用了 reflect.escapes 保证当前值逃逸到堆上，然后通过 reflect.unpackEface 从接口中获取 reflect.Value 结构体： func ValueOf(i interface{}) Value { if i == nil { return Value{} } escapes(i) return unpackEface(i) } func unpackEface(i interface{}) Value { e := (*emptyInterface)(unsafe.Pointer(\u0026i)) t := e.typ if t == nil { return Value{} } f := flag(t.Kind()) if ifaceIndir(t) { f |= flagIndir } return Value{t, e.word, f} } reflect.unpackEface 会将传入的接口转换成 reflect.emptyInterface，然后将具体类型和指针包装成 reflect.Value 结构体后返回。 reflect.TypeOf 和 reflect.ValueOf 的实现都很简单。我们已经分析了这两个函数的实现，现在需要了解编译器在调用函数之前做了哪些工作： package main import ( \"reflect\" ) func main() { i := 20 _ = reflect.TypeOf(i) } $ go build -gcflags=\"-S -N\" main.go ... MOVQ $20, \"\"..autotmp_20+56(SP) // autotmp = 20 LEAQ type.int(SB), AX // AX = type.int(SB) MOVQ AX, \"\"..autotmp_19+280(SP) // autotmp_19+280(SP) = type.int(SB) LEAQ \"\"..autotmp_20+56(SP), CX // CX = 20 MOVQ CX, \"\"..autotmp_19+288(SP) // autotmp_19+288(SP) = 20 ... 从上面这段截取的汇编语言，我们可以发现在函数调用之前已经发生了类型转换，上述指令将 int 类型的变量转换成了占用 16 字节 autotmp_19+280(SP) ~ autotmp_19+288(SP) 的接口，两个 LEAQ 指令分别获取了类型的指针 type.int(SB) 以及变量 i 所在的地址。 当我们想要将一个变量转换成反射对象时，Go 语言会在编译期间完成类型转换，将变量的类型和值转换成了 interface{} 并等待运行期间使用 reflect 包获取接口中存储的信息。 ","date":"2022-09-02","objectID":"/go%E8%AF%AD%E8%A8%80reflect%E5%8C%85%E7%9A%84%E4%BD%BF%E7%94%A8/:3:1","tags":["go编程技巧"],"title":"Go语言reflect包的使用","uri":"/go%E8%AF%AD%E8%A8%80reflect%E5%8C%85%E7%9A%84%E4%BD%BF%E7%94%A8/"},{"categories":["golang"],"content":"更新变量 当我们想要更新 reflect.Value 时，就需要调用 reflect.Value.Set 更新反射对象，该方法会调用 reflect.flag.mustBeAssignable 和 reflect.flag.mustBeExported 分别检查当前反射对象是否是可以被设置的以及字段是否是对外公开的： func (v Value) Set(x Value) { v.mustBeAssignable() x.mustBeExported() var target unsafe.Pointer if v.kind() == Interface { target = v.ptr } x = x.assignTo(\"reflect.Set\", v.typ, target) typedmemmove(v.typ, v.ptr, x.ptr) } reflect.Value.Set 会调用 reflect.Value.assignTo 并返回一个新的反射对象，这个返回的反射对象指针会直接覆盖原反射变量。 func (v Value) assignTo(context string, dst *rtype, target unsafe.Pointer) Value { ... switch { case directlyAssignable(dst, v.typ): ... return Value{dst, v.ptr, fl} case implements(dst, v.typ): if v.Kind() == Interface \u0026\u0026 v.IsNil() { return Value{dst, nil, flag(Interface)} } x := valueInterface(v, false) if dst.NumMethod() == 0 { *(*interface{})(target) = x } else { ifaceE2I(dst, x, target) } return Value{dst, target, flagIndir | flag(Interface)} } panic(context + \": value of type \" + v.typ.String() + \" is not assignable to type \" + dst.String()) } reflect.Value.assignTo 会根据当前和被设置的反射对象类型创建一个新的 reflect.Value 结构体 如果两个反射对象的类型是可以被直接替换，就会直接返回目标反射对象 如果当前反射对象是接口并且目标对象实现了接口，就会把目标对象简单包装成接口值 在变量更新的过程中，reflect.Value.assignTo 返回的 reflect.Value 中的指针会覆盖当前反射对象中的指针实现变量的更新。 ","date":"2022-09-02","objectID":"/go%E8%AF%AD%E8%A8%80reflect%E5%8C%85%E7%9A%84%E4%BD%BF%E7%94%A8/:3:2","tags":["go编程技巧"],"title":"Go语言reflect包的使用","uri":"/go%E8%AF%AD%E8%A8%80reflect%E5%8C%85%E7%9A%84%E4%BD%BF%E7%94%A8/"},{"categories":["golang"],"content":"实现协议 reflect 包还为我们提供了 reflect.rtype.Implements 方法可以用于判断某些类型是否遵循特定的接口。在 Go 语言中获取结构体的反射类型 reflect.Type 还是比较容易的，但是想要获得接口类型需要通过以下方式： reflect.TypeOf((*\u003cinterface\u003e)(nil)).Elem() Go 我们通过一个例子在介绍如何判断一个类型是否实现了某个接口。假设我们需要判断如下代码中的 CustomError 是否实现了 Go 语言标准库中的 error 接口： type CustomError struct{} func (*CustomError) Error() string { return \"\" } func main() { typeOfError := reflect.TypeOf((*error)(nil)).Elem() customErrorPtr := reflect.TypeOf(\u0026CustomError{}) customError := reflect.TypeOf(CustomError{}) fmt.Println(customErrorPtr.Implements(typeOfError)) // #=\u003e true fmt.Println(customError.Implements(typeOfError)) // #=\u003e false } CustomError 类型并没有实现 error 接口； *CustomError 指针类型实现了 error 接口； 抛开上述的执行结果不谈，我们来分析一下 reflect.rtype.Implements 方法的工作原理： func (t *rtype) Implements(u Type) bool { if u == nil { panic(\"reflect: nil type passed to Type.Implements\") } if u.Kind() != Interface { panic(\"reflect: non-interface type passed to Type.Implements\") } return implements(u.(*rtype), t) } reflect.rtype.Implements 会检查传入的类型是不是接口，如果不是接口或者是空值就会直接崩溃并中止当前程序。在参数没有问题的情况下，上述方法会调用私有函数 reflect.implements 判断类型之间是否有实现关系： func implements(T, V *rtype) bool { t := (*interfaceType)(unsafe.Pointer(T)) if len(t.methods) == 0 { return true } ... v := V.uncommon() i := 0 vmethods := v.methods() for j := 0; j \u003c int(v.mcount); j++ { tm := \u0026t.methods[i] tmName := t.nameOff(tm.name) vm := vmethods[j] vmName := V.nameOff(vm.name) if vmName.name() == tmName.name() \u0026\u0026 V.typeOff(vm.mtyp) == t.typeOff(tm.typ) { if i++; i \u003e= len(t.methods) { return true } } } return false } 如果接口中不包含任何方法，就意味着这是一个空的接口，任意类型都自动实现该接口，这时会直接返回 true。 图 类型实现接口 在其他情况下，由于方法都是按照字母序存储的，reflect.implements 会维护两个用于遍历接口和类型方法的索引 i 和 j 判断类型是否实现了接口，因为最多只会进行 n 次比较（类型的方法数量），所以整个过程的时间复杂度是 O(n)。 ","date":"2022-09-02","objectID":"/go%E8%AF%AD%E8%A8%80reflect%E5%8C%85%E7%9A%84%E4%BD%BF%E7%94%A8/:3:3","tags":["go编程技巧"],"title":"Go语言reflect包的使用","uri":"/go%E8%AF%AD%E8%A8%80reflect%E5%8C%85%E7%9A%84%E4%BD%BF%E7%94%A8/"},{"categories":["golang"],"content":"方法调用 作为一门静态语言，如果我们想要通过 reflect 包利用反射在运行期间执行方法不是一件容易的事情，下面的十几行代码就使用反射来执行 Add(0, 1) 函数： func Add(a, b int) int { return a + b } func main() { v := reflect.ValueOf(Add) if v.Kind() != reflect.Func { return } t := v.Type() argv := make([]reflect.Value, t.NumIn()) for i := range argv { if t.In(i).Kind() != reflect.Int { return } argv[i] = reflect.ValueOf(i) } result := v.Call(argv) if len(result) != 1 || result[0].Kind() != reflect.Int { return } fmt.Println(result[0].Int()) // #=\u003e 1 } 通过 reflect.ValueOf 获取函数 Add 对应的反射对象； 调用 reflect.rtype.NumIn 获取函数的入参个数； 多次调用 reflect.ValueOf 函数逐一设置 argv 数组中的各个参数； 调用反射对象 Add 的 reflect.Value.Call 方法并传入参数列表； 获取返回值数组、验证数组的长度以及类型并打印其中的数据； 使用反射来调用方法非常复杂，原本只需要一行代码就能完成的工作，现在需要十几行代码才能完成，但这也是在静态语言中使用动态特性需要付出的成本。 func (v Value) Call(in []Value) []Value { v.mustBe(Func) v.mustBeExported() return v.call(\"Call\", in) } reflect.Value.Call 是运行时调用方法的入口，它通过两个 MustBe 开头的方法确定了当前反射对象的类型是函数以及可见性，随后调用 reflect.Value.call 完成方法调用，这个私有方法的执行过程会分成以下的几个部分 检查输入参数以及类型的合法性 将传入的 reflect.Value 参数数组设置到栈上 通过函数指针和输入参数调用函数 从栈上获取函数的返回值 我们将按照上面的顺序分析使用 reflect 进行函数调用的几个过程。 参数检查 参数检查是通过反射调用方法的第一步，在参数检查期间我们会从反射对象中取出当前的函数指针 unsafe.Pointer，如果该函数指针是方法，那么我们会通过 reflect.methodReceiver 获取方法的接收者和函数指针。 func (v Value) call(op string, in []Value) []Value { t := (*funcType)(unsafe.Pointer(v.typ)) ... if v.flag\u0026flagMethod != 0 { rcvr = v rcvrtype, t, fn = methodReceiver(op, v, int(v.flag)\u003e\u003eflagMethodShift) } else { ... } n := t.NumIn() if len(in) \u003c n { panic(\"reflect: Call with too few input arguments\") } if len(in) \u003e n { panic(\"reflect: Call with too many input arguments\") } for i := 0; i \u003c n; i++ { if xt, targ := in[i].Type(), t.In(i); !xt.AssignableTo(targ) { panic(\"reflect: \" + op + \" using \" + xt.String() + \" as type \" + targ.String()) } } 上述方法还会检查传入参数的个数以及参数的类型与函数签名中的类型是否可以匹配，任何参数的不匹配都会导致整个程序的崩溃中止。 准备参数 当我们已经对当前方法的参数完成验证后，就会进入函数调用的下一个阶段，为函数调用准备参数，在前面函数调用一节中，我们已经介绍过 Go 语言的函数调用惯例，函数或者方法在调用时，所有的参数都会被依次放到栈上。 nout := t.NumOut() frametype, _, retOffset, _, framePool := funcLayout(t, rcvrtype) var args unsafe.Pointer if nout == 0 { args = framePool.Get().(unsafe.Pointer) } else { args = unsafe_New(frametype) } off := uintptr(0) if rcvrtype != nil { storeRcvr(rcvr, args) off = ptrSize } for i, v := range in { targ := t.In(i).(*rtype) a := uintptr(targ.align) off = (off + a - 1) \u0026^ (a - 1) n := targ.size ... addr := add(args, off, \"n \u003e 0\") v = v.assignTo(\"reflect.Value.Call\", targ, addr) *(*unsafe.Pointer)(addr) = v.ptr off += n } 通过 reflect.funcLayout 计算当前函数需要的参数和返回值的栈布局，也就是每一个参数和返回值所占的空间大小； 如果当前函数有返回值，需要为当前函数的参数和返回值分配一片内存空间 args； 如果当前函数是方法，需要向将方法的接收接收者者拷贝到 args 内存中； 将所有函数的参数按照顺序依次拷贝到对应 args 内存中 使用 reflect.funcLayout 返回的参数计算参数在内存中的位置； 将参数拷贝到内存空间中； 准备参数是计算各个参数和返回值占用的内存空间并将所有的参数都拷贝内存空间对应位置的过程，该过程会考虑函数和方法、返回值数量以及参数类型带来的差异。 调用函数 准备好调用函数需要的全部参数后，就会通过下面的代码执行函数指针了。我们会向该函数传入栈类型、函数指针、参数和返回值的内存空间、栈的大小以及返回值的偏移量： call(frametype, fn, args, uint32(frametype.size), uint32(retOffset)) 上述函数实际上并不存在，它会在编译期间链接到 reflect.reflectcall 这个用汇编实现的函数上，我们在这里不会分析该函数的具体实现，感兴趣的读者可以自行了解其实现原理。 处理返回值 当函数调用结束之后，就会开始处理函数的返回值 如果函数没有任何返回值，会直接清空 args 中的全部内容来释放内存空间 如果当前函数有返回值 将 args 中与输入参数有关的内存空间清空 创建一个 nout 长度的切片用于保存由反射对象构成的返回值数组 从函数对象中获取返回值的类型和内存大小，将 args 内存中的数据转换成 reflect.Value 类型并存储到切片中 var ret []Value if nout == 0 { typedmemclr(frametype, args) framePool.Put(args) } else { typedmemclrpartial(frametype, args, 0, retOffset) ret = make([]Value, nout) off = retOffset for i := 0; i \u003c nout; i++ { tv := t.Out(i) a := uintptr(tv.Align()) off = (off + a - 1) \u0026^ (a - 1) if tv.Size() != 0 { fl := flagIndir | flag(tv.Kind()) ret[i] = Value{tv.common(), add(args, off, \"tv.Size() != 0\"), fl} } else { ret[i] = Zero(tv) } off += tv.Size() } } return ret } 由 reflect.Value 构成的 ret 数组会被返回到调用方，到这里为止使用反射实现函数调用的过程就结束了。 ","date":"2022-09-02","objectID":"/go%E8%AF%AD%E8%A8%80reflect%E5%8C%85%E7%9A%84%E4%BD%BF%E7%94%A8/:3:4","tags":["go编程技巧"],"title":"Go语言reflect包的使用","uri":"/go%E8%AF%AD%E8%A8%80reflect%E5%8C%85%E7%9A%84%E4%BD%BF%E7%94%A8/"},{"categories":["golang"],"content":"小结 Go 语言的 reflect 包为我们提供了多种能力，包括如何使用反射来动态修改变量、判断类型是否实现了某些接口以及动态调用方法等功能，通过分析反射包中方法的原理能帮助我们理解之前看起来比较怪异、令人困惑的现象。 ","date":"2022-09-02","objectID":"/go%E8%AF%AD%E8%A8%80reflect%E5%8C%85%E7%9A%84%E4%BD%BF%E7%94%A8/:4:0","tags":["go编程技巧"],"title":"Go语言reflect包的使用","uri":"/go%E8%AF%AD%E8%A8%80reflect%E5%8C%85%E7%9A%84%E4%BD%BF%E7%94%A8/"},{"categories":["golang"],"content":"unsafe使用及底层 ","date":"2022-08-28","objectID":"/go%E8%AF%AD%E8%A8%80unsafe%E5%8C%85%E7%9A%84%E4%BD%BF%E7%94%A8/:0:0","tags":["go编程技巧"],"title":"Go语言unsafe包的使用","uri":"/go%E8%AF%AD%E8%A8%80unsafe%E5%8C%85%E7%9A%84%E4%BD%BF%E7%94%A8/"},{"categories":["golang"],"content":"unsafe实现原理 在使用之前我们先来看一下unsafe包的源码部分，标准库unsafe包中只提供了3种方法，分别是: func Sizeof(x ArbitraryType) uintptr func Offsetof(x ArbitraryType) uintptr func Alignof(x ArbitraryType) uintptr Sizeof(x ArbitrayType)方法主要作用是用返回类型x所占据的字节数，但并不包含x所指向的内容的大小，与C语言标准库中的Sizeof()方法功能一样 Offsetof(x ArbitraryType)方法主要作用是返回结构体成员在内存中的位置离结构体起始处(结构体的第一个字段的偏移量都是0)的字节数，即偏移量，我们在注释中看一看到其入参必须是一个结构体，其返回值是一个常量。 Alignof(x ArbitratyType)的主要作用是返回一个类型的对齐值，也可以叫做对齐系数或者对齐倍数。对齐值是一个和内存对齐有关的值，合理的内存对齐可以提高内存读写的性能。一般对齐值是2^n^，最大不会超过8(受内存对齐影响). 获取对齐值还可以使用反射包的函数，也就是说：unsafe.Alignof(x)等价于reflect.TypeOf(x).Align()。对于任意类型的变量x，unsafe.Alignof(x)至少为1。对于struct结构体类型的变量x，计算x每一个字段f的unsafe.Alignof(x，f)，unsafe.Alignof(x)等于其中的最大值。对于数组类型的变量x，unsafe.Alignof(x)等于构成数组的元素类型的对齐倍数。没有任何字段的空结构体和没有任何元素的数组占据的内存空间大小为0，不同大小为0的变量可能指向同一块地址。 细心的朋友会发发现这三个方法返回的都是uintptr类型，这个目的就是可以和unsafe.poniter类型相互转换，因为*T是不能计算偏移量的，也不能进行计算，但是uintptr是可以的，所以可以使用uintptr类型进行计算，这样就可以可以访问特定的内存了，达到对不同的内存读写的目的。三个方法的入参都是ArbitraryType类型，代表着任意类型的意思，同时还提供了一个Pointer指针类型，即像void *一样的通用型指针。 type ArbitraryType int type Pointer *ArbitraryType // uintptr 是一个整数类型，它足够大，可以存储指针的值，当然已经不是指针了，不是地址 type uintptr uintptr 上面说了这么多，可能会有点懵，在这里对三种指针类型做一个总结： *T：普通类型指针类型，用于传递对象地址，不能进行指针运算。 unsafe.poniter：通用指针类型，用于转换不同类型的指针，不能进行指针运算，不能读取内存存储的值(需转换到某一类型的普通指针) uintptr：用于指针运算，GC不把uintptr当指针，uintptr无法持有对象。uintptr类型的目标会被回收，所以一般不要独立出一个uintptr类型的变量，可能莫名其妙就被GC掉了。 三者关系就是：unsafe.Pointer是桥梁，可以让任意类型的指针实现相互转换，也可以将任意类型的指针转换为uintptr进行指针运算，也就说uintptr是用来与unsafe.Pointer打配合，用于指针运算。画个图表示一下： 基本原理就说到这里啦，接下来我们一起来看看如何使用~ ","date":"2022-08-28","objectID":"/go%E8%AF%AD%E8%A8%80unsafe%E5%8C%85%E7%9A%84%E4%BD%BF%E7%94%A8/:1:0","tags":["go编程技巧"],"title":"Go语言unsafe包的使用","uri":"/go%E8%AF%AD%E8%A8%80unsafe%E5%8C%85%E7%9A%84%E4%BD%BF%E7%94%A8/"},{"categories":["golang"],"content":"unsafe.Pointer基本使用 在atomic.Value源码里，看到atomic/value.go中定义了一个ifaceWords结构，其中typ和data字段类型就是unsafe.Poniter，这里使用unsafe.Poniter类型的原因是传入的值就是interface{}类型，使用unsafe.Pointer强转成ifaceWords类型，这样可以把类型和值都保存了下来，方便后面的写入类型检查。截取部分代码如下： // ifaceWords is interface{} internal representation. type ifaceWords struct { typ unsafe.Pointer data unsafe.Pointer } // Load returns the value set by the most recent Store. // It returns nil if there has been no call to Store for this Value. func (v *Value) Load() (x interface{}) { vp := (*ifaceWords)(unsafe.Pointer(v)) for { typ := LoadPointer(\u0026vp.typ) // 读取已经存在值的类型 /** ..... 中间省略 **/ // First store completed. Check type and overwrite data. if typ != xp.typ { //当前类型与要存入的类型做对比 panic(\"sync/atomic: store of inconsistently typed value into Value\") } } 上面就是源码中使用unsafe.Pointer的一个例子，有一天当你准备读源码时，unsafe.pointer的使用到处可见。好啦，接下来我们写一个简单的例子，看看unsafe.Pointer是如何使用的。 func main() { number := 5 pointer := \u0026number fmt.Printf(\"number:addr:%p, value:%d\\n\",pointer,*pointer) float32Number := (*float32)(unsafe.Pointer(pointer)) *float32Number = *float32Number + 3 fmt.Printf(\"float64:addr:%p, value:%f\\n\",float32Number,*float32Number) } 运行结果： number:addr:0xc000018090, value:5 float64:addr:0xc000018090, value:3.000000 由运行可知使用unsafe.Pointer强制类型转换后指针指向的地址是没有改变，只是类型发生了改变。这个例子本身没什么意义，正常项目中也不会这样使用。 总结一下基本使用：先把*T类型转换成unsafe.Pointer类型，然后在进行强制转换转成你需要的指针类型即可。 ","date":"2022-08-28","objectID":"/go%E8%AF%AD%E8%A8%80unsafe%E5%8C%85%E7%9A%84%E4%BD%BF%E7%94%A8/:2:0","tags":["go编程技巧"],"title":"Go语言unsafe包的使用","uri":"/go%E8%AF%AD%E8%A8%80unsafe%E5%8C%85%E7%9A%84%E4%BD%BF%E7%94%A8/"},{"categories":["golang"],"content":"Sizeof、Alignof、Offsetof三个函数的基本使用 先看一个例子： type User struct { Name string Age uint32 Gender bool // 男:true 女：false } func func_example() { // sizeof fmt.Println(unsafe.Sizeof(true)) fmt.Println(unsafe.Sizeof(int8(0))) fmt.Println(unsafe.Sizeof(int16(10))) fmt.Println(unsafe.Sizeof(int(10))) fmt.Println(unsafe.Sizeof(int32(190))) fmt.Println(unsafe.Sizeof(\"asong\")) fmt.Println(unsafe.Sizeof([]int{1,3,4})) // Offsetof user := User{Name: \"Asong\", Age: 23,Gender: true} userNamePointer := unsafe.Pointer(\u0026user) nNamePointer := (*string)(unsafe.Pointer(userNamePointer)) *nNamePointer = \"Golang梦工厂\" nAgePointer := (*uint32)(unsafe.Pointer(uintptr(userNamePointer) + unsafe.Offsetof(user.Age))) *nAgePointer = 25 nGender := (*bool)(unsafe.Pointer(uintptr(userNamePointer)+unsafe.Offsetof(user.Gender))) *nGender = false fmt.Printf(\"u.Name: %s, u.Age: %d, u.Gender: %v\\n\", user.Name, user.Age,user.Gender) // Alignof var b bool var i8 int8 var i16 int16 var i64 int64 var f32 float32 var s string var m map[string]string var p *int32 fmt.Println(unsafe.Alignof(b)) fmt.Println(unsafe.Alignof(i8)) fmt.Println(unsafe.Alignof(i16)) fmt.Println(unsafe.Alignof(i64)) fmt.Println(unsafe.Alignof(f32)) fmt.Println(unsafe.Alignof(s)) fmt.Println(unsafe.Alignof(m)) fmt.Println(unsafe.Alignof(p)) } 为了省事，把三个函数的使用示例放到了一起。 首先看sizeof方法 Sizeof 采用任何类型的表达式 x，并返回假设变量 v 的大小（以字节为单位），就好像 v 是通过 var v = x 声明的一样。该大小不包括 x 可能引用的任何内存。例如，如果 x 是切片，则 Sizeof 返回切片描述符的大小，而不是切片引用的内存的大小。 我们可以知道基本类型所占字节大小。这里重点说一下int、string、[]byte类型。 Go语言中的int类型的具体大小是跟机器的CPU位数相关的。如果CPU是32位的，那么int就占4字节，如果CPU是64位的，那么int就占8字节，这里我的电脑是64位的，所以结果就是8字节。 Go语言里的string类型，其实是不可变的字符串结构。字符串运行时的结构如下 type StringHeader struct { Data uintptr // 示操作系统cpu而定，64位 --\u003e 64位 Len int // 示操作系统cpu而定，64位 --\u003e 64位 } 因此，使用sizeof取string的大小时，其实是对一个结构体的sizeof，始终都是8+8=16个字节。 Go语言里的[]byte类型。切片运行时的结构如下 type SliceHeader struct { Data uintptr Len int Cap int } 因此，使用sizeof取[]byte的大小时，同string理，始终都是8+8+8=24个字节。当然，数组就不会了。 Offsetof函数 我想要修改结构体中成员变量，第一个成员变量是不需要进行偏移量计算的，直接取出指针后转换为unsafe.pointer，再强制给他转换成字符串类型的指针值即可。如果要修改其他成员变量，需要进行偏移量计算，才可以对其内存地址修改，所以Offsetof方法就可返回成员变量在结构体中的偏移量，也就是返回结构体初始位置到成员变量之间的字节数。 看代码时大家应该要住uintptr的使用，不可以用一个临时变量存储uintptr类型，前面我们提到过用于指针运算，GC不把uintptr当指针，uintptr无法持有对象。uintptr类型的目标会被回收，所以你不知道他什么时候会被GC掉，那样接下来的内存操作会发生什么样的错误，咱也不知道。比如这样一个例子： // 切记不要这样使用 p1 := uintptr(userNamePointer) nAgePointer := (*uint32)(unsafe.Pointer(p1 + unsafe.Offsetof(user.Age))) 最后看一下Alignof函数，主要是获取变量的对齐值. 除了int、uintptr这些依赖CPU位数的类型，基本类型的对齐值都是固定的，结构体中对齐值取他的成员对齐值的最大值，结构体的对齐涉及到内存对齐。 ","date":"2022-08-28","objectID":"/go%E8%AF%AD%E8%A8%80unsafe%E5%8C%85%E7%9A%84%E4%BD%BF%E7%94%A8/:3:0","tags":["go编程技巧"],"title":"Go语言unsafe包的使用","uri":"/go%E8%AF%AD%E8%A8%80unsafe%E5%8C%85%E7%9A%84%E4%BD%BF%E7%94%A8/"},{"categories":["golang"],"content":"经典应用：string与[]byte的相互转换 实现string与byte的转换，正常情况下，我们可能会写出这样的标准转换： // string to []byte str1 := \"Golang梦工厂\" by := []byte(s1) // []byte to string str2 := string(by) 使用这种方式进行转换都会涉及底层数值的拷贝，所以想要实现零拷贝，我们可以使用unsafe.Pointer来实现，通过强转换直接完成指针的指向，从而使string和[]byte指向同一个底层数据。在reflect包中有string和slice对应的结构体，他们的分别是： type StringHeader struct { Data uintptr Len int } type SliceHeader struct { Data uintptr Len int Cap int } StringHeader代表的是string运行时的表现形式(SliceHeader同理)，通过对比string和slice运行时的表达可以看出，他们只有一个Cap字段不同，所以他们的内存布局是对齐的，所以可以通过unsafe.Pointer进行转换，因为可以写出如下代码： func stringToByte(s string) []byte { header := (*reflect.StringHeader)(unsafe.Pointer(\u0026s)) newHeader := reflect.SliceHeader{ Data: header.Data, Len: header.Len, Cap: header.Len, } return *(*[]byte)(unsafe.Pointer(\u0026newHeader)) } func bytesToString(b []byte) string{ header := (*reflect.SliceHeader)(unsafe.Pointer(\u0026b)) newHeader := reflect.StringHeader{ Data: header.Data, Len: header.Len, } return *(*string)(unsafe.Pointer(\u0026newHeader)) } 上面的代码我们通过重新构造slice header和string header完成了类型转换，其实[]byte转换成string可以省略掉自己构造StringHeader的方式，直接使用强转就可以，因为string的底层也是[]byte，强转会自动构造，省略后的代码如下： func bytesToString(b []byte) string { return *(* string)(unsafe.Pointer(\u0026b)) } 虽然这种方式更高效率，但是不推荐大家使用，前面也提高到了，这要是不安全的，使用当不当会出现极大的隐患，一些严重的情况recover也不能捕获。 ","date":"2022-08-28","objectID":"/go%E8%AF%AD%E8%A8%80unsafe%E5%8C%85%E7%9A%84%E4%BD%BF%E7%94%A8/:4:0","tags":["go编程技巧"],"title":"Go语言unsafe包的使用","uri":"/go%E8%AF%AD%E8%A8%80unsafe%E5%8C%85%E7%9A%84%E4%BD%BF%E7%94%A8/"},{"categories":["mysql"],"content":"本文主要补充一些gorm的使用技巧，完整的gorm使用移步官方文档gorm官方文档 ","date":"2022-08-28","objectID":"/gorm%E4%BD%BF%E7%94%A8%E7%BB%8F%E9%AA%8C%E6%80%BB%E7%BB%93/:0:0","tags":["gorm"],"title":"Gorm使用补充点","uri":"/gorm%E4%BD%BF%E7%94%A8%E7%BB%8F%E9%AA%8C%E6%80%BB%E7%BB%93/"},{"categories":["mysql"],"content":"总结 配置单数表名, 再也不用写TableName db, err := gorm.Open(mysql.Open(fmt.Sprintf(dsn, un, pwd, host, port, database)), \u0026gorm.Config{ NamingStrategy: schema.NamingStrategy{SingularTable: true}, }) 当然也可以传入其他参数定制命名策略 创建模型迁移表时，针对string类型一定要给出gorm的type约束，否则默认是创建mysql的字符串最大数据类型longtext较为浪费资源 使用DB前先db.model, 万无一失 创建模型时使用基本类型的指针类型，可以使得零值保存到数据库 CreatedAt, UpdatedAt, mysql类型用datetime(3)(毫秒) 支持db.Create创建和批量创建，创建成功时回填结构体里主键字段或结构体切片每个单元的主键字段 Save方法没有查找到，就会创建记录；查找到就会以Save里的参数替换整行数据，即使是零值也会替换 BeforeCreate创建前自动填写id AfterUpdate更新后自动创建record Where用结构体查询, 自动忽略零值, 不用手打列名 ErrRecordNotFound只会出现在Take, First,Last方法中，如果发生了多个错误，你可以通过 errors.Is 判断错误是否为 ErrRecordNotFound Find结构体时: 等同于Take, 但没有ErrRecordNotFound, RowsAffected为0或1 行锁的写法DB.Clauses(clause.Locking{Strength: \"UPDATE\") Updates支持结构体更新, 自动忽略零值, 不用手打列名. 强行更新零值用Select Update支持gorm.ExprSQL表达式更新 事务直接用db.Transaction方法即可 Update更新时，如果更新失败，但是返回不一定报错。需要综合检测 RowsAffected 零值创建、更新、作为条件等时，使用struct会失效，应该使用map[string]interface{}来指定。 使用map[string]interface{}还可以使用SQL表达式 gorm外键多表联查时可以考虑使用Preload预加载或嵌套预加载 Pluck 用于从数据库查询单个列，并将结果扫描到切片。如果您想要查询多列，您应该使用 Select 和 Scan。某些业务场景下，这个Pluck方法很有用 gorm支持查询创建更新删除等前后的钩子函数 gorm不仅支持创建多条记录，还支持分批创建多条记录（CreateInBatches）或分批查询多条记录（FindInBatches） Scopes 允许你指定常用的查询，可以在调用方法时引用这些查询。可以根据业务需要集成某些常用的业务查询条件。 作用域允许你复用通用的逻辑，这种共享逻辑需要定义为类型func(*gorm.DB) *gorm.DB。 func AmountGreaterThan1000(db *gorm.DB) *gorm.DB { return db.Where(\"amount \u003e ?\", 1000) } func PaidWithCreditCard(db *gorm.DB) *gorm.DB { return db.Where(\"pay_mode_sign = ?\", \"C\") } func PaidWithCod(db *gorm.DB) *gorm.DB { return db.Where(\"pay_mode_sign = ?\", \"C\") } func OrderStatus(status []string) func (db *gorm.DB) *gorm.DB { return func (db *gorm.DB) *gorm.DB { return db.Where(\"status IN (?)\", status) } } db.Scopes(AmountGreaterThan1000, PaidWithCreditCard).Find(\u0026orders) // 查找所有金额大于 1000 的信用卡订单 db.Scopes(AmountGreaterThan1000, PaidWithCod).Find(\u0026orders) // 查找所有金额大于 1000 的 COD 订单 db.Scopes(AmountGreaterThan1000, OrderStatus([]string{\"paid\", \"shipped\"})).Find(\u0026orders) // 查找所有金额大于1000 的已付款或已发货订单 Save 用来更新，会保存所有的字段，即使字段是零值。单列更新推荐使用Update或Updates,多列更新但又不是所有列推荐使用Updates，所有列更新可以使用Save Select选择字段或表、Omit跳过字段或表。使用 Struct 进行 Select（会 select 零值的字段） db.Model(\u0026user).Select(\"Name\", \"Age\").Updates(User{Name: \"new_name\", Age: 0}) // UPDATE users SET name='new_name', age=0 WHERE id=111; 如果在没有任何条件的情况下执行批量更新，默认情况下，GORM 不会执行该操作，并返回 ErrMissingWhereClause 错误.对此，你必须加一些条件，或者使用原生 SQL，或者启用 AllowGlobalUpdate 模式，例如： db.Model(\u0026User{}).Update(\"name\", \"jinzhu\").Error // gorm.ErrMissingWhereClause db.Model(\u0026User{}).Where(\"1 = 1\").Update(\"name\", \"jinzhu\") // UPDATE users SET `name` = \"jinzhu\" WHERE 1=1 db.Exec(\"UPDATE users SET name = ?\", \"jinzhu\") // UPDATE users SET name = \"jinzhu\" db.Session(\u0026gorm.Session{AllowGlobalUpdate: true}).Model(\u0026User{}).Update(\"name\", \"jinzhu\") // UPDATE users SET `name` = \"jinzhu\" 如果您想在更新时跳过 Hook 方法且不追踪更新时间，可以使用 UpdateColumn、UpdateColumns，其用法类似于 Update、Updates 删除一条记录时，删除对象需要指定主键，否则会触发 批量 Delete 如果在没有任何条件的情况下执行批量删除，GORM 不会执行该操作，并返回 ErrMissingWhereClause 错误.对此，你必须加一些条件，或者使用原生 SQL，或者启用 AllowGlobalUpdate 模式 返回被删除的数据，仅适用于支持 Returning 的数据库 // 返回所有列 var users []User DB.Clauses(clause.Returning{}).Where(\"role = ?\", \"admin\").Delete(\u0026users) // DELETE FROM `users` WHERE role = \"admin\" RETURNING * // users =\u003e []User{{ID: 1, Name: \"jinzhu\", Role: \"admin\", Salary: 100}, {ID: 2, Name: \"jinzhu.2\", Role: \"admin\", Salary: 1000}} 如果您的模型包含了一个 gorm.deletedat 字段（gorm.Model 已经包含了该字段)，它将自动获得软删除的能力！拥有软删除能力的模型调用 Delete 时，记录不会从数据库中被真正删除。但 GORM 会将 DeletedAt 置为当前时间， 并且你不能再通过普通的查询方法找到该记录。值得注意的是：软删除不是真真的删除，如果数据表再设计的时候加入太多约束，可能会引发软删除与约束相互矛盾，例如给数据表某个字段加上unique的约束，显然软删除后再添加相同的数据会出问题。 有一种折中的方案：放弃这个唯一，删除的时候还是采用软删除，但是查询的时候需要使用First来查询最新的一条记录，这样来看就是可以的。缺点是，无法保证数据表这个字段的唯一性。 您可以使用 Unscoped 找到被软删除的记录，您也可以使用 Unscoped 永久删除匹配的记录 原生DQL使用Raw，扫描结果使用Scan；原生DML使用Exec 关联关系很好用–\u003e 例子详解：https://juejin.cn/post/7067138794788487181 belongs to belongs to 会与另一个模型建立了一对一的连接。这种模型的每一个实例都“属于”另一个模型的一个实例。 例如，您的应用包含 user 和 company，并且每个 user 能且只能被分配给一个 company。下面的类型就表示这种关系。 注意，在 User 对象中，有一个和 Company 一样的 CompanyID。 默认情况","date":"2022-08-28","objectID":"/gorm%E4%BD%BF%E7%94%A8%E7%BB%8F%E9%AA%8C%E6%80%BB%E7%BB%93/:1:0","tags":["gorm"],"title":"Gorm使用补充点","uri":"/gorm%E4%BD%BF%E7%94%A8%E7%BB%8F%E9%AA%8C%E6%80%BB%E7%BB%93/"},{"categories":["mysql"],"content":"从表名开始 如何定义表名? gorm的默认表名策略是模型名字的复数, 比如: type User Struct{} 默认表名为users, 但是生产习惯常用单数表名, 而且加es的复数或者sheep单复同形容易让人迷惑. 当然, gorm实现了复杂的单数变复数逻辑, 我们可以从 http://github.com/jinzhu/inflection/inflections.go 一探究竟, 下面复习下单复同型/不可数单词, 以及特殊变复数的单词: var uncountableInflections = []string{\"equipment\", \"information\", \"rice\", \"money\", \"species\", \"series\", \"fish\", \"sheep\", \"jeans\", \"police\"} var irregularInflections = IrregularSlice{ {\"person\", \"people\"}, {\"man\", \"men\"}, {\"child\", \"children\"}, {\"sex\", \"sexes\"}, {\"move\", \"moves\"}, {\"mombie\", \"mombies\"}, } 好吧, 写个增删改查还得过专八. 我们可以实现gorm.schema.scheme.go.Tabler.TableName()方法, 重写表名. func (*User) TableName() string {return \"user\"} OK! 除了这种奇奇怪怪的写法, 在gormV2中, 我们可以使用一个配置改为单数表名, 谢天谢地. NamingStrategy: \u0026schema.NamingStrategy{SingularTable: true} schema.NamingStrategy实现了gorm.schema.naming.go.Namer接口, 我们也可以自己实现这个接口,替换gorm的命名策略, 接口简单易懂. type Namer interface { TableName(table string) string ColumnName(table, column string) string JoinTableName(joinTable string) string RelationshipFKName(Relationship) string CheckerName(table, column string) string IndexName(table, column string) string } ","date":"2022-08-28","objectID":"/gorm%E4%BD%BF%E7%94%A8%E7%BB%8F%E9%AA%8C%E6%80%BB%E7%BB%93/:2:0","tags":["gorm"],"title":"Gorm使用补充点","uri":"/gorm%E4%BD%BF%E7%94%A8%E7%BB%8F%E9%AA%8C%E6%80%BB%E7%BB%93/"},{"categories":["mysql"],"content":"列名 列名无可非议, 简简单单的下划线模式. 而且gorm会自动将ID转换成id而不是i_d, 那么, 就遵循golang的语言规范, 放心的在代码中使用ID吧~ 让我们来看下这个特殊变小写数组. 列名变小写? gorm.schema.naming.go // https://github.com/golang/lint/blob/master/lint.go#L770 commonInitialisms = []string{\"API\", \"ASCII\", \"CPU\", \"CSS\", \"DNS\", \"EOF\", \"GUID\", \"HTML\", \"HTTP\", \"HTTPS\", \"ID\", \"IP\", \"JSON\", \"LHS\", \"QPS\", \"RAM\", \"RHS\", \"RPC\", \"SLA\", \"SMTP\", \"SSH\", \"TLS\", \"TTL\", \"UID\", \"UI\", \"UUID\", \"URI\", \"URL\", \"UTF8\", \"VM\", \"XML\", \"XSRF\", \"XSS\"} CreatedAt, UpdatedAt 更加有争议和难以理解的列名,应该是这两个. 最原始的方法 ,mysql自动添加 如果想让mysql来做这件事, 可以这样写: create table user2 ( .... create_time datetime(3) not null DEFAULT CURRENT_TIMESTAMP(3) comment '创建时间', update_time datetime(3) not null DEFAULT CURRENT_TIMESTAMP(3) ON UPDATE CURRENT_TIMESTAMP(3) comment '修改时间', .... ); type User struct { ID int64 `gorm:\"autoIncrement\"` CreatedAt time.Time `gorm:default` UpdatedAt time.Time `gorm:default` } 加上gorm的default标签, 这个标签的意思是: gorm.Create时, 如果该列值为零, 使用建表时的default值, sql语句中也就没有这一列. gorm.Updates如果Update一个map,就是强制更新. 如果Update一个结构体, 不赋值也就是用mysql的默认值, 如果赋值就直接Update, 这一点很巧妙. gorm对默认值的处理: gorm.callbacks.create.go:285 defaultValueFieldsHavingValue := map[*schema.Field][]interface{}{} for _, field := range stmt.Schema.FieldsWithDefaultDBValue { if v, ok := selectColumns[field.DBName]; (ok \u0026\u0026 v) || (!ok \u0026\u0026 !restricted) { if v, isZero := field.ValueOf(rv); !isZero { if len(defaultValueFieldsHavingValue[field]) == 0 { defaultValueFieldsHavingValue[field] = make([]interface{}, stmt.ReflectValue.Len()) } defaultValueFieldsHavingValue[field][i] = v } } } 你要说我能看懂, 那我肯定看不懂, 但你要说看不懂, 我还知道它能干啥, 不错不错… 更好的方法，gorm自动添加 如果想让gorm来添加, sql就可以不用写default type User struct { ID int64 `gorm:\"autoIncrement\"` CreatedAt time.Time UpdatedAt time.Time } 只要列名叫CreatedAt和UpdatedAt即可. gorm.Create即可自动更新CreatedAt和UpdatedAt. 但是gorm.Updates时可要注意了, 如果没有使用db.Model指定model结构体, 就不能更新UpdatedAt. 所以如果这样: db.Table(tableName).Updates(map[string]interface{}) 是无法更新更新时间的. 正确做法是: db.Model(\u0026User{}).Updates(map[string]interface{}) 如果傲娇的你, 偏偏不喜欢这样的列名, 非要用CreateTime, 那好吧, 你可以这样: 不告诉你, 自己去查文档… https://gorm.io/zh_CN/docs/models.html 小建议: 建议建表时使用datetime(3)秒类型 ","date":"2022-08-28","objectID":"/gorm%E4%BD%BF%E7%94%A8%E7%BB%8F%E9%AA%8C%E6%80%BB%E7%BB%93/:3:0","tags":["gorm"],"title":"Gorm使用补充点","uri":"/gorm%E4%BD%BF%E7%94%A8%E7%BB%8F%E9%AA%8C%E6%80%BB%E7%BB%93/"},{"categories":["mysql"],"content":"Create https://gorm.io/zh_CN/docs/create.html gormV2支持创建和批量创建, 你可以这样写: //创建 db.Create(\u0026User{}) //批量创建 users := []*User{{},{},{}} db.Create(\u0026users) // 创建成功会往结构体或结构体切片回填主键的值。 ","date":"2022-08-28","objectID":"/gorm%E4%BD%BF%E7%94%A8%E7%BB%8F%E9%AA%8C%E6%80%BB%E7%BB%93/:4:0","tags":["gorm"],"title":"Gorm使用补充点","uri":"/gorm%E4%BD%BF%E7%94%A8%E7%BB%8F%E9%AA%8C%E6%80%BB%E7%BB%93/"},{"categories":["mysql"],"content":"Hooks 我们可以灵活的使用hooks, 以减少重复代码在logic层对业务逻辑的侵入, 使代码更简洁. 钩子命名好像Vue啊! 哈哈哈 ","date":"2022-08-28","objectID":"/gorm%E4%BD%BF%E7%94%A8%E7%BB%8F%E9%AA%8C%E6%80%BB%E7%BB%93/:5:0","tags":["gorm"],"title":"Gorm使用补充点","uri":"/gorm%E4%BD%BF%E7%94%A8%E7%BB%8F%E9%AA%8C%E6%80%BB%E7%BB%93/"},{"categories":["mysql"],"content":"Where gorm中最常用的语句, gormv1中最常用的方法是: db.Where(\"uid = ?\", 1) gormV2支持两种新的where, Struct和map条件, 本文介绍struct条件: db.Where(\u0026User{phone:\"123\", Age:0}) //Select * from User where phone = 123 //Age没有被查询 结构体查询非常好用, 你不用手动写列名, 避免了因为列名写错而导致的错误. 注意: 当使用结构作为条件查询时，GORM 只会查询非零值字段。这意味着如果您的字段值为0 、'' 、false 或其他零值，该字段不会被用于构建查询条件 ","date":"2022-08-28","objectID":"/gorm%E4%BD%BF%E7%94%A8%E7%BB%8F%E9%AA%8C%E6%80%BB%E7%BB%93/:6:0","tags":["gorm"],"title":"Gorm使用补充点","uri":"/gorm%E4%BD%BF%E7%94%A8%E7%BB%8F%E9%AA%8C%E6%80%BB%E7%BB%93/"},{"categories":["mysql"],"content":"Find, Take, First, Last ErrRecordNotFound 这个东西可是实在太恶心了… 但是golang的反射又没法对ptr赋值为nil, 所以只能通过error返回. 这种处理方式也是可以理解的. err := db.Where(...).First(...).Error // 检查 ErrRecordNotFound 错误 errors.Is(err, gorm.ErrRecordNotFound) 注意: 只有First,Last,Take方法会产生gorm.ErrRecordNotFound错误哦 Find 为了躲避ErrRecordNotFound, 我们来看下这个可爱的Find方法. Find方法可以接受两种参数, 一种是结构体指针, 一种是数组指针. 接受数组指针很好理解, 我们可以通过数组长度来判断是否查到数据: res := []*model.User{} ret := db.Model(\u0026model.User{}).Where(`user_id \u003e ?`, 0).Find(res) 接受结构体指针时呢? 我们只能通过ret.RowsAffected == 0来判断是否查到数据, 所以ErrRecordNotFound还有点可爱? 注意:db.Find(\u0026User{}).RowsAffected只会是0或1 res := \u0026model.User{} ret := db.Model(\u0026model.User{}).Where(`user_id \u003e ?`, 0).Find(res) ret.RowsAffected 从源码来看, Find结构体和Find数组的差别如下: gorm.scan.go:214 switch(){ case reflect.Slice, reflect.Array: for initialized || rows.Next() { } case reflect.Struct: if initialized || rows.Next() { } } OMG! 仅仅是for和if的差别… 那么Find和Take有什么差别呢? 你猜的没错, 只是Take会返回ErrRecordNotFound gorm.scan.go:239 if db.RowsAffected == 0 \u0026\u0026 db.Statement.RaiseErrorOnNotFound { db.AddError(ErrRecordNotFound) } //Find RaiseErrorOnNotFound=false //Take RaiseErrorOnNotFound=true 好家伙! 我直接好家伙! 果然是大佬的代码… 行锁 哪里的代码有version乐观锁和share锁, 我去参观只见过UPDATE锁. gormV2和V1这里确实不一样, 写法如下: DB.Clauses(clause.Locking{Strength: \"UPDATE\"}).Find(\u0026users) mysql行锁只在事务中有用. ","date":"2022-08-28","objectID":"/gorm%E4%BD%BF%E7%94%A8%E7%BB%8F%E9%AA%8C%E6%80%BB%E7%BB%93/:7:0","tags":["gorm"],"title":"Gorm使用补充点","uri":"/gorm%E4%BD%BF%E7%94%A8%E7%BB%8F%E9%AA%8C%E6%80%BB%E7%BB%93/"},{"categories":["mysql"],"content":"Update gormV2的更新是我最喜欢的一部分, 非常的有趣… 零值 我们不再需要这样, 我讨厌的方式: var updateMap := make(map[string]interface{}) if phone != \"\" { updateMap[\"phone\"] = phone } db.Updates(updateMap) 我们只需要这样: db.Updates(\u0026User{Phone:phone}) Updates方法接受一个Model结构体, 如果更新的列为 \"\",0,false,time.Time{} 就会不更新该列. (如果使用gorm-curd, 连数组长度都帮你判断了,真的是太好用了!) 那你要问, 如果我非要设置这个用户的phone为\"\"呢? 你可以这样写: db.Select(\"phone\").Updates(\u0026User{Phone:\"\"}) //或者 db.Updates(map[string]interface{}{\"phone\":\"\"}) 又回去map了… 梅开二度… SQL 表达式 如果需要商品库存 - 1, gormV2不再需要用事务取出, 再-1 , 再存入… 可以这样: DB.Model(\u0026product).Where(\"goods_id = ?\", 10086).Update(\"quantity\", gorm.Expr(\"quantity - ?\", 1)) ","date":"2022-08-28","objectID":"/gorm%E4%BD%BF%E7%94%A8%E7%BB%8F%E9%AA%8C%E6%80%BB%E7%BB%93/:8:0","tags":["gorm"],"title":"Gorm使用补充点","uri":"/gorm%E4%BD%BF%E7%94%A8%E7%BB%8F%E9%AA%8C%E6%80%BB%E7%BB%93/"},{"categories":["mysql"],"content":"事务 要在事务中执行一系列操作，一般流程如下, 这个事务写法真的是太棒了! db.Transaction(func(tx *gorm.DB) error { // 在事务中执行一些 db 操作（从这里开始，您应该使用 'tx' 而不是 'db'） if err := tx.Create(\u0026Animal{Name: \"Giraffe\"}).Error; err != nil { // 返回任何错误都会回滚事务 return err } if err := tx.Create(\u0026Animal{Name: \"Lion\"}).Error; err != nil { return err } // 返回 nil 提交事务 return nil }) 嵌套事务 GORM 支持嵌套事务，可以回滚事务中的事务而不用担心外层事务回滚. ","date":"2022-08-28","objectID":"/gorm%E4%BD%BF%E7%94%A8%E7%BB%8F%E9%AA%8C%E6%80%BB%E7%BB%93/:9:0","tags":["gorm"],"title":"Gorm使用补充点","uri":"/gorm%E4%BD%BF%E7%94%A8%E7%BB%8F%E9%AA%8C%E6%80%BB%E7%BB%93/"},{"categories":["git"],"content":"Git 每次提交代码，都要写 Commit message（提交说明），否则就不允许提交。 $ git commit -m \"hello world\" 上面代码的-m参数，就是用来指定 commit mesage 的。 如果一行不够，可以只执行git commit，就会跳出文本编辑器，让你写多行。 $ git commit 基本上，你写什么都行（这里，这里和这里）。 但是，一般来说，commit message 应该清晰明了，说明本次提交的目的。 目前，社区有多种 Commit message 的写法规范。本文介绍Angular 规范（见上图），这是目前使用最广的写法，比较合理和系统化，并且有配套的工具。 ","date":"2022-08-25","objectID":"/git%E4%B9%8Bcommit%E8%A7%84%E8%8C%83%E6%8C%87%E5%8D%97/:0:0","tags":[],"title":"Git之commit规范指南","uri":"/git%E4%B9%8Bcommit%E8%A7%84%E8%8C%83%E6%8C%87%E5%8D%97/"},{"categories":["git"],"content":"Commit message 的作用 格式化的Commit message，有几个好处。 （1）提供更多的历史信息，方便快速浏览。 比如，下面的命令显示上次发布后的变动，每个commit占据一行。你只看行首，就知道某次 commit 的目的。 $ git log \u003clast tag\u003e HEAD --pretty=format:%s （2）可以过滤某些commit（比如文档改动），便于快速查找信息。 比如，下面的命令仅仅显示本次发布新增加的功能。 $ git log \u003clast release\u003e HEAD --grep feature （3）可以直接从commit生成Change log。 Change Log 是发布新版本时，用来说明与上一个版本差异的文档，详见后文。 ","date":"2022-08-25","objectID":"/git%E4%B9%8Bcommit%E8%A7%84%E8%8C%83%E6%8C%87%E5%8D%97/:1:0","tags":[],"title":"Git之commit规范指南","uri":"/git%E4%B9%8Bcommit%E8%A7%84%E8%8C%83%E6%8C%87%E5%8D%97/"},{"categories":["git"],"content":"Commit message 的格式 每次提交，Commit message 都包括三个部分：Header，Body 和 Footer。 \u003ctype\u003e(\u003cscope\u003e): \u003csubject\u003e // 空一行 \u003cbody\u003e // 空一行 \u003cfooter\u003e 其中，Header 是必需的，Body 和 Footer 可以省略。 不管是哪一个部分，任何一行都不得超过72个字符（或100个字符）。这是为了避免自动换行影响美观。 ","date":"2022-08-25","objectID":"/git%E4%B9%8Bcommit%E8%A7%84%E8%8C%83%E6%8C%87%E5%8D%97/:2:0","tags":[],"title":"Git之commit规范指南","uri":"/git%E4%B9%8Bcommit%E8%A7%84%E8%8C%83%E6%8C%87%E5%8D%97/"},{"categories":["git"],"content":"Header Header部分只有一行，包括三个字段：type（必需）、scope（可选）和subject（必需）。 （1）type type用于说明 commit 的类别，只允许使用下面7个标识。 feat：新功能（feature） fix：修补bug docs：文档（documentation） style： 格式（不影响代码运行的变动） refactor：重构（即不是新增功能，也不是修改bug的代码变动） test：增加测试 chore：构建过程或辅助工具的变动 perf：性能优化 build：构建工具或外部依赖包的修改，比如更新依赖包的版本等 ci：持续集成的配置文件或脚本的修改 如果type为feat和fix，则该 commit 将肯定出现在 Change log 之中。其他情况（docs、chore、style、refactor、test）由你决定，要不要放入 Change log，建议是不要。 （2）scope scope用于说明 commit 影响的范围，比如数据层、控制层、视图层等等，视项目不同而不同。 （3）subject subject是 commit 目的的简短描述，不超过50个字符。 以动词开头，使用第一人称现在时，比如change，而不是changed或changes 第一个字母小写 结尾不加句号（.） ","date":"2022-08-25","objectID":"/git%E4%B9%8Bcommit%E8%A7%84%E8%8C%83%E6%8C%87%E5%8D%97/:2:1","tags":[],"title":"Git之commit规范指南","uri":"/git%E4%B9%8Bcommit%E8%A7%84%E8%8C%83%E6%8C%87%E5%8D%97/"},{"categories":["git"],"content":"Body Body 部分是对本次 commit 的详细描述，可以分成多行。下面是一个范例。 More detailed explanatory text, if necessary. Wrap it to about 72 characters or so. Further paragraphs come after blank lines. - Bullet points are okay, too - Use a hanging indent 有两个注意点。 （1）使用第一人称现在时，比如使用change而不是changed或changes。 （2）应该说明代码变动的动机，以及与以前行为的对比。 ","date":"2022-08-25","objectID":"/git%E4%B9%8Bcommit%E8%A7%84%E8%8C%83%E6%8C%87%E5%8D%97/:2:2","tags":[],"title":"Git之commit规范指南","uri":"/git%E4%B9%8Bcommit%E8%A7%84%E8%8C%83%E6%8C%87%E5%8D%97/"},{"categories":["git"],"content":"Footer Footer 部分只用于两种情况。 （1）不兼容变动 如果当前代码与上一个版本不兼容，则 Footer 部分以BREAKING CHANGE开头，后面是对变动的描述、以及变动理由和迁移方法。 BREAKING CHANGE: isolate scope bindings definition has changed. To migrate the code follow the example below: Before: scope: { myAttr: 'attribute', } After: scope: { myAttr: '@', } The removed `inject` wasn't generaly useful for directives so there should be no code using it. （2）关闭 Issue 如果当前 commit 针对某个issue，那么可以在 Footer 部分关闭这个 issue 。 Closes #234 也可以一次关闭多个 issue 。 Closes #123, #245, #992 ","date":"2022-08-25","objectID":"/git%E4%B9%8Bcommit%E8%A7%84%E8%8C%83%E6%8C%87%E5%8D%97/:2:3","tags":[],"title":"Git之commit规范指南","uri":"/git%E4%B9%8Bcommit%E8%A7%84%E8%8C%83%E6%8C%87%E5%8D%97/"},{"categories":["git"],"content":"Revert 还有一种特殊情况，如果当前 commit 用于撤销以前的 commit，则必须以revert:开头，后面跟着被撤销 Commit 的 Header。 revert: feat(pencil): add 'graphiteWidth' option This reverts commit 667ecc1654a317a13331b17617d973392f415f02. Body部分的格式是固定的，必须写成This reverts commit \u003chash\u003e.，其中的hash是被撤销 commit 的 SHA 标识符。 如果当前 commit 与被撤销的 commit，在同一个发布（release）里面，那么它们都不会出现在 Change log 里面。如果两者在不同的发布，那么当前 commit，会出现在 Change log 的Reverts小标题下面。 ","date":"2022-08-25","objectID":"/git%E4%B9%8Bcommit%E8%A7%84%E8%8C%83%E6%8C%87%E5%8D%97/:2:4","tags":[],"title":"Git之commit规范指南","uri":"/git%E4%B9%8Bcommit%E8%A7%84%E8%8C%83%E6%8C%87%E5%8D%97/"},{"categories":["git"],"content":"Commit规范示例 https://github.com/go-redis/redis https://github.com/gomodule/redigo ","date":"2022-08-25","objectID":"/git%E4%B9%8Bcommit%E8%A7%84%E8%8C%83%E6%8C%87%E5%8D%97/:3:0","tags":[],"title":"Git之commit规范指南","uri":"/git%E4%B9%8Bcommit%E8%A7%84%E8%8C%83%E6%8C%87%E5%8D%97/"},{"categories":["git"],"content":"关于版本控制 什么是版本控制？我为什么要关心它呢？ 版本控制是一种记录一个或若干文件内容变化，以便将来查阅特定版本修订情况的系统。 在本书所展示的例子中，我们仅对保存着软件源代码的文本文件作版本控制管理，但实际上，你可以对任何类型的文件进行版本控制。 如果你是位图形或网页设计师，可能会需要保存某一幅图片或页面布局文件的所有修订版本（这或许是你非常渴望拥有的功能）。采用版本控制系统（VCS）是个明智的选择。有了它你就可以将某个文件回溯到之前的状态，甚至将整个项目都回退到过去某个时间点的状态。你可以比较文件的变化细节，查出最后是谁修改了哪个地方，从而找出导致怪异问题出现的原因，又是谁在何时报告了某个功能缺陷等等。使用版本控制系统通常还意味着，就算你乱来一气把整个项目中的文件改的改删的删，你也照样可以轻松恢复到原先的样子。但额外增加的工作量却微乎其微。 ","date":"2022-08-25","objectID":"/git%E5%B7%A5%E4%BD%9C%E6%B5%81/:1:0","tags":[],"title":"Git工作流","uri":"/git%E5%B7%A5%E4%BD%9C%E6%B5%81/"},{"categories":["git"],"content":"本地版本控制系统 许多人习惯用复制整个项目目录的方式来保存不同的版本，或许还会改名加上备份时间以示区别。这么做唯一的好处就是简单。不过坏处也不少：有时候会混淆所在的工作目录，一旦弄错文件丢了数据就没法撤销恢复。 为了解决这个问题，人们很久以前就开发了许多种本地版本控制系统，大多都是采用某种简单的数据库来记录文件的历次更新差异（见图）。 其中最流行的一种叫做 rcs，现今许多计算机系统上都还看得到它的踪影。甚至在流行的 Mac OS X 系统上安装了开发者工具包之后，也可以使用 rcs 命令。它的工作原理基本上就是保存并管理文件补丁（patch）。文件补丁是一种特定格式的文本文件，记录着对应文件修订前后的内容变化。所以，根据每次修订后的补丁，rcs 可以通过不断打补丁，计算出各个版本的文件内容。 ","date":"2022-08-25","objectID":"/git%E5%B7%A5%E4%BD%9C%E6%B5%81/:1:1","tags":[],"title":"Git工作流","uri":"/git%E5%B7%A5%E4%BD%9C%E6%B5%81/"},{"categories":["git"],"content":"集中化的版本控制系统 接下来人们又遇到一个问题，如何让在不同系统上的开发者协同工作？于是，集中化的版本控制系统（ Centralized Version Control Systems，简称 CVCS ）应运而生。这类系统，诸如 CVS，Subversion 以及 Perforce 等，都有一个单一的集中管理的服务器，保存所有文件的修订版本，而协同工作的人们都通过客户端连到这台服务器，取出最新的文件或者提交更新。多年以来，这已成为版本控制系统的标准做法（见图 1-2）。 这种做法带来了许多好处，特别是相较于老式的本地 VCS 来说。现在，每个人都可以在一定程度上看到项目中的其他人正在做些什么。而管理员也可以轻松掌控每个开发者的权限，并且管理一个 CVCS 要远比在各个客户端上维护本地数据库来得轻松容易。 事分两面，有好有坏。这么做最显而易见的缺点是中央服务器的单点故障。如果宕机一小时，那么在这一小时内，谁都无法提交更新，也就无法协同工作。要是中央服务器的磁盘发生故障，碰巧没做备份，或者备份不够及时，就会有丢失数据的风险。最坏的情况是彻底丢失整个项目的所有历史更改记录，而被客户端偶然提取出来的保存在本地的某些快照数据就成了恢复数据的希望。但这样的话依然是个问题，你不能保证所有的数据都已经有人事先完整提取出来过。本地版本控制系统也存在类似问题，只要整个项目的历史记录被保存在单一位置，就有丢失所有历史更新记录的风险。 ","date":"2022-08-25","objectID":"/git%E5%B7%A5%E4%BD%9C%E6%B5%81/:1:2","tags":[],"title":"Git工作流","uri":"/git%E5%B7%A5%E4%BD%9C%E6%B5%81/"},{"categories":["git"],"content":"分布式版本控制系统 于是分布式版本控制系统（ Distributed Version Control System，简称 DVCS ）面世了。在这类系统中，像 Git，Mercurial，Bazaar 以及 Darcs 等，客户端并不只提取最新版本的文件快照，而是把代码仓库完整地镜像下来。这么一来，任何一处协同工作用的服务器发生故障，事后都可以用任何一个镜像出来的本地仓库恢复。因为每一次的提取操作，实际上都是一次对代码仓库的完整备份（见图）。 更进一步，许多这类系统都可以指定和若干不同的远端代码仓库进行交互。籍此，你就可以在同一个项目中，分别和不同工作小组的人相互协作。你可以根据需要设定不同的协作流程，比如层次模型式的工作流，而这在以前的集中式系统中是无法实现的。 ","date":"2022-08-25","objectID":"/git%E5%B7%A5%E4%BD%9C%E6%B5%81/:1:3","tags":[],"title":"Git工作流","uri":"/git%E5%B7%A5%E4%BD%9C%E6%B5%81/"},{"categories":["git"],"content":"Git 基础 那么，简单地说，Git 究竟是怎样的一个系统呢？请注意，接下来的内容非常重要，若是理解了 Git 的思想和基本工作原理，用起来就会知其所以然，游刃有余。在开始学习 Git 的时候，请不要尝试把各种概念和其他版本控制系统（诸如 Subversion 和 Perforce 等）相比拟，否则容易混淆每个操作的实际意义。Git 在保存和处理各种信息的时候，虽然操作起来的命令形式非常相近，但它与其他版本控制系统的做法颇为不同。理解这些差异将有助于你准确地使用 Git 提供的各种工具。 直接记录快照，而非差异比较 Git 和其他版本控制系统的主要差别在于，Git 只关心文件数据的整体是否发生变化，而大多数其他系统则只关心文件内容的具体差异。这类系统（CVS，Subversion，Perforce，Bazaar 等等）每次记录有哪些文件作了更新，以及都更新了哪些行的什么内容，请看图。 Git 并不保存这些前后变化的差异数据。实际上，Git 更像是把变化的文件作快照后，记录在一个微型的文件系统中。每次提交更新时，它会纵览一遍所有文件的指纹信息并对文件作一快照，然后保存一个指向这次快照的索引。为提高性能，若文件没有变化，Git 不会再次保存，而只对上次保存的快照作一链接。Git 的工作方式就像图 1-5 所示。 这是 Git 同其他系统的重要区别。它完全颠覆了传统版本控制的套路，并对各个环节的实现方式作了新的设计。Git 更像是个小型的文件系统，但它同时还提供了许多以此为基础的超强工具，而不只是一个简单的 VCS。稍后在第三章讨论 Git 分支管理的时候，我们会再看看这样的设计究竟会带来哪些好处。 近乎所有操作都是本地执行 在 Git 中的绝大多数操作都只需要访问本地文件和资源，不用连网。但如果用 CVCS 的话，差不多所有操作都需要连接网络。因为 Git 在本地磁盘上就保存着所有当前项目的历史更新，所以处理起来速度飞快。 举个例子，如果要浏览项目的历史更新摘要，Git 不用跑到外面的服务器上去取数据回来，而直接从本地数据库读取后展示给你看。所以任何时候你都可以马上翻阅，无需等待。如果想要看当前版本的文件和一个月前的版本之间有何差异，Git 会取出一个月前的快照和当前文件作一次差异运算，而不用请求远程服务器来做这件事，或是把老版本的文件拉到本地来作比较。 用 CVCS 的话，没有网络或者断开 VPN 你就无法做任何事情。但用 Git 的话，就算你在飞机或者火车上，都可以非常愉快地频繁提交更新，等到了有网络的时候再上传到远程仓库。同样，在回家的路上，不用连接 VPN 你也可以继续工作。换作其他版本控制系统，这么做几乎不可能，抑或非常麻烦。比如 Perforce，如果不连到服务器，几乎什么都做不了（译注：默认无法发出命令 p4 edit file 开始编辑文件，因为 Perforce 需要联网通知系统声明该文件正在被谁修订。但实际上手工修改文件权限可以绕过这个限制，只是完成后还是无法提交更新。）；如果是 Subversion 或 CVS，虽然可以编辑文件，但无法提交更新，因为数据库在网络上。看上去好像这些都不是什么大问题，但实际体验过之后，你就会惊喜地发现，这其实是会带来很大不同的。 时刻保持数据完整性 在保存到 Git 之前，所有数据都要进行内容的校验和（checksum）计算，并将此结果作为数据的唯一标识和索引。换句话说，不可能在你修改了文件或目录之后，Git 一无所知。这项特性作为 Git 的设计哲学，建在整体架构的最底层。所以如果文件在传输时变得不完整，或者磁盘损坏导致文件数据缺失，Git 都能立即察觉。 Git 使用 SHA-1 算法计算数据的校验和，通过对文件的内容或目录的结构计算出一个 SHA-1 哈希值，作为指纹字符串。该字串由 40 个十六进制字符（0-9 及 a-f）组成，看起来就像是：24b9da6552252987aa493b52f8696cd6d3b00373 Git 的工作完全依赖于这类指纹字串，所以你会经常看到这样的哈希值。实际上，所有保存在 Git 数据库中的东西都是用此哈希值来作索引的，而不是靠文件名。 多数操作仅添加数据 常用的 Git 操作大多仅仅是把数据添加到数据库。因为任何一种不可逆的操作，比如删除数据，都会使回退或重现历史版本变得困难重重。在别的 VCS 中，若还未提交更新，就有可能丢失或者混淆一些修改的内容，但在 Git 里，一旦提交快照之后就完全不用担心丢失数据，特别是养成定期推送到其他仓库的习惯的话。 这种高可靠性令我们的开发工作安心不少，尽管去做各种试验性的尝试好了，再怎样也不会弄丢数据。至于 Git 内部究竟是如何保存和恢复数据的，我们会在第九章讨论 Git 内部原理时再作详述。 文件的三种状态 好，现在请注意，接下来要讲的概念非常重要。对于任何一个文件，在 Git 内都只有三种状态：已提交（committed），已修改（modified）和已暂存（staged）。已提交表示该文件已经被安全地保存在本地数据库中了；已修改表示修改了某个文件，但还没有提交保存；已暂存表示把已修改的文件放在下次提交时要保存的清单中。 由此我们看到 Git 管理项目时，文件流转的三个工作区域：Git 的工作目录，暂存区域，以及本地仓库。 每个项目都有一个 Git 目录（译注：如果 git clone 出来的话，就是其中 .git 的目录；如果 git clone –bare 的话，新建的目录本身就是 Git 目录。），它是 Git 用来保存元数据和对象数据库的地方。该目录非常重要，每次克隆镜像仓库的时候，实际拷贝的就是这个目录里面的数据。 从项目中取出某个版本的所有文件和目录，用以开始后续工作的叫做工作目录。这些文件实际上都是从 Git 目录中的压缩对象数据库中提取出来的，接下来就可以在工作目录中对这些文件进行编辑。 所谓的暂存区域只不过是个简单的文件，一般都放在 Git 目录中。有时候人们会把这个文件叫做索引文件，不过标准说法还是叫暂存区域。 基本的 Git 工作流程如下 在工作目录中修改某些文件。 对修改后的文件进行快照，然后保存到暂存区域。 提交更新，将保存在暂存区域的文件快照永久转储到 Git 目录中。 所以，我们可以从文件所处的位置来判断状态：如果是 Git 目录中保存着的特定版本文件，就属于已提交状态；如果作了修改并已放入暂存区域，就属于已暂存状态；如果自上次取出后，作了修改但还没有放到暂存区域，就是已修改状态。到第二章的时候，我们会进一步了解其中细节，并学会如何根据文件状态实施后续操作，以及怎样跳过暂存直接提交。 ","date":"2022-08-25","objectID":"/git%E5%B7%A5%E4%BD%9C%E6%B5%81/:2:0","tags":[],"title":"Git工作流","uri":"/git%E5%B7%A5%E4%BD%9C%E6%B5%81/"},{"categories":["git"],"content":"Git 常用命令 ","date":"2022-08-25","objectID":"/git%E5%B7%A5%E4%BD%9C%E6%B5%81/:3:0","tags":[],"title":"Git工作流","uri":"/git%E5%B7%A5%E4%BD%9C%E6%B5%81/"},{"categories":["git"],"content":"git add 把要提交的文件的信息添加到暂存区中。当使用 git commit 时，将依据暂存区中的内容提交至本地库。 它通常将现有路径的当前内容作为一个整体添加，但是通过一些选项，它也可以用于添加内容，只对所应用的工作树文件进行一些更改，或删除工作树中不存在的路径了。 “索引”保存工作树内容的快照，并且将该快照作为下一个提交的内容。 因此，在对工作树进行任何更改之后，并且在运行 git commit 命令之前，必须使用 git add 命令将任何新的或修改的文件添加到索引。 该命令可以在提交之前多次执行。它只在运行 git add 命令时添加指定文件的内容; 如果希望随后的更改包含在下一个提交中，那么必须再次运行 git add 将新的内容添加到索引。 # 把指定的文件添加到暂存区中 $ git add \u003c文件路径\u003e # 添加所有修改、已删除的文件到暂存区中 $ git add -u [\u003c文件路径\u003e] $ git add --update [\u003c文件路径\u003e] # 添加所有修改、已删除、新增的文件到暂存区中，省略 \u003c文件路径\u003e 即为当前目录 $ git add -A [\u003c文件路径\u003e] $ git add --all [\u003c文件路径\u003e] # 查看所有修改、已删除但没有提交的文件，进入一个子命令系统 $ git add -i [\u003c文件路径\u003e] $ git add --interactive [\u003c文件路径\u003e] ","date":"2022-08-25","objectID":"/git%E5%B7%A5%E4%BD%9C%E6%B5%81/:3:1","tags":[],"title":"Git工作流","uri":"/git%E5%B7%A5%E4%BD%9C%E6%B5%81/"},{"categories":["git"],"content":"git branch 操作 Git 的分支命令。 # 列出本地的所有分支，当前所在分支以 \"*\" 标出 $ git branch # 列出本地的所有分支并显示最后一次提交，当前所在分支以 \"*\" 标出 $ git branch -v # 创建新分支，新的分支基于上一次提交建立 $ git branch \u003c分支名\u003e # 修改分支名称 # 如果不指定原分支名称则为当前所在分支 $ git branch -m [\u003c原分支名称\u003e] \u003c新的分支名称\u003e # 强制修改分支名称 $ git branch -M [\u003c原分支名称\u003e] \u003c新的分支名称\u003e # 删除指定的本地分支 $ git branch -d \u003c分支名称\u003e # 强制删除指定的本地分支 $ git branch -D \u003c分支名称\u003e ","date":"2022-08-25","objectID":"/git%E5%B7%A5%E4%BD%9C%E6%B5%81/:3:2","tags":[],"title":"Git工作流","uri":"/git%E5%B7%A5%E4%BD%9C%E6%B5%81/"},{"categories":["git"],"content":"git checkout 更新工作树中的文件以匹配索引或指定树中的版本。如果没有给出路径 - git checkout 还会更新 HEAD ，将指定的分支设置为当前分支。 # 切换到已存在的指定分支 $ git checkout \u003c分支名称\u003e # 创建并切换到指定的分支，保留所有的提交记录 # 等同于 \"git branch\" 和 \"git checkout\" 两个命令合并 $ git checkout -b \u003c分支名称\u003e # 创建并切换到指定的分支，删除所有的提交记录 $ git checkout --orphan \u003c分支名称\u003e # 替换掉本地的改动，新增的文件和已经添加到暂存区的内容不受影响 $ git checkout \u003c文件路径\u003e git checkout 是 git 最常用的命令之一，同时也是一个很危险的命令，因为这条命令会重写工作区。 ","date":"2022-08-25","objectID":"/git%E5%B7%A5%E4%BD%9C%E6%B5%81/:3:3","tags":[],"title":"Git工作流","uri":"/git%E5%B7%A5%E4%BD%9C%E6%B5%81/"},{"categories":["git"],"content":"git clone 将存储库克隆到新创建的目录中，为克隆的存储库中的每个分支创建远程跟踪分支(使用 git branch -r 可见)，并从克隆检出的存储库作为当前活动分支的初始分支。 # 默认在当前目录下创建和版本库名相同的文件夹并下载版本到该文件夹下 $ git clone \u003c远程仓库的网址\u003e # 指定本地仓库的目录 $ git clone \u003c远程仓库的网址\u003e \u003c本地目录\u003e # -b 指定要克隆的分支，默认是master分支 $ git clone \u003c远程仓库的网址\u003e -b \u003c分支名称\u003e \u003c本地目录\u003e ","date":"2022-08-25","objectID":"/git%E5%B7%A5%E4%BD%9C%E6%B5%81/:3:4","tags":[],"title":"Git工作流","uri":"/git%E5%B7%A5%E4%BD%9C%E6%B5%81/"},{"categories":["git"],"content":"git stash 暂存当前修改 git stash会把所有未提交的修改（包括暂存的和非暂存的）都保存起来，用于后续恢复当前工作目录。 比如下面的中间状态，通过git stash命令推送一个新的储藏，当前的工作目录就干净了。 注意：stash是本地的，不会通过git push命令上传到git server上。 实际应用中推荐给每个stash加一个message，用于记录版本，使用git stash save取代git stash命令。示例如下： git stash save \"test-cmd-stash\" 重新应用缓存的stash 可以通过git stash pop命令恢复之前缓存的工作目录 git stash pop 这个指令将缓存堆栈中的第一个stash删除，并将对应修改应用到当前的工作目录下。 你也可以使用git stash apply命令，将缓存堆栈中的stash多次应用到工作目录中，但并不删除stash拷贝。 git stash apply 在使用git stash apply命令时可以通过名字指定使用哪个stash，默认使用最近的stash（即stash@{0}）。 查看现有stash 可以使用git stash list命令，一个典型的输出如下： git stash list stash@{0}: WIP on master: 049d078 added the index file stash@{1}: WIP on master: c264051 Revert \"added file_size\" stash@{2}: WIP on master: 21d80a5 added number to log 移除stash 可以使用git stash drop命令，后面可以跟着stash名字。下面是一个示例： $ git stash list stash@{0}: WIP on master: 049d078 added the index file stash@{1}: WIP on master: c264051 Revert \"added file_size\" stash@{2}: WIP on master: 21d80a5 added number to log $ git stash drop stash@{0} Dropped stash@{0} (364e91f3f268f0900bc3ee613f9f733e82aaed43) 或者使用git stash clear命令，删除所有缓存的stash。 暂存未跟踪或忽略的文件 默认情况下，git stash会缓存下列文件： 添加到暂存区的修改（staged changes） Git跟踪的但并未添加到暂存区的修改（unstaged changes） 但不会缓存一下文件： 在工作目录中新的文件（untracked files） 被忽略的文件（ignored files） git stash命令提供了参数用于缓存上面两种类型的文件。使用-u或者--include-untracked可以stash untracked文件。使用-a或者--all命令可以stash当前目录下的所有修改。 ","date":"2022-08-25","objectID":"/git%E5%B7%A5%E4%BD%9C%E6%B5%81/:3:5","tags":[],"title":"Git工作流","uri":"/git%E5%B7%A5%E4%BD%9C%E6%B5%81/"},{"categories":["git"],"content":"git commit 将索引的当前内容与描述更改的用户和日志消息一起存储在新的提交中。 # 把暂存区中的文件提交到本地仓库，调用文本编辑器输入该次提交的描述信息 $ git commit # 把暂存区中的文件提交到本地仓库中并添加描述信息 $ git commit -m \"\u003c提交的描述信息\u003e\" # 把所有修改、已删除的文件提交到本地仓库中 # 不包括未被版本库跟踪的文件，等同于先调用了 \"git add -u\" $ git commit -a -m \"\u003c提交的描述信息\u003e\" # 修改上次提交的描述信息 $ git commit --amend ","date":"2022-08-25","objectID":"/git%E5%B7%A5%E4%BD%9C%E6%B5%81/:3:6","tags":[],"title":"Git工作流","uri":"/git%E5%B7%A5%E4%BD%9C%E6%B5%81/"},{"categories":["git"],"content":"git config 主要是用来配置 Git 的相关参数，其主要操作有： # 查看配置信息 # --local：仓库级，--global：全局级，--system：系统级 $ git config \u003c --local | --global | --system \u003e -l # 查看当前生效的配置信息 $ git config -l # 编辑配置文件 # --local：仓库级，--global：全局级，--system：系统级 $ git config \u003c --local | --global | --system \u003e -e # 添加配置项 # --local：仓库级，--global：全局级，--system：系统级 $ git config \u003c --local | --global | --system \u003e --add \u003cname\u003e \u003cvalue\u003e # 获取配置项 $ git config \u003c --local | --global | --system \u003e --get \u003cname\u003e # 删除配置项 $ git config \u003c --local | --global | --system \u003e --unset \u003cname\u003e # 配置提交记录中的用户信息 $ git config --global user.name \u003c用户名\u003e $ git config --global user.email \u003c邮箱地址\u003e # 更改Git缓存区的大小 # 如果提交的内容较大，默认缓存较小，提交会失败 # 缓存大小单位：B，例如：524288000（500MB） $ git config --global http.postBuffer \u003c缓存大小\u003e # 调用 git status/git diff 命令时以高亮或彩色方式显示改动状态 $ git config --global color.ui true # 配置可以缓存密码，默认缓存时间15分钟 $ git config --global credential.helper cache # 配置密码的缓存时间 # 缓存时间单位：秒 $ git config --global credential.helper 'cache --timeout=\u003c缓存时间\u003e' # 配置长期存储密码 $ git config --global credential.helper store Git 一共有3个配置文件： 仓库级的配置文件：在仓库的 .git/.gitconfig，该配置文件只对所在的仓库有效。 全局配置文件：Mac 系统在 ~/.gitconfig，Windows 系统在 C:\\Users\\\u003c用户名\u003e\\.gitconfig。 系统级的配置文件：在 Git 的安装目录下（Mac 系统下安装目录在 /usr/local/git）的 etc 文件夹中的 gitconfig。 ","date":"2022-08-25","objectID":"/git%E5%B7%A5%E4%BD%9C%E6%B5%81/:3:7","tags":[],"title":"Git工作流","uri":"/git%E5%B7%A5%E4%BD%9C%E6%B5%81/"},{"categories":["git"],"content":"git diff 用于显示提交和工作树等之间的更改。 此命令比较的是工作目录中当前文件和暂存区域快照之间的差异,也就是修改之后还没有暂存起来的变化内容。 # 比较当前文件和暂存区中文件的差异，显示没有暂存起来的更改 $ git diff # 比较暂存区中的文件和上次提交时的差异 $ git diff --cached $ git diff --staged # 比较当前文件和上次提交时的差异 $ git diff HEAD # 查看从指定的版本之后改动的内容 $ git diff \u003ccommit ID\u003e # 比较两个分支之间的差异 $ git diff \u003c分支名称\u003e \u003c分支名称\u003e # 查看两个分支分开后各自的改动内容 $ git diff \u003c分支名称\u003e...\u003c分支名称\u003e ","date":"2022-08-25","objectID":"/git%E5%B7%A5%E4%BD%9C%E6%B5%81/:3:8","tags":[],"title":"Git工作流","uri":"/git%E5%B7%A5%E4%BD%9C%E6%B5%81/"},{"categories":["git"],"content":"git fetch 从远程仓库获取最新的版本到本地的 tmp 分支上。 # 将远程仓库所有分支的最新版本全部取回到本地 $ git fetch \u003c远程仓库的别名\u003e # 将远程仓库指定分支的最新版本取回到本地 $ git fetch \u003c远程主机名\u003e \u003c分支名\u003e ","date":"2022-08-25","objectID":"/git%E5%B7%A5%E4%BD%9C%E6%B5%81/:3:9","tags":[],"title":"Git工作流","uri":"/git%E5%B7%A5%E4%BD%9C%E6%B5%81/"},{"categories":["git"],"content":"git init 初始化项目所在目录，初始化后会在当前目录下出现一个名为 .git 的目录。 # 初始化本地仓库，在当前目录下生成 .git 文件夹 $ git init ","date":"2022-08-25","objectID":"/git%E5%B7%A5%E4%BD%9C%E6%B5%81/:3:10","tags":[],"title":"Git工作流","uri":"/git%E5%B7%A5%E4%BD%9C%E6%B5%81/"},{"categories":["git"],"content":"git log 显示提交的记录。 # 打印所有的提交记录 $ git log # 打印从第一次提交到指定的提交的记录 $ git log \u003ccommit ID\u003e # 打印指定数量的最新提交的记录 $ git log -\u003c指定的数量\u003e ","date":"2022-08-25","objectID":"/git%E5%B7%A5%E4%BD%9C%E6%B5%81/:3:11","tags":[],"title":"Git工作流","uri":"/git%E5%B7%A5%E4%BD%9C%E6%B5%81/"},{"categories":["git"],"content":"git merge 用于将两个或两个以上的开发历史加入(合并)一起。 # 把指定的分支合并到当前所在的分支下，并自动进行新的提交 $ git merge \u003c分支名称\u003e # 把指定的分支合并到当前所在的分支下，不进行新的提交 $ git merge --no-commit \u003c分支名称\u003e ","date":"2022-08-25","objectID":"/git%E5%B7%A5%E4%BD%9C%E6%B5%81/:3:12","tags":[],"title":"Git工作流","uri":"/git%E5%B7%A5%E4%BD%9C%E6%B5%81/"},{"categories":["git"],"content":"git mv 重命名文件或者文件夹。 # 重命名指定的文件或者文件夹 $ git mv \u003c源文件/文件夹\u003e \u003c目标文件/文件夹\u003e ","date":"2022-08-25","objectID":"/git%E5%B7%A5%E4%BD%9C%E6%B5%81/:3:13","tags":[],"title":"Git工作流","uri":"/git%E5%B7%A5%E4%BD%9C%E6%B5%81/"},{"categories":["git"],"content":"git pull 从远程仓库获取最新版本并合并到本地。 首先会执行 git fetch，然后执行 git merge，把获取的分支的 HEAD 合并到当前分支。 # 从远程仓库获取最新版本。 $ git pull # 从origin远程仓库的main分支拉取代码到本地，一般需要加上 --allow-unrelated-histories 参数忽略提交历史的差异。然后在本地合并冲突 $ git remote add origin 连接 #（http or ssh） $ git pull origin main --allow-unrelated-histories ","date":"2022-08-25","objectID":"/git%E5%B7%A5%E4%BD%9C%E6%B5%81/:3:14","tags":[],"title":"Git工作流","uri":"/git%E5%B7%A5%E4%BD%9C%E6%B5%81/"},{"categories":["git"],"content":"git push 把本地仓库的提交推送到远程仓库。 # 把本地仓库的分支推送到远程仓库的指定分支 $ git push \u003c远程仓库的别名\u003e \u003c本地分支名\u003e:\u003c远程分支名\u003e # 删除指定的远程仓库的分支 $ git push \u003c远程仓库的别名\u003e :\u003c远程分支名\u003e $ git push \u003c远程仓库的别名\u003e --delete \u003c远程分支名\u003e ","date":"2022-08-25","objectID":"/git%E5%B7%A5%E4%BD%9C%E6%B5%81/:3:15","tags":[],"title":"Git工作流","uri":"/git%E5%B7%A5%E4%BD%9C%E6%B5%81/"},{"categories":["git"],"content":"git remote 操作远程库。 # 列出已经存在的远程仓库 $ git remote # 列出远程仓库的详细信息，在别名后面列出URL地址 $ git remote -v $ git remote --verbose # 添加远程仓库 $ git remote add \u003c远程仓库的别名\u003e \u003c远程仓库的URL地址\u003e # 修改远程仓库的别名 $ git remote rename \u003c原远程仓库的别名\u003e \u003c新的别名\u003e # 删除指定名称的远程仓库 $ git remote remove \u003c远程仓库的别名\u003e # 修改远程仓库的 URL 地址 $ git remote set-url \u003c远程仓库的别名\u003e \u003c新的远程仓库URL地址\u003e ","date":"2022-08-25","objectID":"/git%E5%B7%A5%E4%BD%9C%E6%B5%81/:3:16","tags":[],"title":"Git工作流","uri":"/git%E5%B7%A5%E4%BD%9C%E6%B5%81/"},{"categories":["git"],"content":"git reset 还原提交记录，版本回退 # 重置暂存区，但文件不受影响 # 相当于将用 \"git add\" 命令更新到暂存区的内容撤出暂存区，可以指定文件 # 没有指定 commit ID 则默认为当前 HEAD $ git reset [\u003c文件路径\u003e] $ git reset --mixed [\u003c文件路径\u003e] # 将 HEAD 的指向改变，撤销到指定的提交记录，文件未修改 $ git reset \u003ccommit ID\u003e $ git reset --mixed \u003ccommit ID\u003e # 将 HEAD 的指向改变，撤销到指定的提交记录，文件未修改 # 相当于调用 \"git reset --mixed\" 命令后又做了一次 \"git add\" $ git reset --soft \u003ccommit ID\u003e # 将 HEAD 的指向改变，撤销到指定的提交记录，文件也修改了 $ git reset --hard \u003ccommit ID\u003e ","date":"2022-08-25","objectID":"/git%E5%B7%A5%E4%BD%9C%E6%B5%81/:3:17","tags":[],"title":"Git工作流","uri":"/git%E5%B7%A5%E4%BD%9C%E6%B5%81/"},{"categories":["git"],"content":"git revert 生成一个新的提交来撤销某次提交，此次提交之前的所有提交都会被保留。 # 生成一个新的提交来撤销某次提交 $ git revert \u003ccommit ID\u003e ","date":"2022-08-25","objectID":"/git%E5%B7%A5%E4%BD%9C%E6%B5%81/:3:18","tags":[],"title":"Git工作流","uri":"/git%E5%B7%A5%E4%BD%9C%E6%B5%81/"},{"categories":["git"],"content":"git rm 删除文件或者文件夹。 # 移除跟踪指定的文件，并从本地仓库的文件夹中删除 $ git rm \u003c文件路径\u003e # 移除跟踪指定的文件夹，并从本地仓库的文件夹中删除 $ git rm -r \u003c文件夹路径\u003e # 移除跟踪指定的文件，在本地仓库的文件夹中保留该文件 $ git rm --cached ","date":"2022-08-25","objectID":"/git%E5%B7%A5%E4%BD%9C%E6%B5%81/:3:19","tags":[],"title":"Git工作流","uri":"/git%E5%B7%A5%E4%BD%9C%E6%B5%81/"},{"categories":["git"],"content":"git status 用于显示工作目录和暂存区的状态。使用此命令能看到那些修改被暂存到了, 哪些没有, 哪些文件没有被 Git tracked 到。 # 查看本地仓库的状态 $ git status git status 不显示已经 commit 到项目历史中去的信息。 看项目历史的信息要使用 git log。 ","date":"2022-08-25","objectID":"/git%E5%B7%A5%E4%BD%9C%E6%B5%81/:3:20","tags":[],"title":"Git工作流","uri":"/git%E5%B7%A5%E4%BD%9C%E6%B5%81/"},{"categories":["git"],"content":"git tag 操作标签的命令。 # 打印所有的标签 $ git tag # 添加轻量标签，指向提交对象的引用，可以指定之前的提交记录 $ git tag \u003c标签名称\u003e [\u003ccommit ID\u003e] # 添加带有描述信息的附注标签，可以指定之前的提交记录 $ git tag -a \u003c标签名称\u003e -m \u003c标签描述信息\u003e [\u003ccommit ID\u003e] # 切换到指定的标签 $ git checkout \u003c标签名称\u003e # 查看标签的信息 $ git show \u003c标签名称\u003e # 删除指定的标签 $ git tag -d \u003c标签名称\u003e # 将指定的标签提交到远程仓库 $ git push \u003c远程仓库的别名\u003e \u003c标签名称\u003e # 将本地所有的标签全部提交到远程仓库 $ git push \u003c远程仓库的别名\u003e –tags ","date":"2022-08-25","objectID":"/git%E5%B7%A5%E4%BD%9C%E6%B5%81/:3:21","tags":[],"title":"Git工作流","uri":"/git%E5%B7%A5%E4%BD%9C%E6%B5%81/"},{"categories":["git"],"content":"分支合并冲突 由于多人同时进行开发，有时候会同时修改一个文件，或者多分支开发，合并的时候就很容易引发冲突，下面是一个制造冲突和解决冲突的例子。 制造冲突： 同学 A 新建码云仓库,同时添加同学 B 为开发者(其实一个人也可以制造冲突的) 同学 A 新建文件 main.js 并提交推送到远程仓库,同学 B 把仓库同步到本地,这时两位同学都有一个 main.js 的空文件 同学 A 在 main.js 的第一行添加\"我是同学 A\",然后添加,提交并推送到远程仓库 同学 B 在 main.js 的第一行添加\"我是同学 B\", 然后执行以下命令 git add . git commit -m\"修改main.js\" git push origin master 出现以下提示 意思是被拒绝了,要先执行 git pull 命令 因为两位同学同时修改了同一行代码,所有 git 不知如何取舍(如果同学 a 修改了第一行,同学 b 修改了第二行,那么 git 会智能的合并),只好把合并代码这个事情交给开发者去处理,上图中\u003c\u003c\u003c\u003c\u003c\u003c\u003c HEAD 到========的代码是 B 同学的代码,======到\u003e\u003e\u003e\u003e\u003e\u003e\u003e a248f68a5fcbcbe4cc887bee3dfc3cfd1cf7147b 的代码是同学a的代码,括号的 Incoming Change 的意思是从外面来的修改,a248f68a5fcbcbe4cc887bee3dfc3cfd1cf7147b 是仓库是冲突的版本号,你可以通过 git log 看到详细的信息 你可以根据具体情况去合并代码,取 a 的代码或者取 b 的代码,或者 a 取一点,b 取一点,具体情况具体分析. 手动删除这些特殊符号，并选择某个修改内容进行提交，就可以解决冲突。 ","date":"2022-08-25","objectID":"/git%E5%B7%A5%E4%BD%9C%E6%B5%81/:4:0","tags":[],"title":"Git工作流","uri":"/git%E5%B7%A5%E4%BD%9C%E6%B5%81/"},{"categories":["git"],"content":"分支管理策略 核心：因为创建、合并和删除分支非常快，所以Git鼓励你使用分支完成某个任务，合并后再删掉分支，这和直接在master分支上工作效果是一样的，但过程更安全。 ","date":"2022-08-25","objectID":"/git%E5%B7%A5%E4%BD%9C%E6%B5%81/:5:0","tags":[],"title":"Git工作流","uri":"/git%E5%B7%A5%E4%BD%9C%E6%B5%81/"},{"categories":["git"],"content":"bug分支 软件开发中，bug就像家常便饭一样。有了bug就需要修复，在Git中，由于分支是如此的强大，所以，每个bug都可以通过一个新的临时分支来修复，修复后合并分支，然后将临时分支删除。 当你接到一个修复一个代号101的bug的任务时，很自然地，你想创建一个分支issue-101来修复它，但是，当前正在dev上进行的工作还没有提交。 并不是你不想提交，而是工作只进行到一半，还没法提交，预计完成还需1天时间。但是，必须在两个小时内修复该bug，怎么办？于是，你就现在要停下并暂存手头的工作，转而去修复bug。 幸好，Git还提供了一个stash功能，可以把当前工作现场“储藏”起来，等修改BUG以后恢复现场继续工作： 将当前工作现场“储藏”起来，等以后恢复现场继续工作 git stash 现在用git status查看工作区，工作区就是干净的（除非没有被Git管理的文件） 查看“储藏”的工作现场 git stash list 恢复最近一次“储藏”的工作现场 git stash apply 恢复指定工作现场，通过git stash list查看工作现场的序号，将（stash@{序号}）追加到命令后面 注意，序号越靠后，说明储藏的时间越长 这样恢复后，stash内容并不会删除，需要手动删除 删除“储藏”的工作现场 git stash drop 恢复并删除“储藏”的工作现场 git stash pop master分支出现bug，说明dev同样也存在，Git专门提供了命令，让我们复制一个特定的提交到当前分支 git cherry-pick 特定的提交id ","date":"2022-08-25","objectID":"/git%E5%B7%A5%E4%BD%9C%E6%B5%81/:5:1","tags":[],"title":"Git工作流","uri":"/git%E5%B7%A5%E4%BD%9C%E6%B5%81/"},{"categories":["git"],"content":"Feature分支 软件开发中，总有无穷无尽的新的功能要不断添加进来。 添加一个新功能时，你肯定不希望因为一些实验性质的代码，把主分支搞乱了，所以，每添加一个新功能，最好新建一个feature分支，在上面开发，完成后，合并，最后，删除该feature分支。 如果在feature分支还没有合并前，需要取消这个功能，删除时会销毁失败，需要使用大写的-D参数强制删除，git branch -D 分支 ","date":"2022-08-25","objectID":"/git%E5%B7%A5%E4%BD%9C%E6%B5%81/:5:2","tags":[],"title":"Git工作流","uri":"/git%E5%B7%A5%E4%BD%9C%E6%B5%81/"},{"categories":["git"],"content":"多人协作 当你从远程仓库克隆时，实际上Git自动把本地的master分支和远程的master分支对应起来了，并且，远程仓库的默认别名是origin。 查看远程库的信息 git remote -v ，-v表示详细信息 推送分支 git push 远程库别名 本地分支名称 抓取分支 git clone 远程仓库的URL，默认只能看到本地的master分支 如果要在dev分支上开发，就必须创建远程origin的dev分支到本地 git checkout -b dev origin/dev，创建并且切换到dev分支，并将远程库origin/dev弄下来 如果最新提交和你推送的提交有冲突，先用git pull把最新提交抓下来，然后在本地合并，解决冲突，再提交。 # git pull 失败 $ git pull There is no tracking information for the current branch. Please specify which branch you want to merge with. See git-pull(1) for details. git pull \u003cremote\u003e \u003cbranch\u003e If you wish to set tracking information for this branch you can do so with: git branch --set-upstream-to=origin/\u003cbranch\u003e dev # 原因是没有指定本地dev分支与远程origin/dev分支的链接，根据提示，设置dev和origin/dev的链接 设置本地分支dev与远程分支origin/dev的链接 git branch --set-upstream-to=origin/dev dev 多人协作工作模式 尽量将冲突在本地解决 pull远程库内容到本地先进行合并 如合并冲突，则解决冲突 如没有冲突或解决后，再commit 并push到远程库 ","date":"2022-08-25","objectID":"/git%E5%B7%A5%E4%BD%9C%E6%B5%81/:5:3","tags":[],"title":"Git工作流","uri":"/git%E5%B7%A5%E4%BD%9C%E6%B5%81/"},{"categories":["mysql"],"content":"[toc] 导入表的问题 导入数据时外键约束问题 数据导入指令： source d:\\xxx.sql 通过FOREIGN_KEY_CHECKS解决，用法如下： set FOREIGN_KEY_CHECKS=0; #在导入前设置为不检查外键约束 set FOREIGN_KEY_CHECKS=1; #在导入后恢复检查外键约束 第三章_最基本的SELECT语句 ","date":"2022-08-24","objectID":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/:0:0","tags":[],"title":"MySQL基础篇","uri":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/"},{"categories":["mysql"],"content":"1. SQL语言的规则和规范 ","date":"2022-08-24","objectID":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/:1:0","tags":[],"title":"MySQL基础篇","uri":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/"},{"categories":["mysql"],"content":"1) 基本规则 SQL 可以写在一行或者多行。为了提高可读性，各子句分行写，必要时使用缩进 每条命令以 ; 或 \\g 或 \\G 结束 关键字不能被缩写也不能分行 关于标点符号 必须保证所有的小括号、单引号、双引号是成对结束的 必须使用英文状态下的半角输入方式 字符串型和日期时间类型的数据可以使用单引号（’ ‘）表示 列的别名，尽量使用双引号（\" “），而且不建议省略as ","date":"2022-08-24","objectID":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/:1:1","tags":[],"title":"MySQL基础篇","uri":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/"},{"categories":["mysql"],"content":"2) SQL大小写规范（建议遵守） MySQL 在Windows环境下是大小写不敏感的 MySQL 在Linux环境下是大小写敏感的 数据库名、表名、表的别名、变量名是严格区分大小写的 关键字、函数名、列名(或字段名)、列的别名(字段的别名) 是忽略大小写的。 推荐采用统一的书写规范： 数据库名、表名、表别名、字段名、字段别名等都小写 SQL关键字、函数名、绑定变量等都大写 ","date":"2022-08-24","objectID":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/:1:2","tags":[],"title":"MySQL基础篇","uri":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/"},{"categories":["mysql"],"content":"3) 注释 单行注释：#注释文字(MySQL特有的方式) 单行注释：-- 注释文字(--后面必须包含一个空格。) 多行注释：/* 注释文字 */ ","date":"2022-08-24","objectID":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/:1:3","tags":[],"title":"MySQL基础篇","uri":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/"},{"categories":["mysql"],"content":"4) 命名规则 数据库、表名不得超过30个字符，变量名限制为29个 必须只能包含 A–Z, a–z, 0–9, _ 共63个字符 数据库名、表名、字段名等对象名中间不要包含空格 同一个MySQL软件中，数据库不能同名；同一个库中，表不能重名； 同一个表中，字段不能重名 必须保证你的字段没有和保留字、数据库系统或常用方法冲突。如果坚持使用，请在SQL语句中使 用`（着重号）引起来 保持字段名和类型的一致性，在命名字段并为其指定数据类型的时候一定要保证一致性。假如数据 类型在一个表里是整数，那在另一个表里可就别变成字符型了 ","date":"2022-08-24","objectID":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/:1:4","tags":[],"title":"MySQL基础篇","uri":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/"},{"categories":["mysql"],"content":"2. 基本的SELECT语句 ","date":"2022-08-24","objectID":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/:2:0","tags":[],"title":"MySQL基础篇","uri":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/"},{"categories":["mysql"],"content":"1) SELECT … FROM 语法 SELECT 标识选择哪些列 FROM 标识从哪个表中选择 选择全部列 SELECT * FROM departments; 选择特定的列： SELECT department_id, location_id FROM departments; ","date":"2022-08-24","objectID":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/:2:1","tags":[],"title":"MySQL基础篇","uri":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/"},{"categories":["mysql"],"content":"2) 列的别名 重命名一个列 便于计算 紧跟列名，也可以在列名和别名之间加入关键字AS，别名使用双引号，以便在别名中包含空格或特 殊的字符并区分大小写。 AS可以省略，最好不要省略 建议别名简短，见名知意 举例： SELECT `last_name` AS `name`, `commission_pct` `comm` FROM `employees`; ","date":"2022-08-24","objectID":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/:2:2","tags":[],"title":"MySQL基础篇","uri":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/"},{"categories":["mysql"],"content":"3) 去除重复行 DISTINCT关键字 SELECT DISTINCT `department_id` FROM `employees`; ","date":"2022-08-24","objectID":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/:2:3","tags":[],"title":"MySQL基础篇","uri":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/"},{"categories":["mysql"],"content":"4) 空值参与运算 空值：null ( 不等同于0, ’ ‘, ’null‘ ) 实际问题的解决方案：引入IFNULL SELECT employee_id, salary \"月工资\", salary * (1 + IFNULL(commission_pct, 0)) * 12 \"年工资\" FROM employees; 这里你一定要注意，在MySQL里面，值不等于空字符串。一个空字符串的长度是0，而一个空值的长度是空。而且，在MySQL里面，空值是占用空间的。 ","date":"2022-08-24","objectID":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/:2:4","tags":[],"title":"MySQL基础篇","uri":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/"},{"categories":["mysql"],"content":"5) 着重号 `` 必须保证你的字段没有和保留字、数据库系统或常见方法冲突。 如果坚持使用，在SQL语句中使用 ` ` 引起来。 SELECT * FROM `order`; ","date":"2022-08-24","objectID":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/:2:5","tags":[],"title":"MySQL基础篇","uri":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/"},{"categories":["mysql"],"content":"6) 查询常数 SELECT '小张科技' as \"公司名\", employee_id, last_name FROM employees; ","date":"2022-08-24","objectID":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/:2:6","tags":[],"title":"MySQL基础篇","uri":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/"},{"categories":["mysql"],"content":"3. 显示表结构 显示表中字段的详细信息 DESCRIBE employees; 或 DESC employees; mysql\u003e desc employees; +----------------+-------------+------+-----+---------+-------+ | Field | Type | Null | Key | Default | Extra | +----------------+-------------+------+-----+---------+-------+ | employee_id | int(6) | NO | PRI | 0 | | | first_name | varchar(20) | YES | | NULL | | | last_name | varchar(25) | NO | | NULL | | | email | varchar(25) | NO | UNI | NULL | | | phone_number | varchar(20) | YES | | NULL | | | hire_date | date | NO | | NULL | | | job_id | varchar(10) | NO | MUL | NULL | | | salary | double(8,2) | YES | | NULL | | | commission_pct | double(2,2) | YES | | NULL | | | manager_id | int(6) | YES | MUL | NULL | | | department_id | int(4) | YES | MUL | NULL | | +----------------+-------------+------+-----+---------+-------+ 11 rows in set (0.00 sec) 其中，各个字段的含义分别解释如下： Field：表示字段名称。 Type：表示字段类型。 Null：表示该列是否可以存储NULL值。 Key：表示该列是否已编制索引。 PRI表示该列是表主键的一部分； UNI表示该列是UNIQUE索引的一部分； MUL表示在列中某个给定值允许出现多次。 Default：表示该列是否有默认值，如果有，那么值是多少。 Extra：表示可以获取的与给定列有关的附加信息，例如AUTO_INCREMENT等。 ","date":"2022-08-24","objectID":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/:3:0","tags":[],"title":"MySQL基础篇","uri":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/"},{"categories":["mysql"],"content":"4. 过滤数据 语法： SELECT 字段1,字段2 FROM 表名 WHERE 过滤条件 使用WHERE子句，将不满足条件的行过滤掉。WHERE子句紧随FROM子句。当然，WHERE条件里不能使用聚合函数 举例： SELECT employee_id, last_name, job_id, department_id FROM employees WHERE department_id = 90; 第四章_运算符 DUAL –\u003e 伪表 ","date":"2022-08-24","objectID":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/:4:0","tags":[],"title":"MySQL基础篇","uri":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/"},{"categories":["mysql"],"content":"1. 算术运算符 SELECT 100 + 0, 100 + 50 * 30, 100 - 35.5 FROM `DUAL`; 一个整数类型的值对整数进行加法和减法操作，结果还是一个整数； 一个整数类型的值对浮点数进行加法和减法操作，结果是一个浮点数； 在Java中，+的左右两边如果有字符串，那么表示字符串的拼接。但是在MySQL中+只表示数值相加。如果遇到非数值类型，先尝试转成数值，如果转失败，就按0计算。（注：MySQL 中字符串拼接要使用字符串函数CONCAT()实现） 在数学运算中，0不能用作除数，在MySQL中，一个数除以0为NULL。 取余是MOD(x,y),x除以y的余数 ","date":"2022-08-24","objectID":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/:5:0","tags":[],"title":"MySQL基础篇","uri":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/"},{"categories":["mysql"],"content":"2. 比较运算符 ","date":"2022-08-24","objectID":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/:6:0","tags":[],"title":"MySQL基础篇","uri":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/"},{"categories":["mysql"],"content":"1) 等号运算符 比较运算符用来对表达式左边的操作数和右边的操作数进行比较，比较的结果为真则返回1，比较的结果为假则返回0，其他情况则返回NULL。 比较运算符经常被用来作为SELECT查询语句的条件来使用，返回符合条件的结果记录。 如果等号两边的值、字符串或表达式中有一个为NULL，则比较结果为NULL。 mysql\u003e SELECT 1 = 1, 1 = '1', 1 = 0, 'a' = 'a', (5 + 3) = (2 + 6), '' = NULL , NULL = NULL; +-------+---------+-------+-----------+-------------------+-----------+-------------+ | 1 = 1 | 1 = '1' | 1 = 0 | 'a' = 'a' | (5 + 3) = (2 + 6) | '' = NULL | NULL = NULL | +-------+---------+-------+-----------+-------------------+-----------+-------------+ | 1 | 1 | 0 | 1 | 1 | NULL | NULL | +-------+---------+-------+-----------+-------------------+-----------+-------------+ 1 row in set (0.00 sec) mysql\u003e SELECT 1 = 2, 0 = 'abc', 1 = 'abc' FROM `DUAL`; +-------+-----------+-----------+ | 1 = 2 | 0 = 'abc' | 1 = 'abc' | +-------+-----------+-----------+ | 0 | 1 | 0 | +-------+-----------+-----------+ 1 row in set, 2 warnings (0.00 sec) 如果等号两边的值都是字符串或表达式都为字符串，则MySQL会按照字符串进行比较，其比较的是每个字符串中字符的ANSI编码是否相等。 如果等号两边的值都是整数，则MySQL会按照整数来比较两个值的大小。 如果等号两边的值一个是整数，另一个是字符串，则MySQL会将字符串转化为数字进行比较。 如果等号两边的值、字符串或表达式中有一个为NULL，则比较结果为NULL。 mysql\u003e SELECT 1 \u003c=\u003e '1', 1 \u003c=\u003e 0, 'a' \u003c=\u003e 'a', (5 + 3) \u003c=\u003e (2 + 6), '' \u003c=\u003e NULL,NULL \u003c=\u003e NULL FROM dual; +-----------+---------+-------------+---------------------+-------------+---------------+ | 1 \u003c=\u003e '1' | 1 \u003c=\u003e 0 | 'a' \u003c=\u003e 'a' | (5 + 3) \u003c=\u003e (2 + 6) | '' \u003c=\u003e NULL | NULL \u003c=\u003e NULL | +-----------+---------+-------------+---------------------+-------------+---------------+ | 1 | 0 | 1 | 1 | 0 | 1 | +-----------+---------+-------------+---------------------+-------------+---------------+ 1 row in set (0.00 sec) 可以看到，使用安全等于运算符时，两边的操作数的值都为NULL时，返回的结果为1而不是NULL，其他返回结果与等于运算符相同。 ","date":"2022-08-24","objectID":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/:6:1","tags":[],"title":"MySQL基础篇","uri":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/"},{"categories":["mysql"],"content":"2) 不等于运算符 不等于运算符（\u003c\u003e和!=）用于判断两边的数字、字符串或者表达式的值是否不相等， 如果不相等则返回1，相等则返回0。不等于运算符不能判断NULL值。如果两边的值有任意一个为NULL， 或两边都为NULL，则结果为NULL。 SQL语句示例如下： mysql\u003e SELECT 1 \u003c\u003e 1, 1 != 2, 'a' != 'b', (3+4) \u003c\u003e (2+6), 'a' != NULL, NULL \u003c\u003e NULL; +--------+--------+------------+----------------+-------------+--------------+ | 1 \u003c\u003e 1 | 1 != 2 | 'a' != 'b' | (3+4) \u003c\u003e (2+6) | 'a' != NULL | NULL \u003c\u003e NULL | +--------+--------+------------+----------------+-------------+--------------+ | 0 | 1 | 1 | 1 | NULL | NULL | +--------+--------+------------+----------------+-------------+--------------+ 1 row in set (0.00 sec) 此外，还有非符号类型的运算符： ","date":"2022-08-24","objectID":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/:6:2","tags":[],"title":"MySQL基础篇","uri":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/"},{"categories":["mysql"],"content":"3) 空运算符 空运算符 (IS NULL 或者 ISNULL) 判断一个值是否为NULL，如果为NULL则返回1，否则返回0。 mysql\u003e SELECT NULL IS NULL, ISNULL(NULL), ISNULL('a'), 1 IS NULL; +--------------+--------------+-------------+-----------+ | NULL IS NULL | ISNULL(NULL) | ISNULL('a') | 1 IS NULL | +--------------+--------------+-------------+-----------+ | 1 | 1 | 0 | 0 | +--------------+--------------+-------------+-----------+ 1 row in set (0.00 sec) ","date":"2022-08-24","objectID":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/:6:3","tags":[],"title":"MySQL基础篇","uri":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/"},{"categories":["mysql"],"content":"4) 非空运算符 非空运算符（IS NOT NULL）判断一个值是否不为NULL，如果不为NULL则返回1，否则返回0。 ","date":"2022-08-24","objectID":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/:6:4","tags":[],"title":"MySQL基础篇","uri":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/"},{"categories":["mysql"],"content":"5) 最小值运算符 语法格式为：LEAST(值1，值2，...，值n)。其中，“值n”表示参数列表中有n个值。在有两个或多个参数的情况下，返回最小值。 mysql\u003e SELECT LEAST (1,0,2), LEAST('b','a','c'), LEAST(1,NULL,2); +---------------+--------------------+-----------------+ | LEAST (1,0,2) | LEAST('b','a','c') | LEAST(1,NULL,2) | +---------------+--------------------+-----------------+ | 0 | a | NULL | +---------------+--------------------+-----------------+ 1 row in set (0.00 sec) 由结果可以看到，当参数是整数或者浮点数时，LEAST将返回其中最小的值；当参数为字符串时，返回字母表中顺序最靠前的字符；当比较值列表中有NULL时，不能判断大小，返回值为NULL。 ","date":"2022-08-24","objectID":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/:6:5","tags":[],"title":"MySQL基础篇","uri":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/"},{"categories":["mysql"],"content":"6) 最大值运算符 语法格式为：GREATEST(值1，值2，...，值n)。其中，n表示参数列表中有n个值。当有两个或多个参数时，返回值为最大值。假如任意一个自变量为NULL，则GREATEST()的返回值为NULL。 mysql\u003e SELECT GREATEST(1,0,2), GREATEST('b','a','c'), GREATEST(1,NULL,2); +-----------------+-----------------------+--------------------+ | GREATEST(1,0,2) | GREATEST('b','a','c') | GREATEST(1,NULL,2) | +-----------------+-----------------------+--------------------+ | 2 | c | NULL | +-----------------+-----------------------+--------------------+ 1 row in set (0.00 sec) 由结果可以看到，当参数中是整数或者浮点数时，GREATEST将返回其中最大的值；当参数为字符串时， 返回字母表中顺序最靠后的字符；当比较值列表中有NULL时，不能判断大小，返回值为NULL。 ","date":"2022-08-24","objectID":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/:6:6","tags":[],"title":"MySQL基础篇","uri":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/"},{"categories":["mysql"],"content":"7) BETWEEN AND运算符 BETWEEN运算符使用的格式通常为SELECT D FROM TABLE WHERE C BETWEEN A AND B，此时，当C大于或等于A，并且C小于或等于B时，结果为1，否则结果为0。 ","date":"2022-08-24","objectID":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/:6:7","tags":[],"title":"MySQL基础篇","uri":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/"},{"categories":["mysql"],"content":"8) IN运算符 IN运算符用于判断给定的值是否是IN列表中的一个值，如果是则返回1，否则返回0。如果给定的值为NULL，或者IN列表中存在NULL，则结果为NULL。 mysql\u003e SELECT 'a' IN ('a','b','c'), 1 IN (2,3), NULL IN ('a','b'), 'a' IN ('a', NULL); +----------------------+------------+-------------------+--------------------+ | 'a' IN ('a','b','c') | 1 IN (2,3) | NULL IN ('a','b') | 'a' IN ('a', NULL) | +----------------------+------------+-------------------+--------------------+ | 1 | 0 | NULL | 1 | +----------------------+------------+-------------------+--------------------+ ","date":"2022-08-24","objectID":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/:6:8","tags":[],"title":"MySQL基础篇","uri":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/"},{"categories":["mysql"],"content":"9) NOT IN运算符 NOT IN运算符用于判断给定的值是否不是IN列表中的一个值，如果不是IN列表中的一 个值，则返回1，否则返回0。 ","date":"2022-08-24","objectID":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/:6:9","tags":[],"title":"MySQL基础篇","uri":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/"},{"categories":["mysql"],"content":"10) LIKE运算符 LIKE运算符主要用来匹配字符串，通常用于模糊匹配，如果满足条件则返回1，否则返回0。如果给定的值或者匹配条件为NULL，则返回结果为NULL。 “%”：匹配0个或多个字符。 “_”：只能匹配一个字符。 ","date":"2022-08-24","objectID":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/:6:10","tags":[],"title":"MySQL基础篇","uri":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/"},{"categories":["mysql"],"content":"11) ESCAPE 回避特殊符号的：使用转义符。就是声明另外一个特殊的转义字符，作用和\\一样 例如：查找某个字段含有IT_的前缀时，需要对其中的特殊字符_进行转义，前面加个\\。 SELECT job_id FROM jobs WHERE job_id LIKE ‘IT\\_%‘; 如果不是\\，则要加上ESCAPE。 SELECT job_id FROM jobs WHERE job_id LIKE ‘IT$_%‘ escape ‘$‘; ","date":"2022-08-24","objectID":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/:6:11","tags":[],"title":"MySQL基础篇","uri":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/"},{"categories":["mysql"],"content":"12) REGEXP运算符(×) REGEXP运算符用来匹配字符串，语法格式为：expr REGEXP匹配条件 。 （1）‘^’匹配以该字符后面的字符开头的字符串。 （2）‘$’匹配以该字符前面的字符结尾的字符串。 （3）‘.’匹配任何一个单字符。 （4）“[…]”匹配在方括号内的任何字符。例如，“[abc]”匹配“a”或“b”或“c”。为了命名字符的范围，使用一 个‘-’。“[a-z]”匹配任何字母，而“[0-9]”匹配任何数字。 （5）*匹配零个或多个在它前面的字符。例如，x*匹配任何数量的x字符，[0-9]*匹配任何数量的数字， 而*匹配任何数量的任何字符。 ","date":"2022-08-24","objectID":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/:6:12","tags":[],"title":"MySQL基础篇","uri":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/"},{"categories":["mysql"],"content":"3. 逻辑运算符 逻辑运算符主要用来判断表达式的真假，在MySQL中，逻辑运算符的返回结果为1、0或者NULL。 MySQL中支持4种逻辑运算符如下： PS: 一般不会采用符号，直接使用英文单词具有更好的语义 ","date":"2022-08-24","objectID":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/:7:0","tags":[],"title":"MySQL基础篇","uri":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/"},{"categories":["mysql"],"content":"4. 位运算(×) 位运算符是在二进制数上进行计算的运算符。位运算符会先将操作数变成二进制数，然后进行位运算， 最后将计算结果从二进制变回十进制数。 MySQL支持的位运算符如下： ","date":"2022-08-24","objectID":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/:8:0","tags":[],"title":"MySQL基础篇","uri":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/"},{"categories":["mysql"],"content":"5. 运算符的优先级 数字编号越大，优先级越高，优先级高的运算符先进行计算。 ","date":"2022-08-24","objectID":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/:9:0","tags":[],"title":"MySQL基础篇","uri":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/"},{"categories":["mysql"],"content":"扩展：使用正则表达式查询(×) 第五章_排序与分页 ","date":"2022-08-24","objectID":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/:10:0","tags":[],"title":"MySQL基础篇","uri":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/"},{"categories":["mysql"],"content":"1. 排序规则 使用ORDER BY子句排序 ASC（ascend）: 升序 DESC（descend）:降序 ORDER BY子句在SELECT语句的结尾。 ","date":"2022-08-24","objectID":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/:11:0","tags":[],"title":"MySQL基础篇","uri":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/"},{"categories":["mysql"],"content":"1) 单列排序 SELECT last_name, job_id, department_id, hire_date FROM employees ORDER BY hire_date DESC;# 降序查找 ","date":"2022-08-24","objectID":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/:11:1","tags":[],"title":"MySQL基础篇","uri":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/"},{"categories":["mysql"],"content":"2) 多列排序 可以使用不在SELECT列表中的列排序。 在对多列进行排序的时候，首先排序的第一列必须有相同的列值，才会对第二列进行排序。如果第一列数据中所有值都是唯一的，将不再对第二列进行排序。 为什么呢？因为，多列排序的原理是：当第一列有多个值相同时，就按照第二列的排序规则排序；如果所有值都是唯一，那么第二列排序就排了个寂寞。 ","date":"2022-08-24","objectID":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/:11:2","tags":[],"title":"MySQL基础篇","uri":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/"},{"categories":["mysql"],"content":"2. 分页 格式： LIMIT [位置偏移量,] 行数 # 位置偏移量从0开始 举例： -- 前10条记录： SELECT * FROM 表名 LIMIT 0,10; # 或者 SELECT * FROM 表名 LIMIT 10; -- 第11至20条记录： SELECT * FROM 表名 LIMIT 10,10; -- 第21至30条记录： SELECT * FROM 表名 LIMIT 20,10; MySQL 8.0中可以使用LIMIT 3 OFFSET 4，意思是获取从第5条记录开始后面的3条记录，和LIMIT 4,3;返回的结果相同。 分页显式公式：（当前页数-1）* 每页条数，每页条数 SELECT * FROM table LIMIT(PageNo - 1) * PageSize, PageSize; 注意：LIMIT子句必须放在整个SELECT语句的最后！ 使用LIMIT的好处 约束返回结果的数量可以减少数据表的网络传输量 ，也可以提升查询效率。如果我们知道返回结果只有1条，就可以使用LIMIT 1 ，告诉SELECT语句只需要返回一条记录即可。这样的好处就是SELECT不需要扫描完整的表，只需要检索到一条符合条件的记录即可返回。而且，当数据库表中数据量过大时（上亿的数据），显然为了避免查找效率过低、减少网络传输量、加快传输速度，分页查询是业务中必须做的。 第六章_多表查询 ","date":"2022-08-24","objectID":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/:12:0","tags":[],"title":"MySQL基础篇","uri":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/"},{"categories":["mysql"],"content":"1. 多表查询分类讲解 Join 是“连接”的意思，顾名思义，SQL JOIN 子句用于将两个或者多个表联合起来进行查询。 联合表时需要在每个表中选择一个字段，并对这些字段的值进行比较，值相同的两条记录将合并为一条。联合表的本质就是将不同表的记录合并起来，形成一张新表。当然，这张新表只是临时的，它仅存在于本次查询期间。 数据库中的表可以通过键将彼此联合起来，一个典型的例子是，将一个表的主键和另一个表的外键进行匹配。在表中，每个主键的值都是唯一的，这样做的目的是在不重复每个表中所有记录的情况下，将表之间的数据交叉捆绑在一起。 ","date":"2022-08-24","objectID":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/:13:0","tags":[],"title":"MySQL基础篇","uri":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/"},{"categories":["mysql"],"content":"1) 自连接 题目：查询employees表，返回 \u003c员工 works for 老板\u003e SELECT CONCAT(worker.last_name , ' works for ', manager.last_name) FROM employees AS worker, employees AS manager WHERE worker.manager_id = manager.employee_id; ","date":"2022-08-24","objectID":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/:13:1","tags":[],"title":"MySQL基础篇","uri":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/"},{"categories":["mysql"],"content":"2) 内连接与外连接 内连接: （默认连接方式）只有当两个表都存在满足on条件的记录时才会返回行。 SQL92语法:(×) SELECT emp.employee_id, dep.department_name FROM employee emp, department dep WHERE emp.`department_id` = dep.`department_id`; SQL99语法: SELECT emp.employee_id, dep.department_name FROM employee emp JOIN department dep ON emp.`department_id` = dep.`department_id`; 外连接: 返回左（右）表中的所有行，即使右（左）表中没有满足条件的行也是如此。没有匹配的行时, 结果表中相应的列为空(NULL)。 如果是左外连接，则连接条件中左边的表也称为主表 ，右边的表称为从表。 LEFT OUTER JOIN（OUTER可以省略）：返回左表中的所有行，即使右表中没有满足条件的行也是如此。右表满足条件的行拼接在左表上，不满足条件的填充null字段值 SELECT last_name, department_name FROM employees emp LEFT OUTER JOIN department dep ON emp.`department_id` = dep.`department_id`; 如果是右外连接，则连接条件中右边的表也称为主表 ，左边的表称为从表。 RIGHT OUTER JOIN（OUTER可以省略）：返回右表中的所有行，即使左表中没有满足条件的行也是如此。左表满足条件的行拼接在右表上，不满足条件的填充null字段值 SELECT last_name, department_name FROM employees emp RIGHT JOIN department dep ON emp.`department_id` = dep.`department_id`; # 没有匹配主表的行数据，会显示null，可以通过where进行筛选 ","date":"2022-08-24","objectID":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/:13:2","tags":[],"title":"MySQL基础篇","uri":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/"},{"categories":["mysql"],"content":"2. UNION的使用 合并查询结果 利用UNION关键字，可以给出多条SELECT语句，并将它们的结果组合成单个结果集。合并时，两个表对应的列数和数据类型必须相同，并且相互对应。各个SELECT语句之间使用UNION或UNION ALL关键字分隔。 语法格式： SELECT column,... FROM table1 UNION [ALL] SELECT column,... FROM table2 UNION操作符 UNION操作符返回两个查询的结果集的并集，去除重复记录。 UNION ALL操作符 UNION ALL操作符返回两个查询的结果集的并集。对于两个结果集的重复部分，不去重。 注意：执行UNION ALL语句时所需要的资源比UNION语句少。如果明确知道合并数据后的结果数据不存在重复数据，或者不需要去除重复的数据，则尽量使用UNION ALL语句，以提高数据查询的效率。 举例：查询部门编号\u003e90或邮箱包含a的员工信息 #方式1 SELECT * FROM employees WHERE email LIKE '%a%' OR department_id\u003e90; #方式2 SELECT * FROM employees WHERE email LIKE '%a%' UNION SELECT * FROM employees WHERE department_id\u003e90; 举例：查询中国用户中男性的信息以及美国用户中年男性的用户信息 SELECT id,cname FROM t_chinamale WHERE csex='男' UNION ALL SELECT id,tname FROM t_usmale WHERE tGender='male'; ","date":"2022-08-24","objectID":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/:14:0","tags":[],"title":"MySQL基础篇","uri":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/"},{"categories":["mysql"],"content":"3.七种SQL JOINS的实现 # 中图：内连接 SELECT employee_id,department_name FROM employees e JOIN departments d ON e.`department_id` = d.`department_id`; # 左上图：左外连接 SELECT employee_id,department_name FROM employees e LEFT JOIN departments d ON e.`department_id` = d.`department_id`; # 右上图：右外连接 SELECT employee_id,department_name FROM employees e RIGHT JOIN departments d ON e.`department_id` = d.`department_id`; # 左中图： SELECT employee_id,department_name FROM employees e LEFT JOIN departments d ON e.`department_id` = d.`department_id` WHERE d.`department_id` IS NULL; # 右中图： SELECT employee_id,department_name FROM employees e RIGHT JOIN departments d ON e.`department_id` = d.`department_id` WHERE e.`department_id` IS NULL; # 左下图：满外连接 # 方式1：左上图 UNION ALL 右中图 SELECT employee_id,department_name FROM employees e LEFT JOIN departments d ON e.`department_id` = d.`department_id` UNION ALL SELECT employee_id,department_name FROM employees e RIGHT JOIN departments d ON e.`department_id` = d.`department_id` WHERE e.`department_id` IS NULL; # 方式2：左中图 UNION ALL 右上图 SELECT employee_id,department_name FROM employees e LEFT JOIN departments d ON e.`department_id` = d.`department_id` WHERE d.`department_id` IS NULL UNION ALL SELECT employee_id,department_name FROM employees e RIGHT JOIN departments d ON e.`department_id` = d.`department_id`; # 右下图：左中图 UNION ALL 右中图 SELECT employee_id,department_name FROM employees e LEFT JOIN departments d ON e.`department_id` = d.`department_id` WHERE d.`department_id` IS NULL UNION ALL SELECT employee_id,department_name FROM employees e RIGHT JOIN departments d ON e.`department_id` = d.`department_id` WHERE e.`department_id` IS NULL; ","date":"2022-08-24","objectID":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/:15:0","tags":[],"title":"MySQL基础篇","uri":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/"},{"categories":["mysql"],"content":"4. SQL99语法的新特性 ","date":"2022-08-24","objectID":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/:16:0","tags":[],"title":"MySQL基础篇","uri":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/"},{"categories":["mysql"],"content":"1) 自然连接 SQL99在SQL92的基础上提供了一些特殊语法，比如NATURAL JOIN用来表示自然连接。我们可以把自然连接理解为SQL92中的等值连接。它会帮你自动查询两张连接表中所有相同的字段，然后进行等值连接。 在SQL92标准中： SELECT employee_id,last_name,department_name FROM employees e JOIN departments d ON e.`department_id` = d.`department_id` AND e.`manager_id` = d.`manager_id`; 在 SQL99 中你可以写成： SELECT employee_id,last_name,department_name FROM employees e NATURAL JOIN departments d; ","date":"2022-08-24","objectID":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/:16:1","tags":[],"title":"MySQL基础篇","uri":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/"},{"categories":["mysql"],"content":"2) USING连接 当我们进行连接的时候，SQL99还支持使用USING指定数据表里的同名字段进行等值连接。但是只能配合JOIN一起使用。比如： SELECT employee_id,last_name,department_name FROM employees e JOIN departments d USING (department_id); 你能看出与自然连接NATURAL JOIN同的是，USING指定了具体的相同的字段名称，你需要在USING的括号 () 中填入要指定的同名字段。同时使用JOIN...USING可以简化 JOIN ON 的等值连接。它与下面的SQL查询结果是相同的： SELECT employee_id,last_name,department_name FROM employees e ,departments d WHERE e.department_id = d.department_id; ","date":"2022-08-24","objectID":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/:16:2","tags":[],"title":"MySQL基础篇","uri":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/"},{"categories":["mysql"],"content":"5. 小结 表连接的约束条件可以有三种方式：WHERE, ON, USING WHERE：适用于所有关联查询，where是针对临时表生成以后再对临时表的数据进行过滤 ON ：join是连接表的条件，决定了连接表的生成。虽然关联条件可以并到WHERE中和其他条件一起写，但使用join分开写可读性更好。 USING：只能和JOIN一起使用，而且要求两个关联字段在关联表中名称一致，而且只能表示关联字段值相等 我们要控制连接表的数量。 多表连接就相当于嵌套for循环一样，非常消耗资源，会让SQL查询性能下降得很严重，因此不要连接不必要的表。 在许多DBMS中，也都会有最大连接表的限制。 # 习题巩固 # 注意：当两个表外连接之后，组成主表和从表，主表的连接字段是不为空的，从表的连接字段可能为空，因此从表的关键字段用来判断是否为空。 # 1.查询哪些部门没有员工 # 方式一 SELECT d.department_id FROM departments d LEFT JOIN employees e ON d.`department_id` = e.`department_id` WHERE e.`department_id` IS NULL; # 方式二 SELECT department_id FROM departments d WHERE NOT EXISTS ( SELECT * FROM employees e WHERE e.`department_id` = d.`department_id` ); # 2.查询哪个城市没有部门 SELECT l.location_id, l.city FROM locations l LEFT JOIN departments d ON l.`location_id` = d.`location_id` WHERE d.`location_id` IS NULL; # 3.查询部门名为 Sales 或 IT 的员工信息 SELECT e.employee_id, e.last_name, e.department_id FROM employees e JOIN department d ON e.`department_id` = d.`department_id` WHERE d.`department_name` IN ('Sales', 'IT'); 第七章_单行函数 ","date":"2022-08-24","objectID":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/:17:0","tags":[],"title":"MySQL基础篇","uri":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/"},{"categories":["mysql"],"content":"1. 数值函数 ","date":"2022-08-24","objectID":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/:18:0","tags":[],"title":"MySQL基础篇","uri":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/"},{"categories":["mysql"],"content":"1) 基本函数 函数 用法 ABS(x) 返回x的绝对值 SIGN(X) 单元格 PI() 返回圆周率的值 CEIL(x)，CEILING(x) 返回大于或等于某个值的最小整数 FLOOR(x) 返回小于或等于某个值的最大整数 LEAST(e1,e2,e3…) 返回列表中的最小值 GREATEST(e1,e2,e3…) 返回列表中的最大值 MOD(x,y) 返回X除以Y后的余数 RAND() 返回0~1的随机值 RAND(x) 返回0~1的随机值，其中x的值用作种子值，相同的X值会产生相同的随机数 ROUND(x) 返回一个对x的值进行四舍五入后，最接近于X的整数 ROUND(x,y) 返回一个对x的值进行四舍五入后最接近X的值，并保留到小数点后面Y位 TRUNCATE(x,y) 返回数字x截断为y位小数的结果 SQRT(x) 返回x的平方根。当X的值为负数时，返回NULL ","date":"2022-08-24","objectID":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/:18:1","tags":[],"title":"MySQL基础篇","uri":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/"},{"categories":["mysql"],"content":"2) 角度与弧度互换函数 函数 用法 RADIANS(x) 将角度转化为弧度，其中，参数x为角度值 DEGREES(x) 将弧度转化为角度，其中，参数x为弧度值 ","date":"2022-08-24","objectID":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/:18:2","tags":[],"title":"MySQL基础篇","uri":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/"},{"categories":["mysql"],"content":"3) 三角函数 函数 用法 SIN(x) 将角度转化为弧度，其中，参数x为角度值 ASIN(x) 将弧度转化为角度，其中，参数x为弧度值 COS(x) 返回x的余弦值，其中，参数x为弧度值 ACOS(x) 返回x的反余弦值，即获取余弦为x的值。如果x的值不在-1到1之间，则返回NULL TAN(x) 返回x的正切值，其中，参数x为弧度值 ATAN(x) 返回x的反正切值，即返回正切值为x的值 ATAN2(m,n) 返回两个参数的反正切值 COT(x) 返回x的余切值，其中，X为弧度值 ","date":"2022-08-24","objectID":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/:18:3","tags":[],"title":"MySQL基础篇","uri":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/"},{"categories":["mysql"],"content":"4) 指数与对数函数 函数 用法 POW(x,y)，POWER(X,Y) 返回x的y次方 EXP(X) 返回e的X次方，其中e是一个常数，2.718281828459045 LN(X)，LOG(X) 返回以e为底的X的对数，当X \u003c= 0 时，返回的结果为NULL LOG10(X) 返回以10为底的X的对数，当X \u003c= 0 时，返回的结果为NULL LOG2(X) 返回以2为底的X的对数，当X \u003c= 0 时，返回NULL ","date":"2022-08-24","objectID":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/:18:4","tags":[],"title":"MySQL基础篇","uri":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/"},{"categories":["mysql"],"content":"5) 进制间的转换 函数 用法 BIN(x) 返回x的二进制编码 HEX(x) 返回x的十六进制编码 OCT(x) 返回x的八进制编码 CONV(x,f1,f2) 返回f1进制数变成f2进制数 ","date":"2022-08-24","objectID":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/:18:5","tags":[],"title":"MySQL基础篇","uri":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/"},{"categories":["mysql"],"content":"2. 字符串函数 函数 用法 ASCII(S) 返回字符串S中的第一个字符的ASCII码值 CHAR_LENGTH(s) 返回字符串s的字符数。作用与CHARACTER_LENGTH(s)相同 LENGTH(s) 返回字符串s的字节数，和字符集有关 CONCAT(s1,s2,......,sn) 连接s1,s2,......,sn为一个字符串 CONCAT_WS(x, s1,s2,......,sn) 同CONCAT(s1,s2,...)函数，但是每个字符串之间要加上x INSERT(str, idx, len, replacestr) 将字符串str从第idx位置开始，len个字符长的子串替换为字符串replacestr REPLACE(str, a, b) 用字符串b替换字符串str中所有出现的字符串a UPPER(s) 或 UCASE(s) 将字符串s的所有字母转成大写字母 ``LOWER(s)或LCASE(s)` 将字符串s的所有字母转成小写字母 LEFT(str,n) 返回字符串str最左边的n个字符 RIGHT(str,n) 返回字符串str最右边的n个字符 LPAD(str, len, pad) 用字符串pad对str最左边进行填充，直到str的长度为len个字符 RPAD(str ,len, pad) 用字符串pad对str最右边进行填充，直到str的长度为len个字符 LTRIM(s) 去掉字符串s左侧的空格 RTRIM(s) 去掉字符串s右侧的空格 TRIM(s) 去掉字符串s开始与结尾的空格 TRIM(s1 FROM s) 去掉字符串s开始与结尾的s1 TRIM(LEADING s1 FROM s) 去掉字符串s开始处的s1 TRIM(TRAILING s1 FROM s) 去掉字符串s结尾处的s1 REPEAT(str, n) 返回str重复n次的结果 SPACE(n) 返回n个空格 STRCMP(s1,s2) 比较字符串s1,s2的ASCII码值的大小 SUBSTR(s,index,len) 返回从字符串s的index位置其len个字符，作用与SUBSTRING(s,n,len)、MID(s,n,len)相同 LOCATE(substr,str) 返回字符串substr在字符串str中首次出现的位置，作用于POSITION(substr IN str)、INSTR(str,substr)相同。未找到，返回0 ELT(m,s1,s2,…,sn) 返回指定位置的字符串，如果m=1，则返回s1，如果m=2，则返回s2，如果m=n，则返回sn FIELD(s,s1,s2,…,sn) 返回字符串s在字符串列表中第一次出现的位置 FIND_IN_SET(s1,s2) 返回字符串s1在字符串s2中出现的位置。其中，字符串s2是一个以逗号分隔的字符串 REVERSE(s) 返回s反转后的字符串 NULLIF(value1,value2) 比较两个字符串，如果value1与value2相等，则返回NULL，否则返回 value1 注意：MySQL中，字符串的位置是从1开始的。 ","date":"2022-08-24","objectID":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/:19:0","tags":[],"title":"MySQL基础篇","uri":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/"},{"categories":["mysql"],"content":"3. 日期和时间函数 ","date":"2022-08-24","objectID":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/:20:0","tags":[],"title":"MySQL基础篇","uri":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/"},{"categories":["mysql"],"content":"1) 获取日期、时间 函数 用法 CURDATE() ，CURRENT_DATE() 返回当前日期，只包含年、 月、日 CURTIME() ， CURRENT_TIME() 返回当前时间，只包含时、 分、秒 NOW() / SYSDATE() / CURRENT_TIMESTAMP() / LOCALTIME() / LOCALTIMESTAMP() 返回当前系统日期和时间 UTC_DATE() 返回UTC（世界标准时间） 日期 UTC_TIME() 返回UTC（世界标准时间） 时间 ","date":"2022-08-24","objectID":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/:20:1","tags":[],"title":"MySQL基础篇","uri":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/"},{"categories":["mysql"],"content":"2) 日期与时间戳的转换 函数 用法 UNIX_TIMESTAMP() 以UNIX时间戳的形式返回当前时间。SELECT UNIX_TIMESTAMP() - \u003e1634348884 UNIX_TIMESTAMP(date) 将时间date以UNIX时间戳的形式返回。 FROM_UNIXTIME(timestamp) 将UNIX时间戳的时间转换为普通格式的时间 ","date":"2022-08-24","objectID":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/:20:2","tags":[],"title":"MySQL基础篇","uri":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/"},{"categories":["mysql"],"content":"3) 获取月份、星期、星期数、天数等函数 函数 用法 YEAR(date) / MONTH(date) / DAY(date) 返回具体的日期值 HOUR(time) / MINUTE(time) / SECOND(time) 返回具体的时间值 FROM_UNIXTIME(timestamp) 将UNIX时间戳的时间转换为普通格式的时间 MONTHNAME(date) 返回月份：January，… DAYNAME(date) 返回星期几：MONDAY，TUESDAY…..SUNDAY WEEKDAY(date) 返回周几，注意，周1是0，周2是1，。。。周日是6 QUARTER(date) 返回日期对应的季度，范围为1～4 WEEK(date) ， WEEKOFYEAR(date) 返回一年中的第几周 DAYOFYEAR(date) 返回日期是一年中的第几天 DAYOFMONTH(date) 返回日期位于所在月份的第几天 DAYOFWEEK(date) 返回周几，注意：周日是1，周一是2，。。。周六是 7 ","date":"2022-08-24","objectID":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/:20:3","tags":[],"title":"MySQL基础篇","uri":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/"},{"categories":["mysql"],"content":"4) 日期的操作函数 函数 用法 EXTRACT(type FROM date) 返回指定日期中特定的部分，type指定返回的值 EXTRACT(type FROM date)函数中type的取值与含义： ","date":"2022-08-24","objectID":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/:20:4","tags":[],"title":"MySQL基础篇","uri":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/"},{"categories":["mysql"],"content":"5) 时间和秒钟转换的函数 函数 用法 TIME_TO_SEC(time) 将 time 转化为秒并返回结果值。转化的公式为： 小时*3600+分钟 *60+秒 SEC_TO_TIME(seconds) 将 seconds 描述转化为包含小时、分钟和秒的时间 ","date":"2022-08-24","objectID":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/:20:5","tags":[],"title":"MySQL基础篇","uri":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/"},{"categories":["mysql"],"content":"6) 计算日期和时间的函数 函数 用法 DATE_ADD(datetime, INTERVAL expr type)，ADDDATE(date,INTERVAL expr type) 返回与给定日期时间相差INTERVAL时间段的日期时间 DATE_SUB(date,INTERVAL expr type)， SUBDATE(date,INTERVAL expr type) 返回与date相差INTERVAL时间间隔的日期 上述函数中type的取值： 函数 用法 ADDTIME(time1,time2) 返回time1加上time2的时间。当time2为一个数字时，代表的是 秒 ，可以为负数 SUBTIME(time1,time2) 返回time1减去time2后的时间。当time2为一个数字时，代表的 是 秒 ，可以为负数 DATEDIFF(date1,date2) 返回date1 - date2的日期间隔天数 TIMEDIFF(time1, time2) 返回time1 - time2的时间间隔 FROM_DAYS(N) 返回从0000年1月1日起，N天以后的日期 TO_DAYS(date) 返回日期date距离0000年1月1日的天数 LAST_DAY(date) 返回date所在月份的最后一天的日期 MAKEDATE(year,n) 针对给定年份与所在年份中的天数返回一个日期 MAKETIME(hour,minute,second) 将给定的小时、分钟和秒组合成时间并返回 PERIOD_ADD(time,n) 返回time加上n后的时间 ","date":"2022-08-24","objectID":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/:20:6","tags":[],"title":"MySQL基础篇","uri":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/"},{"categories":["mysql"],"content":"7) 日期的格式化与解析 函数 用法 DATE_FORMAT(date,fmt) 按照字符串fmt格式化日期date值 TIME_FORMAT(time,fmt) 按照字符串fmt格式化时间time值 GET_FORMAT(date_type,format_type) 返回日期字符串的显示格式 STR_TO_DATE(str, fmt) 按照字符串fmt对str进行解析，解析为一个日期 上述 非GET_FORMAT 函数中fmt参数常用的格式符： 格式符 说明 格式符 说明 %Y 4位数字表示年份 %y 表示两位数字表示年份 %M 月名表示月份（January,….） %m 两位数字表示月份 （01,02,03。。。） %b 缩写的月名（Jan.，Feb.，….） %c 数字表示月份（1,2,3,…） %D 英文后缀表示月中的天数 （1st,2nd,3rd,…） %d 两位数字表示月中的天数(01,02…) %e 数字形式表示月中的天数 （1,2,3,4,5…..） %H 两位数字表示小数，24小时制 （01,02..） %h 和%I 两位数字表示小时，12小时制 （01,02..） %k 数字形式的小时，24小时制(1,2,3) %l 数字形式表示小时，12小时制 （1,2,3,4….） %i 两位数字表示分钟（00,01,02） %S 和%s 两位数字表示秒(00,01,02…) %W 一周中的星期名称（Sunday…） %a 一周中的星期缩写（Sun.， Mon.,Tues.，..） %w 以数字表示周中的天数 (0=Sunday,1=Monday….) %j 以3位数字表示年中的天数(001,002…) %U 以数字表示年中的第几周， （1,2,3。。）其中Sunday为周中第一 天 %u 以数字表示年中的第几周， （1,2,3。。）其中Monday为周中第一 天 %T 24小时制 %r 12小时制 %p AM或PM %% 表示% ","date":"2022-08-24","objectID":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/:20:7","tags":[],"title":"MySQL基础篇","uri":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/"},{"categories":["mysql"],"content":"4. 流程控制函数 流程处理函数可以根据不同的条件，执行不同的处理流程，可以在SQL语句中实现不同的条件选择。 MySQL中的流程处理函数主要包括IF()、IFNULL()和CASE()函数。 函数 用法 IF(value,value1,value2) 如果value的值为TRUE，返回value1， 否则返回value2 IFNULL(value1, value2) 如果value1不为NULL，返回value1，否则返回value2 CASE WHEN 条件1 THEN 结果1 WHEN 条件2 THEN 结果2 .... [ELSE resultn] END 相当于Java的if…else if…else… CASE expr WHEN 常量值1 THEN 值1 WHEN 常量值1 THEN 值1 .... [ELSE 值n] END 相当于Java的switch…case… ","date":"2022-08-24","objectID":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/:21:0","tags":[],"title":"MySQL基础篇","uri":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/"},{"categories":["mysql"],"content":"5. 加密与解密函数 加密与解密函数主要用于对数据库中的数据进行加密和解密处理，以防止数据被他人窃取。这些函数在保证数据库安全时非常有用。 函数 用法 PASSWORD(str) 返回字符串str的加密版本，41位长的字符串。加密结果不可逆 ，常用于用户的密码加密 MD5(str) 返回字符串str的md5加密后的值，也是一种加密方式。若参数为 NULL，则会返回NULL SHA(str) 从原明文密码str计算并返回加密后的密码字符串，当参数为 NULL时，返回NULL。 SHA加密算法比MD5更加安全 。 ENCODE(value,password_seed) 返回使用password_seed作为加密密码加密value DECODE(value,password_seed) 返回使用password_seed作为加密密码解密value ","date":"2022-08-24","objectID":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/:22:0","tags":[],"title":"MySQL基础篇","uri":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/"},{"categories":["mysql"],"content":"6. MySQL信息函数 MySQL中内置了一些可以查询MySQL信息的函数，这些函数主要用于帮助数据库开发或运维人员更好地 对数据库进行维护工作。 函数 用法 VERSION() 返回当前MySQL的版本号 CONNECTION_ID() 返回当前MySQL服务器的连接数 DATABASE()，SCHEMA() 返回MySQL命令行当前所在的数据库 USER()，CURRENT_USER()、SYSTEM_USER()， SESSION_USER() 返回当前连接MySQL的用户名，返回结果格式为 “主机名@用户名” CHARSET(value) 返回字符串value自变量的字符集 COLLATION(value) 返回字符串value的比较规则 MySQL中有些函数无法对其进行具体的分类，但是这些函数在MySQL的开发和运维过程中也是不容忽视 的。 函数 用法 FORMAT(value,n) 返回对数字value进行格式化后的结果数据。n表示四舍五入后保留到小数点后n位 CONV(value,from,to) 将value的值进行不同进制之间的转换 INET_ATON(ipvalue) 将以点分隔的IP地址转化为一个数字 INET_NTOA(value) 将数字形式的IP地址转化为以点分隔的IP地址 BENCHMARK(n,expr) 将表达式expr重复执行n次。用于测试MySQL处理expr表达式所耗费的时间 CONVERT(value USING char_code) 将value所使用的字符编码修改为char_code 第八章_聚合函数 ","date":"2022-08-24","objectID":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/:23:0","tags":[],"title":"MySQL基础篇","uri":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/"},{"categories":["mysql"],"content":"1. 聚合函数介绍 什么是聚合函数 聚合函数作用于一列数据，并对一列数据返回一个值。 聚合函数类型 AVG() SUM() MAX() MIN() COUNT() ","date":"2022-08-24","objectID":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/:24:0","tags":[],"title":"MySQL基础篇","uri":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/"},{"categories":["mysql"],"content":"1) AVG和SUM函数 SELECT AVG(salary), MAX(salary),MIN(salary), SUM(salary) FROM employees WHERE job_id LIKE '%REP%'; ","date":"2022-08-24","objectID":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/:24:1","tags":[],"title":"MySQL基础篇","uri":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/"},{"categories":["mysql"],"content":"2) MIN和MAX函数 可以对任意数据类型的数据使用 MIN 和 MAX 函数。 SELECT MIN(hire_date), MAX(hire_date) FROM employees; ","date":"2022-08-24","objectID":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/:24:2","tags":[],"title":"MySQL基础篇","uri":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/"},{"categories":["mysql"],"content":"3) COUNT函数 COUNT(*)返回表中记录总数，适用于任意数据类型。 SELECT COUNT(*) FROM employees WHERE department_id = 50; COUNT(expr) 返回expr不为空的记录总数。 SELECT COUNT(commission_pct) FROM employees WHERE department_id = 50; 问题：用count(*)，count(1)，count(列名)谁好呢? 其实，对于MyISAM引擎的表是没有区别的。这种引擎内部有一计数器在维护着行数。 InnoDB引擎的表用count(*),count(1)直接读行数，复杂度是O(n)，因为InnoDB真的要去数一遍。但好于具体的count(列名)。 问题：能不能使用count(列名)替换count(*)? 不要使用count(列名)来替代count(*)，count(*)是SQL92定义的标准统计行数的语法，跟数据库无关，跟NULL和非NULL无关。说明：count(*)会统计值为NULL 的行，而count(列名)不会统计此列为NULL值的行。 ","date":"2022-08-24","objectID":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/:24:3","tags":[],"title":"MySQL基础篇","uri":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/"},{"categories":["mysql"],"content":"2. GROUP BY ","date":"2022-08-24","objectID":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/:25:0","tags":[],"title":"MySQL基础篇","uri":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/"},{"categories":["mysql"],"content":"1) 基本使用 可以使用GROUP BY子句将表中的数据分成若干组 SELECT column, group_function(column) FROM table [WHERE condition] [GROUP BY group_by_expression] [ORDER BY column]; 结论1： SELECT中出现的非组函数的字段必须声明在GROUP BY中。 例如，SELECT查询的字段出现*或其他不是分组字段的字段（当然也不是聚合函数）会报错。如果使用group by分组查询时，SELECT查询的字段，要么包含在Group By语句的后面，作为分组的依据；要么就要被包含在聚合函数中，分组的字段可以不出现在查询字段里 结论2： GROUP BY声明在FROM后面、WHERE后面、ORDER BY前面、LIMIT前面。(先分组后排序) ","date":"2022-08-24","objectID":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/:25:1","tags":[],"title":"MySQL基础篇","uri":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/"},{"categories":["mysql"],"content":"2) 使用WITH ROLLUP SELECT department_id,AVG(salary) FROM employees WHERE department_id \u003e 80 GROUP BY department_id WITH ROLLUP; 注意： 当使用ROLLUP时，不能同时使用ORDER BY子句进行结果排序，即ROLLUP和ORDER BY是互相排斥的。 ","date":"2022-08-24","objectID":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/:25:2","tags":[],"title":"MySQL基础篇","uri":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/"},{"categories":["mysql"],"content":"3. HAVING ","date":"2022-08-24","objectID":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/:26:0","tags":[],"title":"MySQL基础篇","uri":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/"},{"categories":["mysql"],"content":"1) 基本使用 过滤分组：HAVING子句 行已经被分组。 使用了聚合函数。 满足HAVING子句中条件的分组将被显示。 HAVING不能单独使用，必须要跟GROUP BY一起使用。 SELECT department_id, MAX(salary) FROM employees GROUP BY department_id HAVING MAX(salary)\u003e10000 ; 要求 如果过滤条件中使用了聚合函数，则必须使用HAVING来替换WHERE。否则，报错。 当过滤条件中没有聚合函数时，则此过滤条件声明在WHERE中或HAVING中都可以。但是，建议声明在WHERE中的执行效率高。 HAVING必须声明在GROUP BY的后面。 开发中，我们使用HAVING的前提是SQL中使用了GROUP BY。 ","date":"2022-08-24","objectID":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/:26:1","tags":[],"title":"MySQL基础篇","uri":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/"},{"categories":["mysql"],"content":"2) WHERE和HAVING的对比 区别1：WHERE 可以直接使用表中的字段作为筛选条件，但不能使用分组中的计算函数作为筛选条件； HAVING 必须要与 GROUP BY 配合使用，可以把分组计算的函数和分组字段作为筛选条件。 这决定了，在需要对数据进行分组统计的时候，HAVING 可以完成 WHERE 不能完成的任务。这是因为，在查询语法结构中，WHERE 在 GROUP BY 之前，所以无法对分组结果进行筛选。HAVING 在 GROUP BY 之 后，可以使用分组字段和分组中的计算函数，对分组的结果集进行筛选，这个功能是 WHERE 无法完成 的。另外，WHERE排除的记录不再包括在分组中。 区别2：如果需要通过连接从关联表中获取需要的数据，WHERE 是先筛选后连接，而 HAVING 是先连接后筛选。 这一点，就决定了在关联查询中，WHERE 比 HAVING 更高效。因为 WHERE 可以先筛选，用一个筛选后的较小数据集和关联表进行连接，这样占用的资源比较少，执行效率也比较高。HAVING 则需要 先把结果集准备好，也就是用未被筛选的数据集进行关联，然后对这个大的数据集进行筛选，这样占用 的资源就比较多，执行效率也较低。 小结如下： 关键字 用法 缺点 WHERE 先筛选数据再关联，执行效率高 不能使用分组中的计算函数进行筛选 HAVING 可以使用分组中的计算函数 在最后的结果集中进行筛选，执行效率较低 开发中的选择： WHERE 和 HAVING 也不是互相排斥的，我们可以在一个查询里面同时使用 WHERE 和 HAVING。包含分组统计函数的条件用 HAVING，普通条件用 WHERE。这样，我们就既利用了 WHERE 条件的高效快速，又发挥了 HAVING 可以使用包含分组统计函数的查询条件的优点。当数据量特别大的时候，运行效率会有很大的差别。 ","date":"2022-08-24","objectID":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/:26:2","tags":[],"title":"MySQL基础篇","uri":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/"},{"categories":["mysql"],"content":"4. SELECT的执行过程 ","date":"2022-08-24","objectID":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/:27:0","tags":[],"title":"MySQL基础篇","uri":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/"},{"categories":["mysql"],"content":"1) 查询的结构 #方式1：一般不采用，可读性不好 SELECT ...,....,... FROM ...,...,.... WHERE 多表的连接条件 AND 不包含组函数的过滤条件 GROUP BY ...,... HAVING 包含组函数的过滤条件 ORDER BY ... ASC/DESC LIMIT ...,... #方式2：采用，可读性良好 SELECT ...,....,... FROM ... JOIN ... ON 多表的连接条件 JOIN ... ON ... WHERE 不包含组函数的过滤条件 [AND/OR] 不包含组函数的过滤条件 GROUP BY ...,... HAVING 包含组函数的过滤条件 ORDER BY ... ASC/DESC LIMIT ...,... #其中： #（1）from：从哪些表中筛选 #（2）on：关联多表查询时，去除笛卡尔积 #（3）where：从表中筛选的条件 #（4）group by：分组依据 #（5）having：在统计结果中再次筛选 #（6）order by：排序 #（7）limit：分页 需要记住 SELECT 查询时的两个顺序： 1. 关键字的顺序是不能颠倒的： SELECT ... FROM ... WHERE ... GROUP BY ... HAVING ... ORDER BY ... LIMIT... 1. SELECT 语句的执行顺序（在 MySQL 和 Oracle 中，SELECT 执行顺序基本相同）： FROM -\u003e WHERE -\u003e GROUP BY -\u003e HAVING -\u003e SELECT 的字段 -\u003e DISTINCT -\u003e ORDER BY -\u003e LIMIT 比如你写了一个 SQL 语句，那么它的关键字顺序和执行顺序是下面这样的： SELECT DISTINCT player_id, player_name, count(*) as num # 顺序 5 FROM player JOIN team ON player.team_id = team.team_id # 顺序 1：找到表并连接表，形成新的虚拟表 WHERE height \u003e 1.80 # 顺序 2 GROUP BY player.team_id # 顺序 3 HAVING num \u003e 2 # 顺序 4 ORDER BY num DESC # 顺序 6 LIMIT 2 # 顺序 7 在 SELECT 语句执行这些步骤的时候，每个步骤都会产生一个虚拟表 ，然后将这个虚拟表传入下一个步骤中作为输入。需要注意的是，这些步骤隐含在 SQL 的执行过程中，对于我们来说是不可见的。 ","date":"2022-08-24","objectID":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/:27:1","tags":[],"title":"MySQL基础篇","uri":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/"},{"categories":["mysql"],"content":"2) SQL的执行原理 SELECT 是先执行 FROM 这一步的。在这个阶段，如果是多张表联查，还会经历下面的几个步骤： 首先先通过 CROSS JOIN 求笛卡尔积，相当于得到虚拟表 vt（virtual table）1-1； 通过 ON 进行筛选，在虚拟表 vt1-1 的基础上进行筛选，得到虚拟表 vt1-2； 添加外部行。如果我们使用的是左连接、右链接或者全连接，就会涉及到外部行，也就是在虚拟 表 vt1-2 的基础上增加外部行，得到虚拟表 vt1-3。 当然如果我们操作的是两张以上的表，还会重复上面的步骤，直到所有表都被处理完为止。这个过程得 到是我们的原始数据。 然后进入第三步和第四步，也就是 GROUP 和 HAVING 阶段 。在这个阶段中，实际上是在虚拟表 vt2 的 基础上进行分组和分组过滤，得到中间的虚拟表 vt3 和 vt4 。 当我们完成了条件筛选部分之后，就可以筛选表中提取的字段，也就是进入到 SELECT 和 DISTINCT 阶段 。 首先在 SELECT 阶段会提取想要的字段，然后在 DISTINCT 阶段过滤掉重复的行，分别得到中间的虚拟表 vt5-1 和 vt5-2 。 当我们提取了想要的字段数据之后，就可以按照指定的字段进行排序，也就是 ORDER BY 阶段 ，得到 虚拟表 vt6 。 最后在 vt6 的基础上，取出指定行的记录，也就是 LIMIT 阶段 ，得到最终的结果，对应的是虚拟表 vt7 。 当然我们在写 SELECT 语句的时候，不一定存在所有的关键字，相应的阶段就会省略。 同时因为 SQL 是一门类似英语的结构化查询语言，所以我们在写 SELECT 语句的时候，还要注意相应的关键字顺序，所谓底层运行的原理，就是我们刚才讲到的执行顺序。 第九章_子查询 ","date":"2022-08-24","objectID":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/:27:2","tags":[],"title":"MySQL基础篇","uri":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/"},{"categories":["mysql"],"content":"1. 基本使用 子查询的基本语法结构： 子查询（内查询）在主查询之前一次执行完成。 子查询的结果被主查询（外查询）使用 。 注意事项 子查询要包含在括号内 将子查询放在比较条件的右侧 单行操作符对应单行子查询，多行操作符对应多行子查询 ","date":"2022-08-24","objectID":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/:28:0","tags":[],"title":"MySQL基础篇","uri":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/"},{"categories":["mysql"],"content":"2. 子查询的分类 分类方式1： 我们按内查询的结果返回一条还是多条记录，将子查询分为 单行子查询 、 多行子查询 。 单行子查询 多行子查询 分类方式2： 我们按内查询是否被执行多次，将子查询划分为 相关(或关联)子查询 和 不相关(或非关联)子查询 。 子查询从数据表中查询了数据结果，如果这个数据结果只执行一次，然后这个数据结果作为主查询的条件进行执行，那么这样的子查询叫做不相关子查询。 同样，如果子查询需要执行多次，即采用循环的方式，先从外部查询开始，每次都传入子查询进行查询，然后再将结果反馈给外部，这种嵌套的执行方式就称为相关子查询。 ","date":"2022-08-24","objectID":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/:29:0","tags":[],"title":"MySQL基础篇","uri":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/"},{"categories":["mysql"],"content":"3. 单行子查询 ","date":"2022-08-24","objectID":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/:30:0","tags":[],"title":"MySQL基础篇","uri":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/"},{"categories":["mysql"],"content":"1) 单行比较操作符 操作符 含义 = equal to \u003e greater than \u003e= greater than or equal to \u003c less than \u003c= less than or equal to \u003c\u003e not equal to ","date":"2022-08-24","objectID":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/:30:1","tags":[],"title":"MySQL基础篇","uri":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/"},{"categories":["mysql"],"content":"2) 代码示例 题目：返回job_id与141号员工相同，salary比143号员工多的员工姓名，job_id和工资 SELECT last_name, job_id, salary FROM eployees WHERE job_id = ( SELECT job_id FROM eployees WHERE employee_id = 141 ) AND salary \u003e ( SELECT salary FROM eployees WHERE employee_id = 143 ); 题目：查询与141号或174号员工的manager_id和department_id相同的其他员工的employee_id，manager_id，department_id # 实现方式一：不成对比较 SELECT employee_id, manager_id, department_id FROM employees WHERE manager_id IN (SELECT manager_id FROM employees WHERE employee_id IN (174,141)) AND department_id IN (SELECT department_id FROM employees WHERE employee_id IN (174,141)) AND employee_id NOT IN(174,141); # 实现方式二：成对比较 SELECT employee_id, manager_id, department_id FROM employees WHERE (manager_id, department_id) IN (SELECT manager_id, department_id FROM employees WHERE employee_id IN (141,174)) AND employee_id NOT IN (141,174); 题目：查询最低工资大于50号部门最低工资的部门id和其最低工资 SELECT department_id, MIN(salary) FROM employees GROUP BY department_id HAVING MIN(salary) \u003e (SELECT MIN(salary) FROM employees WHERE department_id = 50); ","date":"2022-08-24","objectID":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/:30:2","tags":[],"title":"MySQL基础篇","uri":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/"},{"categories":["mysql"],"content":"3) CASE中的子查询 题目：显式员工的employee_id,last_name和location。其中，若员工department_id与location_id为1800 的department_id相同，则location为’Canada’，其余则为’USA’。 SELECT employee_id, last_name, (CASE department_id WHEN (SELECT department_id FROM departments WHERE location_id = 1800) THEN 'Canada' ELSE 'USA' END) location FROM employees; ","date":"2022-08-24","objectID":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/:30:3","tags":[],"title":"MySQL基础篇","uri":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/"},{"categories":["mysql"],"content":"4) 子查询中的空值问题 SELECT last_name, job_id FROM employees WHERE job_id = (SELECT job_id FROM employees WHERE last_name = 'Haas'); 子查询不返回任何行 ","date":"2022-08-24","objectID":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/:30:4","tags":[],"title":"MySQL基础篇","uri":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/"},{"categories":["mysql"],"content":"5) 非法使用子查询(×) SELECT employee_id, last_name FROM employees WHERE salary = (SELECT MIN(salary) FROM employees GROUP BY department_id); 多行子查询使用单行比较符 ","date":"2022-08-24","objectID":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/:30:5","tags":[],"title":"MySQL基础篇","uri":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/"},{"categories":["mysql"],"content":"4. 多行子查询 也称为集合比较子查询 内查询返回多行 使用多行比较操作符 ","date":"2022-08-24","objectID":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/:31:0","tags":[],"title":"MySQL基础篇","uri":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/"},{"categories":["mysql"],"content":"1) 多行比较操作符 操作符 含义 IN 等于列表中的任意一个 ANY 需要和单行比较操作符一起使用，和子查询返回的某一个值比较 ALL 需要和单行比较操作符一起使用，和子查询返回的所有值比较 SOME 实际上是ANY的别名，作用相同，一般常使用ANY ","date":"2022-08-24","objectID":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/:31:1","tags":[],"title":"MySQL基础篇","uri":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/"},{"categories":["mysql"],"content":"2) 代码示例 题目：返回其它job_id中比job_id为‘IT_PROG’部门任一工资低的员工的员工号、姓名、job_id 以及salary SELECT employee_id, last_name, job_id, salary FROM employees WHERE job_id \u003c\u003e 'IT_PROG' AND salary \u003c ANY( SELECT salary FROM emplyees WHERE job_id = 'IT_PROG' ); 题目：查询平均工资最低的部门id #方式1： SELECT department_id FROM employees GROUP BY department_id HAVING AVG(salary) = ( SELECT MIN(avg_sal) FROM ( SELECT AVG(salary) avg_sal FROM employees GROUP BY department_id ) dept_avg_sal ); #方式2： SELECT department_id FROM employees GROUP BY department_id HAVING AVG(salary) \u003c= ALL ( SELECT AVG(salary) avg_sal FROM employees GROUP BY department_id ); ","date":"2022-08-24","objectID":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/:31:2","tags":[],"title":"MySQL基础篇","uri":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/"},{"categories":["mysql"],"content":"3) 空值问题 SELECT last_name FROM employees WHERE employee_id NOT IN ( SELECT manager_id FROM employees WHERE manager_id IS NOT NULL ); ","date":"2022-08-24","objectID":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/:31:3","tags":[],"title":"MySQL基础篇","uri":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/"},{"categories":["mysql"],"content":"5. 关联子查询 如果子查询的执行依赖于外部查询，通常情况下都是因为子查询中的表用到了外部的表，并进行了条件关联，因此每执行一次外部查询，子查询都要重新计算一次，这样的子查询就称之为关联子查询 。 关联子查询按照一行接一行的顺序执行，主查询的每一行都执行一次子查询。 说明：子查询中使用主查询中的列 ","date":"2022-08-24","objectID":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/:32:0","tags":[],"title":"MySQL基础篇","uri":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/"},{"categories":["mysql"],"content":"1) 代码示例 题目：查询员工中工资大于本部门平均工资的员工的last_name,salary和其department_id # 方式一：使用相关子查询 SELECT last_name, salary, department_id FROM employees e1 WHERE salary \u003e ( SELECT AVG(salary) FROM employees e2 WHERE department_id = e1.`department_id` ); # 方式二：在FROM中声明子查询 SELECT e.last_name, e.salary, e.department_id FROM employees e, ( SELECT department_id, AVG(salary) avg_sal FROM employees GROUP BY department_id) t_dept_avg_salary WHERE e.department_id = t_dept_avg_salary.department_id AND e.salary \u003e t_dept_avg_salary.avg_sal; 在ORDER BY中使用子查询： 查询员工的id,salary,按照department_name排序 SELECT employee_id, salary FROM employees e ORDER BY ( SELECT department_name FROM departments d WHERE e.`department_id` = d.`department_id` ); 题目：若employees表中employee_id与job_history表中employee_id相同的数目不小于2，输出这些相同id的员工的employee_id,last_name和其job_id SELECT e.employee_id, last_name,e.job_id FROM employees e WHERE 2 \u003c= (SELECT COUNT(*) FROM job_history WHERE employee_id = e.employee_id ); ","date":"2022-08-24","objectID":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/:32:1","tags":[],"title":"MySQL基础篇","uri":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/"},{"categories":["mysql"],"content":"2) EXISTS 与 NOT EXISTS 关键字 关联子查询通常也会和EXISTS操作符一起来使用，用来检查在子查询中是否存在满足条件的行。 如果在子查询中不存在满足条件的行： 条件返回FALSE 继续在子查询中查找 如果在子查询中存在满足条件的行： 不在子查询中继续查找 条件返回TRUE NOT EXISTS关键字表示如果不存在某种条件，则返回TRUE，否则返回FALSE。 题目：查询公司管理者的employee_id，last_name，job_id，department_id信息 # 方式一：EXISTS SELECT employee_id, last_name, job_id, department_id FROM employees e1 WHERE EXISTS ( SELECT * FROM employees e2 WHERE e2.manager_id = e1.employee_id ); # 方式二：自连接 SELECT DISTINCT e1.employee_id, e1.last_name, e1.job_id, e1.department_id FROM employees e1 JOIN employees e2 ON e1.employee_id = e2.manager_id; # 方式三：IN SELECT employee_id, last_name, job_id, department_id WHERE employee_id IN ( SELECT DISTINCT manager_id FROM employees ); 题目：查询departments表中，不存在于employees表中的部门的department_id和department_name # 方式一： SELECT d.department_id, d.department_name FROM departments e RIGHT JOIN departments d ON e.`department_id` = d.`department_id` WHERE e.`department_id` IS NULL; # 方式二： SELECT department_id, department_name FROM departments d WHERE NOT EXISTS ( SELECT * FROM employees e WHERE d.`department_id` = e.`department_id` ); ","date":"2022-08-24","objectID":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/:32:2","tags":[],"title":"MySQL基础篇","uri":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/"},{"categories":["mysql"],"content":"3) 相关更新 UPDATE table1 alias1 SET column = (SELECT expression FROM table2 alias2 WHERE alias1.column = alias2.column); 使用相关子查询依据一个表中的数据更新另一个表的数据。 题目：在employees中增加一个department_name字段，数据为员工对应的部门名称 # 1） ALTER TABLE employees ADD(`department_name` VARCHAR(14)); # 2） UPDATE employees e SET department_name = (SELECT department_name FROM departments d WHERE e.department_id = d.department_id); ","date":"2022-08-24","objectID":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/:32:3","tags":[],"title":"MySQL基础篇","uri":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/"},{"categories":["mysql"],"content":"4) 相关删除 DELETE FROM table1 alias1 WHERE column operator (SELECT expression FROM table2 alias2 WHERE alias1.column = alias2.column); 使用相关子查询依据一个表中的数据删除另一个表的数据。 题目：删除表employees中，其与emp_history表皆有的数据 DELETE FROM employees e WHERE employee_id in( SELECT employee_id FROM emp_history WHERE employee_id = e.employee_id ); ","date":"2022-08-24","objectID":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/:32:4","tags":[],"title":"MySQL基础篇","uri":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/"},{"categories":["mysql"],"content":"6. 思考题 问题：谁的工资比Abel的高？ 解答： #方式1：自连接 SELECT e2.last_name,e2.salary FROM employees e1,employees e2 WHERE e1.last_name = 'Abel' AND e1.`salary` \u003c e2.`salary`; #方式2：子查询 SELECT last_name,salary FROM employees WHERE salary \u003e ( SELECT salary FROM employees WHERE last_name = 'Abel' ); 问题：以上两种方式有好坏之分吗？ 解答：自连接方式好！ 题目中可以使用子查询，也可以使用自连接。一般情况建议你使用自连接，因为在许多 DBMS 的处理过程中，对于自连接的处理速度要比子查询快得多。 可以这样理解：子查询实际上是通过未知表进行查询后的条件判断，而自连接是通过已知的自身数据表 进行条件判断，因此在大部分 DBMS 中都对自连接处理进行了优化。 ","date":"2022-08-24","objectID":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/:33:0","tags":[],"title":"MySQL基础篇","uri":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/"},{"categories":["mysql"],"content":"7. 课后练习 查询和Zlotkey相同部门的员工姓名和工资 # 单行子查询，确定查询结果一定只有一行 SELECT last_name, salary FROM employees WHERE department_id = ( SELECT department_id FROM employees WHERE last_name = 'Zlotkey' ); 查询工资比公司平均工资高的员工的员工号，姓名和工资。 SELECT employee_id, last_name, salary FROM employees WHERE salary \u003e ( SELECT AVG(salary) FROM employee ); 选择工资大于所有JOB_ID = 'SA_MAN' 的员工的工资的员工的last_name, job_id, salary # 使用多行子查询：all，比较每一行 SELECT last_name, job_id, salary FROM employees WHERE salary \u003e ALL ( SELECT salary FROM employees WHERE job_id = 'SA_MAN' ); # 单行子查询：只比较最大值即可 SELECT last_name, job_id, salary FROM employees WHERE salary \u003e ( SELECT MAX(salary) FROM employees WHERE job_id = 'SA_MAN' ); 查询姓名中包含字母u的员工在相同部门员工的员工号和姓名 SELECT employee_id, last_name FROM eployees WHERE department_id IN ( SELECT DISTINCT department_id FROM employees WHERE last_name LIKE '%u%' ); 查询在部门的department_id为1700的部门员工的员工号 SELECT employee_id FROM employees WHERE department_id IN ( SELECT department_id FROM departments WHERE location_id = 1700 ); 查询管理者是King的员工姓名和工资 SELECT last_name, salary FROM employees WHERE manage_id IN ( SELECT employee_id FROM employees WHERE last_name = 'King' ); 查询工资最低的员工信息 (last_name, salary) SELECT last_name, salary FROM employees WHERE salary = ( SELECT MIN(salary) FROM employees ); 查询平均工资最低的部门信息 # 方式一 SELECT * FROM departments WHERE department_id = ( SELECT department_id FROM employees GROUP BY department_id HAVING AVG(salary) = ( SELECT MIN(avg_sal) FROM ( SELECT AVG(salary) avg_sal FROM employees GROUP BY department_id ) t_dept_avg_sal ) ); # 方式二 SELECT * FROM departments WHERE department_id = ( SELECT department_id FROM employees GROUP BY department_id HAVING AVG(salary) \u003c= ALL ( SELECT AVG(salary) avg_sal FROM employees GROUP BY department_id ) ); # 方式三: LIMIT SELECT * FROM departments WHERE department_id IN ( SELECT department_id FROM employees GROUP BY department_id HAVING AVG(salary) = ( SELECT AVG(salary) avg_sal FROM employees GROUP BY department_id ORDER BY avg_sal ASC LIMIT 1 ) ); # 方式四 SELECT d.* FROM departments d, ( SELECT department_id, AVG(salary) avg_sal FROM employees GROUP BY department_id ORDER BY avg_sal ASC LIMIT 0,1 ) t_dept_avg_sal WHERE d.`department_id` = t_dept_avg_sal.`department_id`; 查询平均工资最低的部门信息和该部门的平均工资 (相关子查询) SELECT d.*, (SELECT AVG(salary) FROM employees WHERE department_id = d.`department_id`) avg_sal FROM departments d, ( SELECT department_id, AVG(salary) avg_sal FROM employees GROUP BY department_id ORDER BY avg_sal ASC LIMIT 0,1 ) t_dept_avg_sal WHERE d.`department_id` = t_dept_avg_sal.`department_id`; 查询平均工资最高的job信息 SELECT * FROM jobs WHERE job_id = ( SELECT job_id FROM employees GROUP BY job_id HAVING AVG(salary) = ( SELECT MAX(avg_sal) FROM ( SELECT AVG(salary) avg_sal FROM employees GROUP BY job_id ) t_job_avg_sal ) ); 查询平均工资高于公司平均工资的部门有哪些？ SELECT depatment_id FROM employees WHERE department_id IS NOT NULL GROUP BY department_id HAVING AVG(salary) \u003e ( SELECT AVG(salary) FROM eployees ); 查询出公司中所有manager的详细信息 # 方式1：自连接 SELECT DISTINCT * FROM employees emp, employees manager WHERE emp.`manager_id` = manager.`employee_id`; SELECT DISTINCT * FROM employees emp JOIN employees manager ON emp.`manager_id` = manager.`employee_id`; # 方式2：子查询 SELECT * FROM employees WHERE employee_id IN ( SELECT manager_id FROM employees ); # 方式3：EXISTS SELECT * FROM employees manager WHERE EXISTS ( SELECT * FROM employees emp WHERE manager.`employee_id` = emp.`manager_id` ); 各个部门中，最高工资中最低的那个部门的最低工资是多少？ # 方式一： SELECT MIN(salary) FROM employees WHERE department_id = ( SELECT department_id FROM employees GROUP BY department_id HAVING MAX(salary) = ( SELECT MIN(max_sal) FROM ( SELECT MAX(salary) max_sal FROM employees GROUP BY department_id ) t_dept_max_sal ) ); # 方式二： SELECT MIN(salary) FROM employees WHERE department_id = ( SELECT department_id FROM employees GROUP BY department_id HAVING MAX(salary) \u003c= ALL ( SELECT MAX(salary) FROM employees GROUP BY department_id ) ); # 方式三： SELECT MIN(salary) FROM employees WHERE department_id = ( SELECT departmen","date":"2022-08-24","objectID":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/:34:0","tags":[],"title":"MySQL基础篇","uri":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/"},{"categories":["mysql"],"content":"1. 基础知识 ","date":"2022-08-24","objectID":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/:35:0","tags":[],"title":"MySQL基础篇","uri":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/"},{"categories":["mysql"],"content":"1) 标识符命名规则 数据库名、表名不得超过30个字符，变量名限制为29个 必须只能包含 A–Z, a–z, 0–9, _共63个字符 数据库名、表名、字段名等对象名中间不要包含空格 同一个MySQL软件中，数据库不能同名；同一个库中，表不能重名；同一个表中，字段不能重名 必须保证你的字段没有和保留字、数据库系统或常用方法冲突。如果坚持使用，请在SQL语句中使 用`（着重号）引起来 保持字段名和类型的一致性：在命名字段并为其指定数据类型的时候一定要保证一致性，假如数据 类型在一个表里是整数，那在另一个表里可就别变成字符型了 ","date":"2022-08-24","objectID":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/:35:1","tags":[],"title":"MySQL基础篇","uri":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/"},{"categories":["mysql"],"content":"2) MySQL中的数据类型 类型 数据变量 整数类型 TINYINT、SMALLINT、MEDIUMINT、INT(或INTEGER)、BIGINT 浮点类型 FLOAT、DOUBLE 定点数类型 DECIMAL 位类型 BIT 日期时间类型 YEAR、TIME、DATE、DATETIME、TIMESTAMP 文本字符串类型 CHAR、VARCHAR、TINYTEXT、TEXT、MEDIUMTEXT、LONGTEXT 枚举类型 ENUM 集合类型 SET 二进制字符串类型 BINARY、VARBINARY、TINYBLOB、BLOB、MEDIUMBLOB、LONGBLOB JSON类型 JSON对象、JSON数组 空间数据类型 单值：GEOMETRY、POINT、LINESTRING、POLYGON； 集合：MULTIPOINT、MULTILINESTRING、MULTIPOLYGON、 GEOMETRYCOLLECTION 其中，常用的几类类型介绍如下： ① 数值类型 出现“数值类型(n)”的表示，例如: TINYINT(1)、INT(2)表示数据库显示时的最小长度，而不是数据大小 类型 大小 范围（有符号） 范围（无符号） 用途 TINYINT 1 Bytes (-128，127) (0，255) 小整数值 SMALLINT 2 Bytes (-32 768，32 767) (0，65 535) 大整数值 MEDIUMINT 3 Bytes (-8 388 608，8 388 607) (0，16 777 215) 大整数值 INT或INTEGER 4 Bytes (-2 147 483 648，2 147 483 647) (0，4 294 967 295) 大整数值 BIGINT 8 Bytes (-9,223,372,036,854,775,808，9 223 372 036 854 775 807) (0，18 446 744 073 709 551 615) 极大整数值 FLOAT 4 Bytes (-3.402 823 466 E+38，-1.175 494 351 E-38)，0，(1.175 494 351 E-38，3.402 823 466 351 E+38) 0，(1.175 494 351 E-38，3.402 823 466 E+38) 单精度 浮点数值 DOUBLE 8 Bytes (-1.797 693 134 862 315 7 E+308，-2.225 073 858 507 201 4 E-308)，0，(2.225 073 858 507 201 4 E-308，1.797 693 134 862 315 7 E+308) 0，(2.225 073 858 507 201 4 E-308，1.797 693 134 862 315 7 E+308) 双精度 浮点数值 DECIMAL 对DECIMAL(M,D) ，如果M\u003eD，为M+2,否则为D+2 依赖于M和D的值 依赖于M和D的值 小数值 ② 日期类型 每个时间类型有一个有效值范围和一个\"零\"值，当指定不合法的MySQL不能表示的值时使用\"零\"值。 类型 大小 (bytes) 范围 格式 用途 DATE 3 1000-01-01/9999-12-31 YYYY-MM-DD 日期值 TIME 3 ‘-838:59:59’/‘838:59:59’ HH:MM:SS 时间值或持续时间 YEAR 1 1901/2155 YYYY 年份值 DATETIME 8 ‘1000-01-01 00:00:00’ 到 ‘9999-12-31 23:59:59’ YYYY-MM-DD hh:mm:ss 混合日期和时间值 TIMESTAMP 4 ‘1970-01-01 00:00:01’ UTC 到 ‘2038-01-19 03:14:07’ UTC结束时间是第 2147483647 秒，北京时间 2038-1-19 11:14:07，格林尼治时间 2038年1月19日 凌晨 03:14:07 YYYY-MM-DD hh:mm:ss 混合日期和时间值，时间戳 ③ 字符串类型 类型 大小 用途 CHAR 0-255 bytes 定长字符串 VARCHAR 0-65535 bytes 变长字符串 TINYBLOB 0-255 bytes 不超过 255 个字符的二进制字符串 TINYTEXT 0-255 bytes 短文本字符串 BLOB 0-65 535 bytes 二进制形式的长文本数据 TEXT 0-65 535 bytes 长文本数据 MEDIUMBLOB 0-16 777 215 bytes 二进制形式的中等长度文本数据 MEDIUMTEXT 0-16 777 215 bytes 中等长度文本数据 LONGBLOB 0-4 294 967 295 bytes 二进制形式的极大文本数据 LONGTEXT 0-4 294 967 295 bytes 极大文本数据 注意： char(n) 和 varchar(n) 中括号中 n 代表字符的个数，并不代表字节个数，比如 CHAR(30) 就可以存储 30 个字符，有时有些特殊字符占有字节数大于一个byte. CHAR(n)定义的列的长度为固定的，n取值可以为0～255之间. 当保存CHAR值时，在它们的右边填充空格以达到指定的长度。当检索到CHAR值时，尾部的空格被删除掉。在存储或检索过程中不进行大小写转换。CHAR存储定长数据很方便，CHAR字段上的索引效率级高，比如定义 char(10)，那么不论你存储的数据是否达到了10个字节，都要占去10个字节的空间,不足的自动用空格填充。但是如果使用的是Innodb引擎的话，推荐使用varchar代替char BINARY 和 VARBINARY 类似于 CHAR 和 VARCHAR，不同的是它们包含二进制字符串而不要非二进制字符串。也就是说，它们包含字节字符串而不是字符字符串。这说明它们没有字符集，并且排序和比较基于列值字节的数值值。 BLOB 是一个二进制大对象，可以容纳可变数量的数据。有 4 种 BLOB 类型：TINYBLOB、BLOB、MEDIUMBLOB 和 LONGBLOB。它们区别在于可容纳存储范围不同。 有 4 种 TEXT 类型：TINYTEXT、TEXT、MEDIUMTEXT 和 LONGTEXT。对应的这 4 种 BLOB 类型，可存储的最大长度不同，可根据实际情况选择。 ","date":"2022-08-24","objectID":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/:35:2","tags":[],"title":"MySQL基础篇","uri":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/"},{"categories":["mysql"],"content":"2. 创建和管理数据库 ","date":"2022-08-24","objectID":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/:36:0","tags":[],"title":"MySQL基础篇","uri":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/"},{"categories":["mysql"],"content":"1) 创建数据库 方式1：创建数据库 CREATE DATABASE 数据库名; 方式2：创建数据库并指定字符集 CREATE DATABASE 数据库名 CHARACTER SET 字符集; 方式3：判断数据库是否已经存在，不存在则创建数据库（ 推荐 ） CREATE DATABASE IF NOT EXISTS 数据库名; 如果MySQL中已经存在相关的数据库，则忽略创建语句，不再创建数据库。 注意：DATABASE 不能改名。一些可视化工具可以改名，它是建新库，把所有表复制到新库，再删旧库完成的。 ","date":"2022-08-24","objectID":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/:36:1","tags":[],"title":"MySQL基础篇","uri":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/"},{"categories":["mysql"],"content":"2) 使用数据库 查看当前所有的数据库 SHOW DATABASES; #有一个S，代表多个数据库 查看当前正在使用的数据库 SELECT DATABASE(); #使用的一个 mysql 中的全局函数 查看指定库下所有的表 SHOW TABLES FROM 数据库名 查看数据库的创建信息 SHOW CREATE DATABASE 数据库名; # 或者： SHOW CREATE DATABASE 数据库名\\G 使用use切换数据库 USE 数据库名; 注意：要操作表格和数据之前必须先说明是对哪个数据库进行操作，否则就要对所有对象加上“use 据库名.”。 ","date":"2022-08-24","objectID":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/:36:2","tags":[],"title":"MySQL基础篇","uri":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/"},{"categories":["mysql"],"content":"3) 修改数据库 更改数据库字符集 ALTER DATABASE 数据库名 CHARACTER SET 字符集; #比如：gbk、utf8等 方式1：删除指定的数据库 DROP DATABASE 数据库名; 方式2：删除指定的数据库（ 推荐 ） DROP DATABASE IF EXISTS 数据库名; ","date":"2022-08-24","objectID":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/:36:3","tags":[],"title":"MySQL基础篇","uri":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/"},{"categories":["mysql"],"content":"3. 创建表 ","date":"2022-08-24","objectID":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/:37:0","tags":[],"title":"MySQL基础篇","uri":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/"},{"categories":["mysql"],"content":"1) 创建方式1 语法格式： CREATE TABLE [IF NOT EXISTS] 表名( 字段1, 数据类型 [约束条件] [默认值], 字段2, 数据类型 [约束条件] [默认值], 字段3, 数据类型 [约束条件] [默认值], …… [表约束条件] ); 加上了IF NOT EXISTS关键字，则表示：如果当前数据库中不存在要创建的数据表，则创建数据表； 如果当前数据库中已经存在要创建的数据表，则忽略建表语句，不再创建数据表。 ","date":"2022-08-24","objectID":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/:37:1","tags":[],"title":"MySQL基础篇","uri":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/"},{"categories":["mysql"],"content":"2) 创建方式2 使用 AS subquery 选项，将创建表和插入数据结合起来 CREATE TABLE 表名 [(column, column, ...)] AS subquery; 指定的列和子查询中的列要一一对应 通过列名和默认值定义列 CREATE TABLE dept80 AS SELECT employee_id, last_name, salary*12 ANNSAL, hire_date FROM employees WHERE department_id = 80; ","date":"2022-08-24","objectID":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/:37:2","tags":[],"title":"MySQL基础篇","uri":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/"},{"categories":["mysql"],"content":"3) 查看数据表结构 在MySQL中创建好数据表之后，可以查看数据表的结构。MySQL支持使用 DESCRIBE/DESC 语句查看数据 表结构，也支持使用 SHOW CREATE TABLE 语句查看数据表结构。 语法格式如下： SHOW CREATE TABLE 表名\\G 使用SHOW CREATE TABLE语句不仅可以查看表创建时的详细语句，还可以查看存储引擎和字符编码。 ","date":"2022-08-24","objectID":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/:37:3","tags":[],"title":"MySQL基础篇","uri":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/"},{"categories":["mysql"],"content":"4. 修改表 修改表指的是修改数据库中已经存在的数据表的结构。 使用 ALTER TABLE 语句可以实现： 向已有的表中添加列 修改现有表中的列 删除现有表中的列 重命名现有表中的列 ","date":"2022-08-24","objectID":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/:38:0","tags":[],"title":"MySQL基础篇","uri":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/"},{"categories":["mysql"],"content":"1) 追加一个列 语法格式如下： ALTER TABLE 表名 ADD [COLUMN] 字段名 字段类型 [FIRST]|[AFTER 字段名]; 举例： ALTER TABLE dept80 ADD job_id varchar(15); ","date":"2022-08-24","objectID":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/:38:1","tags":[],"title":"MySQL基础篇","uri":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/"},{"categories":["mysql"],"content":"2) 修改一个列 可以修改列的数据类型，长度、默认值和位置 修改字段数据类型、长度、默认值、位置的语法格式如下： ALTER TABLE 表名 MODIFY [COLUMN] 字段名1 字段类型 [DEFAULT 默认值] [FIRST]|[AFTER 字段名2]; 举例： ALTER TABLE dept80 MODIFY salary double(9,2) default 1000; 对默认值的修改只影响今后对表的修改 此外，还可以通过此种方式修改列的约束。 ","date":"2022-08-24","objectID":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/:38:2","tags":[],"title":"MySQL基础篇","uri":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/"},{"categories":["mysql"],"content":"3) 重命名一个列 使用 CHANGE old_column new_column dataType子句重命名列。语法格式如下： ALTER TABLE 表名 CHANGE [column] 列名 新列名 新数据类型; 举例： ALTER TABLE dept80 CHANGE department_name dept_name varchar(15); ","date":"2022-08-24","objectID":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/:38:3","tags":[],"title":"MySQL基础篇","uri":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/"},{"categories":["mysql"],"content":"4) 删除一个列 删除表中某个字段的语法格式如下： ALTER TABLE 表名 DROP [COLUMN] 字段名 ","date":"2022-08-24","objectID":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/:38:4","tags":[],"title":"MySQL基础篇","uri":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/"},{"categories":["mysql"],"content":"5) 更改表名 方式一：使用RENAME RENAME TABLE emp TO myemp; 方式二： ALTER table dept RENAME [TO] detail_dept; -- [TO]可以省略 必须是对象的拥有者 ","date":"2022-08-24","objectID":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/:38:5","tags":[],"title":"MySQL基础篇","uri":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/"},{"categories":["mysql"],"content":"5. 删除表 在MySQL中，当一张数据表没有与其他任何数据表形成关联关系时，可以将当前数据表直接删除。 数据和结构都被删除 所有正在运行的相关事务被提交 所有相关索引被删除 语法格式： DROP TABLE [IF EXISTS] 数据表1 [, 数据表2, …, 数据表n]; IF EXISTS 的含义为：如果当前数据库中存在相应的数据表，则删除数据表；如果当前数据库中不存在相应的数据表，则忽略删除语句，不再执行删除数据表的操作。 举例： DROP TABLE dept80; DROP TABLE 语句不能回滚 ","date":"2022-08-24","objectID":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/:39:0","tags":[],"title":"MySQL基础篇","uri":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/"},{"categories":["mysql"],"content":"6. 清空表 TRUNCATE TABLE语句： 删除表中所有的数据 释放表的存储空间 举例： TRUNCATE TABLE detail_dept; TRUNCATE语句不能回滚，而使用 DELETE 语句删除数据，可以回滚 阿里开发规范： 【参考】TRUNCATE TABLE 比 DELETE 速度快，且使用的系统和事务日志资源少，但 TRUNCATE 无事务且不触发 TRIGGER，有可能造成事故，故不建议在开发代码中使用此语句。 说明：TRUNCATE TABLE 在功能上与不带 WHERE 子句的 DELETE 语句相同。 ","date":"2022-08-24","objectID":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/:40:0","tags":[],"title":"MySQL基础篇","uri":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/"},{"categories":["mysql"],"content":"7. 内容扩展 ","date":"2022-08-24","objectID":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/:41:0","tags":[],"title":"MySQL基础篇","uri":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/"},{"categories":["mysql"],"content":"拓展1：阿里巴巴《Java开发手册》之MySQL字段命名 【 强制 】表名、字段名必须使用小写字母或数字，禁止出现数字开头，禁止两个下划线中间只出现数字。数据库字段名的修改代价很大，因为无法进行预发布，所以字段名称需要慎重考虑。 正例：aliyun_admin，rdc_config，level3_name 反例：AliyunAdmin，rdcConfig，level_3_name 【 强制 】禁用保留字，如 desc、range、match、delayed 等，请参考 MySQL 官方保留字。 【 强制 】表必备三字段：id, gmt_create, gmt_modified。 说明：其中 id 必为主键，类型为BIGINT UNSIGNED、单表时自增、步长为1。gmt_create, gmt_modified 的类型均为 DATETIME类型，前者现在时表示主动式创建，后者过去分词表示被动式更新 【 推荐 】表的命名最好是遵循 “业务名称_表的作用”。 正例：alipay_task 、 force_project、 trade_config 【 推荐 】库名与应用名称尽量一致。 【参考】合适的字符存储长度，不但节约数据库表空间、节约索引存储，更重要的是提升检索速度。 正例：无符号值可以避免误存负数，且扩大了表示范围。 ","date":"2022-08-24","objectID":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/:41:1","tags":[],"title":"MySQL基础篇","uri":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/"},{"categories":["mysql"],"content":"扩展2：操作注意要求 表删除操作将把表的定义和表中的数据一起删除，并且MySQL在执行删除操作时，不会有任何的确认信息提示，因此执行删除操时应当慎重。在删除表前，最好对表中的数据进行 备份，这样当操作失误时可以对数据进行恢复，以免造成无法挽回的后果。 同样的，在使用 ALTER TABLE 进行表的基本修改操作时，在执行操作过程之前，也应该确保对数据进行完整的备份，因为数据库的改变是无法撤销的，如果添加了一个不需要的字段，可以将其删除；相同的，如果删除了一个需要的列，该列下面的所有数据都将会丢失。 ","date":"2022-08-24","objectID":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/:41:2","tags":[],"title":"MySQL基础篇","uri":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/"},{"categories":["mysql"],"content":"扩展3：MySQL8新特性—DDL的原子化 在MySQL 8.0版本中，InnoDB表的DDL支持事务完整性，即DDL操作要么成功要么回滚。DDL操作回滚日志写入到data dictionary数据字典表mysql.innodb_ddl_log（该表是隐藏的表，通过show tables无法看到）中，用于回滚操作。通过设置参数，可将DDL操作日志打印输出到MySQL错误日志中。 第11章_数据处理之增删改 ","date":"2022-08-24","objectID":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/:41:3","tags":[],"title":"MySQL基础篇","uri":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/"},{"categories":["mysql"],"content":"1. 插入数据 ","date":"2022-08-24","objectID":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/:42:0","tags":[],"title":"MySQL基础篇","uri":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/"},{"categories":["mysql"],"content":"1) 方式1：VALUES的方式添加 使用这种语法一次只能向表中插入一条数据。 情况1：为表的所有字段按默认顺序插入数据 INSERT INTO 表名 VALUES (value1,value2,....); 值列表中需要为表的每一个字段指定值，并且值的顺序必须和数据表中字段定义时的顺序相同。 举例： INSERT INTO departments VALUES (70, 'Pub', 100, 1700); 情况2: 指定字段名插入数据 为表的指定字段插入数据，就是在INSERT语句中只向部分字段中插入值，而其他字段的值为表定义时的默认值或null。 在 INSERT 子句中随意列出列名，但是一旦列出，VALUES中要插入的value1,....valuen需要与 column1,...columnn列一一对应。如果类型不同，将无法插入，并且MySQL会产生错误。 举例： INSERT INTO departments(department_id, department_name) VALUES (80, 'IT'); 情况3：同时插入多条记录 INSERT语句可以同时向数据表中插入多条记录，插入时指定多个值列表，每个值列表之间用逗号分隔开，基本语法格式如下： INSERT INTO table_name VALUES (value1 [,value2, …, valuen]), (value1 [,value2, …, valuen]), …… (value1 [,value2, …, valuen]); 或者 INSERT INTO table_name(column1 [, column2, …, columnn]) VALUES (value1 [,value2, …, valuen]), (value1 [,value2, …, valuen]), …… (value1 [,value2, …, valuen]); 使用INSERT同时插入多条记录时，MySQL会返回一些在执行单行插入时没有的额外信息，这些信息的含义如下： Records：表明插入的记录条数。 Duplicates：表明插入时被忽略的记录，原因可能是这些记录包含了重复的主键值。 Warnings：表明有问题的数据值，例如发生数据类型转换。 一个同时插入多行记录的INSERT语句等同于多个单行插入的INSERT语句，但是多行的INSERT语句在处理过程中效率更高 。因为MySQL执行单条INSERT语句插入多行数据比使用多条INSERT语句快，所以在插入多条记录时最好选择使用单条INSERT语句的方式插入. ","date":"2022-08-24","objectID":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/:42:1","tags":[],"title":"MySQL基础篇","uri":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/"},{"categories":["mysql"],"content":"2) 方式2：将查询结果插入到表中 INSERT还可以将SELECT语句查询的结果插入到表中，此时不需要把每一条记录的值一个一个输入，只需要使用一条INSERT语句和一条SELECT语句组成的组合语句即可快速地从一个或多个表中向一个表中插入多行 INSET INTO 目标表名 (tar_column1 [, tar_column2, ..., tar_columnn]) SELECT (src_column1 [, src_column2, …, src_columnn]) FROM 源表名 [WHERE condition] 在 INSERT 语句中加入子查询。 不必书写 VALUES 子句。 子查询中的值列表应与 INSERT 子句中的列名对应。 INSERT INTO emp2 SELECT * FROM employees WHERE department_id = 90; INSERT INTO sales_reps(id, name, salary, commission_pct) SELECT employee_id, last_name, salary, commission_pct FROM employees WHERE job_id LIKE '%REP%'; ","date":"2022-08-24","objectID":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/:42:2","tags":[],"title":"MySQL基础篇","uri":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/"},{"categories":["mysql"],"content":"2. 更新数据 使用 UPDATE 语句更新数据。语法如下： UPDATE table_name SET column1=value1, column2=value2, ..., column=valuen [WHERE condition] 可以一次更新多条数据。 如果需要回滚数据，需要保证在DML前，进行设置：SET AUTOCOMMIT = FALSE; 使用 WHERE 子句指定需要更新的数据。 UPDATE employees SET department_id = 70 WHERE employee_id = 113; 如果省略 WHERE 子句，则表中的所有数据都将被更新。 ","date":"2022-08-24","objectID":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/:43:0","tags":[],"title":"MySQL基础篇","uri":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/"},{"categories":["mysql"],"content":"3. 删除数据 DELETE FROM table_name [WHERE \u003ccondition\u003e]; table_name指定要执行删除操作的表；“[WHERE ]”为可选参数，指定删除条件，如果没有WHERE子句， DELETE语句将删除表中的所有记录。 ","date":"2022-08-24","objectID":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/:44:0","tags":[],"title":"MySQL基础篇","uri":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/"},{"categories":["mysql"],"content":"4. MySQL8新特性：计算列 什么叫计算列呢？简单来说就是某一列的值是通过别的列计算得来的。例如，a列值为1、b列值为2，c列 不需要手动插入，定义a+b的结果为c的值，那么c就是计算列，是通过别的列计算得来的。 在MySQL 8.0中，CREATE TABLE 和 ALTER TABLE 中都支持增加计算列。下面以CREATE TABLE为例进行讲解。 举例：定义数据表tb1，然后定义字段id、字段a、字段b和字段c，其中字段c为计算列，用于计算a+b的 值。 首先创建测试表tb1，语句如下： CREATE TABLE tb1( id INT, a INT, b INT, c INT GENERATED ALWAYS AS (a + b) VIRTUAL ); 第12章_MySQL数据类型精讲 ","date":"2022-08-24","objectID":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/:45:0","tags":[],"title":"MySQL基础篇","uri":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/"},{"categories":["mysql"],"content":"1. MySQL中的数据类型 类型 举例 整数类型 TINYINT、SMALLINT、MEDIUMINT、INT(或INTEGER)、BIGINT 浮点类型 FLOAT、DOUBLE 定点数类型 DECIMAL 位类型 BIT 日期时间类型 YEAR、TIME、DATE、DATETIME、TIMESTAMP 文本字符串类型 CHAR、VARCHAR、TINYTEXT、TEXT、MEDIUMTEXT、LONGTEXT 枚举类型 ENUM 集合类型 SET 二进制字符串类型 BINARY、VARBINARY、TINYBLOB、BLOB、MEDIUMBLOB、LONGBLOB JSON类型 JSON对象、JSON数组 空间数据类型 单值类型：GEOMETRY、POINT、LINESTRING、POLYGON； 集合类型：MULTIPOINT、MULTILINESTRING、MULTIPOLYGON、 GEOMETRYCOLLECTION 常见数据类型的属性，如下： MySQL关键字 含义 NULL 空 NOT NULL 非空 DEFAULT 默认值 PRIMARY KEY 主键约束 AUTO_INCREMENT 自增约束 UNSIGNED 无符号 CHARACTER SET name 设置字符集 ","date":"2022-08-24","objectID":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/:46:0","tags":[],"title":"MySQL基础篇","uri":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/"},{"categories":["mysql"],"content":"2. 整数类型 ","date":"2022-08-24","objectID":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/:47:0","tags":[],"title":"MySQL基础篇","uri":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/"},{"categories":["mysql"],"content":"1) 类型介绍 整数类型一共有 5 种，包括 TINYINT、SMALLINT、MEDIUMINT、INT（INTEGER）和 BIGINT。 它们的区别如下表所示： 整数类型 字节 有符号数取值范围 无符号数取值范围 TINYINT 1 -128~127 0~255 SMALLINT 2 -32768~32767 0~65535 MEDIUMINT 3 -8388608~8388607 0~16777215 INT、INTEGER 4 -2147483648~2147483647 0~4294967295 BIGINT 8 -9223372036854775808~9223372036854775807 0~18446744073709551615 ","date":"2022-08-24","objectID":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/:47:1","tags":[],"title":"MySQL基础篇","uri":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/"},{"categories":["mysql"],"content":"2) 可选属性 整数类型的可选属性有三个： M M : 表示显示宽度，M的取值范围是(0, 255)。例如，int(5)：当数据宽度小于5位的时候在数字前面需要用’ ‘字符填满宽度。该项功能需要配合“ ZEROFILL ”使用，表示用“0”填满宽度，否则指定显示宽度无效。如果设置了显示宽度，那么插入的数据宽度超过显示宽度限制，会不会截断或插入失败？ 答案：不会对插入的数据有任何影响，还是按照类型的实际宽度进行保存，即显示宽度与类型可以存储的值范围无关 。从MySQL 8.0.17开始，整数数据类型不推荐使用显示宽度属性。 整型数据类型可以在定义表结构时指定所需要的显示宽度，如果不指定，则系统为每一种类型指定默认的宽度值。 举例： CREATE TABLE test_int1 ( x TINYINT, y SMALLINT, z MEDIUMINT, m INT, n BIGINT ); 查看表结构 （MySQL5.7中显式如下，MySQL8中不再显式范围） mysql\u003e desc test_int1; +-------+--------------+------+-----+---------+-------+ | Field | Type | Null | Key | Default | Extra | +-------+--------------+------+-----+---------+-------+ | x | tinyint(4) | YES | | NULL | | | y | smallint(6) | YES | | NULL | | | z | mediumint(9) | YES | | NULL | | | m | int(11) | YES | | NULL | | | n | bigint(20) | YES | | NULL | | +-------+--------------+------+-----+---------+-------+ 5 rows in set (0.00 sec) TINYINT有符号数和无符号数的取值范围分别为-128~127和0~255，由于负号占了一个数字位，因此TINYINT默认的显示宽度为4。同理，其他整数类型的默认显示宽度与其有符号数的最小值的宽度相同。 UNSIGNED UNSIGNED : 无符号类型（非负），所有的整数类型都有一个可选的属性UNSIGNED（无符号属性），无符号整数类型的最小取值为0。所以，如果需要在MySQL数据库中保存非负整数值时，可以将整数类型设置为无符号类型。 int类型默认显示宽度为int(11)，无符号int类型默认显示宽度为int(10)。 ZEROFILL ZEROFILL : 0填充,（如果某列是ZEROFILL，那么MySQL会自动为当前列添加UNSIGNED属性），如果指定了ZEROFILL只是表示不够M位时，用0在左边填充，如果超过M位，只要不超过数据存储范围即可。 原来，在 int(M) 中，M 的值跟 int(M) 所占多少存储空间并无任何关系。 **int(3)、int(4)、int(8) 在磁盘上都 是占用 4 bytes 的存储空间。**也就是说，int(M)，必须和UNSIGNED、 ZEROFILL一起使用才有意义。如果整数值超过M位，就按照实际位数存储。只是无须再用字符 0 进行填充。 ","date":"2022-08-24","objectID":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/:47:2","tags":[],"title":"MySQL基础篇","uri":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/"},{"categories":["mysql"],"content":"3) 适用场景 TINYINT ：一般用于枚举数据，比如系统设定取值范围很小且固定的场景。 SMALLINT ：可以用于较小范围的统计数据，比如统计工厂的固定资产库存数量等。 MEDIUMINT ：用于较大整数的计算，比如车站每日的客流量等。 INT、INTEGER ：取值范围足够大，一般情况下不用考虑超限问题，用得最多。比如商品编号。 BIGINT ：只有当你处理特别巨大的整数时才会用到。比如双十一的交易量、大型门户网站点击量、证券公司衍生产品持仓等。 ","date":"2022-08-24","objectID":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/:47:3","tags":[],"title":"MySQL基础篇","uri":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/"},{"categories":["mysql"],"content":"4) 如何选择？ 在评估用哪种整数类型的时候，你需要考虑 存储空间 和 可靠性 的平衡问题：一方面，用占用字节数少的整数类型可以节省存储空间；另一方面，要是为了节省存储空间， 使用的整数类型取值范围太小，一旦遇到超出取值范围的情况，就可能引起系统错误，影响可靠性。 举个例子，商品编号采用的数据类型是 INT。原因就在于，客户门店中流通的商品种类较多，而且，每天都有旧商品下架，新商品上架，这样不断迭代，日积月累。 如果使用 SMALLINT 类型，虽然占用字节数比 INT 类型的整数少，但是却不能保证数据不会超出范围 65535。相反，使用 INT，就能确保有足够大的取值范围，不用担心数据超出范围影响可靠性的问题。 你要注意的是，在实际工作中，系统故障产生的成本远远超过增加几个字段存储空间所产生的成本。因此，我建议你首先确保数据不会超过取值范围，在这个前提之下，再去考虑如何节省存储空间。 ","date":"2022-08-24","objectID":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/:47:4","tags":[],"title":"MySQL基础篇","uri":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/"},{"categories":["mysql"],"content":"3. 浮点类型 ","date":"2022-08-24","objectID":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/:48:0","tags":[],"title":"MySQL基础篇","uri":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/"},{"categories":["mysql"],"content":"1) 类型介绍 浮点数和定点数类型的特点是可以 处理小数 ，你可以把整数看成小数的一个特例。因此，浮点数和定点数的使用场景，比整数大多了。 MySQL支持的浮点数类型，分别是FLOAT、DOUBLE、REAL。 FLOAT 表示单精度浮点数； DOUBLE 表示双精度浮点数； REAL默认就是 DOUBLE。如果你把 SQL 模式设定为启用“ REAL_AS_FLOAT ”，那 么，MySQL 就认为REAL 是 FLOAT。如果要启用“REAL_AS_FLOAT”，可以通过以下 SQL 语句实现： SET sql_mode = “REAL_AS_FLOAT”; 问题：为什么浮点数类型的无符号数取值范围，只相当于有符号数取值范围的一半，也就是只相当于有符号数取值范围大于等于零的部分呢？ MySQL 存储浮点数的格式为： 符号(S) 、 尾数(M) 和 阶码(E) 。因此，无论有没有符号，MySQL 的浮点数都会存储表示符号的部分。因此，所谓的无符号数取值范围，其实就是有符号数取值范围大于等于零的部分。 ","date":"2022-08-24","objectID":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/:48:1","tags":[],"title":"MySQL基础篇","uri":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/"},{"categories":["mysql"],"content":"2) 数据精度说明 对于浮点类型，在MySQL中单精度值使用 4 个字节，双精度值使用 8 个字节。 MySQL允许使用非标准语法 （其他数据库未必支持，因此如果涉及到数据迁移，则最好不要这么用）： FLOAT(M,D) 或 DOUBLE(M,D) 。这里，M称为 精度 ，D称为 标度 。(M,D)中 M=整数位+小数位，D=小数位。 D\u003c=M\u003c=255，0\u003c=D\u003c=30。 例如，定义为FLOAT(5,2)的一个列可以显示为-999.99-999.99。如果超过这个范围会报错。 FLOAT和DOUBLE类型在不指定(M,D)时，默认会按照实际的精度（由实际的硬件和操作系统决定） 来显示。 说明：浮点类型，也可以加 UNSIGNED ，但是不会改变数据范围，例如：FLOAT(3,2) UNSIGNED仍然 只能表示0-9.99的范围。 不管是否显式设置了精度(M,D)，这里MySQL的处理方案如下： 如果存储时，整数部分超出了范围，MySQL就会报错，不允许存这样的值 如果存储时，小数点部分若超出范围，就分以下情况： 若四舍五入后，整数部分没有超出范围，则只警告，但能成功操作并四舍五入删除多余的小数位后保存。例如在FLOAT(5,2)列内插入999.009，近似结果是999.01。 若四舍五入后，整数部分超出范围，则MySQL报错，并拒绝处理。如FLOAT(5,2)列内插入 999.995和-999.995都会报错。 从MySQL 8.0.17开始，FLOAT(M,D) 和DOUBLE(M,D)用法在官方文档中已经明确不推荐使用，将来可能被移除。另外，关于浮点型FLOAT和DOUBLE的UNSIGNED也不推荐使用了，将来也可能被移除。 ","date":"2022-08-24","objectID":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/:48:2","tags":[],"title":"MySQL基础篇","uri":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/"},{"categories":["mysql"],"content":"3) 精度误差说明 浮点数类型有个缺陷，就是不精准。下面我来重点解释一下为什么 MySQL 的浮点数不够精准。比如，我们设计一个表，有f1这个字段，插入值分别为0.47,0.44,0.19，我们期待的运行结果是：0.47 + 0.44 + 0.19 = 1.1。而使用sum之后查询： CREATE TABLE test_double2( f1 DOUBLE ); INSERT INTO test_double2 VALUES(0.47),(0.44),(0.19); mysql\u003e SELECT SUM(f1) -\u003e FROM test_double2; +--------------------+ | SUM(f1) | +--------------------+ | 1.0999999999999999 | +--------------------+ 1 row in set (0.00 sec) 查询结果是 1.0999999999999999。看到了吗？虽然误差很小，但确实有误差。你也可以尝试把数据类型 改成 FLOAT，然后运行求和查询，得到的是， 1.0999999940395355。显然，误差更大了。 那么，为什么会存在这样的误差呢？问题还是出在 MySQL 对浮点类型数据的存储方式上。 MySQL 用 4 个字节存储 FLOAT 类型数据，用 8 个字节来存储 DOUBLE 类型数据。无论哪个，都是采用二进制的方式来进行存储的。比如 9.625，用二进制来表达，就是 1001.101，或者表达成 1.001101×2^3。如 果尾数不是 0 或 5（比如 9.624），你就无法用一个二进制数来精确表达。进而，就只好在取值允许的范围内进行四舍五入。 在编程中，如果用到浮点数，要特别注意误差问题，因为浮点数是不准确的，所以我们要避免使用“=”来 判断两个数是否相等。同时，在一些对精确度要求较高的项目中，千万不要使用浮点数，不然会导致结果错误，甚至是造成不可挽回的损失。那么，MySQL 有没有精准的数据类型呢？当然有，这就是定点数 类型：DECIMAL 。 ","date":"2022-08-24","objectID":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/:48:3","tags":[],"title":"MySQL基础篇","uri":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/"},{"categories":["mysql"],"content":"4. 定点数类型 ","date":"2022-08-24","objectID":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/:49:0","tags":[],"title":"MySQL基础篇","uri":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/"},{"categories":["mysql"],"content":"1) 类型介绍 MySQL中的定点数类型只有DECIMAL一种类型。 类型 字节 有符号数取值范围 DECIMAL(M,D),DEC,NUMERIC M+2字节 有效范围由M和D决定 使用 DECIMAL(M,D) 的方式表示高精度小数。其中，M被称为精度，D被称为标度。0\u003c=M\u003c=65， 0\u003c=D\u003c=30，D DECIMAL(M,D)的最大取值范围与DOUBLE类型一样，但是有效的数据范围是由M和D决定的。 DECIMAL 的存储空间并不是固定的，由精度值M决定，总共占用的存储空间为M+2个字节。也就是说，在一些对精度要求不高的场景下，比起占用同样字节长度的定点数，浮点数表达的数值范围可以更大一些。 定点数在MySQL内部是以字符串的形式进行存储，这就决定了它一定是精准的。 当DECIMAL类型不指定精度和标度时，其默认为DECIMAL(10,0)。当数据的精度超出了定点数类型的精度范围时，则MySQL同样会进行四舍五入处理。 浮点数 vs 定点数 浮点数相对于定点数的优点是在长度一定的情况下，浮点类型取值范围大，但是不精准，适用于需要取值范围大，又可以容忍微小误差的科学计算场景（比如计算化学、分子建模、流体动 力学等） 定点数类型取值范围相对小，但是精准，没有误差，适合于对精度要求极高的场景（比如涉及金额计算的场景） ","date":"2022-08-24","objectID":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/:49:1","tags":[],"title":"MySQL基础篇","uri":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/"},{"categories":["mysql"],"content":"2) 开发中的经验 “由于DECIMAL数据类型的精准性，在我们的项目中，除了极少数（比如商品编号）用到整数类型外，其他的数值都用的是 DECIMAL，原因就是这个项目所处的零售行业，要求精准，一分钱也不能差。 ” ——来自某项目经理 ","date":"2022-08-24","objectID":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/:49:2","tags":[],"title":"MySQL基础篇","uri":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/"},{"categories":["mysql"],"content":"5. 位类型：BIT BIT类型中存储的是二进制值，类似010110。 二进制字符串类型 长度 长度范围 占用空间 BIT(M) M 1 \u003c= M \u003c= 64 约为(M + 7)/8个字节 BIT类型，如果没有指定(M)，默认是1位。这个1位，表示只能存1位的二进制值。这里(M)是表示二进制的位数，位数最小值为1，最大值为64。 ","date":"2022-08-24","objectID":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/:50:0","tags":[],"title":"MySQL基础篇","uri":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/"},{"categories":["mysql"],"content":"6. 日期与时间类型 日期与时间是重要的信息，在我们的系统中，几乎所有的数据表都用得到。原因是客户需要知道数据的 时间标签，从而进行数据查询、统计和处理。 MySQL有多种表示日期和时间的数据类型，不同的版本可能有所差异，MySQL8.0版本支持的日期和时间 类型主要有：YEAR类型、TIME类型、DATE类型、DATETIME类型和TIMESTAMP类型。 YEAR 类型通常用来表示年 DATE 类型通常用来表示年、月、日 TIME 类型通常用来表示时、分、秒 DATETIME 类型通常用来表示年、月、日、时、分、秒 TIMESTAMP 类型通常用来表示带时区的年、月、日、时、分、秒 类型 名称 字节 日期格式 最小值 最大值 YEAR 年 1 YYYY或YY 1901 2155 TIME 时间 3 HH:MM:SS -838:59:59 838:59:59 DATE 日期 3 YYYY-MM-DD 1000-01-01 9999-12-03 DATETIME 日期时间 8 YYYY-MM-DD HH:MM:SS 1000-01-01 00:00:00 9999-12-31 23:59:59 TIMESTAMP 日期时间 4 YYYY-MM-DD HH:MM:SS 1970-01-01 00:00:00 UTC 2038-01-19 03:14:07UTC 可以看到，不同数据类型表示的时间内容不同、取值范围不同，而且占用的字节数也不一样，你要根据 实际需要灵活选取。 为什么时间类型 TIME 的取值范围不是 -23:59:59～23:59:59 呢？原因是 MySQL 设计的 TIME 类型，不光表示一天之内的时间，而且可以用来表示一个时间间隔，这个时间间隔可以超过 24 小时。 ","date":"2022-08-24","objectID":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/:51:0","tags":[],"title":"MySQL基础篇","uri":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/"},{"categories":["mysql"],"content":"7. 文本字符串类型 MySQL中，文本字符串总体上分为 CHAR 、 VARCHAR 、 TINYTEXT 、 TEXT 、 MEDIUMTEXT 、 LONGTEXT 、 ENUM 、 SET 等类型。 ","date":"2022-08-24","objectID":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/:52:0","tags":[],"title":"MySQL基础篇","uri":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/"},{"categories":["mysql"],"content":"8. ENUM类型 ENUM类型也叫作枚举类型，ENUM类型的取值范围需要在定义字段时进行指定。设置字段值时，ENUM 类型只允许从成员中选取单个值，不能一次选取多个值。 其所需要的存储空间由定义ENUM类型时指定的成员个数决定。 文本字符串类型 长度 长度范围 占用的存储空间 ENUM L 1 \u003c= L \u003c= 65535 1或2个字节 当ENUM类型包含1～255个成员时，需要1个字节的存储空间； 当ENUM类型包含256～65535个成员时，需要2个字节的存储空间。 ENUM类型的成员个数的上限为65535个。 ","date":"2022-08-24","objectID":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/:53:0","tags":[],"title":"MySQL基础篇","uri":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/"},{"categories":["mysql"],"content":"9. SET类型 当SET类型包含的成员个数不同时，其所占用的存储空间也是不同的，具体如下： 成员个数范围（L表示实际成员个数） 占用的存储空间 1 \u003c= L \u003c= 8 1个字节 9 \u003c= L \u003c= 16 2个字节 17 \u003c= L \u003c= 24 3个字节 25 \u003c= L \u003c= 32 4个字节 33 \u003c= L \u003c= 64 8个字节 SET类型在存储数据时成员个数越多，其占用的存储空间越大。注意：SET类型在选取成员时，可以一次选择多个成员，这一点与ENUM类型不同。 ","date":"2022-08-24","objectID":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/:54:0","tags":[],"title":"MySQL基础篇","uri":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/"},{"categories":["mysql"],"content":"13. 小结及选择建议 在定义数据类型时，如果确定是整数，就用 INT ； 如果是小数 ，一定用定点数类型 DECIMAL(M,D) ； 如果是日期与时间，就用 DATETIME 。 这样做的好处是，首先确保你的系统不会因为数据类型定义出错。不过，凡事都是有两面的，可靠性好，并不意味着高效。比如，TEXT 虽然使用方便，但是效率不如 CHAR(M) 和 VARCHAR(M)。 阿里巴巴《Java开发手册》之MySQL数据库： 任何字段如果为非负数，必须是 UNSIGNED 【 强制 】小数类型为 DECIMAL，禁止使用 FLOAT 和 DOUBLE。 说明：在存储的时候，FLOAT 和 DOUBLE 都存在精度损失的问题，很可能在比较值的时候，得到不正确的结果。如果存储的数据范围超过 DECIMAL 的范围，建议将数据拆成整数和小数并分开存储。 【 强制 】如果存储的字符串长度几乎相等，使用 CHAR 定长字符串类型。 【 强制 】VARCHAR 是可变长字符串，不预先分配存储空间，长度不要超过 5000。如果存储长度大于此值，定义字段类型为 TEXT，独立出来一张表，用主键来对应，避免影响其它字段索引效率。 第13章_约束 ","date":"2022-08-24","objectID":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/:55:0","tags":[],"title":"MySQL基础篇","uri":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/"},{"categories":["mysql"],"content":"1. 约束的分类 根据约束数据列的限制，约束可分为： 单列约束：每个约束只约束一列 多列约束：每个约束可约束多列数据 根据约束的作用范围，约束可分为： 列级约束：只能作用在一个列上，跟在列的定义后面 表级约束：可以作用在多个列上，不与列一起，而是单独定义 根据约束起的作用，约束可分为： NOT NULL 非空约束，规定某个字段不能为空 UNIQUE 唯一约束，规定某个字段在整个表中是唯一的 PRIMARY KEY 主键(非空且唯一)约束 FOREIGN KEY 外键约束 CHECK 检查约束 DEFAULT 默认值约束 注意： MySQL不支持check约束，但可以使用check约束，而没有任何效果 如何添加/删除约束？ CREATE TABLE时添加约束 ALTER TABLE时增加约束、删除约束 查看某个表已有的约束 #information_schema数据库名（系统库） #table_constraints表名称（专门存储各个表的约束） SELECT * FROM information_schema.table_constraints WHERE table_name = '表名称'; ","date":"2022-08-24","objectID":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/:56:0","tags":[],"title":"MySQL基础篇","uri":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/"},{"categories":["mysql"],"content":"2. 非空约束 ","date":"2022-08-24","objectID":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/:57:0","tags":[],"title":"MySQL基础篇","uri":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/"},{"categories":["mysql"],"content":"1) 作用 限定某个字段/ 某列的值不允许为空 ","date":"2022-08-24","objectID":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/:57:1","tags":[],"title":"MySQL基础篇","uri":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/"},{"categories":["mysql"],"content":"2) 关键字 NOT NULL ","date":"2022-08-24","objectID":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/:57:2","tags":[],"title":"MySQL基础篇","uri":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/"},{"categories":["mysql"],"content":"3) 特点 默认，所有的类型的值都可以是NULL，包括INT、FLOAT等数据类型 非空约束只能出现在表对象的列上，只能某个列单独限定非空，不能组合非空 一个表可以有很多列都分别限定了非空 空字符串''不等于NULL，0也不等于NULL ","date":"2022-08-24","objectID":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/:57:3","tags":[],"title":"MySQL基础篇","uri":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/"},{"categories":["mysql"],"content":"4) 添加非空约束 1. 建表时 CREATE TABLE 表名称( 字段名 数据类型, 字段名 数据类型 NOT NULL, 字段名 数据类型 NOT NULL ); 2. 建表后 alter table 表名称 modify 字段名 数据类型 not null; ","date":"2022-08-24","objectID":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/:57:4","tags":[],"title":"MySQL基础篇","uri":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/"},{"categories":["mysql"],"content":"5) 删除非空约束 alter table 表名称 modify 字段名 数据类型 NULL;#去掉not null，相当于修改某个非注解字段，该字段允许为空 或 alter table 表名称 modify 字段名 数据类型;#去掉not null，相当于修改某个非注解字段，该字段允许为空 ","date":"2022-08-24","objectID":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/:57:5","tags":[],"title":"MySQL基础篇","uri":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/"},{"categories":["mysql"],"content":"3. 唯一性约束 ","date":"2022-08-24","objectID":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/:58:0","tags":[],"title":"MySQL基础篇","uri":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/"},{"categories":["mysql"],"content":"1) 作用 用来限制某个字段/某列的值不能重复。 ","date":"2022-08-24","objectID":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/:58:1","tags":[],"title":"MySQL基础篇","uri":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/"},{"categories":["mysql"],"content":"2) 关键字 UNIQUE ","date":"2022-08-24","objectID":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/:58:2","tags":[],"title":"MySQL基础篇","uri":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/"},{"categories":["mysql"],"content":"3) 特点 同一个表可以有多个唯一约束。 唯一约束可以是某一个列的值唯一，也可以多个列组合的值唯一。 唯一性约束允许列值为空。 在创建唯一约束的时候，如果不给唯一约束命名，就默认和列名相同。 MySQL会给唯一约束的列上默认创建一个唯一索引。 ","date":"2022-08-24","objectID":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/:58:3","tags":[],"title":"MySQL基础篇","uri":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/"},{"categories":["mysql"],"content":"4) 添加唯一约束 1. 建表时 create table 表名称( 字段名 数据类型, 字段名 数据类型 unique, 字段名 数据类型 unique key, 字段名 数据类型 ); create table 表名称( 字段名 数据类型, 字段名 数据类型, 字段名 数据类型, [constraint 约束名] unique key(字段名) ); 举例： CREATE TABLE USER( id INT NOT NULL, NAME VARCHAR(25), PASSWORD VARCHAR(16), -- 使用表级约束语法 CONSTRAINT uk_name_pwd UNIQUE(NAME,PASSWORD) ); 表示用户名和密码组合不能重复 2. 建表后指定唯一键约束 #字段列表中如果是一个字段，表示该列的值唯一。如果是两个或更多个字段，那么复合唯一，即多个字段的组合是唯 一的 #方式1： alter table 表名称 add unique key(字段列表); #方式2： alter table 表名称 modify 字段名 字段类型 unique; ","date":"2022-08-24","objectID":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/:58:4","tags":[],"title":"MySQL基础篇","uri":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/"},{"categories":["mysql"],"content":"5) 关于复合唯一约束 create table 表名称( 字段名 数据类型, 字段名 数据类型, 字段名 数据类型, unique key(字段列表) #字段列表中写的是多个字段名，多个字段名用逗号分隔，表示那么是复合唯一，即多个字段的组合是唯一的 ); ","date":"2022-08-24","objectID":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/:58:5","tags":[],"title":"MySQL基础篇","uri":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/"},{"categories":["mysql"],"content":"6) 删除唯一约束 添加唯一性约束的列上也会自动创建唯一索引。 删除唯一约束只能通过删除唯一索引的方式删除。 删除时需要指定唯一索引名，唯一索引名就和唯一约束名一样。 如果创建唯一约束时未指定名称，如果是单列，就默认和列名相同； 如果是组合列，那么默认和()中排在第一个的列名相同。也可以自定义唯一性约束名。 SELECT * FROM information_schema.table_constraints WHERE table_name = '表名'; #查看都有哪些约束 ALTER TABLE `USER` DROP INDEX uk_name_pwd; 注意：可以通过 show index from 表名称; 查看表的索引 ","date":"2022-08-24","objectID":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/:58:6","tags":[],"title":"MySQL基础篇","uri":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/"},{"categories":["mysql"],"content":"4. PRIMARY KEY 约束 ","date":"2022-08-24","objectID":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/:59:0","tags":[],"title":"MySQL基础篇","uri":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/"},{"categories":["mysql"],"content":"1) 作用 用来唯一标识表中的一行记录。 ","date":"2022-08-24","objectID":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/:59:1","tags":[],"title":"MySQL基础篇","uri":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/"},{"categories":["mysql"],"content":"2) 关键字 primary key ","date":"2022-08-24","objectID":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/:59:2","tags":[],"title":"MySQL基础篇","uri":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/"},{"categories":["mysql"],"content":"3) 特点 主键约束相当于 [唯一约束+非空约束] 的组合，主键约束列不允许重复，也不允许出现空值。 一个表最多只能有一个主键约束，建立主键约束可以在列级别创建，也可以在表级别上创建。 主键约束对应着表中的一列或者多列（复合主键） 如果是多列组合的复合主键约束，那么这些列都不允许为空值，并且组合的值不允许重复。 MySQL的主键名总是PRIMARY，就算自己命名了主键约束名也没用。 当创建主键约束时，系统默认会在所在的列或列组合上建立对应的主键索引（能够根据主键查询的，就根据主键查询，效率更高。如果删除主键约束了，主键约束对应的索引就自动删除了。 需要注意的一点是，不要修改主键字段的值。因为主键是数据记录的唯一标识，如果修改了主键的值，就有可能会破坏数据的完整性。 ","date":"2022-08-24","objectID":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/:59:3","tags":[],"title":"MySQL基础篇","uri":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/"},{"categories":["mysql"],"content":"4) 添加主键约束 1. 建表时指定主键约束 create table 表名称( 字段名 数据类型 primary key, #列级模式 字段名 数据类型, 字段名 数据类型 ); create table 表名称( 字段名 数据类型, 字段名 数据类型, 字段名 数据类型, [constraint 约束名] primary key(字段名) #表级模式 ); 2. 建表后增加主键约束 ALTER TABLE 表名称 ADD PRIMARY KEY(字段列表); #字段列表可以是一个字段，也可以是多个字段，如果是多个字段的话，是复合主键 ","date":"2022-08-24","objectID":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/:59:4","tags":[],"title":"MySQL基础篇","uri":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/"},{"categories":["mysql"],"content":"5) 关于复合主键 create table 表名称( 字段名 数据类型, 字段名 数据类型, 字段名 数据类型, primary key(字段名1,字段名2) #表示字段1和字段2的组合是唯一的，也可以有更多个字段 ); ","date":"2022-08-24","objectID":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/:59:5","tags":[],"title":"MySQL基础篇","uri":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/"},{"categories":["mysql"],"content":"6) 删除主键约束 alter table 表名称 drop primary key 说明：删除主键约束，不需要指定主键名，因为一个表只有一个主键，删除主键约束后，非空还存在。 ","date":"2022-08-24","objectID":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/:59:6","tags":[],"title":"MySQL基础篇","uri":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/"},{"categories":["mysql"],"content":"5. 自增列：AUTO_INCREMENT ","date":"2022-08-24","objectID":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/:60:0","tags":[],"title":"MySQL基础篇","uri":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/"},{"categories":["mysql"],"content":"1) 作用 某个字段的值自增 ","date":"2022-08-24","objectID":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/:60:1","tags":[],"title":"MySQL基础篇","uri":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/"},{"categories":["mysql"],"content":"2) 关键字 auto_increment ","date":"2022-08-24","objectID":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/:60:2","tags":[],"title":"MySQL基础篇","uri":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/"},{"categories":["mysql"],"content":"3) 特点 （1）一个表最多只能有一个自增长列 （2）当需要产生唯一标识符或顺序值时，可设置自增长 （3）自增长列约束的列必须是键列（主键列，唯一键列） （4）自增约束的列的数据类型必须是整数类型 （5）如果自增列指定了 0 和 null，会在当前最大值的基础上自增；如果自增列手动指定了具体值，直接赋值为具体值。 ","date":"2022-08-24","objectID":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/:60:3","tags":[],"title":"MySQL基础篇","uri":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/"},{"categories":["mysql"],"content":"4) 如何指定自增约束 1. 建表时 create table 表名称( 字段名 数据类型 primary key auto_increment, 字段名 数据类型 unique key not null, 字段名 数据类型 unique key, 字段名 数据类型 not null default 默认值, ); create table 表名称( 字段名 数据类型 default 默认值 , 字段名 数据类型 unique key auto_increment,# 字段类型必须为整数型 字段名 数据类型 not null default 默认值, primary key(字段名) ); 2. 建表后 alter table 表名称 modify 字段名 数据类型 auto_increment; ","date":"2022-08-24","objectID":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/:60:4","tags":[],"title":"MySQL基础篇","uri":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/"},{"categories":["mysql"],"content":"5) 删除自增约束 #alter table 表名称 modify 字段名 数据类型 auto_increment;#给这个字段增加自增约束 alter table 表名称 modify 字段名 数据类型; #去掉auto_increment相当于删除 ","date":"2022-08-24","objectID":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/:60:5","tags":[],"title":"MySQL基础篇","uri":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/"},{"categories":["mysql"],"content":"6) MySQL 8.0新特性—自增变量的持久化 在MySQL 8.0之前，自增主键AUTO_INCREMENT的值如果大于max(primary key)+1，在MySQL重启后，会重置AUTO_INCREMENT=max(primary key)+1，这种现象在某些情况下会导致业务主键冲突或者其他难以发现的问题。下面通过案例来对比不同的版本中自增变量是否持久化。 在MySQL 5.7版本中，测试步骤如下：创建的数据表中包含自增主键的id字段，语句如下： CREATE TABLE test1( id INT PRIMARY KEY AUTO_INCREMENT, ); 在MySQL 5.7系统中，对于自增主键的分配规则，是由InnoDB数据字典内部一个计数器来决定的，而该计数器只在内存中维护，并不会持久化到磁盘中。当数据库重启时，该计数器会被初始化。 在MySQL 8.0将自增主键的计数器持久化到重做日志中。每次计数器发生改变，都会将其写入重做日志中。如果数据库重启，InnoDB会根据重做日志中的信息来初始化计数器的内存值。 ","date":"2022-08-24","objectID":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/:60:6","tags":[],"title":"MySQL基础篇","uri":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/"},{"categories":["mysql"],"content":"6. FOREIGN KEY 约束 ","date":"2022-08-24","objectID":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/:61:0","tags":[],"title":"MySQL基础篇","uri":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/"},{"categories":["mysql"],"content":"1) 作用 限定某个表的某个字段的引用完整性。 ","date":"2022-08-24","objectID":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/:61:1","tags":[],"title":"MySQL基础篇","uri":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/"},{"categories":["mysql"],"content":"2) 关键字 FOREIGN KEY ","date":"2022-08-24","objectID":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/:61:2","tags":[],"title":"MySQL基础篇","uri":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/"},{"categories":["mysql"],"content":"3) 主表和从表/父表和子表 主表（父表）：被引用的表，被参考的表 从表（子表）：引用别人的表，参考别人的表 ","date":"2022-08-24","objectID":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/:61:3","tags":[],"title":"MySQL基础篇","uri":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/"},{"categories":["mysql"],"content":"4) 特点 （1）从表的外键列，必须引用/参考主表的主键或唯一约束的列为什么？因为被依赖/被参考的值必须是唯一的 （2）在创建外键约束时，如果不给外键约束命名，默认名不是列名，而是自动产生一个外键名（例如 student_ibfk_1;），也可以指定外键约束名。 （3）创建(CREATE)表时就指定外键约束的话，先创建主表，再创建从表 （4）删表时，先删从表（或先删除外键约束），再删除主表 （5）当主表的记录被从表参照时，主表的记录将不允许删除，如果要删除数据，需要先删除从表中依赖该记录的数据，然后才可以删除主表的数据 （6）在“从表”中指定外键约束，并且一个表可以建立多个外键约束 （7）从表的外键列与主表被参照的列名字可以不相同，但是数据类型必须一样，逻辑意义一致。如果类型不一样，创建子表时，就会出现错误“ERROR 1005 (HY000): Can't create table'database.tablename'(errno: 150)”。 例如：都是表示部门编号，都是int类型。 （8）当创建外键约束时，系统默认会在所在的列上建立对应的普通索引。但是索引名是外键的约束名。（根据外键查询效率很高） （9）删除外键约束后，必须手动删除对应的索引 ","date":"2022-08-24","objectID":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/:61:4","tags":[],"title":"MySQL基础篇","uri":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/"},{"categories":["mysql"],"content":"5) 添加外键约束 1. 建表时 create table 主表名称( 字段1 数据类型 primary key, 字段2 数据类型 ); create table 从表名称( 字段1 数据类型 primary key, 字段2 数据类型, [CONSTRAINT \u003c外键约束名称\u003e] FOREIGN KEY（从表的某个字段) references 主表名(被参考字段) ); #(从表的某个字段)的数据类型必须与主表名(被参考字段)的数据类型一致，逻辑意义也一样 #(从表的某个字段)的字段名可以与主表名(被参考字段)的字段名一样，也可以不一样 -- FOREIGN KEY: 在表级指定子表中的列 -- REFERENCES: 标示在父表中的列 create table dept( #主表 did int primary key, #部门编号 dname varchar(50) #部门名称 ); create table emp(#从表 eid int primary key, #员工编号 ename varchar(5), #员工姓名 deptid int, #员工所在的部门 foreign key (deptid) references dept(did) #在从表中指定外键约束 #emp表的deptid和和dept表的did的数据类型一致，意义都是表示部门的编号 ); 说明： （1）主表dept必须先创建成功，然后才能创建emp表，指定外键成功。 （2）删除表时，先删除从表emp，再删除主表dept 2. 建表后 一般情况下，表与表的关联都是提前设计好了的，因此，会在创建表的时候就把外键约束定义好。不过，如果需要修改表的设计（比如添加新的字段，增加新的关联关系），但没有预先定义外键约束，那么，就要用修改表的方式来补充定义。 格式： ALTER TABLE 从表名 ADD [CONSTRAINT 约束名] FOREIGN KEY (从表的字段) REFERENCES 主表名(被引用字段) [on update xx][on delete xx]; 举例： ALTER TABLE emp1 ADD [CONSTRAINT emp_dept_id_fk] FOREIGN KEY(dept_id) REFERENCES dept(dept_id); ","date":"2022-08-24","objectID":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/:61:5","tags":[],"title":"MySQL基础篇","uri":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/"},{"categories":["mysql"],"content":"6) 约束等级 Cascade方式 ：在父表上update/delete记录时，同步update/delete掉子表的匹配记录 Set null方式 ：在父表上update/delete记录时，将子表上匹配记录的列设为null，但是要注意子表的外键列不能为not null No action方式 ：如果子表中有匹配的记录，则不允许对父表对应候选键进行update/delete操作 Restrict方式 ：同no action，都是立即检查外键约束 Set default方式 （在可视化工具SQLyog中可能显示空白）：父表有变更时，子表将外键列设置成一个默认的值，但Innodb不能识别x 如果没有指定等级，就相当于Restrict方式。 对于外键约束，最好是采用: ON UPDATE CASCADE; ON DELETE RESTRICT 的方式。 ","date":"2022-08-24","objectID":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/:61:6","tags":[],"title":"MySQL基础篇","uri":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/"},{"categories":["mysql"],"content":"7) 删除外键约束 流程如下： (1)第一步先查看约束名和删除外键约束 SELECT * FROM information_schema.table_constraints WHERE table_name = '表名称'; #查看某个表的约束名 ALTER TABLE 从表名 DROP FOREIGN KEY 外键约束名; (2)第二步查看索引名和删除索引。（注意，只能手动删除） SHOW INDEX FROM 表名称; #查看某个表的索引名 ALTER TABLE 从表名 DROP INDEX 索引名; ","date":"2022-08-24","objectID":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/:61:7","tags":[],"title":"MySQL基础篇","uri":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/"},{"categories":["mysql"],"content":"8) 开发场景 问题1：如果两个表之间有关系（一对一、一对多），比如：员工表和部门表（一对多），它们之间是否一定要建外键约束？ 答：不是的 问题2：建和不建外键约束有什么区别？ 答：建外键约束，你的操作（创建表、删除表、添加、修改、删除）会受到限制，从语法层面受到限制。例如：在员工表中不可能添加一个员工信息，它的部门的值在部门表中找不到。 不建外键约束，你的操作（创建表、删除表、添加、修改、删除）不受限制，要保证数据的引用完整性 ，只能依靠程序员的自觉 ，或者是在Java程序中进行限定。例如：在员工表中，可以添加一个员工的信息，它的部门指定为一个完全不存在的部门。 问题3：那么建和不建外键约束和查询有没有关系？ 答：没有 在 MySQL 里，外键约束是有成本的，需要消耗系统资源。对于大并发的 SQL 操作，有可能会不适合。比如大型网站的中央数据库，可能会因为外键约束的系统开销而变得非常慢。所以， MySQL 允许你不使用系统自带的外键约束，在应用层面完成检查数据一致性的逻辑。也就是说，即使你不用外键约束，也要想办法通过应用层面的附加逻辑，来实现外键约束的功能，确保数据的一致性。 ","date":"2022-08-24","objectID":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/:61:8","tags":[],"title":"MySQL基础篇","uri":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/"},{"categories":["mysql"],"content":"9) 阿里开发规范 【 强制 】不得使用外键与级联，一切外键概念必须在应用层解决。 说明：（概念解释）学生表中的 student_id 是主键，那么成绩表中的 student_id 则为外键。如果更新学生表中的 student_id，同时触发成绩表中的 student_id 更新，即为级联更新。外键与级联更新适用于单机低并发，不适合分布式、高并发集群 ；级联更新是强阻塞，存在数据库更新风暴的风险；外键影响数据库的插入速度 。 ","date":"2022-08-24","objectID":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/:61:9","tags":[],"title":"MySQL基础篇","uri":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/"},{"categories":["mysql"],"content":"7. CHECK 约束 ","date":"2022-08-24","objectID":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/:62:0","tags":[],"title":"MySQL基础篇","uri":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/"},{"categories":["mysql"],"content":"1) 作用 检查某个字段的值是否符号xx要求，一般指的是值的范围 ","date":"2022-08-24","objectID":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/:62:1","tags":[],"title":"MySQL基础篇","uri":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/"},{"categories":["mysql"],"content":"2) 关键字 CHECK ","date":"2022-08-24","objectID":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/:62:2","tags":[],"title":"MySQL基础篇","uri":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/"},{"categories":["mysql"],"content":"3) 说明 MySQL5.7 可以使用check约束，但check约束对数据验证没有任何作用。添加数据时，没有任何错误或警告 但是MySQL 8.0中可以使用check约束了。 create table employee( eid int primary key, ename varchar(5), gender char check ('男' or '女') ); ","date":"2022-08-24","objectID":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/:62:3","tags":[],"title":"MySQL基础篇","uri":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/"},{"categories":["mysql"],"content":"8. DEFAULT约束 ","date":"2022-08-24","objectID":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/:63:0","tags":[],"title":"MySQL基础篇","uri":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/"},{"categories":["mysql"],"content":"1) 作用 给某个字段/某列指定默认值，一旦设置默认值，在插入数据时，如果此字段没有显式赋值，则赋值为默认值。 ","date":"2022-08-24","objectID":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/:63:1","tags":[],"title":"MySQL基础篇","uri":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/"},{"categories":["mysql"],"content":"2) 关键字 DEFAULT ","date":"2022-08-24","objectID":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/:63:2","tags":[],"title":"MySQL基础篇","uri":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/"},{"categories":["mysql"],"content":"3) 添加默认值 1. 建表时 create table 表名称( 字段名 数据类型 primary key, 字段名 数据类型 unique key not null, 字段名 数据类型 unique key, 字段名 数据类型 not null default 默认值, ); 2. 建表后 alter table 表名称 modify 字段名 数据类型 default 默认值; #如果这个字段原来有非空约束，你还保留非空约束，那么在加默认值约束时，还得保留非空约束，否则非空约束就被删除了 #同理，在给某个字段加非空约束也一样，如果这个字段原来有默认值约束，你想保留，也要在modify语句中保留默认值约束，否则就删除了 alter table 表名称 modify 字段名 数据类型 default 默认值 not null; 删除默认值 alter table 表名称 modify 字段名 数据类型; #删除默认值约束，也不保留非空约束 alter table 表名称 modify 字段名 数据类型 not null; #删除默认值约束，保留非空约束 ","date":"2022-08-24","objectID":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/:63:3","tags":[],"title":"MySQL基础篇","uri":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/"},{"categories":["mysql"],"content":"9. 面试 面试1、为什么建表时，加 not null default '' 或 default 0 答：不想让表中出现null值。 面试2、为什么不想要 null 的值 答: （1）不好比较。null是一种特殊值，比较时只能用专门的is null 和 is not null来比较。碰到运算符，通常返回null。 （2）效率不高。影响提高索引效果。因此，我们往往在建表时 not null default '' 或 default 0 面试3、带AUTO_INCREMENT约束的字段值是从1开始的吗？ 在MySQL中，默认AUTO_INCREMENT的初始值是1，每新增一条记录，字段值自动加1。设置自增属性（AUTO_INCREMENT）的时候，还可以指定第一条插入记录的自增字段的值，这样新插入的记录的自增字段值从初始值开始递增，如在表中插入第一条记录，同时指定id值为5，则以后插入的记录的id值就会从6开始往上增加。添加主键约束时，往往需要设置字段自动增加属性。 面试4、并不是每个表都可以任意选择存储引擎？ 外键约束（FOREIGN KEY）不能跨引擎使用。 MySQL支持多种存储引擎，每一个表都可以指定一个不同的存储引擎，需要注意的是：外键约束是用来保证数据的参照完整性的，如果表之间需要关联外键，却指定了不同的存储引擎，那么这些表之间是不能创建外键约束的。所以说，存储引擎的选择也不完全是随意的。 第14章_视图(×) ","date":"2022-08-24","objectID":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/:64:0","tags":[],"title":"MySQL基础篇","uri":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/"},{"categories":["mysql"],"content":"1. 常见的数据库对象 对象 描述 表(TABLE) 表是存储数据的逻辑单元，以行和列的形式存在，列就是字段，行就是记录 数据字典 就是系统表，存放数据库相关信息的表。系统表的数据通常由数据库系统维护， 程序员通常不应该修改，只可查看 约束 (CONSTRAINT) 执行数据校验的规则，用于保证数据完整性的规则 视图(VIEW) 一个或者多个数据表里的数据的逻辑显示，视图并不存储数据 索引(INDEX) 用于提高查询性能，相当于书的目录 存储过程 (PROCEDURE) 用于完成一次完整的业务处理，没有返回值，但可通过传出参数将多个值传给调用环境 存储函数 (FUNCTION) 用于完成一次特定的计算，具有一个返回值 触发器 (TRIGGER) 相当于一个事件监听器，当数据库发生特定事件后，触发器被触发，完成相应的处理 ","date":"2022-08-24","objectID":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/:65:0","tags":[],"title":"MySQL基础篇","uri":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/"},{"categories":["mysql"],"content":"2. 视图概述 视图是一种虚拟表 ，本身是不具有数据的，占用很少的内存空间，它是 SQL 中的一个重要概念。 视图建立在已有表的基础上, 视图赖以建立的这些表称为基表。 视图的创建和删除只影响视图本身，不影响对应的基表。但是当对视图中的数据进行增加、删除和修改操作时，数据表中的数据会相应地发生变化，反之亦然。 视图提供数据内容的语句为 SELECT 语句, 可以将视图理解为存储起来的 SELECT 语句 在数据库中，视图不会保存数据，数据真正保存在数据表中。当对视图中的数据进行增加、删 除和修改操作时，数据表中的数据会相应地发生变化；反之亦然。 视图，是向用户提供基表数据的另一种表现形式。通常情况下，小型项目的数据库可以不使用视 图，但是在大型项目中，以及数据表比较复杂的情况下，视图的价值就凸显出来了，它可以帮助我 们把经常查询的结果集放到虚拟表中，提升使用效率。理解和使用起来都非常方便。 ","date":"2022-08-24","objectID":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/:66:0","tags":[],"title":"MySQL基础篇","uri":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/"},{"categories":["mysql"],"content":"3. 创建视图 在 CREATE VIEW 语句中嵌入子查询 CREATE [OR REPLACE] [ALGORITHM = {UNDEFINED | MERGE | TEMPTABLE}] VIEW 视图名称 [(字段列表)] AS 查询语句 [WITH [CASCADED|LOCAL] CHECK OPTION] 精简版 CREATE VIEW 视图名称 AS 查询语句 ","date":"2022-08-24","objectID":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/:67:0","tags":[],"title":"MySQL基础篇","uri":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/"},{"categories":["mysql"],"content":"1) 创建单表视图 举例： # 方式一： CREATE VIEW empvu80 AS SELECT employee_id, last_name, salary FROM employees WHERE department_id = 80; # 方式二： CREATE VIEW empsalary8000(emp_id, NAME, monthly_sal) # 小括号内字段个数与SELECT中字段个数相同 AS SELECT employee_id, last_name, salary FROM employees WHERE salary \u003e 8000; 查询视图： SELECT * FROM salvu80; ","date":"2022-08-24","objectID":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/:67:1","tags":[],"title":"MySQL基础篇","uri":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/"},{"categories":["mysql"],"content":"2) 创建多表联合视图 举例： CREATE VIEW empview AS SELECT employee_id emp_id,last_name NAME,department_name FROM employees e,departments d WHERE e.department_id = d.department_id; CREATE VIEW dept_sum_vu (name, minsal, maxsal, avgsal) AS SELECT d.department_name, MIN(e.salary), MAX(e.salary),AVG(e.salary) FROM employees e, departments d WHERE e.department_id = d.department_id GROUP BY d.department_name; 利用视图对数据进行格式化 常需要输出某个格式的内容，比如我们想输出员工姓名和对应的部门名，对应格式为 emp_name(department_name)，就可以使用视图来完成数据格式化的操作： CREATE VIEW emp_depart AS SELECT CONCAT(last_name,'(',department_name,')') AS emp_dept FROM employees e JOIN departments d WHERE e.department_id = d.department_id; ","date":"2022-08-24","objectID":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/:67:2","tags":[],"title":"MySQL基础篇","uri":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/"},{"categories":["mysql"],"content":"3) 基于视图创建视图 当我们创建好一张视图之后，还可以在它的基础上继续创建视图。 举例：联合“emp_dept”视图和“emp_year_salary”视图查询员工姓名、部门名称、年薪信息创建 “emp_dept_ysalary”视图。 CREATE VIEW emp_dept_ysalary AS SELECT emp_dept.ename,dname,year_salary FROM emp_dept INNER JOIN emp_year_salary ON emp_dept.ename = emp_year_salary.ename; ","date":"2022-08-24","objectID":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/:67:3","tags":[],"title":"MySQL基础篇","uri":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/"},{"categories":["mysql"],"content":"4. 查看视图 语法1：查看数据库的表对象、视图对象 SHOW TABLES; 语法2：查看视图的结构 DESC / DESCRIBE 视图名称; 语法3：查看视图的属性信息 # 查看视图信息（显示数据表的存储引擎、版本、数据行数和数据大小等） SHOW TABLE STATUS LIKE '视图名称'\\G 执行结果显示，注释Comment为VIEW，说明该表为视图，其他的信息为NULL，说明这是一个虚表。 语法4：查看视图的详细定义信息 SHOW CREATE VIEW 视图名称; ","date":"2022-08-24","objectID":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/:68:0","tags":[],"title":"MySQL基础篇","uri":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/"},{"categories":["mysql"],"content":"5. 更新视图的数据 ","date":"2022-08-24","objectID":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/:69:0","tags":[],"title":"MySQL基础篇","uri":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/"},{"categories":["mysql"],"content":"1) 一般情况 MySQL支持使用INSERT、UPDATE和DELETE语句对视图中的数据进行插入、更新和删除操作。当视图中的 数据发生变化时，数据表中的数据也会发生变化，反之亦然。 举例：UPDATE操作 UPDATE emp_tel SET tel = '13789091234' WHERE ename = '孙洪亮'; 举例：DELETE操作 DELETE FROM emp_tel WHERE ename = '孙洪亮'; ","date":"2022-08-24","objectID":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/:69:1","tags":[],"title":"MySQL基础篇","uri":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/"},{"categories":["mysql"],"content":"2) 不可更新的视图 要使视图可更新，视图中的行和底层基本表中的行之间必须存在 一对一 的关系。另外当视图定义出现如下情况时，视图不支持更新操作： 在定义视图的时候指定了“ALGORITHM = TEMPTABLE”，视图将不支持INSERT和DELETE操作； 视图中不包含基表中所有被定义为非空又未指定默认值的列，视图将不支持INSERT操作； 在定义视图的SELECT语句中使用了 JOIN联合查询 ，视图将不支持INSERT和DELETE操作； 在定义视图的SELECT语句后的字段列表中使用了 数学表达式 或 子查询 ，视图将不支持INSERT，也 不支持UPDATE使用了数学表达式、子查询的字段值； 在定义视图的SELECT语句后的字段列表中使用 DISTINCT 、 聚合函数 、 GROUP BY 、 HAVING 、 UNION 等，视图将不支持INSERT、UPDATE、DELETE； 在定义视图的SELECT语句中包含了子查询，而子查询中引用了FROM后面的表，视图将不支持 INSERT、UPDATE、DELETE； 视图定义基于一个 不可更新视图 ； 常量视图。 虽然可以更新视图数据，但总的来说，视图作为虚拟表 ，主要用于方便查询 ，不建议更新视图的数据。对视图数据的更改，都是通过对实际数据表里数据的操作来完成的。 ","date":"2022-08-24","objectID":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/:69:2","tags":[],"title":"MySQL基础篇","uri":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/"},{"categories":["mysql"],"content":"6. 修改、删除视图 ","date":"2022-08-24","objectID":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/:70:0","tags":[],"title":"MySQL基础篇","uri":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/"},{"categories":["mysql"],"content":"1) 修改视图 方式1：使用CREATE OR REPLACE VIEW 子句修改视图 CREATE OR REPLACE VIEW empvu80 (id_number, name, sal, department_id) AS SELECT employee_id, first_name || ' ' || last_name, salary, department_id FROM employees WHERE department_id = 80; 说明：CREATE VIEW 子句中各列的别名应和子查询中各列相对应。 方式2：ALTER VIEW 修改视图的语法是： ALTER VIEW 视图名称 AS 查询语句 ","date":"2022-08-24","objectID":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/:70:1","tags":[],"title":"MySQL基础篇","uri":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/"},{"categories":["mysql"],"content":"2) 删除视图 删除视图只是删除视图的定义，并不会删除基表的数据。 删除视图的语法是： DROP VIEW IF EXISTS 视图名称; 举例： DROP VIEW empvu80; 说明：基于视图a、b创建了新的视图c，如果将视图a或者视图b删除，会导致视图c的查询失败。这 样的视图c需要手动删除或修改，否则影响使用。 ","date":"2022-08-24","objectID":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/:70:2","tags":[],"title":"MySQL基础篇","uri":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/"},{"categories":["mysql"],"content":"7. 总结 ","date":"2022-08-24","objectID":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/:71:0","tags":[],"title":"MySQL基础篇","uri":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/"},{"categories":["mysql"],"content":"1) 优点 1. 操作简单 将经常使用的查询操作定义为视图，可以使开发人员不需要关心视图对应的数据表的结构、表与表之间的关联关系，也不需要关心数据表之间的业务逻辑和查询条件，而只需要简单地操作视图即可，极大简化了开发人员对数据库的操作。 2. 减少数据冗余 视图跟实际数据表不一样，它存储的是查询语句。所以，在使用的时候，我们要通过定义视图的查询语句来获取结果集。而视图本身不存储数据，不占用数据存储的资源，减少了数据冗余。 3. 数据安全 MySQL将用户对数据的访问限制在某些数据的结果集上，而这些数据的结果集可以使用视图来实现。用户不必直接查询或操作数据表。这也可以理解为视图具有隔离性 。视图相当于在用户和实际的数据表之间加了一层虚拟表。 同时，MySQL可以根据权限将用户对数据的访问限制在某些视图上，用户不需要查询数据表，可以直接通过视图获取数据表中的信息。这在一定程度上保障了数据表中数据的安全性。 4. 适应灵活多变的需求 当业务系统的需求发生变化后，如果需要改动数据表的结构，则工作量相对较大，可以使用视图来减少改动的工作量。这种方式在实际工作中使用得比较多。 5. 能够分解复杂的查询逻辑 数据库中如果存在复杂的查询逻辑，则可以将问题进行分解，创建多个视图获取数据，再将创建的多个视图结合起来，完成复杂的查询逻辑。 ","date":"2022-08-24","objectID":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/:71:1","tags":[],"title":"MySQL基础篇","uri":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/"},{"categories":["mysql"],"content":"2) 不足 如果我们在实际数据表的基础上创建了视图，那么，如果实际数据表的结构变更了，我们就需要及时对相关的视图进行相应的维护。特别是嵌套的视图（就是在视图的基础上创建视图），维护会变得比较复杂，可读性不好，容易变成系统的潜在隐患。因为创建视图的 SQL 查询可能会对字段重命名，也可能包含复杂的逻辑，这些都会增加维护的成本。 实际项目中，如果视图过多，会导致数据库维护成本的问题。 所以，在创建视图的时候，你要结合实际项目需求，综合考虑视图的优点和不足，这样才能正确使用视图，使系统整体达到最优。 第15章_存储过程与函数(×) MySQL从5.0版本开始支持存储过程和函数。存储过程和函数能够将复杂的SQL逻辑封装在一起，应用程序无须关注存储过程和函数内部复杂的SQL逻辑，而只需要简单地调用存储过程和函数即可。 ","date":"2022-08-24","objectID":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/:71:2","tags":[],"title":"MySQL基础篇","uri":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/"},{"categories":["mysql"],"content":"1. 存储过程概述 ","date":"2022-08-24","objectID":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/:72:0","tags":[],"title":"MySQL基础篇","uri":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/"},{"categories":["mysql"],"content":"1) 理解 **含义：**存储过程的英文是 Stored Procedure 。它的思想很简单，就是一组经过 预先编译的 SQL 语句 的封装。 执行过程：存储过程预先存储在 MySQL 服务器上，需要执行的时候，客户端只需要向服务器端发出调用存储过程的命令，服务器端就可以把预先存储好的这一系列 SQL 语句全部执行。 好处： 1、简化操作，提高了sql语句的重用性，减少了开发程序员的压力。 2、减少操作过程中的失误，提高效率。 3、减少网络传输量（客户端不需要把所有的 SQL 语句通过网络发给服务器）。 4、减少了 SQL 语句暴露在网上的风险，也提高了数据查询的安全性。 和视图、函数的对比： 它和视图有着同样的优点，清晰、安全，还可以减少网络传输量。不过它和视图不同，视图是虚拟表，通常不对底层数据表直接操作，而存储过程是程序化的 SQL，可以 直接操作底层数据表 ，相比于面向集合的操作方式，能够实现一些更复杂的数据处理。 一旦存储过程被创建出来，使用它就像使用函数一样简单，我们直接通过调用存储过程名即可。相较于函数，存储过程是 没有返回值 的。 ","date":"2022-08-24","objectID":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/:72:1","tags":[],"title":"MySQL基础篇","uri":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/"},{"categories":["mysql"],"content":"2) 分类 存储过程的参数类型可以是IN、OUT和INOUT。根据这点分类如下： 1、没有参数（无参数无返回） 2、仅仅带 IN 类型（有参数无返回） 3、仅仅带 OUT 类型（无参数有返回） 4、既带 IN 又带 OUT（有参数有返回） 5、带 INOUT（有参数有返回） 注意：IN、OUT、INOUT 都可以在一个存储过程中带多个。 ","date":"2022-08-24","objectID":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/:72:2","tags":[],"title":"MySQL基础篇","uri":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/"},{"categories":["mysql"],"content":"2. 创建存储过程 ","date":"2022-08-24","objectID":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/:73:0","tags":[],"title":"MySQL基础篇","uri":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/"},{"categories":["mysql"],"content":"1) 语法分析 语法： CREATE PROCEDURE 存储过程名(IN|OUT|INOUT 参数名 参数类型,...) [characteristics ...] BEGIN 存储过程体 END 说明： 1、参数前面的符号的意思 IN ：当前参数为输入参数，也就是表示入参； 存储过程只是读取这个参数的值。如果没有定义参数种类， 默认就是 IN ，表示输入参数。 OUT ：当前参数为输出参数，也就是表示出参； 执行完成之后，调用这个存储过程的客户端或者应用程序就可以读取这个参数返回的值了。 INOUT ：当前参数既可以为输入参数，也可以为输出参数。 2、形参类型可以是 MySQL数据库中的任意类型。 3、characteristics 表示创建存储过程时指定的对存储过程的约束条件，其取值信息如下： LANGUAGE SQL | [NOT] DETERMINISTIC | { CONTAINS SQL | NO SQL | READS SQL DATA | MODIFIES SQL DATA } | SQL SECURITY { DEFINER | INVOKER } | COMMENT 'string' LANGUAGE SQL ：说明存储过程执行体是由SQL语句组成的，当前系统支持的语言为SQL。 [NOT] DETERMINISTIC ：指明存储过程执行的结果是否确定。DETERMINISTIC表示结果是确定 的。每次执行存储过程时，相同的输入会得到相同的输出。NOT DETERMINISTIC表示结果是不确定 的，相同的输入可能得到不同的输出。如果没有指定任意一个值，默认为NOT DETERMINISTIC。 { CONTAINS SQL | NO SQL | READS SQL DATA | MODIFIES SQL DATA } ：指明子程序使 用SQL语句的限制。 CONTAINS SQL表示当前存储过程的子程序包含SQL语句，但是并不包含读写数据的SQL语句； NO SQL表示当前存储过程的子程序中不包含任何SQL语句； READS SQL DATA表示当前存储过程的子程序中包含读数据的SQL语句； MODIFIES SQL DATA表示当前存储过程的子程序中包含写数据的SQL语句。 默认情况下，系统会指定为CONTAINS SQL。 SQL SECURITY { DEFINER | INVOKER } ：执行当前存储过程的权限，即指明哪些用户能够执行当前存储过程。 DEFINER 表示只有当前存储过程的创建者或者定义者才能执行当前存储过程； INVOKER 表示拥有当前存储过程的访问权限的用户能够执行当前存储过程。 COMMENT ‘string’ ：注释信息，可以用来描述存储过程。 4、存储过程体中可以有多条 SQL 语句，如果仅仅一条SQL 语句，则可以省略 BEGIN 和 END 1. BEGIN…END：BEGIN…END 中间包含了多个语句，每个语句都以（;）号为结束符。 2. DECLARE：DECLARE 用来声明变量，使用的位置在于 BEGIN…END 语句中间，而且需要在其他语句使用之前进 行变量的声明。 3. SET：赋值语句，用于对变量进行赋值。 4. SELECT… INTO：把从数据表中查询的结果存放到变量中，也就是为变量赋值。 5、需要设置新的结束标记 DELIMITER 新的结束标记 因为MySQL默认的语句结束符号为分号‘;’。为了避免与存储过程中SQL语句结束符相冲突，需要使用 DELIMITER改变存储过程的结束符。 比如：“DELIMITER //”语句的作用是将MySQL的结束符设置为//，并以“END //”结束存储过程。存储过程定 义完毕之后再使用“DELIMITER ;”恢复默认结束符。DELIMITER也可以指定其他符号作为结束符。 当使用DELIMITER命令时，应该避免使用反斜杠（‘\\’）字符，因为反斜线是MySQL的转义字符。 示例： DELIMITER $ CREATE PROCEDURE 存储过程名(IN|OUT|INOUT 参数名 参数类型,...) [characteristics ...] BEGIN sql语句1; sql语句2; END $ ","date":"2022-08-24","objectID":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/:73:1","tags":[],"title":"MySQL基础篇","uri":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/"},{"categories":["mysql"],"content":"2) 代码举例 举例1：创建存储过程select_all_data()，查看 emps 表的所有数据 DELIMITER $ CREATE PROCEDURE select_all_data() BEGIN SELECT * FROM emps; END $ DELIMITER ; 举例2：创建存储过程avg_employee_salary()，返回所有员工的平均工资 DELIMITER // CREATE PROCEDURE avg_employee_salary () BEGIN SELECT AVG(salary) AS avg_salary FROM emps; END // DELIMITER ; ","date":"2022-08-24","objectID":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/:73:2","tags":[],"title":"MySQL基础篇","uri":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/"},{"categories":["mysql"],"content":"3. 调用存储过程 ","date":"2022-08-24","objectID":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/:74:0","tags":[],"title":"MySQL基础篇","uri":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/"},{"categories":["mysql"],"content":"1) 调用格式 存储过程有多种调用方法。存储过程必须使用CALL语句调用，并且存储过程和数据库相关，如果要执行其他数据库中的存储过程，需要指定数据库名称，例如CALL dbname.procname。 CALL 存储过程名(实参列表) 格式： 1、调用in模式的参数： CALL sp1('值'); 2、调用out模式的参数： SET @name; CALL sp1(@name); SELECT @name; 3、调用inout模式的参数： SET @name=值; CALL sp1(@name); SELECT @name; ","date":"2022-08-24","objectID":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/:74:1","tags":[],"title":"MySQL基础篇","uri":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/"},{"categories":["mysql"],"content":"2) 代码举例 举例1： DELIMITER // CREATE PROCEDURE CountProc(IN sid INT,OUT num INT) BEGIN SELECT COUNT(*) INTO num FROM fruits WHERE s_id = sid; END // DELIMITER ; 调用存储过程： CALL CountProc (101, @num); 查看返回结果： SELECT @num; **举例2：**创建存储过程，实现累加运算，计算 1+2+…+n 等于多少。具体的代码如下： DELIMITER // CREATE PROCEDURE `add_num`(IN n INT) BEGIN DECLARE i INT; DECLARE sum INT; SET i = 1; SET sum = 0; WHILE i \u003c= n DO SET sum = sum + i; SET i = i +1; END WHILE; SELECT sum; END // DELIMITER ; 直接使用 CALL add_num(50); 即可。这里我传入的参数为 50，也就是统计 1+2+…+50 的积累之和。 ","date":"2022-08-24","objectID":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/:74:2","tags":[],"title":"MySQL基础篇","uri":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/"},{"categories":["mysql"],"content":"3) 如何调试 在 MySQL 中，存储过程不像普通的编程语言（比如 VC++、Java 等）那样有专门的集成开发环境。因此，你可以通过 SELECT 语句，把程序执行的中间结果查询出来，来调试一个 SQL 语句的正确性。调试成功之后，把 SELECT 语句后移到下一个 SQL 语句之后，再调试下一个 SQL 语句。这样 逐步推进 ，就可以完成对存储过程中所有操作的调试了。当然，你也可以把存储过程中的 SQL 语句复制出来，逐段单独调试。 ","date":"2022-08-24","objectID":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/:74:3","tags":[],"title":"MySQL基础篇","uri":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/"},{"categories":["mysql"],"content":"4. 存储函数的使用 ","date":"2022-08-24","objectID":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/:75:0","tags":[],"title":"MySQL基础篇","uri":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/"},{"categories":["mysql"],"content":"1) 语法分析 学过的函数：LENGTH、SUBSTR、CONCAT等 语法格式： CREATE FUNCTION 函数名(参数名 参数类型,...) RETURNS 返回值类型 [characteristics ...] BEGIN 函数体 #函数体中肯定有 RETURN 语句 END 说明： 1、参数列表：指定参数为IN、OUT或INOUT只对PROCEDURE是合法的，FUNCTION中总是默认为IN参数。 2、RETURNS type 语句表示函数返回数据的类型； RETURNS子句只能对FUNCTION做指定，对函数而言这是 强制 的。它用来指定函数的返回类型，而且函 数体必须包含一个 RETURN value 语句。 3、characteristic 创建函数时指定的对函数的约束。取值与创建存储过程时相同，这里不再赘述。 4、函数体也可以用BEGIN…END来表示SQL代码的开始和结束。如果函数体只有一条语句，也可以省略 BEGIN…END。 ","date":"2022-08-24","objectID":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/:75:1","tags":[],"title":"MySQL基础篇","uri":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/"},{"categories":["mysql"],"content":"2) 调用存储函数 在MySQL中，存储函数的使用方法与MySQL内部函数的使用方法是一样的。换言之，用户自己定义的存储函数与MySQL内部函数是一个性质的。区别在于，存储函数是 用户自己定义 的，而内部函数是MySQL 的 开发者定义 的。 SELECT 函数名(实参列表) ","date":"2022-08-24","objectID":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/:75:2","tags":[],"title":"MySQL基础篇","uri":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/"},{"categories":["mysql"],"content":"3) 代码举例 举例1： 创建存储函数，名称为email_by_name()，参数定义为空，该函数查询Abel的email，并返回，数据类型为字符串型。 DELIMITER // CREATE FUNCTION email_by_name() RETURNS VARCHAR(25) DETERMINISTIC CONTAINS SQL BEGIN RETURN (SELECT email FROM employees WHERE last_name = 'Abel'); END // DELIMITER ; 调用： SELECT email_by_name(); 举例2： 创建存储函数，名称为email_by_id()，参数传入emp_id，该函数查询emp_id的email，并返回，数据类型 为字符串型。 DELIMITER // CREATE FUNCTION email_by_id(emp_id INT) RETURNS VARCHAR(25) DETERMINISTIC CONTAINS SQL BEGIN RETURN (SELECT email FROM employees WHERE employee_id = emp_id); END // DELIMITER ; 调用： SET @emp_id = 102; SELECT email_by_id(@emp_id); 注意： 若在创建存储函数中报错“ you might want to use the less safe log_bin_trust_function_creators variable ”，有两种处理方法： 方式1： 加上必要的函数特性“[NOT] DETERMINISTIC”和“{CONTAINS SQL | NO SQL | READS SQL DATA | MODIFIES SQL DATA}” 方式2： SET GLOBAL log_bin_trust_function_creators = 1; ","date":"2022-08-24","objectID":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/:75:3","tags":[],"title":"MySQL基础篇","uri":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/"},{"categories":["mysql"],"content":"4) 对比存储函数与存储过程 关键字 调用语法 返回值 应用场景 存储过程 PROCEDURE CALL 存储过程() 理解为有0个或多个 一般用于更新 存储函数 FUNCTION SELECT 函数 () 只能是一个 一般用于查询结果为一个值并返回时 此外，存储函数可以放在查询语句中使用，存储过程不行。反之，存储过程的功能更加强大，包括能够 执行对表的操作（比如创建表，删除表等）和事务操作，这些功能是存储函数不具备的。 ","date":"2022-08-24","objectID":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/:75:4","tags":[],"title":"MySQL基础篇","uri":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/"},{"categories":["mysql"],"content":"5. 存储过程和函数的查看、修改、删除 ","date":"2022-08-24","objectID":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/:76:0","tags":[],"title":"MySQL基础篇","uri":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/"},{"categories":["mysql"],"content":"1) 查看 创建完之后，怎么知道我们创建的存储过程、存储函数是否成功了呢？ MySQL存储了存储过程和函数的状态信息，用户可以使用SHOW STATUS语句或SHOW CREATE语句来查 看，也可直接从系统的information_schema数据库中查询。这里介绍3种方法。 使用SHOW CREATE语句查看存储过程和函数的创建信息 SHOW CREATE {PROCEDURE | FUNCTION} 存储过程名或函数名 使用SHOW STATUS语句查看存储过程和函数的状态信息 SHOW {PROCEDURE | FUNCTION} STATUS [LIKE 'pattern'] 从information_schema.Routines表中查看存储过程和函数的信息 MySQL中存储过程和函数的信息存储在information_schema数据库下的Routines表中。可以通过查询该表的记录来查询存储过程和函数的信息。其基本语法形式如下： SELECT * FROM information_schema.Routines WHERE ROUTINE_NAME='存储过程或函数的名' [AND ROUTINE_TYPE = {'PROCEDURE|FUNCTION'}]; 说明：如果在MySQL数据库中存在存储过程和函数名称相同的情况，最好指定ROUTINE_TYPE查询条件来 指明查询的是存储过程还是函数。 ","date":"2022-08-24","objectID":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/:76:1","tags":[],"title":"MySQL基础篇","uri":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/"},{"categories":["mysql"],"content":"2) 修改 修改存储过程或函数，不影响存储过程或函数功能，只是修改相关特性。使用ALTER语句实现。 ALTER {PROCEDURE | FUNCTION} 存储过程或函数的名 [characteristic ...] 其中，characteristic指定存储过程或函数的特性，其取值信息与创建存储过程、函数时的取值信息略有不同。 { CONTAINS SQL | NO SQL | READS SQL DATA | MODIFIES SQL DATA } | SQL SECURITY { DEFINER | INVOKER } | COMMENT 'string' CONTAINS SQL ，表示子程序包含SQL语句，但不包含读或写数据的语句。 NO SQL ，表示子程序中不包含SQL语句。 READS SQL DATA ，表示子程序中包含读数据的语句。 MODIFIES SQL DATA ，表示子程序中包含写数据的语句。 SQL SECURITY { DEFINER | INVOKER } ，指明谁有权限来执行。 DEFINER ，表示只有定义者自己才能够执行。 INVOKER ，表示调用者可以执行。 COMMENT ‘string’ ，表示注释信息。 修改存储过程使用ALTER PROCEDURE语句，修改存储函数使用ALTER FUNCTION语句。但是，这两 个语句的结构是一样的，语句中的所有参数也是一样的。 ","date":"2022-08-24","objectID":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/:76:2","tags":[],"title":"MySQL基础篇","uri":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/"},{"categories":["mysql"],"content":"3) 删除 删除存储过程和函数，可以使用DROP语句，其语法结构如下： DROP {PROCEDURE | FUNCTION} [IF EXISTS] 存储过程或函数的名 ","date":"2022-08-24","objectID":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/:76:3","tags":[],"title":"MySQL基础篇","uri":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/"},{"categories":["mysql"],"content":"6. 关于存储过程使用的争议 ","date":"2022-08-24","objectID":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/:77:0","tags":[],"title":"MySQL基础篇","uri":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/"},{"categories":["mysql"],"content":"1) 优点 1、存储过程可以一次编译多次使用。存储过程只在创建时进行编译，之后的使用都不需要重新编译， 这就提升了 SQL 的执行效率。 2、可以减少开发工作量。将代码 封装 成模块，实际上是编程的核心思想之一，这样可以把复杂的问题拆解成不同的模块，然后模块之间可以重复使用 ，在减少开发工作量的同时，还能保证代码的结构清晰。 3、存储过程的安全性强。我们在设定存储过程的时候可以 设置对用户的使用权限 ，这样就和视图一样具有较强的安全性。 4、可以减少网络传输量。因为代码封装到存储过程中，每次使用只需要调用存储过程即可，这样就减少了网络传输量。 5、良好的封装性。在进行相对复杂的数据库操作时，原本需要使用一条一条的 SQL 语句，可能要连接多次数据库才能完成的操作，现在变成了一次存储过程，只需要连接一次即可 。 ","date":"2022-08-24","objectID":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/:77:1","tags":[],"title":"MySQL基础篇","uri":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/"},{"categories":["mysql"],"content":"2) 缺点 阿里开发规范 【强制】禁止使用存储过程，存储过程难以调试和扩展，更没有移植性。 1、可移植性差。存储过程不能跨数据库移植，比如在 MySQL、Oracle 和 SQL Server 里编写的存储过程，在换成其他数据库时都需要重新编写。 2、调试困难。只有少数 DBMS 支持存储过程的调试。对于复杂的存储过程来说，开发和维护都不容易。虽然也有一些第三方工具可以对存储过程进行调试，但要收费。 3、存储过程的版本管理很困难。比如数据表索引发生变化了，可能会导致存储过程失效。我们在开发软件的时候往往需要进行版本管理，但是存储过程本身没有版本控制，版本迭代更新的时候很麻烦。 4、它不适合高并发的场景。高并发的场景需要减少数据库的压力，有时数据库会采用分库分表的方式，而且对可扩展性要求很高，在这种情况下，存储过程会变得难以维护， 增加数据库的压力 ，显然就不适用了。 ","date":"2022-08-24","objectID":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/:77:2","tags":[],"title":"MySQL基础篇","uri":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/"},{"categories":["mysql"],"content":"3) 小结 存储过程既方便，又有局限性。尽管不同的公司对存储过程的态度不一，但是对于我们开发人员来说，不论怎样，掌握存储过程都是必备的技能之一。 第16章_变量、流程控制与游标 在MySQL数据库的存储过程和函数中，可以使用变量来存储查询或计算的中间结果数据，或者输出最终的结果数据。 ","date":"2022-08-24","objectID":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/:77:3","tags":[],"title":"MySQL基础篇","uri":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/"},{"categories":["mysql"],"content":"1. 变量 在MySQL数据库的存储过程和函数中，可以使用变量来存储查询或计算的中间结果数据，或者输出最终 的结果数据。 在 MySQL 数据库中，变量分为 系统变量 以及 用户自定义变量 。 ","date":"2022-08-24","objectID":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/:78:0","tags":[],"title":"MySQL基础篇","uri":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/"},{"categories":["mysql"],"content":"1) 系统变量 系统变量分类 变量由系统定义，不是用户定义，属于 服务器 层面。启动MySQL服务，生成MySQL服务实例期间， MySQL将为MySQL服务器内存中的系统变量赋值，这些系统变量定义了当前MySQL服务实例的属性、特 征。这些系统变量的值要么是 编译MySQL时参数 的默认值，要么是 配置文件 （例如my.ini等）中的参数 值。大家可以通过网址 https://dev.mysql.com/doc/refman/8.0/en/server-systemvariables.html 查看MySQL文档的系统变量。 系统变量分为全局系统变量（需要添加 global 关键字）以及会话系统变量（需要添加 session 关键字），有时也把全局系统变量简称为全局变量，有时也把会话系统变量称为local变量。如果不写，默认会话级别。静态变量（在 MySQL 服务实例运行期间它们的值不能使用 set 动态修改）属于特殊的全局系统变量。 每一个MySQL客户机成功连接MySQL服务器后，都会产生与之对应的会话。会话期间，MySQL服务实例会在MySQL服务器内存中生成与该会话对应的会话系统变量，这些会话系统变量的初始值是全局系统变量值的复制。如下图： 全局系统变量针对于所有会话（连接）有效，但 不能跨重启 会话系统变量仅针对于当前会话（连接）有效。会话期间，当前会话对某个会话系统变量值的修改，不会影响其他会话同一个会话系统变量的值。 会话1对某个全局系统变量值的修改会导致会话2中同一个全局系统变量值的修改。 在MySQL中有些系统变量只能是全局的，例如 max_connections 用于限制服务器的最大连接数；有些系 统变量作用域既可以是全局又可以是会话，例如 character_set_client 用于设置客户端的字符集；有些系 统变量的作用域只能是当前会话，例如 pseudo_thread_id 用于标记当前会话的 MySQL 连接 ID。 查看系统变量 查看所有或部分系统变量 #查看所有全局变量 SHOW GLOBAL VARIABLES; #查看所有会话变量 SHOW SESSION VARIABLES; # 或 SHOW VARIABLES; #查看满足条件的部分系统变量。 SHOW GLOBAL VARIABLES LIKE '%标识符%'; #查看满足条件的部分会话变量 SHOW SESSION VARIABLES LIKE '%标识符%'; 查看指定系统变量 作为 MySQL 编码规范，MySQL 中的系统变量以 两个“@” 开头，其中“@@global”仅用于标记全局系统变量，“@@session”仅用于标记会话系统变量。“@@”首先标记会话系统变量，如果会话系统变量不存在， 则标记全局系统变量。 #查看指定的系统变量的值 SELECT @@global.变量名; #查看指定的会话变量的值 SELECT @@session.变量名; # 或 SELECT @@变量名; 修改系统变量的值 有些时候，数据库管理员需要修改系统变量的默认值，以便修改当前会话或者MySQL服务实例的属性、 特征。具体方法： 方式1：修改MySQL 配置文件 ，继而修改MySQL系统变量的值（该方法需要重启MySQL服务） 方式2：在MySQL服务运行期间，使用“set”命令重新设置系统变量的值 #为某个系统变量赋值 #方式1： SET @@global.变量名=变量值; #方式2： SET GLOBAL 变量名=变量值; #为某个会话变量赋值 #方式1： SET @@session.变量名=变量值; #方式2： SET SESSION 变量名=变量值; ","date":"2022-08-24","objectID":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/:78:1","tags":[],"title":"MySQL基础篇","uri":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/"},{"categories":["mysql"],"content":"2) 用户变量 用户变量分类 用户变量是用户自己定义的，作为 MySQL 编码规范，MySQL 中的用户变量以一个“@” 开头。根据作用范围不同，又分为 会话用户变量 和 局部变量 。 会话用户变量：作用域和会话变量一样，只对 当前连接 会话有效。 局部变量：只在 BEGIN 和 END 语句块中有效。局部变量只能在 存储过程和函数 中使用。 会话用户变量 变量的定义 #方式1：“=”或“:=” SET @用户变量 = 值; SET @用户变量 := 值; #方式2：“:=” 或 INTO关键字 SELECT @用户变量 := 表达式 [FROM 等子句]; SELECT 表达式 INTO @用户变量 [FROM 等子句]; 查看用户变量的值 (查看、比较、运算等) SELECT @用户变量 局部变量 定义：可以使用 DECLARE 语句定义一个局部变量 作用域：仅仅在定义它的 BEGIN … END 中有效 位置：只能放在 BEGIN … END 中，而且只能放在第一句 BEGIN #声明局部变量 DECLARE 变量名1 变量数据类型 [DEFAULT 变量默认值]; DECLARE 变量名2,变量名3,... 变量数据类型 [DEFAULT 变量默认值]; #为局部变量赋值 SET 变量名1 = 值; SELECT 值 INTO 变量名2 [FROM 子句]; #查看局部变量的值 SELECT 变量1,变量2,变量3; END 定义变量 DECLARE 变量名 类型 [default 值]; # 如果没有DEFAULT子句，初始值为NULL 变量赋值 方式1：一般用于赋简单的值 SET 变量名=值; SET 变量名:=值; 方式2：一般用于赋表中的字段值 SELECT 字段名或表达式 INTO 变量名 FROM 表; 使用变量 (查看、比较、运算等) SELECT 局部变量名; 举例1：声明局部变量，并分别赋值为employees表中employee_id为102的last_name和salary DELIMITER // CREATE PROCEDURE set_value() BEGIN DECLARE emp_name VARCHAR(25); DECLARE sal DOUBLE(10,2); SELECT last_name, salary INTO emp_name,sal FROM employees WHERE employee_id = 102; SELECT emp_name, sal; END // DELIMITER ; 举例2：声明两个变量，求和并打印 （分别使用会话用户变量、局部变量的方式实现） #方式1：使用用户变量 SET @m=1; SET @n=1; SET @sum=@m+@n; SELECT @sum; #方式2：使用局部变量 DELIMITER // CREATE PROCEDURE add_value() BEGIN #局部变量 DECLARE m INT DEFAULT 1; DECLARE n INT DEFAULT 3; DECLARE SUM INT; SET SUM = m+n; SELECT SUM; END // DELIMITER ; 对比会话用户变量与局部变量 作用域 定义位置 语法 会话用户变量 当前会话 会话的任何地方 加@符号，不用指定类型 局部变量 定义它的BEGIN END中 BEGIN END的第一句话 一般不用加@,需要指定类型 ","date":"2022-08-24","objectID":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/:78:2","tags":[],"title":"MySQL基础篇","uri":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/"},{"categories":["mysql"],"content":"2. 定义条件与处理程序 定义条件 是事先定义程序执行过程中可能遇到的问题， 处理程序 定义了在遇到问题时应当采取的处理方式，并且保证存储过程或函数在遇到警告或错误时能继续执行。这样可以增强存储程序处理问题的能力，避免程序异常停止运行。 说明：定义条件和处理程序在存储过程、存储函数中都是支持的。 ","date":"2022-08-24","objectID":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/:79:0","tags":[],"title":"MySQL基础篇","uri":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/"},{"categories":["mysql"],"content":"1) 案例分析 案例分析：创建一个名称为“UpdateDataNoCondition”的存储过程。代码如下： DELIMITER // CREATE PROCEDURE UpdateDataNoCondition() BEGIN SET @x = 1; UPDATE employees SET email = NULL WHERE last_name = 'Abel'; SET @x = 2; UPDATE employees SET email = 'aabbel' WHERE last_name = 'Abel'; SET @x = 3; END // DELIMITER ; 调用存储过程： mysql\u003e CALL UpdateDataNoCondition(); ERROR 1048 (23000): Column 'email' cannot be null mysql\u003e SELECT @x; +------+ | @x | +------+ | 1 | +------+ 1 row in set (0.00 sec) 可以看到，此时@x变量的值为1。结合创建存储过程的SQL语句代码可以得出：在存储过程中未定义条件 和处理程序，且当存储过程中执行的SQL语句报错时，MySQL数据库会抛出错误，并退出当前SQL逻辑， 不再向下继续执行。 ","date":"2022-08-24","objectID":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/:79:1","tags":[],"title":"MySQL基础篇","uri":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/"},{"categories":["mysql"],"content":"2) 定义条件 定义条件就是给MySQL中的错误码命名，这有助于存储的程序代码更清晰。它将一个 错误名字 和 指定的 错误条件 关联起来。这个名字可以随后被用在定义处理程序的 DECLARE HANDLER 语句中。 定义条件使用DECLARE语句，语法格式如下： DECLARE 错误名称 CONDITION FOR 错误码（或错误条件） 错误码的说明： MySQL_error_code 和 sqlstate_value 都可以表示MySQL的错误。 MySQL_error_code是数值类型错误代码。 sqlstate_value是长度为5的字符串类型错误代码。 例如，在ERROR 1418 (HY000)中，1418是MySQL_error_code，‘HY000’是sqlstate_value。 例如，在ERROR 1142（42000）中，1142是MySQL_error_code，‘42000’是sqlstate_value。 举例1：定义“Field_Not_Be_NULL”错误名与MySQL中违反非空约束的错误类型是“ERROR 1048 (23000)”对应。 #使用MySQL_error_code DECLARE Field_Not_Be_NULL CONDITION FOR 1048; #使用sqlstate_value DECLARE Field_Not_Be_NULL CONDITION FOR SQLSTATE '23000'; ","date":"2022-08-24","objectID":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/:79:2","tags":[],"title":"MySQL基础篇","uri":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/"},{"categories":["mysql"],"content":"3) 定义处理程序 可以为SQL执行过程中发生的某种类型的错误定义特殊的处理程序。定义处理程序时，使用DECLARE语句 的语法如下： DECLARE 处理方式 HANDLER FOR 错误类型 处理语句 处理方式：处理方式有3个取值：CONTINUE、EXIT、UNDO。 CONTINUE ：表示遇到错误不处理，继续执行。 EXIT ：表示遇到错误马上退出。 UNDO ：表示遇到错误后撤回之前的操作。MySQL中暂时不支持这样的操作。 错误类型（即条件）可以有如下取值： SQLSTATE ‘字符串错误码’ ：表示长度为5的sqlstate_value类型的错误代码； MySQL_error_code ：匹配数值类型错误代码； 错误名称 ：表示DECLARE … CONDITION定义的错误条件名称。 SQLWARNING ：匹配所有以01开头的SQLSTATE错误代码； NOT FOUND ：匹配所有以02开头的SQLSTATE错误代码； SQLEXCEPTION ：匹配所有没有被SQLWARNING或NOT FOUND捕获的SQLSTATE错误代码； 处理语句：如果出现上述条件之一，则采用对应的处理方式，并执行指定的处理语句。语句可以是 像“ SET 变量 = 值 ”这样的简单语句，也可以是使用 BEGIN … END 编写的复合语句。 定义处理程序的几种方式，代码如下： #方法1：捕获sqlstate_value DECLARE CONTINUE HANDLER FOR SQLSTATE '42S02' SET @info = 'NO_SUCH_TABLE'; #方法2：捕获mysql_error_value DECLARE CONTINUE HANDLER FOR 1146 SET @info = 'NO_SUCH_TABLE'; #方法3：先定义条件，再调用 DECLARE no_such_table CONDITION FOR 1146; DECLARE CONTINUE HANDLER FOR NO_SUCH_TABLE SET @info = 'NO_SUCH_TABLE'; #方法4：使用SQLWARNING DECLARE EXIT HANDLER FOR SQLWARNING SET @info = 'ERROR'; #方法5：使用NOT FOUND DECLARE EXIT HANDLER FOR NOT FOUND SET @info = 'NO_SUCH_TABLE'; #方法6：使用SQLEXCEPTION DECLARE EXIT HANDLER FOR SQLEXCEPTION SET @info = 'ERROR'; ","date":"2022-08-24","objectID":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/:79:3","tags":[],"title":"MySQL基础篇","uri":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/"},{"categories":["mysql"],"content":"4) 案例解决 在存储过程中，定义处理程序，捕获sqlstate_value值，当遇到MySQL_error_code值为1048时，执行 CONTINUE操作，并且将@proc_value的值设置为-1。 DELIMITER // CREATE PROCEDURE UpdateDataNoCondition() BEGIN #定义处理程序 DECLARE CONTINUE HANDLER FOR 1048 SET @proc_value = -1; SET @x = 1; UPDATE employees SET email = NULL WHERE last_name = 'Abel'; SET @x = 2; UPDATE employees SET email = 'aabbel' WHERE last_name = 'Abel'; SET @x = 3; END // DELIMITER ; ","date":"2022-08-24","objectID":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/:79:4","tags":[],"title":"MySQL基础篇","uri":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/"},{"categories":["mysql"],"content":"3. 流程控制 解决复杂问题不可能通过一个 SQL 语句完成，我们需要执行多个 SQL 操作。流程控制语句的作用就是控 制存储过程中 SQL 语句的执行顺序，是我们完成复杂操作必不可少的一部分。只要是执行的程序，流程就分为三大类： 顺序结构 ：程序从上往下依次执行 分支结构 ：程序按条件进行选择执行，从两条或多条路径中选择一条执行 循环结构 ：程序满足一定条件下，重复执行一组语句 针对于MySQL 的流程控制语句主要有 3 类。注意：只能用于存储程序。 条件判断语句 ：IF 语句和 CASE 语句 循环语句 ：LOOP、WHILE 和 REPEAT 语句 跳转语句 ：ITERATE 和 LEAVE 语句 ","date":"2022-08-24","objectID":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/:80:0","tags":[],"title":"MySQL基础篇","uri":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/"},{"categories":["mysql"],"content":"1) 分支结构之 IF IF 语句的语法结构是： IF 表达式1 THEN 操作1 [ELSEIF 表达式2 THEN 操作2]…… [ELSE 操作N] END IF 根据表达式的结果为TRUE或FALSE执行相应的语句。这里“[]”中的内容是可选的。 特点：① 不同的表达式对应不同的操作 ② 使用在begin end中 举例1： IF val IS NULL THEN SELECT 'val is null'; ELSE SELECT 'val is not null'; END IF; 举例2：声明存储过程“update_salary_by_eid1”，定义IN参数emp_id，输入员工编号。判断该员工薪资如果低于8000元并且入职时间超过5年，就涨薪500元；否则就不变。 DELIMITER // CREATE PROCEDURE update_salary_by_eid1(IN emp_id INT) BEGIN DECLARE emp_salary DOUBLE; DECLARE hire_year DOUBLE; SELECT salary INTO emp_salary FROM employees WHERE employee_id = emp_id; SELECT DATEDIFF(CURDATE(),hire_date)/365 INTO hire_year FROM employees WHERE employee_id = emp_id; IF emp_salary \u003c 8000 AND hire_year \u003e 5 THEN UPDATE employees SET salary = salary + 500 WHERE employee_id = emp_id; END IF; END // DELIMITER ; ","date":"2022-08-24","objectID":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/:80:1","tags":[],"title":"MySQL基础篇","uri":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/"},{"categories":["mysql"],"content":"2) 分支结构之 CASE CASE 语句的语法结构1： #情况一：类似于switch CASE 表达式 WHEN 值1 THEN 结果1或语句1(如果是语句，需要加分号) WHEN 值2 THEN 结果2或语句2(如果是语句，需要加分号) ... ELSE 结果n或语句n(如果是语句，需要加分号) END [case]（如果是放在begin end中需要加上case，如果放在select后面不需要） CASE 语句的语法结构2： #情况二：类似于多重if CASE WHEN 条件1 THEN 结果1或语句1(如果是语句，需要加分号) WHEN 条件2 THEN 结果2或语句2(如果是语句，需要加分号) ... ELSE 结果n或语句n(如果是语句，需要加分号) END [case]（如果是放在begin end中需要加上case，如果放在select后面不需要） 举例1：使用CASE流程控制语句的第1种格式，判断val值等于1、等于2，或者两者都不等。 CASE val WHEN 1 THEN SELECT 'val is 1'; WHEN 2 THEN SELECT 'val is 2'; ELSE SELECT 'val is not 1 or 2'; END CASE; 举例2：声明存储过程“update_salary_by_eid4”，定义IN参数emp_id，输入员工编号。判断该员工 薪资如果低于9000元，就更新薪资为9000元；薪资大于等于9000元且低于10000的，但是奖金比例 为NULL的，就更新奖金比例为0.01；其他的涨薪100元。 DELIMITER // CREATE PROCEDURE update_salary_by_eid4(IN emp_id INT) BEGIN DECLARE emp_sal DOUBLE; DECLARE bonus DECIMAL(3,2); SELECT salary INTO emp_sal FROM employees WHERE employee_id = emp_id; SELECT commission_pct INTO bonus FROM employees WHERE employee_id = emp_id; CASE WHEN emp_sal\u003c9000 THEN UPDATE employees SET salary=9000 WHERE employee_id = emp_id; WHEN emp_sal\u003c10000 AND bonus IS NULL THEN UPDATE employees SET commission_pct=0.01 WHERE employee_id = emp_id; ELSE UPDATE employees SET salary=salary+100 WHERE employee_id = emp_id; END CASE; END // DELIMITER ; 举例3：声明存储过程update_salary_by_eid5，定义IN参数emp_id，输入员工编号。判断该员工的 入职年限，如果是0年，薪资涨50；如果是1年，薪资涨100；如果是2年，薪资涨200；如果是3年， 薪资涨300；如果是4年，薪资涨400；其他的涨薪500。 DELIMITER // CREATE PROCEDURE update_salary_by_eid5(IN emp_id INT) BEGIN DECLARE emp_sal DOUBLE; DECLARE hire_year DOUBLE; SELECT salary INTO emp_sal FROM employees WHERE employee_id = emp_id; SELECT ROUND(DATEDIFF(CURDATE(),hire_date)/365) INTO hire_year FROM employees WHERE employee_id = emp_id; CASE hire_year WHEN 0 THEN UPDATE employees SET salary=salary+50 WHERE employee_id = emp_id; WHEN 1 THEN UPDATE employees SET salary=salary+100 WHERE employee_id = emp_id; WHEN 2 THEN UPDATE employees SET salary=salary+200 WHERE employee_id = emp_id; WHEN 3 THEN UPDATE employees SET salary=salary+300 WHERE employee_id = emp_id; WHEN 4 THEN UPDATE employees SET salary=salary+400 WHERE employee_id = emp_id; ELSE UPDATE employees SET salary=salary+500 WHERE employee_id = emp_id; END CASE; END // DELIMITER ; ","date":"2022-08-24","objectID":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/:80:2","tags":[],"title":"MySQL基础篇","uri":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/"},{"categories":["mysql"],"content":"3) 循环结构之LOOP LOOP循环语句用来重复执行某些语句。LOOP内的语句一直重复执行直到循环被退出（使用LEAVE子 句），跳出循环过程。 LOOP语句的基本格式如下： [loop_label:] LOOP 循环执行的语句 END LOOP [loop_label] 其中，loop_label表示LOOP语句的标注名称，该参数可以省略。 举例1：使用LOOP语句进行循环操作，id值小于10时将重复执行循环过程。 DECLARE id INT DEFAULT 0; add_loop:LOOP SET id = id +1; IF id \u003e= 10 THEN LEAVE add_loop; END IF; END LOOP add_loop; 举例2：当市场环境变好时，公司为了奖励大家，决定给大家涨工资。声明存储过程 “update_salary_loop()”，声明OUT参数num，输出循环次数。存储过程中实现循环给大家涨薪，薪资涨为原来的1.1倍。直到全公司的平均薪资达到12000结束。并统计循环次数。 DELIMITER // CREATE PROCEDURE update_salary_loop(OUT num INT) BEGIN DECLARE avg_salary DOUBLE; DECLARE loop_count INT DEFAULT 0; SELECT AVG(salary) INTO avg_salary FROM employees; label_loop:LOOP IF avg_salary \u003e= 12000 THEN LEAVE label_loop; END IF; UPDATE employees SET salary = salary * 1.1; SET loop_count = loop_count + 1; SELECT AVG(salary) INTO avg_salary FROM employees; END LOOP label_loop; SET num = loop_count; END // DELIMITER ; ","date":"2022-08-24","objectID":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/:80:3","tags":[],"title":"MySQL基础篇","uri":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/"},{"categories":["mysql"],"content":"4) 循环结构之WHILE WHILE语句创建一个带条件判断的循环过程。WHILE在执行语句执行时，先对指定的表达式进行判断，如 果为真，就执行循环内的语句，否则退出循环。WHILE语句的基本格式如下： [while_label:] WHILE 循环条件 DO 循环体 END WHILE [while_label]; while_label为WHILE语句的标注名称；如果循环条件结果为真，WHILE语句内的语句或语句群被执行，直 至循环条件为假，退出循环。 举例1：WHILE语句示例，i值小于10时，将重复执行循环过程，代码如下： DELIMITER // CREATE PROCEDURE test_while() BEGIN DECLARE i INT DEFAULT 0; WHILE i \u003c 10 DO SET i = i + 1; END WHILE; SELECT i; END // DELIMITER ; #调用 CALL test_while(); 举例2：市场环境不好时，公司为了渡过难关，决定暂时降低大家的薪资。声明存储过程 “update_salary_while()”，声明OUT参数num，输出循环次数。存储过程中实现循环给大家降薪，薪资降 为原来的90%。直到全公司的平均薪资达到5000结束。并统计循环次数。 DELIMITER // CREATE PROCEDURE update_salary_while(OUT num INT) BEGIN DECLARE avg_sal DOUBLE ; DECLARE while_count INT DEFAULT 0; SELECT AVG(salary) INTO avg_sal FROM employees; WHILE avg_sal \u003e 5000 DO UPDATE employees SET salary = salary * 0.9; SET while_count = while_count + 1; SELECT AVG(salary) INTO avg_sal FROM employees; END WHILE; SET num = while_count; END // DELIMITER ; ","date":"2022-08-24","objectID":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/:80:4","tags":[],"title":"MySQL基础篇","uri":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/"},{"categories":["mysql"],"content":"5) 循环结构之REPEAT REPEAT语句创建一个带条件判断的循环过程。与WHILE循环不同的是，REPEAT 循环首先会执行一次循环，然后在 UNTIL 中进行表达式的判断，如果满足条件就退出，即 END REPEAT；如果条件不满足，则会就继续执行循环，直到满足退出条件为止。 REPEAT语句的基本格式如下： [repeat_label:] REPEAT 循环体的语句 UNTIL 结束循环的条件表达式 END REPEAT [repeat_label] repeat_label为REPEAT语句的标注名称，该参数可以省略；REPEAT语句内的语句或语句群被重复，直至 expr_condition为真。 举例1： DELIMITER // CREATE PROCEDURE test_repeat() BEGIN DECLARE i INT DEFAULT 0; REPEAT SET i = i + 1; UNTIL i \u003e= 10 END REPEAT; SELECT i; END // DELIMITER ; 举例2：当市场环境变好时，公司为了奖励大家，决定给大家涨工资。声明存储过程 “update_salary_repeat()”，声明OUT参数num，输出循环次数。存储过程中实现循环给大家涨薪，薪资涨 为原来的1.15倍。直到全公司的平均薪资达到13000结束。并统计循环次数。 DELIMITER // CREATE PROCEDURE update_salary_repeat(OUT num INT) BEGIN DECLARE avg_sal DOUBLE ; DECLARE repeat_count INT DEFAULT 0; SELECT AVG(salary) INTO avg_sal FROM employees; REPEAT UPDATE employees SET salary = salary * 1.15; SET repeat_count = repeat_count + 1; SELECT AVG(salary) INTO avg_sal FROM employees; UNTIL avg_sal \u003e= 13000 END REPEAT; SET num = repeat_count; END // DELIMITER ; 对比三种循环结构： 这三种循环都可以省略名称，但如果循环中添加了循环控制语句（LEAVE或ITERATE）则必须添加名称。 LOOP：一般用于实现简单的\"死\"循环 WHILE：先判断后执行 REPEAT：先执行后判断，无条件至少执行一次 ","date":"2022-08-24","objectID":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/:80:5","tags":[],"title":"MySQL基础篇","uri":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/"},{"categories":["mysql"],"content":"6) 跳转语句之LEAVE语句 LEAVE语句：可以用在循环语句内，或者以 BEGIN 和 END 包裹起来的程序体内，表示跳出循环或者跳出 程序体的操作。如果你有面向过程的编程语言的使用经验，你可以把 LEAVE 理解为 break。 基本格式如下： LEAVE 标记名 其中，label参数表示循环的标志。LEAVE和BEGIN … END或循环一起被使用。 举例1：创建存储过程 “leave_begin()”，声明INT类型的IN参数num。给BEGIN…END加标记名，并在 BEGIN…END中使用IF语句判断num参数的值。 如果num\u003c=0，则使用LEAVE语句退出BEGIN…END； 如果num=1，则查询“employees”表的平均薪资； 如果num=2，则查询“employees”表的最低薪资； 如果num\u003e2，则查询“employees”表的最高薪资。 IF语句结束后查询“employees”表的总人数。 DELIMITER // CREATE PROCEDURE leave_begin(IN num INT) begin_label: BEGIN IF num\u003c=0 THEN LEAVE begin_label; ELSEIF num=1 THEN SELECT AVG(salary) FROM employees; ELSEIF num=2 THEN SELECT MIN(salary) FROM employees; ELSE SELECT MAX(salary) FROM employees; END IF; SELECT COUNT(*) FROM employees; END // DELIMITER ; 举例2： 当市场环境不好时，公司为了渡过难关，决定暂时降低大家的薪资。声明存储过程“leave_while()”，声明 OUT参数num，输出循环次数，存储过程中使用WHILE循环给大家降低薪资为原来薪资的90%，直到全公司的平均薪资小于等于10000，并统计循环次数。 DELIMITER // CREATE PROCEDURE leave_while(OUT num INT) BEGIN DECLARE avg_sal DOUBLE;#记录平均工资 DECLARE while_count INT DEFAULT 0; #记录循环次数 SELECT AVG(salary) INTO avg_sal FROM employees; #① 初始化条件 while_label:WHILE TRUE DO #② 循环条件 #③ 循环体 IF avg_sal \u003c= 10000 THEN LEAVE while_label; END IF; UPDATE employees SET salary = salary * 0.9; SET while_count = while_count + 1; #④ 迭代条件 SELECT AVG(salary) INTO avg_sal FROM employees; END WHILE; #赋值 SET num = while_count; END // DELIMITER ; ","date":"2022-08-24","objectID":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/:80:6","tags":[],"title":"MySQL基础篇","uri":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/"},{"categories":["mysql"],"content":"7) 跳转语句之ITERATE语句 ITERATE语句：只能用在循环语句（LOOP、REPEAT和WHILE语句）内，表示重新开始循环，将执行顺序转到语句段开头处。如果你有面向过程的编程语言的使用经验，你可以把 ITERATE 理解为 continue，意思为“再次循环”。 语句基本格式如下： ITERATE label label参数表示循环的标志。ITERATE语句必须跟在循环标志前面。 举例： 定义局部变量num，初始值为0。循环结构中执行num + 1操作。 如果num \u003c 10，则继续执行循环； 如果num \u003e 15，则退出循环结构； DELIMITER // CREATE PROCEDURE test_iterate() BEGIN DECLARE num INT DEFAULT 0; my_loop:LOOP SET num = num + 1; IF num \u003c 10 THEN ITERATE my_loop; ELSEIF num \u003e 15 THEN LEAVE my_loop; END IF; SELECT 'MySQL'; END LOOP my_loop; END // DELIMITER ; ","date":"2022-08-24","objectID":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/:80:7","tags":[],"title":"MySQL基础篇","uri":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/"},{"categories":["mysql"],"content":"4. 游标 ","date":"2022-08-24","objectID":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/:81:0","tags":[],"title":"MySQL基础篇","uri":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/"},{"categories":["mysql"],"content":"1) 什么是游标（或光标） 虽然我们也可以通过筛选条件 WHERE 和 HAVING，或者是限定返回记录的关键字 LIMIT 返回一条记录，但是，却无法在结果集中像指针一样，向前定位一条记录、向后定位一条记录，或者是随意定位到某一条记录 ，并对记录的数据进行处理。 这个时候，就可以用到游标。游标，提供了一种灵活的操作方式，让我们能够对结果集中的每一条记录进行定位，并对指向的记录中的数据进行操作的数据结构。游标让 SQL 这种面向集合的语言有了面向过程开发的能力。 在 SQL 中，游标是一种临时的数据库对象，可以指向存储在数据库表中的数据行指针。这里游标充当了指针的作用 ，我们可以通过操作游标来对数据行进行操作。 MySQL中游标可以在存储过程和函数中使用。 ","date":"2022-08-24","objectID":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/:81:1","tags":[],"title":"MySQL基础篇","uri":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/"},{"categories":["mysql"],"content":"2) 使用游标步骤 游标必须在声明处理程序之前被声明，并且变量和条件还必须在声明游标或处理程序之前被声明。 如果我们想要使用游标，一般需要经历四个步骤。不同的 DBMS 中，使用游标的语法可能略有不同。 第一步，声明游标 在MySQL中，使用DECLARE关键字来声明游标，其语法的基本形式如下： DECLARE cursor_name CURSOR FOR select_statement; 这个语法适用于 MySQL，SQL Server，DB2 和 MariaDB。如果是用 Oracle 或者 PostgreSQL，需要写成： DECLARE cursor_name CURSOR IS select_statement; 要使用 SELECT 语句来获取数据结果集，而此时还没有开始遍历数据，这里 select_statement 代表的是 SELECT 语句，返回一个用于创建游标的结果集。 比如： DECLARE cur_emp CURSOR FOR SELECT employee_id,salary FROM employees; 第二步，打开游标 打开游标的语法如下： OPEN cursor_name 当我们定义好游标之后，如果想要使用游标，必须先打开游标。打开游标的时候 SELECT 语句的查询结果集就会送到游标工作区，为后面游标的 逐条读取 结果集中的记录做准备。 OPEN cur_emp; 第三步，使用游标（从游标中取得数据） 语法如下： FETCH cursor_name INTO var_name [, var_name] ... 这句的作用是使用 cursor_name 这个游标来读取当前行，并且将数据保存到 var_name 这个变量中，游标指针指到下一行。如果游标读取的数据行有多个列名，则在 INTO 关键字后面赋值给多个变量名即可。 注意：var_name必须在声明游标之前就定义好。 FETCH cur_emp INTO emp_id, emp_sal ; 注意：游标的查询结果集中的字段数，必须跟 INTO 后面的变量数一致，否则，在存储过程执行的时 候，MySQL 会提示错误。 第四步，关闭游标 CLOSE cursor_name 有 OPEN 就会有 CLOSE，也就是打开和关闭游标。当我们使用完游标后需要关闭掉该游标。因为游标会 占用系统资源 ，如果不及时关闭，游标会一直保持到存储过程结束，影响系统运行的效率。而关闭游标 的操作，会释放游标占用的系统资源。 关闭游标之后，我们就不能再检索查询结果中的数据行，如果需要检索只能再次打开游标。 CLOSE cur_emp; ","date":"2022-08-24","objectID":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/:81:2","tags":[],"title":"MySQL基础篇","uri":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/"},{"categories":["mysql"],"content":"3) 举例 创建存储过程“get_count_by_limit_total_salary()”，声明IN参数 limit_total_salary，DOUBLE类型；声明 OUT参数total_count，INT类型。函数的功能可以实现累加薪资最高的几个员工的薪资值，直到薪资总和达到limit_total_salary参数的值，返回累加的人数给total_count。 DELIMITER // CREATE PROCEDURE get_count_by_limit_total_salary(IN limit_total_salary DOUBLE, OUT total_count INT) BEGIN DECLARE sum_salary DOUBLE DEFAULT 0; # 记录累加的总工资 DECLARE cursor_salary DOUBLE DEFAULT 0; # 记录某一个工资值 DECLARE emp_count INT DEFAULT 0; # 记录循环个数 # 定义游标 DECLARE emp_cursor CURSOR FOR SELECT salary FROM employees ORDER BY salary DESC; # 打开游标 OPEN emp_cursor; REPEAT # 使用游标(从游标中获取数据) FETCH emp_cursor INTO cursor_salary; SET sum_salary = sum_salary + cursor_salary; SET emp_count = emp_count + 1; UNTIL sum_salary \u003e= limit_total_salary END REPEAT; set total_count = emp_count; # 关闭游标 CLOSE emp_cursor; END // DELIMITER; ","date":"2022-08-24","objectID":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/:81:3","tags":[],"title":"MySQL基础篇","uri":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/"},{"categories":["mysql"],"content":"4) 小结 游标是 MySQL 的一个重要的功能，为 逐条读取 结果集中的数据，提供了完美的解决方案。跟在应用层面实现相同的功能相比，游标可以在存储程序中使用，效率高，程序也更加简洁。 但同时也会带来一些性能问题，比如在使用游标的过程中，会对数据行进行 加锁 ，这样在业务并发量大 的时候，不仅会影响业务之间的效率，还会 消耗系统资源 ，造成内存不足，这是因为游标是在内存中进行的处理。 建议：养成用完之后就关闭的习惯，这样才能提高系统的整体效率。 ","date":"2022-08-24","objectID":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/:81:4","tags":[],"title":"MySQL基础篇","uri":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/"},{"categories":["mysql"],"content":"补充：MySQL 8.0的新特性—全局变量的持久化 在MySQL数据库中，全局变量可以通过SET GLOBAL语句来设置。例如，设置服务器语句超时的限制，可以通过设置系统变量max_execution_time来实现： SET GLOBAL MAX_EXECUTION_TIME=2000; 使用SET GLOBAL语句设置的变量值只会临时生效 。 数据库重启后，服务器又会从MySQL配置文件中读取变量的默认值。 MySQL 8.0版本新增了 SET PERSIST 命令。例如，设置服务器的最大连接数为1000： SET PERSIST global max_connections = 1000; MySQL会将该命令的配置保存到数据目录下的 mysqld-auto.cnf文件中，下次启动时会读取该文件，用其中的配置来覆盖默认的配置文件。 第17章_触发器 在实际开发中，我们经常会遇到这样的情况：有 2 个或者多个相互关联的表，如 商品信息 和 库存信息分别存放在 2 个不同的数据表中，我们在添加一条新商品记录的时候，为了保证数据的完整性，必须同时在库存表中添加一条库存记录。 这样一来，我们就必须把这两个关联的操作步骤写到程序里面，而且要用事务包裹起来，确保这两个操作成为一个原子操作 ，要么全部执行，要么全部不执行。要是遇到特殊情况，可能还需要对数据进行手动维护，这样就很容易忘记其中的一步，导致数据缺失。 这个时候，咱们可以使用触发器。你可以创建一个触发器，让商品信息数据的插入操作自动触发库存数据的插入操作。这样一来，就不用担心因为忘记添加库存数据而导致的数据缺失了。 ","date":"2022-08-24","objectID":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/:82:0","tags":[],"title":"MySQL基础篇","uri":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/"},{"categories":["mysql"],"content":"1. 触发器概述 触发器是由 事件来触发某个操作，这些事件包括 INSERT 、 UPDATE 、 DELETE 事件。所谓事件就是指用户的动作或者触发某项行为。如果定义了触发程序，当数据库执行这些语句时候，就相当于事件发生了，就会自动激发触发器执行相应的操作。 当对数据表中的数据执行插入、更新和删除操作，需要自动执行一些数据库逻辑时，可以使用触发器来实现。 ","date":"2022-08-24","objectID":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/:83:0","tags":[],"title":"MySQL基础篇","uri":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/"},{"categories":["mysql"],"content":"2. 触发器的创建 ","date":"2022-08-24","objectID":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/:84:0","tags":[],"title":"MySQL基础篇","uri":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/"},{"categories":["mysql"],"content":"1) 语法 CREATE TRIGGER 触发器名称 {BEFORE|AFTER} {INSERT|UPDATE|DELETE} ON 表名 FOR EACH ROW 触发器执行的语句块 说明： 表名 ：表示触发器监控的对象。 BEFORE|AFTER ：表示触发的时间。BEFORE 表示在事件之前触发；AFTER 表示在事件之后触发。 INSERT|UPDATE|DELETE ：表示触发的事件。 INSERT 表示插入记录时触发； UPDATE 表示更新记录时触发； DELETE 表示删除记录时触发。 触发器执行的语句块 ：可以是单条SQL语句，也可以是由BEGIN…END结构组成的复合语句块。 ","date":"2022-08-24","objectID":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/:84:1","tags":[],"title":"MySQL基础篇","uri":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/"},{"categories":["mysql"],"content":"2) 代码举例 举例1： 创建数据表： CREATE TABLE test_trigger ( id INT PRIMARY KEY AUTO_INCREMENT, t_note VARCHAR(30) ); CREATE TABLE test_trigger_log ( id INT PRIMARY KEY AUTO_INCREMENT, t_log VARCHAR(30) ); 创建触发器：创建名称为before_insert的触发器，向test_trigger数据表插入数据之前，向 test_trigger_log数据表中插入before_insert的日志信息。 DELIMITER // CREATE TRIGGER before_insert BEFORE INSERT ON test_trigger FOR EACH ROW BEGIN INSERT INTO test_trigger_log (t_log) VALUES('before_insert'); END // DELIMITER ; 向test_trigger数据表中插入数据 INSERT INTO test_trigger (t_note) VALUES ('测试 BEFORE INSERT 触发器'); 查看test_trigger_log数据表中的数据 mysql\u003e SELECT * FROM test_trigger_log; +----+---------------+ | id | t_log | +----+---------------+ | 1 | before_insert | +----+---------------+ 1 row in set (0.00 sec) 举例2： 定义触发器“salary_check_trigger”，基于员工表“employees”的INSERT事件，在INSERT之前检查 将要添加的新员工薪资是否大于他领导的薪资，如果大于领导薪资，则报sqlstate_value为’HY000’的错误，从而使得添加失败。 DELIMITER // CREATE TRIGGER salary_check_trigger BEFORE INSERT ON employees FOR EACH ROW BEGIN DECLARE mgrsalary DOUBLE; SELECT salary INTO mgrsalary FROM employees WHERE employee_id = NEW.manager_id; IF NEW.salary \u003e mgrsalary THEN SIGNAL SQLSTATE 'HY000' SET MESSAGE_TEXT = '薪资高于领导薪资错误'; END IF; END // DELIMITER ; 上面触发器声明过程中的NEW关键字代表INSERT添加语句的新记录。 ","date":"2022-08-24","objectID":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/:84:2","tags":[],"title":"MySQL基础篇","uri":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/"},{"categories":["mysql"],"content":"3. 查看、删除触发器 ","date":"2022-08-24","objectID":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/:85:0","tags":[],"title":"MySQL基础篇","uri":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/"},{"categories":["mysql"],"content":"1) 查看触发器 查看触发器是查看数据库中已经存在的触发器的定义、状态和语法信息等。 方式1：查看当前数据库的所有触发器的定义 SHOW TRIGGERS\\G 方式2：查看当前数据库中某个触发器的定义 SHOW CREATE TRIGGER 触发器名 方式3：从系统库information_schema的TRIGGERS表中查询“salary_check_trigger”触发器的信息。 SELECT * FROM information_schema.TRIGGERS; ","date":"2022-08-24","objectID":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/:85:1","tags":[],"title":"MySQL基础篇","uri":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/"},{"categories":["mysql"],"content":"2) 删除触发器 触发器也是数据库对象，删除触发器也用DROP语句，语法格式如下： DROP TRIGGER IF EXISTS 触发器名称; ","date":"2022-08-24","objectID":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/:85:2","tags":[],"title":"MySQL基础篇","uri":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/"},{"categories":["mysql"],"content":"4. 触发器的优缺点 ","date":"2022-08-24","objectID":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/:86:0","tags":[],"title":"MySQL基础篇","uri":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/"},{"categories":["mysql"],"content":"1) 优点 1、触发器可以确保数据的完整性。 假设我们用进货单头表 （demo.importhead）来保存进货单的总体信息，包括进货单编号、供货商编号、仓库编号、总计进货数量、总计进货金额和验收日期。 listnumber (进货单编号) supplierid (进货商编号) stockid (参库编号) quantity (总计数量) importvalue (总计金额) confirmationdate （验收日期) 用进货单明细表 （demo.importdetails）来保存进货商品的明细，包括进货单编号、商品编号、进货数 量、进货价格和进货金额。 listnumber (进货单编号) itemnumber (商品编号) quantity (进货数量) importprice (进货价格) importvalue （进货金额) 每当我们录入、删除和修改一条进货单明细数据的时候，进货单明细表里的数据就会发生变动。这个时候，在进货单头表中的总计数量和总计金额就必须重新计算，否则，进货单头表中的总计数量和总计金 额就不等于进货单明细表中数量合计和金额合计了，这就是数据不一致。 为了解决这个问题，我们就可以使用触发器，规定每当进货单明细表有数据插入、修改和删除的操作时，自动触发 2 步操作： 1）重新计算进货单明细表中的数量合计和金额合计； 2）用第一步中计算出来的值更新进货单头表中的合计数量与合计金额。 这样一来，进货单头表中的合计数量与合计金额的值，就始终与进货单明细表中计算出来的合计数量与 合计金额的值相同，数据就是一致的，不会互相矛盾。 2、触发器可以帮助我们记录操作日志。 利用触发器，可以具体记录什么时间发生了什么。比如，记录修改会员储值金额的触发器，就是一个很好的例子。这对我们还原操作执行时的具体场景，更好地定位问题原因很有帮助。 3、触发器还可以用在操作数据前，对数据进行合法性检查。 比如，超市进货的时候，需要库管录入进货价格。但是，人为操作很容易犯错误，比如说在录入数量的时候，把条形码扫进去了；录入金额的时候，看串了行，录入的价格远超售价，导致账面上的巨亏…… 这些都可以通过触发器，在实际插入或者更新操作之前，对相应的数据进行检查，及时提示错误，防止错误数据进入系统。 ","date":"2022-08-24","objectID":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/:86:1","tags":[],"title":"MySQL基础篇","uri":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/"},{"categories":["mysql"],"content":"2) 缺点 1、触发器最大的一个问题就是可读性差。 因为触发器存储在数据库中，并且由事件驱动，这就意味着触发器有可能不受应用层的控制 。这对系统维护是非常有挑战的。 2、相关数据的变更，可能会导致触发器出错。 特别是数据表结构的变更，都可能会导致触发器出错，进而影响数据操作的正常运行。这些都会由于触发器本身的隐蔽性，影响到应用中错误原因排查的效率。 ","date":"2022-08-24","objectID":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/:86:2","tags":[],"title":"MySQL基础篇","uri":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/"},{"categories":["mysql"],"content":"3) 注意点 注意，如果在子表中定义了外键约束，并且外键指定了ON UPDATE/DELETE CASCADE/SET NULL子句，此时修改父表被引用的键值或删除父表被引用的记录行时，也会引起子表的修改和删除操作，此时基于子表的UPDATE和DELETE语句定义的触发器并不会被激活。 例如：基于子表员工表（t_employee）的DELETE语句定义了触发器t1，而子表的部门编号（did）字段定义了外键约束引用了父表部门表（t_department）的主键列部门编号（did），并且该外键加了“ON DELETE SET NULL”子句，那么如果此时删除父表部门表（t_department）在子表员工表（t_employee） 有匹配记录的部门记录时，会引起子表员工表（t_employee）匹配记录的部门编号（did）修改为NULL， mysql\u003e update demo.membermaster set memberdeposit=20 where memberid = 2; ERROR 1054 (42S22): Unknown column ‘aa’ in ‘field list’ 但是此时不会激活触发器t1。只有直接对子表员工表（t_employee）执行DELETE语句时才会激活触发器 t1。 第18章_MySQL8其他新特性 ","date":"2022-08-24","objectID":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/:86:3","tags":[],"title":"MySQL基础篇","uri":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/"},{"categories":["mysql"],"content":"1. MySQL8新特性概述 MySQL从5.7版本直接跳跃发布了8.0版本 ，可见这是一个令人兴奋的里程碑版本。MySQL 8版本在功能上做了显著的改进与增强，开发者对MySQL的源代码进行了重构，最突出的一点是多MySQL Optimizer优化器进行了改进。不仅在速度上得到了改善，还为用户带来了更好的性能和更棒的体验。 ","date":"2022-08-24","objectID":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/:87:0","tags":[],"title":"MySQL基础篇","uri":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/"},{"categories":["mysql"],"content":"1) MySQL8.0 新增特性 更简便的NoSQL支持 NoSQL泛指非关系型数据库和数据存储。随着互联网平台的规模飞速发展，传统 的关系型数据库已经越来越不能满足需求。从5.6版本开始，MySQL就开始支持简单的NoSQL存储功能。 MySQL 8对这一功能做了优化，以更灵活的方式实现NoSQL功能，不再依赖模式（schema）。 更好的索引 在查询中，正确地使用索引可以提高查询的效率。MySQL 8中新增了 隐藏索引 和 降序索引 。隐藏索引可以用来测试去掉索引对查询性能的影响。在查询中混合存在多列索引时，使用降序索引 可以提高查询的性能。 更完善的JSON支持 MySQL从5.7开始支持原生JSON数据的存储，MySQL 8对这一功能做了优化，增加 了聚合函数 JSON_ARRAYAGG() 和 JSON_OBJECTAGG() ，将参数聚合为JSON数组或对象，新增了行内 操作符 -»，是列路径运算符 -\u003e的增强，对JSON排序做了提升，并优化了JSON的更新操作。 安全和账户管理 MySQL 8中新增了 caching_sha2_password 授权插件、角色、密码历史记录和FIPS 模式支持，这些特性提高了数据库的安全性和性能，使数据库管理员能够更灵活地进行账户管理工作。 InnoDB的变化 InnoDB是MySQL默认的存储引擎 ，是事务型数据库的首选引擎，支持事务安全表 （ACID），支持行锁定和外键。在MySQL 8 版本中，InnoDB在自增、索引、加密、死锁、共享锁等方面 做了大量的改进和优化 ，并且支持原子数据定义语言（DDL），提高了数据安全性，对事务提供更好的支持。 数据字典 在之前的MySQL版本中，字典数据都存储在元数据文件和非事务表中。从MySQL 8开始新增了事务数据字典，在这个字典里存储着数据库对象信息，这些数据字典存储在内部事务表中。 原子数据定义语句 MySQL 8开始支持原子数据定义语句（Automic DDL），即 原子DDL 。目前，只有 InnoDB存储引擎支持原子DDL。原子数据定义语句（DDL）将与DDL操作相关的数据字典更新、存储引擎 操作、二进制日志写入结合到一个单独的原子事务中，这使得即使服务器崩溃，事务也会提交或回滚。 使用支持原子操作的存储引擎所创建的表，在执行DROP TABLE、CREATE TABLE、ALTER TABLE、 RENAME TABLE、TRUNCATE TABLE、CREATE TABLESPACE、DROP TABLESPACE等操作时，都支持原子操 作，即事务要么完全操作成功，要么失败后回滚，不再进行部分提交。 对于从MySQL 5.7复制到MySQL 8 版本中的语句，可以添加 IF EXISTS 或 IF NOT EXISTS 语句来避免发生错误。 资源管理 MySQL 8开始支持创建和管理资源组，允许将服务器内运行的线程分配给特定的分组，以便 线程根据组内可用资源执行。组属性能够控制组内资源，启用或限制组内资源消耗。数据库管理员能够 根据不同的工作负载适当地更改这些属性。 目前，CPU时间是可控资源，由“虚拟CPU”这个概念来表 示，此术语包含CPU的核心数，超线程，硬件线程等等。服务器在启动时确定可用的虚拟CPU数量。拥有 对应权限的数据库管理员可以将这些CPU与资源组关联，并为资源组分配线程。 资源组组件为MySQL中的资源组管理提供了SQL接口。资源组的属性用于定义资源组。MySQL中存在两个默认组，系统组和用户 组，默认的组不能被删除，其属性也不能被更改。对于用户自定义的组，资源组创建时可初始化所有的 属性，除去名字和类型，其他属性都可在创建之后进行更改。 在一些平台下，或进行了某些MySQL的配 置时，资源管理的功能将受到限制，甚至不可用。例如，如果安装了线程池插件，或者使用的是macOS 系统，资源管理将处于不可用状态。在FreeBSD和Solaris系统中，资源线程优先级将失效。在Linux系统 中，只有配置了CAP_SYS_NICE属性，资源管理优先级才能发挥作用。 字符集支持 MySQL 8中默认的字符集由 latin1 更改为 utf8mb4 ，并首次增加了日语所特定使用的集合，utf8mb4_ja_0900_as_cs。 优化器增强 MySQL优化器开始支持隐藏索引和降序索引。隐藏索引不会被优化器使用，验证索引的必 要性时不需要删除索引，先将索引隐藏，如果优化器性能无影响就可以真正地删除索引。降序索引允许 优化器对多个列进行排序，并且允许排序顺序不一致。 公用表表达式 (Common Table Expressions）简称为CTE，MySQL现在支持递归和非递归两种形式的CTE。CTE通过在SELECT语句或其他特定语句前使用WITH语句对临时结果集进行命名。 基础语法如下： WITH cte_name (col_name1,col_name2 ...) AS (Subquery) SELECT * FROM cte_name; ​ Subquery代表子查询，子查询前使用WITH语句将结果集命名为cte_name，在后续的查询中即可使用 cte_name进行查询。 窗口函数 MySQL 8开始支持窗口函数。在之前的版本中已存在的大部分 聚合函数 在MySQL 8中也可以作为窗口函数来使用。 正则表达式支持 MySQL在8.0.4以后的版本中采用支持Unicode的国际化组件库实现正则表达式操作， 这种方式不仅能提供完全的Unicode支持，而且是多字节安全编码。MySQL增加了REGEXP_LIKE()、 EGEXP_INSTR()、REGEXP_REPLACE()和 REGEXP_SUBSTR()等函数来提升性能。另外，regexp_stack_limit和 regexp_time_limit 系统变量能够通过匹配引擎来控制资源消耗。 内部临时表 TempTable存储引擎取代MEMORY存储引擎成为内部临时表的默认存储引擎 。TempTable存储 引擎为VARCHAR和VARBINARY列提供高效存储。internal_tmp_mem_storage_engine会话变量定义了内部 临时表的存储引擎，可选的值有两个，TempTable和MEMORY，其中TempTable为默认的存储引擎。 temptable_max_ram系统配置项定义了TempTable存储引擎可使用的最大内存数量。 日志记录 在MySQL 8中错误日志子系统由一系列MySQL组件构成。这些组件的构成由系统变量 log_error_services来配置，能够实现日志事件的过滤和写入。 WITH cte_name (col_name1,col_name2 …) AS (Subquery) SELECT * FROM cte_name; 备份锁 新的备份锁允许在线备份期间执行数据操作语句，同时阻止可能造成快照不一致的操作。新 备份锁由 LOCK INSTANCE FOR BACKUP 和 UNLOCK INSTANCE 语法提供支持，执行这些操作需要备份管理 员特权。 增强的MySQL复制 MySQL 8复制支持对 JSON文档 进行部分更新的二进制日志记录 ，该记录 使用紧凑 的二进制格式 ，从而节省记录完整JSON文档的空间。当使用基于语句的日志记录时，这种紧凑的日志记 录会自动完成，并且可以通过将新的binlog_row_value_options系统变量值设置为PARTIAL_JSON来启用。 ","date":"2022-08-24","objectID":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/:87:1","tags":[],"title":"MySQL基础篇","uri":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/"},{"categories":["mysql"],"content":"2) MySQL8.0 移除的旧特性 在MySQL 5.7版本上开发的应用程序如果使用了MySQL8.0 移除的特性，语句可能会失败，或者产生不同 的执行结果。为了避免这些问题，对于使用了移除特性的应用，应当尽力修正避免使用这些特性，并尽 可能使用替代方法。 查询缓存 查询缓存已被移除 ，删除的项有： （1）语句：FLUSH QUERY CACHE和RESET QUERY CACHE。 （2）系统变量：query_cache_limit、query_cache_min_res_unit、query_cache_size、 query_cache_type、query_cache_wlock_invalidate。 （3）状态变量：Qcache_free_blocks、 Qcache_free_memory、Qcache_hits、Qcache_inserts、Qcache_lowmem_prunes、Qcache_not_cached、 Qcache_queries_in_cache、Qcache_total_blocks。 （4）线程状态：checking privileges on cached query、checking query cache for query、invalidating query cache entries、sending cached result to client、storing result in query cache、waiting for query cache lock。 加密相关 删除的加密相关的内容有：ENCODE()、DECODE()、ENCRYPT()、DES_ENCRYPT()和 DES_DECRYPT()函数，配置项des-key-file，系统变量have_crypt，FLUSH语句的DES_KEY_FILE选项， HAVE_CRYPT CMake选项。 对于移除的ENCRYPT()函数，考虑使用SHA2()替代，对于其他移除的函数，使 用AES_ENCRYPT()和AES_DECRYPT()替代。 空间函数相关 在MySQL 5.7版本中，多个空间函数已被标记为过时。这些过时函数在MySQL 8中都已被 移除，只保留了对应的ST_和MBR函数。 \\N和NULL 在SQL语句中，解析器不再将\\N视为NULL，所以在SQL语句中应使用NULL代替\\N。这项变化 不会影响使用LOAD DATA INFILE或者SELECT…INTO OUTFILE操作文件的导入和导出。在这类操作中，NULL 仍等同于\\N。 mysql_install_db 在MySQL分布中，已移除了mysql_install_db程序，数据字典初始化需要调用带着– initialize或者–initialize-insecure选项的mysqld来代替实现。另外，–bootstrap和INSTALL_SCRIPTDIR CMake也已被删除。 通用分区处理程序 通用分区处理程序已从MySQL服务中被移除。为了实现给定表分区，表所使用的存 储引擎需要自有的分区处理程序。 提供本地分区支持的MySQL存储引擎有两个，即InnoDB和NDB，而在 MySQL 8中只支持InnoDB。 系统和状态变量信息 在INFORMATION_SCHEMA数据库中，对系统和状态变量信息不再进行维护。 GLOBAL_VARIABLES、SESSION_VARIABLES、GLOBAL_STATUS、SESSION_STATUS表都已被删除。另外，系 统变量show_compatibility_56也已被删除。被删除的状态变量有Slave_heartbeat_period、 Slave_last_heartbeat,Slave_received_heartbeats、Slave_retried_transactions、Slave_running。以上被删除 的内容都可使用性能模式中对应的内容进行替代。 mysql_plugin工具 mysql_plugin工具用来配置MySQL服务器插件，现已被删除，可使用–plugin-load或- -plugin-load-add选项在服务器启动时加载插件或者在运行时使用INSTALL PLUGIN语句加载插件来替代该 工具。 ","date":"2022-08-24","objectID":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/:87:2","tags":[],"title":"MySQL基础篇","uri":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/"},{"categories":["mysql"],"content":"2. 新特性1：窗口函数 ","date":"2022-08-24","objectID":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/:88:0","tags":[],"title":"MySQL基础篇","uri":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/"},{"categories":["mysql"],"content":"1) 使用窗口函数前后对比 假设我现在有这样一个数据表，它显示了某购物网站在每个城市每个区的销售额： CREATE TABLE sales( id INT PRIMARY KEY AUTO_INCREMENT, city VARCHAR(15), county VARCHAR(15), sales_value DECIMAL ); INSERT INTO sales(city,county,sales_value) VALUES ('北京','海淀',10.00), ('北京','朝阳',20.00), ('上海','黄埔',30.00), ('上海','长宁',10.00); 查询： mysql\u003e SELECT * FROM sales; +----+------+--------+-------------+ | id | city | county | sales_value | +----+------+--------+-------------+ | 1 | 北京 | 海淀 | 10 | | 2 | 北京 | 朝阳 | 20 | | 3 | 上海 | 黄埔 | 30 | | 4 | 上海 | 长宁 | 10 | +----+------+--------+-------------+ 4 rows in set (0.00 sec) 需求：现在计算这个网站在每个城市的销售总额、在全国的销售总额、每个区的销售额占所在城市销售额中的比率，以及占总销售额中的比率。 如果用分组和聚合函数，就需要分好几步来计算。 第一步，计算总销售金额，并存入临时表 a： CREATE TEMPORARY TABLE a -- 创建临时表 SELECT SUM(sales_value) AS sales_value -- 计算总计金额 FROM sales; 查看一下临时表 a ： mysql\u003e SELECT * FROM a; +-------------+ | sales_value | +-------------+ | 70 | +-------------+ 1 row in set (0.00 sec) 第二步，计算每个城市的销售总额并存入临时表 b： CREATE TEMPORARY TABLE b -- 创建临时表 SELECT city, SUM(sales_value) AS sales_value -- 计算城市销售合计 FROM sales GROUP BY city; 查看临时表 b ： mysql\u003e SELECT * FROM b; +------+-------------+ | city | sales_value | +------+-------------+ | 北京 | 30 | | 上海 | 40 | +------+-------------+ 2 rows in set (0.00 sec) 第三步，计算各区的销售占所在城市的总计金额的比例，和占全部销售总计金额的比例。我们可以通过下面的连接查询获得需要的结果： mysql\u003e SELECT s.city AS 城市,s.county AS 区,s.sales_value AS 区销售额, -\u003e b.sales_value AS 市销售额,s.sales_value/b.sales_value AS 市比率, -\u003e a.sales_value AS 总销售额,s.sales_value/a.sales_value AS 总比率 -\u003e FROM sales s -\u003e JOIN b ON (s.city=b.city) -- 连接市统计结果临时表 -\u003e JOIN a -- 连接总计金额临时表 -\u003e ORDER BY s.city,s.county; +------+------+----------+----------+--------+----------+--------+ | 城市 | 区 | 区销售额 | 市销售额 | 市比率 | 总销售额 | 总比率 | +------+------+----------+----------+--------+----------+--------+ | 上海 | 长宁 | 10 | 40 | 0.2500 | 70 | 0.1429 | | 上海 | 黄埔 | 30 | 40 | 0.7500 | 70 | 0.4286 | | 北京 | 朝阳 | 20 | 30 | 0.6667 | 70 | 0.2857 | | 北京 | 海淀 | 10 | 30 | 0.3333 | 70 | 0.1429 | +------+------+----------+----------+--------+----------+--------+ 4 rows in set (0.00 sec) 结果显示：市销售金额、市销售占比、总销售金额、总销售占比都计算出来了。 同样的查询，如果用窗口函数，就简单多了。我们可以用下面的代码来实现： mysql\u003e SELECT city AS 城市,county AS 区,sales_value AS 区销售额, -\u003e SUM(sales_value) OVER(PARTITION BY city) AS 市销售额, -- 计算市销售额 -\u003e sales_value/SUM(sales_value) OVER(PARTITION BY city) AS 市比率, -\u003e SUM(sales_value) OVER() AS 总销售额, -- 计算总销售额 -\u003e sales_value/SUM(sales_value) OVER() AS 总比率 -\u003e FROM sales -\u003e ORDER BY city,county; +------+------+----------+----------+--------+----------+--------+ | 城市 | 区 | 区销售额 | 市销售额 | 市比率 | 总销售额 | 总比率 | +------+------+----------+----------+--------+----------+--------+ | 上海 | 长宁 | 10 | 40 | 0.2500 | 70 | 0.1429 | | 上海 | 黄埔 | 30 | 40 | 0.7500 | 70 | 0.4286 | | 北京 | 朝阳 | 20 | 30 | 0.6667 | 70 | 0.2857 | | 北京 | 海淀 | 10 | 30 | 0.3333 | 70 | 0.1429 | +------+------+----------+-----------+--------+----------+--------+ 4 rows in set (0.00 sec) 结果显示，我们得到了与上面那种查询同样的结果。 使用窗口函数，只用了一步就完成了查询。而且，由于没有用到临时表，执行的效率也更高了。很显然，在这种需要用到分组统计的结果对每一条记录进行计算的场景下，使用窗口函数更好。 ","date":"2022-08-24","objectID":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/:88:1","tags":[],"title":"MySQL基础篇","uri":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/"},{"categories":["mysql"],"content":"2) 窗口函数分类 MySQL从8.0版本开始支持窗口函数。窗口函数的作用类似于在查询中对数据进行分组，不同的是，分组操作会把分组的结果聚合成一条记录，而窗口函数是将结果置于每一条数据记录中。 窗口函数可以分为 静态窗口函数 和 动态窗口函数 。 静态窗口函数的窗口大小是固定的，不会因为记录的不同而不同； 动态窗口函数的窗口大小会随着记录的不同而变化。 MySQL官方网站窗口函数的网址为https://dev.mysql.com/doc/refman/8.0/en/window-function-descriptio ns.html#function_row-number。 窗口函数总体上可以分为序号函数、分布函数、前后函数、首尾函数和其他函数，如下表： ","date":"2022-08-24","objectID":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/:88:2","tags":[],"title":"MySQL基础篇","uri":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/"},{"categories":["mysql"],"content":"3) 语法结构 窗口函数的语法结构是： 函数 OVER（[PARTITION BY 字段名 ORDER BY 字段名 ASC|DESC]） 或者是： 函数 OVER 窗口名 … WINDOW 窗口名 AS （[PARTITION BY 字段名 ORDER BY 字段名 ASC|DESC]） OVER 关键字指定函数窗口的范围。 如果省略后面括号中的内容，则窗口会包含满足WHERE条件的所有记录，窗口函数会基于所有满足WHERE条件的记录进行计算。 如果OVER关键字后面的括号不为空，则可以使用如下语法设置窗口。 窗口名：为窗口设置一个别名，用来标识窗口。 PARTITION BY子句：指定窗口函数按照哪些字段进行分组。分组后，窗口函数可以在每个分组中分别执行。 ORDER BY子句：指定窗口函数按照哪些字段进行排序。执行排序操作使窗口函数按照排序后的数据记录的顺序进行编号。 FRAME子句：为分区中的某个子集定义规则，可以用来作为滑动窗口使用。 ","date":"2022-08-24","objectID":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/:88:3","tags":[],"title":"MySQL基础篇","uri":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/"},{"categories":["mysql"],"content":"4) 分类讲解 创建表： CREATE TABLE goods( id INT PRIMARY KEY AUTO_INCREMENT, category_id INT, category VARCHAR(15), NAME VARCHAR(30), price DECIMAL(10,2), stock INT, upper_time DATETIME ); 添加数据： INSERT INTO goods(category_id,category,NAME,price,stock,upper_time) VALUES (1, '女装/女士精品', 'T恤', 39.90, 1000, '2020-11-10 00:00:00'), (1, '女装/女士精品', '连衣裙', 79.90, 2500, '2020-11-10 00:00:00'), (1, '女装/女士精品', '卫衣', 89.90, 1500, '2020-11-10 00:00:00'), (1, '女装/女士精品', '牛仔裤', 89.90, 3500, '2020-11-10 00:00:00'), (1, '女装/女士精品', '百褶裙', 29.90, 500, '2020-11-10 00:00:00'), (1, '女装/女士精品', '呢绒外套', 399.90, 1200, '2020-11-10 00:00:00'), (2, '户外运动', '自行车', 399.90, 1000, '2020-11-10 00:00:00'), (2, '户外运动', '山地自行车', 1399.90, 2500, '2020-11-10 00:00:00'), (2, '户外运动', '登山杖', 59.90, 1500, '2020-11-10 00:00:00'), (2, '户外运动', '骑行装备', 399.90, 3500, '2020-11-10 00:00:00'), (2, '户外运动', '运动外套', 799.90, 500, '2020-11-10 00:00:00'), (2, '户外运动', '滑板', 499.90, 1200, '2020-11-10 00:00:00'); 下面针对goods表中的数据来验证每个窗口函数的功能。 1) 序号函数 1. ROW_NUMBER()函数 ROW_NUMBER()函数能够对数据中的序号进行顺序显示。 举例：查询 goods 数据表中每个商品分类下价格降序排列的各个商品信息。 mysql\u003e SELECT ROW_NUMBER() OVER(PARTITION BY category_id ORDER BY price DESC) AS row_num, id, category_id, category, NAME, price, stock FROM goods; +---------+----+-------------+---------------+------------+---------+-------+ | row_num | id | category_id | category | NAME | price | stock | +---------+----+-------------+---------------+------------+---------+-------+ | 1 | 6 | 1 | 女装/女士精品 | 呢绒外套 | 399.90 | 1200 | | 2 | 3 | 1 | 女装/女士精品 | 卫衣 | 89.90 | 1500 | | 3 | 4 | 1 | 女装/女士精品 | 牛仔裤 | 89.90 | 3500 | | 4 | 2 | 1 | 女装/女士精品 | 连衣裙 | 79.90 | 2500 | | 5 | 1 | 1 | 女装/女士精品 | T恤 | 39.90 | 1000 | | 6 | 5 | 1 | 女装/女士精品 | 百褶裙 | 29.90 | 500 | | 1 | 8 | 2 | 户外运动 | 山地自行车 | 1399.90 | 2500 | | 2 | 11 | 2 | 户外运动 | 运动外套 | 799.90 | 500 | | 3 | 12 | 2 | 户外运动 | 滑板 | 499.90 | 1200 | | 4 | 7 | 2 | 户外运动 | 自行车 | 399.90 | 1000 | | 5 | 10 | 2 | 户外运动 | 骑行装备 | 399.90 | 3500 | | 6 | 9 | 2 | 户外运动 | 登山杖 | 59.90 | 1500 | +---------+----+-------------+---------------+------------+---------+-------+ 12 rows in set (0.00 sec) 举例：查询 goods 数据表中每个商品分类下价格最高的3种商品信息。 mysql\u003e SELECT * -\u003e FROM ( -\u003e SELECT ROW_NUMBER() OVER(PARTITION BY category_id ORDER BY price DESC) AS row_num, -\u003e id, category_id, category, NAME, price, stock -\u003e FROM goods) t -\u003e WHERE row_num \u003c= 3; +---------+----+-------------+---------------+------------+---------+-------+ | row_num | id | category_id | category | NAME | price | stock | +---------+----+-------------+---------------+------------+---------+-------+ | 1 | 6 | 1 | 女装/女士精品 | 呢绒外套 | 399.90 | 1200 | | 2 | 3 | 1 | 女装/女士精品 | 卫衣 | 89.90 | 1500 | | 3 | 4 | 1 | 女装/女士精品 | 牛仔裤 | 89.90 | 3500 | | 1 | 8 | 2 | 户外运动 | 山地自行车 | 1399.90 | 2500 | | 2 | 11 | 2 | 户外运动 | 运动外套 | 799.90 | 500 | | 3 | 12 | 2 | 户外运动 | 滑板 | 499.90 | 1200 | +---------+----+-------------+---------------+------------+----------+-------+ 6 rows in set (0.00 sec) 在名称为“女装/女士精品”的商品类别中，有两款商品的价格为89.90元，分别是卫衣和牛仔裤。两款商品 的序号都应该为2，而不是一个为2，另一个为3。此时，可以使用RANK()函数和DENSE_RANK()函数解 决。 2．RANK()函数 使用RANK()函数能够对序号进行并列排序，并且会跳过重复的序号，比如序号为1、1、3。 举例：使用RANK()函数获取 goods 数据表中各类别的价格从高到低排序的各商品信息。 mysql\u003e SELECT RANK() OVER(PARTITION BY category_id ORDER BY price DESC) AS row_num, -\u003e id, category_id, category, NAME, price, stock -\u003e FROM goods; +---------+----+-------------+---------------+------------+---------+-------+ | row_num | id | category_id | category | NAME | price | stock | +---------+----+-------------+---------------+------------+---------+-------+ | 1 | 6 | 1 | 女装/女士精品 | 呢绒外套 | 399.90 | 1200 | | 2 | 3 | 1 | 女装/女士精品 | 卫衣 | 89.90 | 1500 | | 2 | 4 | 1 | 女装/女士精品 | 牛仔裤 | 89.90 | 3500 | | 4 | 2 | 1 | 女装/女士精品 | 连衣裙 | 79.90 | 2500 | | 5 | 1 | 1 | 女装/女士精品 | T恤 | 39.90 | 1000 | | 6 | 5 | 1 | 女装/女士精品 | 百褶裙 | 29.90 | 500 | | 1 | 8 | 2 | 户外运动 | 山地自行车 | 1399.90 | 2500 | | 2 | 11 | 2 | 户外运动 | 运动外套 | 799.90 | 500 | | 3 | 12 | 2 | 户外运动 | 滑板 | 499.90 | 1200 | | 4 | 7 | 2 | 户外运动 | 自行车 | 399.90 | 1000 | | 4 | 10 | 2 | 户外运动 | 骑行装备 | 399.90 | 3500 | | 6 | 9 | 2 |","date":"2022-08-24","objectID":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/:88:4","tags":[],"title":"MySQL基础篇","uri":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/"},{"categories":["mysql"],"content":"5) 小结 窗口函数的特点是可以分组，而且可以在分组内排序。另外，窗口函数不会因为分组而减少原表中的行 数，这对我们在原表数据的基础上进行统计和排序非常有用。 ","date":"2022-08-24","objectID":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/:88:5","tags":[],"title":"MySQL基础篇","uri":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/"},{"categories":["mysql"],"content":"3. 新特性2：公用表表达式 公用表表达式（或通用表表达式）简称为CTE（Common Table Expressions）。CTE是一个命名的临时结果集，作用范围是当前语句。CTE可以理解成一个可以复用的子查询，当然跟子查询还是有点区别的， CTE可以引用其他CTE，但子查询不能引用其他子查询。所以，可以考虑代替子查询。 依据语法结构和执行方式的不同，公用表表达式分为 普通公用表表达式 和 递归公用表表达式 2 种。 ","date":"2022-08-24","objectID":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/:89:0","tags":[],"title":"MySQL基础篇","uri":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/"},{"categories":["mysql"],"content":"1) 普通公用表表达式 普通公用表表达式的语法结构是： WITH CTE名称 AS （子查询） SELECT|DELETE|UPDATE 语句; 普通公用表表达式类似于子查询，不过，跟子查询不同的是，它可以被多次引用，而且可以被其他的普 通公用表表达式所引用。 举例：查询员工所在的部门的详细信息。 mysql\u003e SELECT * FROM departments -\u003e WHERE department_id IN ( -\u003e SELECT DISTINCT department_id -\u003e FROM employees -\u003e ); +---------------+------------------+------------+-------------+ | department_id | department_name | manager_id | location_id | +---------------+------------------+------------+-------------+ | 10 | Administration | 200 | 1700 | | 20 | Marketing | 201 | 1800 | | 30 | Purchasing | 114 | 1700 | | 40 | Human Resources | 203 | 2400 | | 50 | Shipping | 121 | 1500 | | 60 | IT | 103 | 1400 | | 70 | Public Relations | 204 | 2700 | | 80 | Sales | 145 | 2500 | | 90 | Executive | 100 | 1700 | | 100 | Finance | 108 | 1700 | | 110 | Accounting | 205 | 1700 | +---------------+------------------+------------+-------------+ 11 rows in set (0.00 sec) 这个查询也可以用普通公用表表达式的方式完成： mysql\u003e WITH emp_dept_id -\u003e AS (SELECT DISTINCT department_id FROM employees) -\u003e SELECT * -\u003e FROM departments d JOIN emp_dept_id e -\u003e ON d.department_id = e.department_id; +---------------+------------------+------------+-------------+---------------+ | department_id | department_name | manager_id | location_id | department_id | +---------------+------------------+------------+-------------+---------------+ | 90 | Executive | 100 | 1700 | 90 | | 60 | IT | 103 | 1400 | 60 | | 100 | Finance | 108 | 1700 | 100 | | 30 | Purchasing | 114 | 1700 | 30 | | 50 | Shipping | 121 | 1500 | 50 | | 80 | Sales | 145 | 2500 | 80 | | 10 | Administration | 200 | 1700 | 10 | | 20 | Marketing | 201 | 1800 | 20 | | 40 | Human Resources | 203 | 2400 | 40 | | 70 | Public Relations | 204 | 2700 | 70 | | 110 | Accounting | 205 | 1700 | 110 | +---------------+------------------+------------+-------------+---------------+ 11 rows in set (0.00 sec) 例子说明，公用表表达式可以起到子查询的作用。以后如果遇到需要使用子查询的场景，你可以在查询 之前，先定义公用表表达式，然后在查询中用它来代替子查询。而且，跟子查询相比，公用表表达式有 一个优点，就是定义过公用表表达式之后的查询，可以像一个表一样多次引用公用表表达式，而子查询 则不能。 ","date":"2022-08-24","objectID":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/:89:1","tags":[],"title":"MySQL基础篇","uri":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/"},{"categories":["mysql"],"content":"2) 递归公用表表达式 递归公用表表达式也是一种公用表表达式，只不过，除了普通公用表表达式的特点以外，它还有自己的特点，就是可以调用自己。它的语法结构是： WITH RECURSIVE CTE名称 AS （子查询） SELECT|DELETE|UPDATE 语句; 递归公用表表达式由 2 部分组成，分别是种子查询和递归查询，中间通过关键字 UNION [ALL]进行连接。 这里的种子查询，意思就是获得递归的初始值。这个查询只会运行一次，以创建初始数据集，之后递归 查询会一直执行，直到没有任何新的查询数据产生，递归返回。 案例：针对于我们常用的employees表，包含employee_id，last_name和manager_id三个字段。如果a是b 的管理者，那么，我们可以把b叫做a的下属，如果同时b又是c的管理者，那么c就是b的下属，是a的下下 属。 下面我们尝试用查询语句列出所有具有下下属身份的人员信息。 如果用我们之前学过的知识来解决，会比较复杂，至少要进行 4 次查询才能搞定： 第一步，先找出初代管理者，就是不以任何别人为管理者的人，把结果存入临时表； 第二步，找出所有以初代管理者为管理者的人，得到一个下属集，把结果存入临时表； 第三步，找出所有以下属为管理者的人，得到一个下下属集，把结果存入临时表。 第四步，找出所有以下下属为管理者的人，得到一个结果集。 如果第四步的结果集为空，则计算结束，第三步的结果集就是我们需要的下下属集了，否则就必须继续 进行第四步，一直到结果集为空为止。比如上面的这个数据表，就需要到第五步，才能得到空结果集。 而且，最后还要进行第六步：把第三步和第四步的结果集合并，这样才能最终获得我们需要的结果集。 如果用递归公用表表达式，就非常简单了。我介绍下具体的思路。 用递归公用表表达式中的种子查询，找出初代管理者。字段 n 表示代次，初始值为 1，表示是第一 代管理者。 用递归公用表表达式中的递归查询，查出以这个递归公用表表达式中的人为管理者的人，并且代次 的值加 1。直到没有人以这个递归公用表表达式中的人为管理者了，递归返回。 在最后的查询中，选出所有代次大于等于 3 的人，他们肯定是第三代及以上代次的下属了，也就是 下下属了。这样就得到了我们需要的结果集。 这里看似也是 3 步，实际上是一个查询的 3 个部分，只需要执行一次就可以了。而且也不需要用临时表 保存中间结果，比刚刚的方法简单多了。 代码实现： WITH RECURSIVE cte AS ( SELECT employee_id,last_name,manager_id,1 AS n FROM employees WHERE employee_id = 100 -- 种子查询，找到第一代领导 UNION ALL SELECT a.employee_id,a.last_name,a.manager_id,n+1 FROM employees AS a JOIN cte ON (a.manager_id = cte.employee_id) -- 递归查询，找出以递归公用表表达式的人为领导的人 ) SELECT employee_id,last_name FROM cte WHERE n \u003e= 3; 总之，递归公用表表达式对于查询一个有共同的根节点的树形结构数据，非常有用。它可以不受层级的 限制，轻松查出所有节点的数据。如果用其他的查询方式，就比较复杂了。 ","date":"2022-08-24","objectID":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/:89:2","tags":[],"title":"MySQL基础篇","uri":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/"},{"categories":["mysql"],"content":"3) 小结 公用表表达式的作用是可以替代子查询，而且可以被多次引用。递归公用表表达式对查询有一个共同根节点的树形结构数据非常高效，可以轻松搞定其他查询方式难以处理的查询。 ","date":"2022-08-24","objectID":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/:89:3","tags":[],"title":"MySQL基础篇","uri":"/mysql%E5%9F%BA%E7%A1%80%E7%AF%87/"},{"categories":["golang"],"content":"一. 什么是内存对齐, 为啥要内存对齐？ 在解释什么是内存对齐之前，我们需要先了解一下CPU和内存数据交互的过程。CPU和内存是通过总线进行数据交互的。其中地址总线用来传递CPU需要的数据地址，内存将数据通过数据总线传递给CPU， 或者CPU将数据通过数据总线回传给内存。 首先我们需要知道以下概念： ","date":"2022-07-17","objectID":"/go%E7%9A%84%E7%BB%93%E6%9E%84%E4%BD%93%E5%86%85%E5%AD%98%E5%AF%B9%E9%BD%90/:1:0","tags":[],"title":"Go的结构体内存对齐","uri":"/go%E7%9A%84%E7%BB%93%E6%9E%84%E4%BD%93%E5%86%85%E5%AD%98%E5%AF%B9%E9%BD%90/"},{"categories":["golang"],"content":"(1) 机器字长 在计算机领域，对于某种特定的计算机设计而言，字（word）是用于表示其自然的数据单位的术语，是用来表示一次性处理事务的固定长度。一个字的位数，即字长。 ","date":"2022-07-17","objectID":"/go%E7%9A%84%E7%BB%93%E6%9E%84%E4%BD%93%E5%86%85%E5%AD%98%E5%AF%B9%E9%BD%90/:1:1","tags":[],"title":"Go的结构体内存对齐","uri":"/go%E7%9A%84%E7%BB%93%E6%9E%84%E4%BD%93%E5%86%85%E5%AD%98%E5%AF%B9%E9%BD%90/"},{"categories":["golang"],"content":"(2) 地址总线 专门用来传送地址的，由于地址只能从CPU传向外部存储器或I／O端口，所以地址总线总是单向的。地址总线的位数决定了CPU可直接寻址的内存空间大小，比如8位微型机的地址总线为16位，则其最大可寻址空间为2^16＝64KB，16位微型机的地址总线为20位，其可寻址空间为2^20＝1MB。 ","date":"2022-07-17","objectID":"/go%E7%9A%84%E7%BB%93%E6%9E%84%E4%BD%93%E5%86%85%E5%AD%98%E5%AF%B9%E9%BD%90/:1:2","tags":[],"title":"Go的结构体内存对齐","uri":"/go%E7%9A%84%E7%BB%93%E6%9E%84%E4%BD%93%E5%86%85%E5%AD%98%E5%AF%B9%E9%BD%90/"},{"categories":["golang"],"content":"(3) 数据总线 是CPU与内存或其他器件之间的数据传送的通道。每条传输线一次只能传输1位二进制数据, 数据总线每次可以传输的字节总数就称为机器字长或者数据总线的宽度。 它决定了CPU和外界的数据传送速度。我们现在日常使用的基本上是32位(每次可以传输4字节)或者64位(每次可以传输8字节)机器字长的机器。 由于数据是通过总线进行传输，若数据未经一定规则的对齐，CPU的访址操作与总线的传输操作将会异常的复杂，所以编译器在程序编译期间会对各种类型的数据按照一定的规则进行对齐, 对齐过程会按一定规则对内存的数据段进行的字节填充， 这就是字节对齐。 例如: 现在要存储变量A（int32）和B（int64）那么不做任何字节对齐优化的情况下，内存布局是这样的 字节对齐优化后是这样子的： 一看感觉字节对齐后浪费了内存， 但是当我们去读取内存中的数据给CPU时，64位的机器（一次可以原子读取8字节）在内存对齐和不对齐的情况下A变量都只需要原子读取一次就行， 但是对齐后B变量的读取只需一次， 而不对齐的情况下，B需要读取2次，且需要额外的处理牺牲性能来保证2次读取的原子性。所以本质上，内存填充是一种以空间换时间， 通过额外的内存填充来提高内存读取的效率的手段。 总的来说，内存对齐主要解决以下两个问题： 【1】跨平台问题：如果数据不对齐，那么在64位字长机器存储的数据可能在32位字长的机器可能就无法正常的读取。 【2】性能问题：如果不对齐，那么每个数据要通过多少次总线传输是未知的，如果每次都要处理这些复杂的情况，那么数据的读/写性能将会收到很大的影响。之所以有些CPU支持访问任意地址，是因为处理器在后面多做了很多额外处理。 ","date":"2022-07-17","objectID":"/go%E7%9A%84%E7%BB%93%E6%9E%84%E4%BD%93%E5%86%85%E5%AD%98%E5%AF%B9%E9%BD%90/:1:3","tags":[],"title":"Go的结构体内存对齐","uri":"/go%E7%9A%84%E7%BB%93%E6%9E%84%E4%BD%93%E5%86%85%E5%AD%98%E5%AF%B9%E9%BD%90/"},{"categories":["golang"],"content":"二. 内存对齐的规则是什么? 内存对齐主要是为了保证数据的原子读取， 因此内存对齐的最大边界只可能为当前机器的字长。当然如果每种类型都使用最大的对齐边界，那么对内存将是一种浪费，实际上我们只要保证同一个数据不要分开在多次总线事务中便可。 Go在其官方文档 Size and alignment guarantees - Golang spec 就描述了其在内存对齐方面的细节。 Go也提供了unsafe.Alignof(x)来返回一个类型的对齐值，并且作出了如下约定： 对于任意类型的变量 x ，unsafe.Alignof(x) 至少为 1。 对于 struct 结构体类型的变量 x，计算 x 每一个字段 f 的 unsafe.Alignof(x.f)，unsafe.Alignof(x) 等于其中的最大值。 对于 array 数组类型的变量 x，unsafe.Alignof(x) 等于构成数组的元素类型的对齐倍数。 没有任何字段的空 struct{} 和没有任何元素的 array 占据的内存空间大小为 0，不同的大小为 0 的变量可能指向同一块地址。 总结来说，分为基本类型对齐和结构体类型对齐 ","date":"2022-07-17","objectID":"/go%E7%9A%84%E7%BB%93%E6%9E%84%E4%BD%93%E5%86%85%E5%AD%98%E5%AF%B9%E9%BD%90/:2:0","tags":[],"title":"Go的结构体内存对齐","uri":"/go%E7%9A%84%E7%BB%93%E6%9E%84%E4%BD%93%E5%86%85%E5%AD%98%E5%AF%B9%E9%BD%90/"},{"categories":["golang"],"content":"(1) 基本类型对齐 go语言的基本类型的内存对齐是按照基本类型的大小和机器字长中最小值进行对齐 数据类型 类型大小（32/64位） 最大对齐边界（32位） 最大对齐边界（64位） int8/uint8/byte 1字节 1 1 int16/uint16 2字节 2 2 int32/uint32/rune/float32/complex32 4字节 4 4 int64/uint64/float64/complex64 8字节 4 8 string 8字节/16字节 4 8 slice 12字节/24字节 4 8 我们可以在自己的机器上编码测试了一下（我的机器是64位的 Mac OS X）： package service import ( \"testing\" \"unsafe\" ) func TestAlign(t *testing.T) { var byteTest byte = 'a' var int8Test int8 = 0 var int16Test int16 = 0 var int32Test int32 = 0 var int64Test int64 = 0 var uint8Test uint8 = 0 var uint16Test uint16 = 0 var uint32Test uint32 = 0 var uint64Test uint64 = 0 var float32Test float32 = 0.0 var float64Test float64 = 0.0 println(\"byte max align size =\u003e\", unsafe.Alignof(byteTest)) println(\"int8/uint8 max align size =\u003e\", unsafe.Alignof(int8Test), \"/\" , unsafe.Alignof(uint8Test)) println(\"int16/uint16 max align size =\u003e\", unsafe.Alignof(int16Test), \"/\" , unsafe.Alignof(uint16Test)) println(\"int32/uint32/float32 max align size =\u003e\", unsafe.Alignof(int32Test), \"/\" , unsafe.Alignof(uint32Test), \"/\", unsafe.Alignof(float32Test)) println(\"int64/uint64/float64 max align size =\u003e\", unsafe.Alignof(int64Test), \"/\" , unsafe.Alignof(uint64Test), \"/\", unsafe.Alignof(float64Test)) var s string = \"343240000000000\" println(\"string max align size =\u003e\", unsafe.Alignof(s)) var sliceTest []string println(\"slice's size/max align size =\u003e\", unsafe.Alignof(sliceTest), \"/\" , unsafe.Sizeof(sliceTest)) var structTest struct{} println(\"struct{}'s size / max align size =\u003e\", unsafe.Alignof(structTest), \"/\" , unsafe.Sizeof(structTest)) } 复制 运行结果： byte max align size =\u003e 1 int8/uint8 max align size =\u003e 1 / 1 int16/uint16 max align size =\u003e 2 / 2 int32/uint32/float32 max align size =\u003e 4 / 4 / 4 int64/uint64/float64 max align size =\u003e 8 / 8 / 8 string max align size =\u003e 8 slice's size/max align size =\u003e 8 / 24 struct{}'s size / max align size =\u003e 1 / 0 复制 ","date":"2022-07-17","objectID":"/go%E7%9A%84%E7%BB%93%E6%9E%84%E4%BD%93%E5%86%85%E5%AD%98%E5%AF%B9%E9%BD%90/:2:1","tags":[],"title":"Go的结构体内存对齐","uri":"/go%E7%9A%84%E7%BB%93%E6%9E%84%E4%BD%93%E5%86%85%E5%AD%98%E5%AF%B9%E9%BD%90/"},{"categories":["golang"],"content":"(2) 结构体类型对齐 go语言的结构体的对齐是先对结构体的每个字段进行对齐，然后对总体的大小按照最大对齐边界的整数倍进行对齐。有一个特殊的情况就是，如果空结构体嵌套到一个结构体尾部，那么这个结构体也是要额外对齐的，因为如果有指针指向该字段, 返回的地址将在结构体之外，如果此指针一直存活不释放对应的内存，就会有内存泄露的问题。 下面通过一些列的例子来说明一下结构体对齐的规则，需要额外说明的是结构体内的字段位置其实都是通过计算到结构体首地址的偏移量来确定的，对所有的字段来说，首地址就是结构体内索引值为0的地址。 案例一 type TestStruct1 struct { a int8 // 1 字节====\u003e max align 1 字节 b int32 // 4 字节====\u003e max align 4 字节 c []string // 24 字节====\u003e max align 8 字节 } 复制 TestStruct1在编译期就会进行字节对齐的优化。优化后各个变量的相对位置如下图(以64位字长下环境为例)： image.png TestStruct1 内存占用大小分析：最大对齐边界为8，总体字节数 = 1 + （align 3） + 4 + 24 = 32, 由于32刚好是8的倍数，所以末尾无需额外填充，最后这个结构体的大小为32字节。 案例二 type TestStruct2 struct { a []string // 24 字节====\u003e max align 8 字节 b int64 // 8 字节====\u003e max align 8 字节 c int32 // 4 字节====\u003e max align 4 字节 } 复制 image.png TestStruct2 内存占用大小分析：最大对齐边界为8字节，总体字节数 = 24（a） + 8（b） + 4（c） + 4（填充） = 40, 由于40刚好是8的倍数，所以c字段填充完后无需额外填充了。 案例三 type TestStruct3 struct { a int8 b int64 c struct{} } 复制 image.png TestStruct3 内存占用大小分析：最大对齐边界为8字节，总体字节数 = 1（a）+ 7(填充) + 8（b） + 8（c填充）=24, 空结构体理论上不占字节，但是如果在另一个结构体尾部则需要进行额外字节对齐 。 案例四 type TestStruct4 struct { a struct{} b int8 c int32 } 复制 image.png TestStruct4 内存占用大小分析：最大对齐边界为4字节，总体字节数 = 0(a)+ 1（b）+ 7(填充) + 4（c) = 8。 ","date":"2022-07-17","objectID":"/go%E7%9A%84%E7%BB%93%E6%9E%84%E4%BD%93%E5%86%85%E5%AD%98%E5%AF%B9%E9%BD%90/:2:2","tags":[],"title":"Go的结构体内存对齐","uri":"/go%E7%9A%84%E7%BB%93%E6%9E%84%E4%BD%93%E5%86%85%E5%AD%98%E5%AF%B9%E9%BD%90/"},{"categories":["golang"],"content":"(3) 测试验证 执行以下代码（环境是64位机器字长的）就可以看到我们之前案例中分析的结果。 func TestAlignStruct(t *testing.T) { var testStruct1 TestStruct1 println(\"size of testStruct1:\", unsafe.Sizeof(testStruct1)) var testStruct2 TestStruct2 println(\"size of testStruct2:\", unsafe.Sizeof(testStruct2)) var testStruct3 TestStruct3 println(\"size of testStruct4 / testStruct4's a size:\", unsafe.Sizeof(testStruct3), \"/\" , unsafe.Sizeof(testStruct3.c)) var testStruct4 TestStruct4 println(\"size of testStruct4 / testStruct4's a size:\", unsafe.Sizeof(testStruct4), \"/\" , unsafe.Sizeof(testStruct4.a)) } 输出为： === RUN TestAlignStruct size of testStruct1: 32 size of testStruct2: 40 size of testStruct4 / testStruct4's a size: 24 / 0 size of testStruct4 / testStruct4's a size: 8 / 0 --- PASS: TestAlignStruct (0.00s) PASS ","date":"2022-07-17","objectID":"/go%E7%9A%84%E7%BB%93%E6%9E%84%E4%BD%93%E5%86%85%E5%AD%98%E5%AF%B9%E9%BD%90/:2:3","tags":[],"title":"Go的结构体内存对齐","uri":"/go%E7%9A%84%E7%BB%93%E6%9E%84%E4%BD%93%E5%86%85%E5%AD%98%E5%AF%B9%E9%BD%90/"},{"categories":["golang"],"content":"Make介绍 make命令是GNU的工程化编译工具，用以实现工程化的管理，提高开发效率。 Make解释Makefile 中的指令（应该说是规则）。在Makefile文件中描述了整个工程所有文件的编译顺序、编译规则。Makefile 有自己的书写格式、关键字、函数。像C 语言有自己的格式、关键字和函数一样。而且在Makefile 中可以使用系统shell所提供的任何命令来完成想要的工作。 Makefile文件 构建规则都写在Makefile文件里面，要学会如何Make命令，就必须学会如何编写Makefile文件。 ","date":"2022-07-16","objectID":"/makefile%E5%9C%A8go%E9%A1%B9%E7%9B%AE%E7%9A%84%E5%AE%9E%E8%B7%B5/:0:0","tags":["makefile","go编程技巧"],"title":"Makefile在go项目的实践","uri":"/makefile%E5%9C%A8go%E9%A1%B9%E7%9B%AE%E7%9A%84%E5%AE%9E%E8%B7%B5/"},{"categories":["golang"],"content":"文件格式 Makefile文件由一系列规则（rules）构成。 每条规则要说明构建的依赖条件，及怎么样去构建。那么格式如下， # rule \u003ctarget\u003e : \u003cprerequisites\u003e [tab] \u003ccommands\u003e target: 目标 prerequisites： 先决条件，或者说依赖条件 tab: 使用tab来缩进 command: 要执行的命令（可以说是小型的shell代码块） ","date":"2022-07-16","objectID":"/makefile%E5%9C%A8go%E9%A1%B9%E7%9B%AE%E7%9A%84%E5%AE%9E%E8%B7%B5/:1:0","tags":["makefile","go编程技巧"],"title":"Makefile在go项目的实践","uri":"/makefile%E5%9C%A8go%E9%A1%B9%E7%9B%AE%E7%9A%84%E5%AE%9E%E8%B7%B5/"},{"categories":["golang"],"content":"target目标 一个目标，可以是文件名，也可以是某个操作的名字（伪目标），这个名字由自己定义，用来指明要构建的对象。 create: touch newfile 比如上面这条规则，伪目标为create，命令作用为创建一个文件。要想构建这个操作，调用make create(指定目标进行构建编译)即可。 但是如果目录下，存在一个文件名为create，那么构建命令就不会去执行。为了解决这个问题，当使用伪目标时，可以明确声明create是“伪目标“，告诉make跳过文件检查。 .PHONY: clean create: touch newfile 如果Make命令运行时没有指定目标，默认会执行Makefile文件的第一个目标。 ","date":"2022-07-16","objectID":"/makefile%E5%9C%A8go%E9%A1%B9%E7%9B%AE%E7%9A%84%E5%AE%9E%E8%B7%B5/:1:1","tags":["makefile","go编程技巧"],"title":"Makefile在go项目的实践","uri":"/makefile%E5%9C%A8go%E9%A1%B9%E7%9B%AE%E7%9A%84%E5%AE%9E%E8%B7%B5/"},{"categories":["golang"],"content":"prerequisites先决条件 先决条件，通常是文件名，多个名字用空格分隔。 它定义了一个是否进行重新构建的判断标准： 如果有任何一个先决文件发生改变（时间戳更新），就要重新构建。 result.txt: source.txt cp source.txt result.txt 上面代码中，构建 result.txt 的前置条件是 source.txt 。如果当前目录中，source.txt 已经存在，那么make result.txt可以正常运行，否则必须再写一条规则，来生成 source.txt 。 source.txt: echo \"this is the source\" \u003e source.txt 上面代码中，source.txt后面没有前置条件，就意味着它跟其他文件都无关，只要这个文件还不存在，每次调用make source.txt，它都会生成。 make result.txt make result.txt 上面命令连续执行两次make result.txt。第一次执行会先新建 source.txt，然后再新建 result.txt。第二次执行，make发现 source.txt 没有变动（时间戳晚于 result.txt），就不会执行任何操作，result.txt 也不会重新生成。 如果需要生成多个文件，往往采用下面的写法。 source: file1 file2 file3 上面代码中，source 是一个伪目标，只有三个前置文件，没有任何对应的命令。 make source 执行make source命令后，就会一次性生成 file1，file2，file3 三个文件。这比下面的写法要方便很多。 make file1 make file2 make file3 如果先决条件也是伪目标，即不是一个实实在在的、真实存在的文件，而是仅仅是一个目标的标识符。那么在当前目标执行前，就会先去先决条件对应的伪目标去递归执行。当前目标被指定make时，会先调用先决条件的目标，如果先决条件也具备这样的依赖时，也会如此递归调用下去 target: target1 target2 ​ echo \u003e “this tartget” target1: ​ echo \u003e “this is target1” target2: ​ echo \u003e “this is target2” ","date":"2022-07-16","objectID":"/makefile%E5%9C%A8go%E9%A1%B9%E7%9B%AE%E7%9A%84%E5%AE%9E%E8%B7%B5/:1:2","tags":["makefile","go编程技巧"],"title":"Makefile在go项目的实践","uri":"/makefile%E5%9C%A8go%E9%A1%B9%E7%9B%AE%E7%9A%84%E5%AE%9E%E8%B7%B5/"},{"categories":["golang"],"content":"commands命令 命令是构建目标时具体执行的指令，由一行或多行shell组成。每行命令之前必须有一个tab键缩进。如果想用其他键缩进，可以用内置变量.RECIPEPREFIX声明。 .RECIPEPREFIX = \u003e hello: \u003e echo Hello, world 需要注意的是，每行shell在一个单独的bash进程中执行，多进程间没有继承关系。 var: export name=wangpeng echo \"myname is $name\" 运行上面的构建 ，发现变量name是取不到的，因为两行shell在两个独立的bash中运行。 最直接的方法就是将两行shell写到一行中， var: export name=wangpeng; echo \"myname is $name\" 第二种办法，在换行前加反斜杠\\转义， var: export name=wangpeng \\ echo \"myname is $name\" 还有第三种办法是使用。ONESHELL内置命令。 .ONESHELL: var: export name=wangpeng echo \"myname is $name\" ","date":"2022-07-16","objectID":"/makefile%E5%9C%A8go%E9%A1%B9%E7%9B%AE%E7%9A%84%E5%AE%9E%E8%B7%B5/:1:3","tags":["makefile","go编程技巧"],"title":"Makefile在go项目的实践","uri":"/makefile%E5%9C%A8go%E9%A1%B9%E7%9B%AE%E7%9A%84%E5%AE%9E%E8%B7%B5/"},{"categories":["golang"],"content":"文件语法 ","date":"2022-07-16","objectID":"/makefile%E5%9C%A8go%E9%A1%B9%E7%9B%AE%E7%9A%84%E5%AE%9E%E8%B7%B5/:2:0","tags":["makefile","go编程技巧"],"title":"Makefile在go项目的实践","uri":"/makefile%E5%9C%A8go%E9%A1%B9%E7%9B%AE%E7%9A%84%E5%AE%9E%E8%B7%B5/"},{"categories":["golang"],"content":"注释 行首井号（#）表示注释。 ","date":"2022-07-16","objectID":"/makefile%E5%9C%A8go%E9%A1%B9%E7%9B%AE%E7%9A%84%E5%AE%9E%E8%B7%B5/:2:1","tags":["makefile","go编程技巧"],"title":"Makefile在go项目的实践","uri":"/makefile%E5%9C%A8go%E9%A1%B9%E7%9B%AE%E7%9A%84%E5%AE%9E%E8%B7%B5/"},{"categories":["golang"],"content":"回显 回显是指，在执行到每行命令前，将命令本身打印出来。 test: # 这是测试 执行上面构建会输出 make test # 这是测试 在命令的前面加上@，就可以关闭回声。 test: @# 这是测试 这下构建时就不会有任何输出。 ","date":"2022-07-16","objectID":"/makefile%E5%9C%A8go%E9%A1%B9%E7%9B%AE%E7%9A%84%E5%AE%9E%E8%B7%B5/:2:2","tags":["makefile","go编程技巧"],"title":"Makefile在go项目的实践","uri":"/makefile%E5%9C%A8go%E9%A1%B9%E7%9B%AE%E7%9A%84%E5%AE%9E%E8%B7%B5/"},{"categories":["golang"],"content":"通配符 Makefile 的通配符与 Bash 一致，主要有星号（）、问号（？）.比如 .text 表示所有后缀名为text的文件。 ","date":"2022-07-16","objectID":"/makefile%E5%9C%A8go%E9%A1%B9%E7%9B%AE%E7%9A%84%E5%AE%9E%E8%B7%B5/:2:3","tags":["makefile","go编程技巧"],"title":"Makefile在go项目的实践","uri":"/makefile%E5%9C%A8go%E9%A1%B9%E7%9B%AE%E7%9A%84%E5%AE%9E%E8%B7%B5/"},{"categories":["golang"],"content":"模式匹配 Make命令允许对文件名，进行类似正则运算的匹配，主要用到的匹配符是%。比如，假定当前目录下有 f1.c 和 f2.c 两个源码文件，需要将它们编译为对应的对象文件。 %.o: %.c 等同于下面的写法。 f1.o: f1.c f2.o: f2.c 使用匹配符%，可以将大量同类型的文件，只用一条规则就完成构建。 ","date":"2022-07-16","objectID":"/makefile%E5%9C%A8go%E9%A1%B9%E7%9B%AE%E7%9A%84%E5%AE%9E%E8%B7%B5/:2:4","tags":["makefile","go编程技巧"],"title":"Makefile在go项目的实践","uri":"/makefile%E5%9C%A8go%E9%A1%B9%E7%9B%AE%E7%9A%84%E5%AE%9E%E8%B7%B5/"},{"categories":["golang"],"content":"变量和赋值符 Makefile 允许自定义变量。 txt = Hello World test: @echo $(txt) 调用shell中的变量，需要使用两个美元符号$$。 Makefile一共提供了四个赋值运算符 （=、:=、?=、+=），它们的区别请看StackOverflow。 VARIABLE = value # 在执行时扩展，允许递归扩展。 VARIABLE := value # 在定义时扩展。 VARIABLE ?= value # 只有在该变量为空时才设置值。 VARIABLE += value # 将值追加到变量的尾端。 ","date":"2022-07-16","objectID":"/makefile%E5%9C%A8go%E9%A1%B9%E7%9B%AE%E7%9A%84%E5%AE%9E%E8%B7%B5/:2:5","tags":["makefile","go编程技巧"],"title":"Makefile在go项目的实践","uri":"/makefile%E5%9C%A8go%E9%A1%B9%E7%9B%AE%E7%9A%84%E5%AE%9E%E8%B7%B5/"},{"categories":["golang"],"content":"内置变量 Make命令提供一系列内置变量，比如，$(CC)指向当前使用的编译器，$(MAKE) 指向当前使用的Make工具。这主要是为了跨平台的兼容性，详细的内置变量清单见手册。 output: $(CC) -o output input.c ","date":"2022-07-16","objectID":"/makefile%E5%9C%A8go%E9%A1%B9%E7%9B%AE%E7%9A%84%E5%AE%9E%E8%B7%B5/:2:6","tags":["makefile","go编程技巧"],"title":"Makefile在go项目的实践","uri":"/makefile%E5%9C%A8go%E9%A1%B9%E7%9B%AE%E7%9A%84%E5%AE%9E%E8%B7%B5/"},{"categories":["golang"],"content":"判断和循环 Makefile使用 Bash 语法，完成判断和循环。 ifeq ($(CC),gcc) libs=$(libs_for_gcc) else libs=$(normal_libs) endif 上面代码判断当前编译器是否 gcc ，然后指定不同的库文件。 LIST = one two three all: for i in $(LIST); do \\ echo $$i; \\ done # 等同于 all: for i in one two three; do \\ echo $i; \\ done 上面代码的运行结果。 one two three Makefile例子演示 ","date":"2022-07-16","objectID":"/makefile%E5%9C%A8go%E9%A1%B9%E7%9B%AE%E7%9A%84%E5%AE%9E%E8%B7%B5/:2:7","tags":["makefile","go编程技巧"],"title":"Makefile在go项目的实践","uri":"/makefile%E5%9C%A8go%E9%A1%B9%E7%9B%AE%E7%9A%84%E5%AE%9E%E8%B7%B5/"},{"categories":["golang"],"content":"1. 执行多个目标 .PHONY: cleanall cleanobj cleandiff cleanall: cleanobj cleandiff rm all cleanobj: rm *.o cleandiff: rm *.diff 上面代码可以调用不同目标，删除不同后缀名的文件，也可以调用一个目标（cleanall），删除所有指定类型的文件。 ","date":"2022-07-16","objectID":"/makefile%E5%9C%A8go%E9%A1%B9%E7%9B%AE%E7%9A%84%E5%AE%9E%E8%B7%B5/:3:0","tags":["makefile","go编程技巧"],"title":"Makefile在go项目的实践","uri":"/makefile%E5%9C%A8go%E9%A1%B9%E7%9B%AE%E7%9A%84%E5%AE%9E%E8%B7%B5/"},{"categories":["golang"],"content":"2. 构建golang项目 以下Makefile仅供参考，项目仓库 # 伪目标，如果未指定终极目标，将默认使用Makefile里的第一个目标， # 即Makefile里最靠前的规则（本Makefile是all这个目标） # 因此，当执行make时，会默认第一个目标为终极目标，所以all后面的依赖都会执行下去 # 因此，可以将all后面放置一些必须编译的默认选项，在只使用make时；当然也可以使用make指定终极目标进行构建 .PHONY: openssl format build build_linux build_win clean swag \\ docker-build help format test run all: openssl format build # 声明编译项目的文件名 BUILD_NAME=web_app # swagger接口文档初始化 swag: @swag init # 在./config目录，签发自建的tls证书 # 或者使用go标准库：go run $GOROOT/src/crypto/tls/generate_cert.go --host localhost openssl: @openssl genrsa -out ./config/key.pem 2048;openssl req -new -x509 -key ./config/key.pem -out ./config/cert.pem -days 3650 # 使用Dockerfile对项目打包编译出镜像 docker-build: swag @docker build -t ${BUILD_NAME}:1.0 # 格式化项目 format: @go fmt ./ @go vet ./ # 测试代码 test: swag @go test -v #回归测试 # 直接运行项目根目录下已经编译好的二进制文件 run: ./${BUILD_NAME} # 默认编译 build: test @go build -o ${BUILD_NAME} ${SOURCE} # 交叉编译--适应linux系统 build_linux: test @CGO_ENABLED=0 GOOS=linux GOARCH=amd64 go build -o ${BUILD_NAME} . # 交叉编译--适应windows系统 build_win: test @CGO_ENABLED=0 GOOS=windows GOARCH=amd64 go build -o ${BUILD_NAME} . # 清除编译文件 clean: @go clean # 帮助命令 help: @echo \"make - 格式化 Go 代码、更新swagger文档、生成tls证书、测试代码、编译生成二进制文件\" @echo \"make docker-build - 构建本项目的Docker镜像\" @echo \"make build - 编译 Go 代码, 生成当前环境默认的二进制文件\" @echo \"make build_linux - 编译 Go 代码, 生成linux环境二进制文件\" @echo \"make build_win - 编译 Go 代码, 生成windows环境二进制文件\" @echo \"make run - 直接运行 Go 代码\" @echo \"make clean - 移除二进制文件和 vim swap files\" @echo \"make format - 运行 Go 工具 'fmt' and 'vet'\" 另一个例子： .PHONY: all build clean run check cover lint docker help BIN_FILE=hello all: check build build: @go build -o \"${BIN_FILE}\" clean: @go clean rm --force \"xx.out\" test: @go test check: @go fmt ./ @go vet ./ cover: @go test -coverprofile xx.out @go tool cover -html=xx.out run: ./\"${BIN_FILE}\" lint: golangci-lint run --enable-all docker: @docker build -t leo/hello:latest . help: @echo \"make 格式化go代码 并编译生成二进制文件\" @echo \"make build 编译go代码生成二进制文件\" @echo \"make clean 清理中间目标文件\" @echo \"make test 执行测试case\" @echo \"make check 格式化go代码\" @echo \"make cover 检查测试覆盖率\" @echo \"make run 直接运行程序\" @echo \"make lint 执行代码检查\" @echo \"make docker 构建docker镜像\" 这样就很方便地通过一个make命令完成对项目的构建。 ","date":"2022-07-16","objectID":"/makefile%E5%9C%A8go%E9%A1%B9%E7%9B%AE%E7%9A%84%E5%AE%9E%E8%B7%B5/:4:0","tags":["makefile","go编程技巧"],"title":"Makefile在go项目的实践","uri":"/makefile%E5%9C%A8go%E9%A1%B9%E7%9B%AE%E7%9A%84%E5%AE%9E%E8%B7%B5/"},{"categories":["golang"],"content":"Go语言内置的flag包实现了命令行参数的解析，flag包使得开发命令行工具更为简单。 os.Args 如果你只是简单的想要获取命令行参数，可以像下面的代码示例一样使用os.Args来获取命令行参数。 package main import ( \"fmt\" \"os\" ) //os.Args demo func main() { //os.Args是一个[]string if len(os.Args) \u003e 0 { for index, arg := range os.Args { fmt.Printf(\"args[%d]=%v\\n\", index, arg) } } } 将上面的代码执行go build -o \"args_demo\"编译之后，执行： $ ./args_demo a b c d args[0]=./args_demo args[1]=a args[2]=b args[3]=c args[4]=d os.Args是一个存储命令行参数的字符串切片，它的第一个元素是执行文件的名称。 flag包基本使用 本文介绍了flag包的常用函数和基本用法，更详细的内容请查看官方文档。 ","date":"2022-07-16","objectID":"/go%E8%AF%AD%E8%A8%80%E6%A0%87%E5%87%86%E5%BA%93flag%E5%9F%BA%E6%9C%AC%E4%BD%BF%E7%94%A8/:0:0","tags":["go编程技巧"],"title":"Go语言标准库flag基本使用","uri":"/go%E8%AF%AD%E8%A8%80%E6%A0%87%E5%87%86%E5%BA%93flag%E5%9F%BA%E6%9C%AC%E4%BD%BF%E7%94%A8/"},{"categories":["golang"],"content":"导入flag包 import flag ","date":"2022-07-16","objectID":"/go%E8%AF%AD%E8%A8%80%E6%A0%87%E5%87%86%E5%BA%93flag%E5%9F%BA%E6%9C%AC%E4%BD%BF%E7%94%A8/:1:0","tags":["go编程技巧"],"title":"Go语言标准库flag基本使用","uri":"/go%E8%AF%AD%E8%A8%80%E6%A0%87%E5%87%86%E5%BA%93flag%E5%9F%BA%E6%9C%AC%E4%BD%BF%E7%94%A8/"},{"categories":["golang"],"content":"flag参数类型 flag包支持的命令行参数类型有bool、int、int64、uint、uint64、float float64、string、duration。 flag参数 有效值 字符串flag 合法字符串 整数flag 1234、0664、0x1234等类型，也可以是负数。 浮点数flag 合法浮点数 bool类型flag 1, 0, t, f, T, F, true, false, TRUE, FALSE, True, False。 时间段flag 任何合法的时间段字符串。如”300ms”、”-1.5h”、”2h45m”。 合法的单位有”ns”、”us” 、“µs”、”ms”、”s”、”m”、”h”。 ","date":"2022-07-16","objectID":"/go%E8%AF%AD%E8%A8%80%E6%A0%87%E5%87%86%E5%BA%93flag%E5%9F%BA%E6%9C%AC%E4%BD%BF%E7%94%A8/:2:0","tags":["go编程技巧"],"title":"Go语言标准库flag基本使用","uri":"/go%E8%AF%AD%E8%A8%80%E6%A0%87%E5%87%86%E5%BA%93flag%E5%9F%BA%E6%9C%AC%E4%BD%BF%E7%94%A8/"},{"categories":["golang"],"content":"定义命令行flag参数 有以下两种常用的定义命令行flag参数的方法。 ","date":"2022-07-16","objectID":"/go%E8%AF%AD%E8%A8%80%E6%A0%87%E5%87%86%E5%BA%93flag%E5%9F%BA%E6%9C%AC%E4%BD%BF%E7%94%A8/:3:0","tags":["go编程技巧"],"title":"Go语言标准库flag基本使用","uri":"/go%E8%AF%AD%E8%A8%80%E6%A0%87%E5%87%86%E5%BA%93flag%E5%9F%BA%E6%9C%AC%E4%BD%BF%E7%94%A8/"},{"categories":["golang"],"content":"flag.Type() 基本格式如下： flag.Type(flag名, 默认值, 帮助信息)*Type 例如我们要定义姓名、年龄、婚否三个命令行参数，我们可以按如下方式定义： name := flag.String(\"name\", \"张三\", \"姓名\") age := flag.Int(\"age\", 18, \"年龄\") married := flag.Bool(\"married\", false, \"婚否\") delay := flag.Duration(\"d\", 0, \"时间间隔\") 需要注意的是，此时name、age、married、delay均为对应类型的指针。 ","date":"2022-07-16","objectID":"/go%E8%AF%AD%E8%A8%80%E6%A0%87%E5%87%86%E5%BA%93flag%E5%9F%BA%E6%9C%AC%E4%BD%BF%E7%94%A8/:3:1","tags":["go编程技巧"],"title":"Go语言标准库flag基本使用","uri":"/go%E8%AF%AD%E8%A8%80%E6%A0%87%E5%87%86%E5%BA%93flag%E5%9F%BA%E6%9C%AC%E4%BD%BF%E7%94%A8/"},{"categories":["golang"],"content":"flag.TypeVar() 基本格式如下： flag.TypeVar(Type指针, flag名, 默认值, 帮助信息) 例如我们要定义姓名、年龄、婚否三个命令行参数，我们可以按如下方式定义： var name string var age int var married bool var delay time.Duration flag.StringVar(\u0026name, \"name\", \"张三\", \"姓名\") flag.IntVar(\u0026age, \"age\", 18, \"年龄\") flag.BoolVar(\u0026married, \"married\", false, \"婚否\") flag.DurationVar(\u0026delay, \"d\", 0, \"时间间隔\") ","date":"2022-07-16","objectID":"/go%E8%AF%AD%E8%A8%80%E6%A0%87%E5%87%86%E5%BA%93flag%E5%9F%BA%E6%9C%AC%E4%BD%BF%E7%94%A8/:3:2","tags":["go编程技巧"],"title":"Go语言标准库flag基本使用","uri":"/go%E8%AF%AD%E8%A8%80%E6%A0%87%E5%87%86%E5%BA%93flag%E5%9F%BA%E6%9C%AC%E4%BD%BF%E7%94%A8/"},{"categories":["golang"],"content":"flag.Parse() 通过以上两种方法定义好命令行flag参数后，需要通过调用flag.Parse()来对命令行参数进行解析。 支持的命令行参数格式有以下几种： -flag xxx （使用空格，一个-符号） --flag xxx （使用空格，两个-符号） -flag=xxx （使用等号，一个-符号） --flag=xxx （使用等号，两个-符号） 其中，布尔类型的参数必须使用等号的方式指定。 Flag解析在第一个非flag参数（单个”-“不是flag参数）之前停止，或者在终止符”–“之后停止。 ","date":"2022-07-16","objectID":"/go%E8%AF%AD%E8%A8%80%E6%A0%87%E5%87%86%E5%BA%93flag%E5%9F%BA%E6%9C%AC%E4%BD%BF%E7%94%A8/:4:0","tags":["go编程技巧"],"title":"Go语言标准库flag基本使用","uri":"/go%E8%AF%AD%E8%A8%80%E6%A0%87%E5%87%86%E5%BA%93flag%E5%9F%BA%E6%9C%AC%E4%BD%BF%E7%94%A8/"},{"categories":["golang"],"content":"flag其他函数 flag.Args() ////返回命令行参数后的其他参数，以[]string类型 flag.NArg() //返回命令行参数后的其他参数个数 flag.NFlag() //返回使用的命令行参数个数 ","date":"2022-07-16","objectID":"/go%E8%AF%AD%E8%A8%80%E6%A0%87%E5%87%86%E5%BA%93flag%E5%9F%BA%E6%9C%AC%E4%BD%BF%E7%94%A8/:5:0","tags":["go编程技巧"],"title":"Go语言标准库flag基本使用","uri":"/go%E8%AF%AD%E8%A8%80%E6%A0%87%E5%87%86%E5%BA%93flag%E5%9F%BA%E6%9C%AC%E4%BD%BF%E7%94%A8/"},{"categories":["golang"],"content":"完整示例 ","date":"2022-07-16","objectID":"/go%E8%AF%AD%E8%A8%80%E6%A0%87%E5%87%86%E5%BA%93flag%E5%9F%BA%E6%9C%AC%E4%BD%BF%E7%94%A8/:6:0","tags":["go编程技巧"],"title":"Go语言标准库flag基本使用","uri":"/go%E8%AF%AD%E8%A8%80%E6%A0%87%E5%87%86%E5%BA%93flag%E5%9F%BA%E6%9C%AC%E4%BD%BF%E7%94%A8/"},{"categories":["golang"],"content":"定义 func main() { //定义命令行参数方式1 var name string var age int var married bool var delay time.Duration flag.StringVar(\u0026name, \"name\", \"张三\", \"姓名\") flag.IntVar(\u0026age, \"age\", 18, \"年龄\") flag.BoolVar(\u0026married, \"married\", false, \"婚否\") flag.DurationVar(\u0026delay, \"d\", 0, \"延迟的时间间隔\") //解析命令行参数 flag.Parse() fmt.Println(name, age, married, delay) //返回命令行参数后的其他参数 fmt.Println(flag.Args()) //返回命令行参数后的其他参数个数 fmt.Println(flag.NArg()) //返回使用的命令行参数个数 fmt.Println(flag.NFlag()) } ","date":"2022-07-16","objectID":"/go%E8%AF%AD%E8%A8%80%E6%A0%87%E5%87%86%E5%BA%93flag%E5%9F%BA%E6%9C%AC%E4%BD%BF%E7%94%A8/:6:1","tags":["go编程技巧"],"title":"Go语言标准库flag基本使用","uri":"/go%E8%AF%AD%E8%A8%80%E6%A0%87%E5%87%86%E5%BA%93flag%E5%9F%BA%E6%9C%AC%E4%BD%BF%E7%94%A8/"},{"categories":["golang"],"content":"使用 命令行参数使用提示： $ ./flag_demo -help Usage of ./flag_demo: -age int 年龄 (default 18) -d duration 时间间隔 -married 婚否 -name string 姓名 (default \"张三\") 正常使用命令行flag参数： $ ./flag_demo -name 沙河娜扎 --age 28 -married=false -d=1h30m 沙河娜扎 28 false 1h30m0s [] 0 4 使用非flag命令行参数： $ ./flag_demo a b c 张三 18 false 0s [a b c] 3 0 ","date":"2022-07-16","objectID":"/go%E8%AF%AD%E8%A8%80%E6%A0%87%E5%87%86%E5%BA%93flag%E5%9F%BA%E6%9C%AC%E4%BD%BF%E7%94%A8/:6:2","tags":["go编程技巧"],"title":"Go语言标准库flag基本使用","uri":"/go%E8%AF%AD%E8%A8%80%E6%A0%87%E5%87%86%E5%BA%93flag%E5%9F%BA%E6%9C%AC%E4%BD%BF%E7%94%A8/"},{"categories":["golang"],"content":"Golang创建最简单的HTTP和HTTPS服务 ","date":"2022-07-16","objectID":"/https%E5%9C%A8golang%E7%9A%84%E5%AE%9E%E8%B7%B5/:1:0","tags":["https"],"title":"Https在golang的实践","uri":"/https%E5%9C%A8golang%E7%9A%84%E5%AE%9E%E8%B7%B5/"},{"categories":["golang"],"content":"HTTP服务 HTTP是基于传输层TCP协议的。 package main import ( \"net/http\" \"fmt\" \"log\" ) func main() { http.HandleFunc(\"/\", func(w http.ResponseWriter, r *http.Request){ fmt.Fprint(w, \"Hello world\") }) log.Fatal(http.ListenAndServe(\":5001\", nil)) } ","date":"2022-07-16","objectID":"/https%E5%9C%A8golang%E7%9A%84%E5%AE%9E%E8%B7%B5/:1:1","tags":["https"],"title":"Https在golang的实践","uri":"/https%E5%9C%A8golang%E7%9A%84%E5%AE%9E%E8%B7%B5/"},{"categories":["golang"],"content":"HTTPS服务 HTTPS服务不同于HTTP服务，HTTPS是HTTP over SSL或HTTP over TLS。 SSL是“Secure Sockets Layer”的缩写，中文叫做“安全套接层”。它是在上世纪90年代中期，由NetScape公司设计的。为啥要发明 SSL 这个协议捏？因为原先互联网上使用的 HTTP 协议是明文的，存在很多缺点——比如传输内容会被偷窥（嗅探）和篡改。发明 SSL 协议，就是为了解决这些问题。 到了1999年，SSL 因为应用广泛，已经成为互联网上的事实标准。IETF 就在那年把 SSL 标准化。标准化之后的名称改为TLS是“Transport Layer Security”的缩写，中文叫做“传输层安全协议”。 很多相关的文章都把这两者并列称呼（SSL/TLS），因为这两者可以视作同一个东西的不同阶段。参考 要启用HTTPS首先需要创建私钥和证书。 有两种方式生成私钥和证书： OpenSSL方式，生成私钥key.pem和证书cert.pem，3650代表有效期为10年 其他OpenSSL使用方式 openssl genrsa -out key.pem 2048 openssl req -new -x509 -key key.pem -out cert.pem -days 3650 Golang标准库crypto/tls里有generate_cert.go，可以生成私钥key.pem和证书cert.pem，host参数是必须的，需要注意的是默认有效期是1年 go run $GOROOT/src/crypto/tls/generate_cert.go --host localhost 将生成的key.pem、cert.pem和以下代码放在同一目录下 package main import ( \"log\" \"net/http\" ) func handler(w http.ResponseWriter, req *http.Request) { w.Header().Set(\"Content-Type\", \"text/plain\") w.Write([]byte(\"This is an example server.\\n\")) } func main() { http.HandleFunc(\"/\", handler) log.Printf(\"About to listen on 10443. Go to https://127.0.0.1:10443/\") // One can use generate_cert.go in crypto/tls to generate cert.pem and key.pem. // ListenAndServeTLS always returns a non-nil error. err := http.ListenAndServeTLS(\":10443\", \"cert.pem\", \"key.pem\", nil) log.Fatal(err) } 当然鉴于以上go的https方式，是自建证书，并不是CA机构签发的证书，虽然也使用了https加密，但是不是权威机构，浏览器访问时，会提示不安全，容易被中间人劫持。 ","date":"2022-07-16","objectID":"/https%E5%9C%A8golang%E7%9A%84%E5%AE%9E%E8%B7%B5/:1:2","tags":["https"],"title":"Https在golang的实践","uri":"/https%E5%9C%A8golang%E7%9A%84%E5%AE%9E%E8%B7%B5/"},{"categories":["golang"],"content":"https原理 ","date":"2022-07-16","objectID":"/https%E5%9C%A8golang%E7%9A%84%E5%AE%9E%E8%B7%B5/:1:3","tags":["https"],"title":"Https在golang的实践","uri":"/https%E5%9C%A8golang%E7%9A%84%E5%AE%9E%E8%B7%B5/"},{"categories":["golang"],"content":"HTTPS 随着 HTTPS 建站的成本下降，现在大部分的网站都已经开始用上 HTTPS 协议。大家都知道 HTTPS 比 HTTP 安全，也听说过与 HTTPS 协议相关的概念有 SSL 、非对称加密、 CA证书等，但对于以下灵魂三拷问可能就答不上了： 1.为什么用了 HTTPS 就是安全的？ 2.HTTPS 的底层原理如何实现？ 3.用了 HTTPS 就一定安全吗？ 本文将层层深入，从原理上把 HTTPS 的安全性讲透。 ","date":"2022-07-16","objectID":"/https%E5%9C%A8golang%E7%9A%84%E5%AE%9E%E8%B7%B5/:2:0","tags":["https"],"title":"Https在golang的实践","uri":"/https%E5%9C%A8golang%E7%9A%84%E5%AE%9E%E8%B7%B5/"},{"categories":["golang"],"content":"HTTPS 的实现原理 大家可能都听说过 HTTPS 协议之所以是安全的是因为 HTTPS 协议会对传输的数据进行加密，而加密过程是使用了非对称加密实现。但其实，HTTPS 在内容传输的加密上使用的是对称加密，非对称加密只作用在证书验证阶段。 HTTPS的整体过程分为证书验证和数据传输阶段，具体的交互过程如下： ① 证书验证阶段 浏览器发起 HTTPS 请求 服务端返回 HTTPS 证书 客户端验证证书是否合法，如果不合法则提示告警 ② 数据传输阶段 当证书验证合法后，在本地生成随机数 通过公钥加密随机数，并把加密后的随机数传输到服务端 服务端通过私钥对随机数进行解密 服务端通过客户端传入的随机数构造对称加密算法，对返回结果内容进行加密后传输 ","date":"2022-07-16","objectID":"/https%E5%9C%A8golang%E7%9A%84%E5%AE%9E%E8%B7%B5/:3:0","tags":["https"],"title":"Https在golang的实践","uri":"/https%E5%9C%A8golang%E7%9A%84%E5%AE%9E%E8%B7%B5/"},{"categories":["golang"],"content":"为什么数据传输是用对称加密？ 首先，非对称加密的加解密效率是非常低的，而 http 的应用场景中通常端与端之间存在大量的交互，非对称加密的效率是无法接受的； 另外，在 HTTPS 的场景中只有服务端保存了私钥，一对公私钥只能实现单向的加解密，所以 HTTPS 中内容传输加密采取的是对称加密，而不是非对称加密。 ","date":"2022-07-16","objectID":"/https%E5%9C%A8golang%E7%9A%84%E5%AE%9E%E8%B7%B5/:3:1","tags":["https"],"title":"Https在golang的实践","uri":"/https%E5%9C%A8golang%E7%9A%84%E5%AE%9E%E8%B7%B5/"},{"categories":["golang"],"content":"为什么需要 CA 认证机构颁发证书？ HTTP 协议被认为不安全是因为传输过程容易被监听者勾线监听、伪造服务器，而 HTTPS 协议主要解决的便是网络传输的安全性问题。 首先我们假设不存在认证机构，任何人都可以制作证书，这带来的安全风险便是经典的“中间人攻击”问题。 “中间人攻击”的具体过程如下： 过程原理： 1.本地请求被劫持（如DNS劫持等），所有请求均发送到中间人的服务器 2.中间人服务器返回中间人自己的证书 3.客户端创建随机数，通过中间人证书的公钥对随机数加密后传送给中间人，然后凭随机数构造对称加密对传输内容进行加密传输 4.中间人因为拥有客户端的随机数，可以通过对称加密算法进行内容解密 5.中间人以客户端的请求内容再向正规网站发起请求 6.因为中间人与服务器的通信过程是合法的，正规网站通过建立的安全通道返回加密后的数据 7.中间人凭借与正规网站建立的对称加密算法对内容进行解密 8.中间人通过与客户端建立的对称加密算法对正规内容返回的数据进行加密传输 9.客户端通过与中间人建立的对称加密算法对返回结果数据进行解密 由于缺少对证书的验证，所以客户端虽然发起的是 HTTPS 请求，但客户端完全不知道自己的网络已被拦截，传输内容被中间人全部窃取。 推荐阅读：用户密码到底要怎么加密存储？ ","date":"2022-07-16","objectID":"/https%E5%9C%A8golang%E7%9A%84%E5%AE%9E%E8%B7%B5/:3:2","tags":["https"],"title":"Https在golang的实践","uri":"/https%E5%9C%A8golang%E7%9A%84%E5%AE%9E%E8%B7%B5/"},{"categories":["golang"],"content":"浏览器是如何确保 CA 证书的合法性？ 1. 证书包含什么信息？ 颁发机构信息 公钥 公司信息 域名 有效期 指纹 …… 2. 证书的合法性依据是什么？ 首先，权威机构是要有认证的，不是随便一个机构都有资格颁发证书，不然也不叫做权威机构。 另外，证书的可信性基于信任制，权威机构需要对其颁发的证书进行信用背书，只要是权威机构生成的证书，我们就认为是合法的。 所以权威机构会对申请者的信息进行审核，不同等级的权威机构对审核的要求也不一样，于是证书也分为免费的、便宜的和贵的。 3. 浏览器如何验证证书的合法性？ 浏览器发起 HTTPS 请求时，服务器会返回网站的 SSL 证书，浏览器需要对证书做以下验证： 验证域名、有效期等信息是否正确。证书上都有包含这些信息，比较容易完成验证； 判断证书来源是否合法。每份签发证书都可以根据验证链查找到对应的根证书，操作系统、浏览器会在本地存储权威机构的根证书，利用本地根证书可以对对应机构签发证书完成来源验证； 判断证书是否被篡改。需要与 CA 服务器进行校验； 判断证书是否已吊销。通过CRL（Certificate Revocation List 证书注销列表）和 OCSP（Online Certificate Status Protocol 在线证书状态协议）实现，其中 OCSP 可用于第3步中以减少与 CA 服务器的交互，提高验证效率 以上任意一步都满足的情况下浏览器才认为证书是合法的。 这里插一个我想了很久的但其实答案很简单的问题： 既然证书是公开的，如果要发起中间人攻击，我在官网上下载一份证书作为我的服务器证书，那客户端肯定会认同这个证书是合法的，如何避免这种证书冒用的情况？ 其实这就是非加密对称中公私钥的用处，虽然中间人可以得到证书，但私钥是无法获取的，一份公钥是不可能推算出其对应的私钥，中间人即使拿到证书也无法伪装成合法服务端，因为无法对客户端传入的加密数据进行解密。 4. 只有认证机构可以生成证书吗？ 如果需要浏览器不提示安全风险，那只能使用认证机构签发的证书。但浏览器通常只是提示安全风险，并不限制网站不能访问，所以从技术上谁都可以生成证书，只要有证书就可以完成网站的 HTTPS 传输。例如早期的 12306 采用的便是手动安装私有证书的形式实现 HTTPS 访问。 推荐阅读：12306 的架构也太 “牛X” 了吧！ ","date":"2022-07-16","objectID":"/https%E5%9C%A8golang%E7%9A%84%E5%AE%9E%E8%B7%B5/:3:3","tags":["https"],"title":"Https在golang的实践","uri":"/https%E5%9C%A8golang%E7%9A%84%E5%AE%9E%E8%B7%B5/"},{"categories":["golang"],"content":"本地随机数被窃取怎么办？ 证书验证是采用非对称加密实现，但是传输过程是采用对称加密，而其中对称加密算法中重要的随机数是由本地生成并且存储于本地的，HTTPS 如何保证随机数不会被窃取？ 其实 HTTPS 并不包含对随机数的安全保证，HTTPS 保证的只是传输过程安全，而随机数存储于本地，本地的安全属于另一安全范畴，应对的措施有安装杀毒软件、反木马、浏览器升级修复漏洞等。 ","date":"2022-07-16","objectID":"/https%E5%9C%A8golang%E7%9A%84%E5%AE%9E%E8%B7%B5/:3:4","tags":["https"],"title":"Https在golang的实践","uri":"/https%E5%9C%A8golang%E7%9A%84%E5%AE%9E%E8%B7%B5/"},{"categories":["golang"],"content":"用了 HTTPS 会被抓包吗？ HTTPS 的数据是加密的，常规下抓包工具代理请求后抓到的包内容是加密状态，无法直接查看。关注微信公众号：Java技术栈，在后台回复：工具，可以获取我整理的 N 篇最新开发工具教程，都是干货。 但是，正如前文所说，浏览器只会提示安全风险，如果用户授权仍然可以继续访问网站，完成请求。因此，只要客户端是我们自己的终端，我们授权的情况下，便可以组建中间人网络，而抓包工具便是作为中间人的代理。 通常 HTTPS 抓包工具的使用方法是会生成一个证书，用户需要手动把证书安装到客户端中，然后终端发起的所有请求通过该证书完成与抓包工具的交互，然后抓包工具再转发请求到服务器，最后把服务器返回的结果在控制台输出后再返回给终端，从而完成整个请求的闭环。 既然 HTTPS 不能防抓包，那 HTTPS 有什么意义？ A: 客户端发起 HTTPS 请求，服务端返回证书，客户端对证书进行验证，验证通过后本地生成用于改造对称加密算法的随机数，通过证书中的公钥对随机数进行加密传输到服务端，服务端接收后通过私钥解密得到随机数，之后的数据交互通过对称加密算法进行加解密。 Q: 为什么需要证书？ A: 防止”中间人“攻击，同时可以为网站提供身份证明。 Q: 使用 HTTPS 会被抓包吗？ A: 会被抓包，HTTPS 只防止用户在不知情的情况下通信被监听，如果用户主动授信，是可以构建“中间人”网络，代理软件可以对传输内容进行解密。 出自腾讯云《终于有人把 HTTPS 原理讲清楚了！》1 https://cloud.tencent.com/developer/article/1601995 ↩ ↩︎ ","date":"2022-07-16","objectID":"/https%E5%9C%A8golang%E7%9A%84%E5%AE%9E%E8%B7%B5/:3:5","tags":["https"],"title":"Https在golang的实践","uri":"/https%E5%9C%A8golang%E7%9A%84%E5%AE%9E%E8%B7%B5/"},{"categories":["数据结构与算法"],"content":"[TOC] ","date":"2022-07-07","objectID":"/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95java%E7%89%88/:0:0","tags":[],"title":"数据结构与算法（java版）","uri":"/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95java%E7%89%88/"},{"categories":["数据结构与算法"],"content":"引子 数据结构包括：线性结构和非线性结构 线性结构 特点：数据元素一对一的线性关系 线性结构有两种不同的存储结构：一种是顺序存储结构，元素处于相邻地址空间，顺序存储的线性表又称为顺序表；另一种是链式存储结构，元素节点保存数据元素和相邻元素地址信息，相邻元素不一定在地址空间上连续。 常见的线性结构：数组、队列、链表和栈 非线性结构 常见的有：二维数组、多维数组、广义表、树结构、图结构 ","date":"2022-07-07","objectID":"/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95java%E7%89%88/:1:0","tags":[],"title":"数据结构与算法（java版）","uri":"/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95java%E7%89%88/"},{"categories":["数据结构与算法"],"content":"时间复杂度 时间频度：一个算法花费的时间与算法中语句的执行次数成正比例，哪个算法中语句执行次数多，它花费时间就多。**一个算法中的语句执行次数成为语句频度或时间频度。**记为T(n) (注意：是语句重复执行的次数，而不是语句条数) 时间复杂度：一般情况下，算法的基本操作语句的重复执行次数是问题规模n的某个函数，用T(n)表示，若有某个辅助函数f(n)，使得当n趋于无穷大时，T(n)/f(n)的极限值为不等于零的常熟，则成f(n)是T(n)的同数量级函数，记作**T(n)=O{f(n)}，称O{f(n)}为算法的渐进时间复杂度，简称时间复杂度**。 时间频度不同，但时间复杂度可能相同。 举例说明 T(n)=n²+7n+6 与 T(n)=3n²+2n+2 它们的T(n) 不同，但时间复杂度相同，都为O(n²) 计算时间复杂度的方法 用常数1代替运行时间中的所有加法常数 T(n) = n²+7n+6 =\u003e T(n) = n²+7n+1 修改后的运行次数函数中，只保留最高阶项 T(n) = n²+7n+1 =\u003e T(n) = n² 去除最高阶项的系数 T(n) = n² =\u003e T(n) = n² =\u003e O(n²) 常见的时间复杂度 常数阶O(1) 对数阶O(log2n)：以2为底，n为对数 线性阶O(n) 线性对数阶O(nlog2n) 平方阶O(n^2) 立方阶O(n^3) 参考上面的O(n²) 去理解就好了，O(n³)相当于三层n循环，其它的类似 k次方阶O(n^k) 参考上面的O(n²) 去理解就好了，O(n³)相当于三层n循环，其它的类似 指数阶O(2^n) 我们应该尽可能避免使用指数阶的算法 常见的算法时间复杂度由小到大依次为：Ο(1)＜Ο(log2n)＜Ο(n)＜Ο(nlog2n)＜Ο(n2)＜Ο(n3)＜ Ο(nk) ＜Ο(2n) ，随着问题规模n的不断增大，上述时间复杂度不断增大，算法的执行效率越低 平均时间复杂度和最坏时间复杂度 平均时间复杂度是指所有可能的输入实例均以等概率出现的情况下，该算法的运行时间。 最坏情况下的时间复杂度称最坏时间复杂度。一般讨论的时间复杂度均是最坏情况下的时间复杂度。 这样做的原因是：最坏情况下的时间复杂度是算法在任何输入实例上运行时间的界限，这就保证了算法的运行时间不会比最坏情况更长。 平均时间复杂度和最坏时间复杂度是否一致，和算法有关。 ","date":"2022-07-07","objectID":"/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95java%E7%89%88/:2:0","tags":[],"title":"数据结构与算法（java版）","uri":"/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95java%E7%89%88/"},{"categories":["数据结构与算法"],"content":"空间复杂度 类似于时间复杂度的讨论，一个算法的空间复杂度(Space Complexity)定义为该算法所耗费的存储空间，它也是问题规模n的函数。 空间复杂度(Space Complexity)是对一个算法在运行过程中临时占用存储空间大小的量度。有的算法需要占用的临时工作单元数与解决问题的规模n有关，它随着n的增大而增大，当n较大时，将占用较多的存储单元，例如快速排序和归并排序算法就属于这种情况 在做算法分析时，主要讨论的是时间复杂度。从用户使用体验上看，更看重的程序执行的速度。一些缓存产品(redis, memcache)和算法(基数排序)本质就是用空间换时间. ","date":"2022-07-07","objectID":"/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95java%E7%89%88/:3:0","tags":[],"title":"数据结构与算法（java版）","uri":"/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95java%E7%89%88/"},{"categories":["数据结构与算法"],"content":"一、稀疏数组 原理 原始数组之中有很多重复或无用的数据，可以通过记录大量重复数据的位置和值来代替原来的重复数据，从而达到压缩的目的。 处理方法 记录数组一共几行几列，有多少个不同的值 把具有不同数值的元素行列记录至一个新的数组中（这个数组肯定会小于原先的数组），从而达到压缩的目的。 代码 import java.util.Arrays; public class first { public static void main(String[] args) { //初始化二维数组 int[][] oldArray =new int[10][10]; oldArray[1][3]=1; oldArray[2][7]=3; oldArray[5][2]=10; System.out.println(Arrays.deepToString(oldArray)); int validNum =0; //获取不同值个数 for (int i=0;i\u003coldArray.length;i++){ for (int j=0;j\u003coldArray[i].length;j++){ //将非重复元素存到压缩数组 if (oldArray[i][j]!=0){ validNum++; } } } //初始化稀疏数组，确定深度 int[][] parseArray =new int[validNum+1][3]; int index=0; //初始化稀疏数组第一个索引，用来表示原数组的个数及值重值 parseArray[0][0]=oldArray.length; parseArray[0][1]=oldArray[0].length; parseArray[0][2]=0; for (int i=0;i\u003coldArray.length;i++){ for (int j=0;j\u003coldArray[i].length;j++){ //将非重复元素存到压缩数组 if (oldArray[i][j]!=0){ index++; //记录非零值 parseArray[index][0]=i; parseArray[index][1]=j; parseArray[index][2]=oldArray[i][j]; } } } //稀疏数组结果 System.out.println(Arrays.deepToString(parseArray)); //还原稀疏数组 //初始化稀疏数组大小 int[][] newArray =new int[parseArray[0][0]][parseArray[0][1]]; //读取稀疏数组数值 for (int i=1;i\u003cparseArray.length;i++){ newArray[parseArray[i][0]][parseArray[i][1]]=parseArray[i][2]; } System.out.println(Arrays.deepToString(newArray)); } } ","date":"2022-07-07","objectID":"/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95java%E7%89%88/:4:0","tags":[],"title":"数据结构与算法（java版）","uri":"/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95java%E7%89%88/"},{"categories":["数据结构与算法"],"content":"二、队列 ","date":"2022-07-07","objectID":"/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95java%E7%89%88/:5:0","tags":[],"title":"数据结构与算法（java版）","uri":"/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95java%E7%89%88/"},{"categories":["数据结构与算法"],"content":"1. 非循环队列(一般不使用) 原理 队列是一个有序列表，可以使用数组或者链表实现。数组的特点是相邻地址索引快，读操作效率更高；链表是链式存储，写操作效率更高 遵循先入先出的原则 示意图 处理方法（数组） 队列初始化时，默认front==rear==-1，头指针始终指向队列第一个元素的前一个索引位置（不一定存在），；尾指针始终指向队列队后一个元素的索引位置 队列元素增加：尾指针后移，rear+1，当rear=maxSize-1表示队列已满 队列元素取出：头指针后移，front-1，当front==rear表示队列已空 代码实现 public class queue { //队列深度 private int maxSize; //队头指针 private int front; //队尾指针 private int rear; //队列元素使用数组作为队列结构 private int[] arr; //添加队列元素 public void addQueue(int e){ if (isFull()){ System.out.println(\"the queue is full...\"); return; } this.rear++; this.arr[this.rear]=e; } //取出队列元素 public int getQueue(){ //判断队列是否为空 if (isEmpty()){ throw new RuntimeException(\"the queue is empty...\"); } this.front++; return this.arr[this.front]; } //显示队列数据 public void list() { for (int v : arr) { System.out.print(v+\" \"); } } //判断队列是否已满 public boolean isFull(){ return this.maxSize-1==this.rear; } //判断队列是否为空 public boolean isEmpty(){ return this.rear==this.front; } //构造队列 public queue(int maxSize) { this.maxSize = maxSize; this.front = -1; this.rear = -1; this.arr = new int[maxSize]; } } ","date":"2022-07-07","objectID":"/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95java%E7%89%88/:5:1","tags":[],"title":"数据结构与算法（java版）","uri":"/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95java%E7%89%88/"},{"categories":["数据结构与算法"],"content":"2. 循环队列 非循环队列的缺陷 非循环队列的空间不能反复利用，队列空间一旦填满不能再对队列进行写操作。其问题在于，非循环队列元素添加的实现只是简单的将队列进行后移，每添加一次元素，rear后移一个位置，直到rear==maxSize-1时，不能再往后移（队列已满)。也就是说，目前数组使用一次就不能使用了，比较浪费 但是可以通过对队列的指针取余（也叫取模），可以解决这个缺陷。内存上并没有环形的结构，因此环形队列实际上是数组的线性空间来实现的。当数据到了尾部应该怎么处理呢？它将转回到原来位置进行处理，通过取模操作来实现 所谓取模，只是将不断增大的rear通过取模（取余）的方式，将rear大小不断缩小在一个“环”里，rear沿着“环”走 原理图 处理方法 front头指针指向队列第一个元素；rear尾指针指向队列最后一个元素的后一个位置。（与非循环队列相反） 队满时，(rear+1)%maxSize==front，意即rear后一个位置的下一个元素如果是front，表示队列已满 队空时，rear==front 队列初始化时，rear==front==0 队列之中的有效数据个数：(rear+maxSize-front)%maxSize或者Math.abs(rear-front)，两者结果一样，因为rear与front的差值不可能查过maxSize 代码实现 public class circleQueue { private int maxSize; private int front; private int rear; private int[] arr; //判断队列是否为空 public boolean isEmpty(){ return this.front==this.rear; } //判断队列是否满了 public boolean isFull(){ return (this.rear+1)%maxSize==this.front; } //添加队列元素 public void addQueue(int e){ if (isFull()){ throw new RuntimeException(\"queue is full\\n\"); } this.arr[this.rear]=e; //注意取模 this.rear=(this.rear+1)%this.maxSize; } //取出队列元素 public int getQueue(){ if (isEmpty()){ throw new RuntimeException(\"queue is empty\\n\"); } int v = arr[front]; //取模 front=(front+1)%maxSize; return v; } //获取有效个数 public void list(){ for (int i = 0; i \u003c (rear + maxSize - front) % maxSize; i++) { System.out.print(arr[front+i]+\" \"); } } public circleQueue(int maxSize) { this.maxSize = maxSize; this.front=0; this.rear=0; this.arr=new int[maxSize]; } } ","date":"2022-07-07","objectID":"/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95java%E7%89%88/:5:2","tags":[],"title":"数据结构与算法（java版）","uri":"/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95java%E7%89%88/"},{"categories":["数据结构与算法"],"content":"三、链表 ","date":"2022-07-07","objectID":"/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95java%E7%89%88/:6:0","tags":[],"title":"数据结构与算法（java版）","uri":"/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95java%E7%89%88/"},{"categories":["数据结构与算法"],"content":"1. 单链表 原理 链表是以节点的方式存储的，每个节点包含data域（存放数据）、next域（指向下一个节点） 链表的各个节点地址不一定是连续 链表分类：带头节点的链表和没有头节点的链表。 原理图 head头节点：不存放数据 处理方法 创建一个head头节点。作用是表示整个链表的地址 添加一个节点就将链表的最后一个节点的next域指向这个节点 插入一个节点就先遍历链表找出需要插入的位置，然后让指针插入对应即可 删除一个节点就先遍历找到这个节点，然后将这个节点的前后节点指针相连，这个节点就没有指针指向他，在java中会被垃圾回收机制回收 代码实现 public class singleLinkedList { public static void main(String[] args) { heroNode h1 = new heroNode(1, \"刘海斌\"); heroNode h2 = new heroNode(2, \"包文杰\"); heroNode h3 = new heroNode(3, \"王铁霖\"); heroNode h4 = new heroNode(4, \"牟国柱\"); managerSingleList manager = new managerSingleList(); manager.addNode(h1); manager.addNode(h2); manager.addNode(h3); manager.addNode(h4); manager.showList(); manager.updateList(\"cold bin\", 1); manager.insertNode(new heroNode(0, \"无\"), 1); manager.showList(); manager.delNode(3); manager.showList(); } } class managerSingleList { //头节点 private heroNode head; public managerSingleList() { this.head = new heroNode(0, \"\"); } //添加 public void addNode(heroNode node) { //遍历链表至最后一个节点 heroNode temp = head; while (temp.next != null) { temp = temp.next; } //插入尾节点 temp.next = node; node.next = null; System.out.println(\"添加成功\"); } //删除 public void delNode(int no) { heroNode head = this.head; if (head.next == null) { System.out.println(\"链表为空\"); return; } heroNode temp = head; while (temp.next != null) { //记录要删除节点的前一个结点 heroNode beforeNode = temp; temp = temp.next; if (temp.no == no) { //将删除节点的前一个节点next指向删除节点的下一个节点 beforeNode.next = temp.next; System.out.println(\"删除成功\"); return; } } System.out.println(\"没有找到编号\"); } //插入：在编号之前 public void insertNode(heroNode node, int no) { heroNode head = this.head; if (head.next == null) { System.out.println(\"链表为空\"); return; } heroNode temp = head; while (temp.next != null) { heroNode beforeNode = temp; temp = temp.next; if (temp.no == no) { //记录对应编号节点的前一个节点位置，将新节点插入这个位置，意即beforeNode之后，temp之前； beforeNode.next = node; node.next = temp; System.out.println(\"插入成功\"); return; } } System.out.println(\"没有找到编号\"); } //修改 public void updateList(String newName, int no) { heroNode head = this.head; if (head.next == null) { System.out.println(\"链表为空\"); return; } heroNode temp = head; while (temp.next != null) { temp = temp.next; if (temp.no == no) { temp.name = newName; System.out.println(\"更新成功\"); return; } } System.out.println(\"没有找到编号\"); } //查询 public void showList() { heroNode head = this.head; if (head.next == null) { System.out.println(\"链表为空\"); return; } heroNode temp = head; while (temp.next != null) { //头节点无数据，因此跳过 temp = temp.next; System.out.println(temp); } } } class heroNode { int no; String name; heroNode next; public heroNode(int no, String name) { this.no = no; this.name = name; } @Override public String toString() { return \"heroNode{\" + \"no=\" + no + \", name=\" + name + '}'; } } 删除的原理：被删除的节点将不会有其他引用指向，会被垃圾回收机制回收 应用场景： 单链表的应用的话一般能用在 2 种场景下： 第 1 种基操。 在你应用的场景中，插入和删除的操作特别多，你不想因为这俩操作浪费你太多的时间，此时用单链表，可以改善插入和删除操作浪费的时间。 第 2 种骚操。在你应用的场景种，不知道有多少个元素，那这个时候你用单链表，每来一个新的元素你就链在表里，这种情况是用链表处理的绝佳方式。也是很容易被大家忽略的。 单链表常见面试题 单链表中节点的个数：遍历单链表有效节点个数（不算头节点） 单链表反转：先定义一个反转之后的头节点；然后遍历原始链表，每遍历一个链表元素就将这个链表元素插进紧跟反转链表的头节点之后的位置。然后再将原来头节点换成现在的头节点，意即，反转链表。 逆序打印单链表： 打印链表反转之后的链表，这样做的问题是会破坏单链表的结构；（不建议） 栈，将各个节点压入栈中，打印时，从栈中取出（不同的语言对栈的使用不一样，也可以考虑自己利用队列实现一个栈） ","date":"2022-07-07","objectID":"/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95java%E7%89%88/:6:1","tags":[],"title":"数据结构与算法（java版）","uri":"/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95java%E7%89%88/"},{"categories":["数据结构与算法"],"content":"2. 双链表 单链表存在的问题 单向链表查找的方向只能是一个方向（即头节点往后），而双向链表可以向前或者向后查找。 单向链表不能自我删除，需要借助辅助节点，而双向链表可以自我删除 示意图 处理方法 遍历和单链表一样，只是多了一个pre指针，可以双向遍历（向前向后查找） 添加节点，将节点的两个指针连在双向链表最后一个节点即可 修改节点，先遍历找到节点，然后改变其值 删除节点，先便利找到这个节点temp，将前节点的next指针指向后节点，后节点的pre指针指向前节点（此处可以temp.pre.next=temp.next\u0026\u0026temp.next.pre=temp.pre） 按顺序插入节点，先确定插入位置，然后插入，原理如下图（分四步，顺序不能乱） 代码实现 package linkedList; public class doublyLinkedList { public static void main(String[] args) { heroNode1 h1 = new heroNode1(1, \"刘海斌\"); heroNode1 h2 = new heroNode1(2, \"包文杰\"); heroNode1 h3 = new heroNode1(3, \"王铁霖\"); heroNode1 h4 = new heroNode1(4, \"牟国柱\"); managerHeroNode1 m =new managerHeroNode1(); m.addNode(h1); m.addNode(h2); m.addNode(h3); m.addNode(h4); m.listNode(); m.updateNode(new heroNode1(2,\"小包\")); m.insertNode(new heroNode1(3,\"邓涔浩\")); m.deleteNode(4); m.listNode(); } } class managerHeroNode1 { heroNode1 head; public managerHeroNode1() { this.head = new heroNode1(-1, \"\"); } public boolean isEmpty() { if (head.next != null) return false; else return true; } //末尾添加node public void addNode(heroNode1 node) { heroNode1 temp = head; //遍历到末尾 while (temp.next != null) { temp = temp.next; } temp.next = node; node.pre = temp; System.out.println(\"成功添加\"); } //修改节点 public void updateNode(heroNode1 node) { if (isEmpty()) { System.out.println(\"链表为空\"); return; } heroNode1 temp = head; while (temp.no != node.no) { temp = temp.next; if (temp == null) { System.out.println(\"没找到该节点\"); return; } } temp.name = node.name; System.out.println(\"修改成功:\" + temp); } //删除节点 public void deleteNode(int no) { if (isEmpty()) { System.out.println(\"链表为空\"); return; } heroNode1 temp = head; while (temp.no != no) { temp = temp.next; } //temp的下一个节点的pre指针指向temp的上一个节点 // temp.next.pre=temp.pre;//注意如果temp是最后一个节点，这里有错： if (temp.next != null) temp.next.pre = temp.pre; //temp的上一个节点的next指针指向temp的下一个节点 temp.pre.next = temp.next; System.out.println(\"删除成功:\" + temp); } //按序插入节点 public void insertNode(heroNode1 node) { if (isEmpty()) { System.out.println(\"链表为空\"); return; } heroNode1 temp = head; while (temp.no \u003c= node.no) { temp = temp.next; //如果插入节点在最后，直接添加 if (temp==null){ addNode(node); return; } } //此操作注意先后顺序，否则会因为链条一部分断裂而导致后面的节点变成null //首先 node.pre=temp; node.next=temp.next; temp.next=node; temp.next.pre=node; System.out.println(\"插入成功:\" + node); } //遍历链表 public void listNode() { if (isEmpty()) { System.out.println(\"链表为空\"); return; } heroNode1 temp = head; while (temp.next != null) { temp = temp.next; System.out.println(temp); } } } class heroNode1 { //节点data属性 int no; String name; //指针域 heroNode1 pre; heroNode1 next; public heroNode1(int no, String name) { this.no = no; this.name = name; } @Override public String toString() { return \"heroNode1{\" + \"no=\" + no + \", name='\" + name + '\\'' + '}'; } } ","date":"2022-07-07","objectID":"/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95java%E7%89%88/:6:2","tags":[],"title":"数据结构与算法（java版）","uri":"/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95java%E7%89%88/"},{"categories":["数据结构与算法"],"content":"3. 单向环形链表 原理 约瑟夫问题 在罗马人占领乔塔帕特后，39个犹太人和Josephus及他的朋友躲进一个洞里，39个犹太人决定宁愿死也不要被敌人抓到，于是决定了一个自杀方式，41个人排成一个圆圈，由第一个人开始报数，报数到3的人就自杀，再由下一个人重新报1，报数到3的人就自杀，这样依次下去，知道剩下最后一个人时，那个人可以自由选择自己的命运。这就是著名的约瑟夫问题。现在请用单向链表描述该结构并呈现整个自杀过程。 单向环形链表解决 代码实现 package linkedList; public class singleRingLinkedList { public static void main(String[] args) { managerList m = new managerList(); m.addNode(41); m.list(); System.out.println(); m.pops(1, 3, 5); } } class managerList { boyNode head; public managerList() { //头节点，标记首位置 head = new boyNode(-1); head.next = head;//注意闭环 } //添加 public void addNode(int num) { if (num \u003c 1) { System.out.println(\"参数错误\"); return; } boyNode curNode = head; for (int i = 1; i \u003c= num; i++) { boyNode node = new boyNode(i); if (i == 1) { head.no = node.no; continue; } curNode.next = node; node.next = head; //记录当前节点 curNode = node; } System.out.println(\"添加成功\"); } //遍历链表 public void list() { if (head == null) { System.out.println(\"链表为空\"); return; } boyNode curNode = head; while (curNode.next != head) { System.out.println(curNode); curNode = curNode.next; } System.out.println(curNode); } //取出节点 public void pops(int startNo, int countNum, int nums) { //先对数据进行检验 if (head==null||countNum\u003c1||countNum\u003enums){ System.out.println(\"参数输入有误\"); return; } //helper指针指向链表尾节点 boyNode helper = head; while (helper.next != head) { helper = helper.next; } //移动到报数节点 for (int i = 0; i \u003c startNo-1; i++) { helper = helper.next; head = head.next; } while (head != helper) { //报数移动countNum-1次 for (int i = 1; i \u003c countNum; i++) { helper = helper.next; head = head.next; } int number = head.no; head = head.next; helper.next = head; System.out.println(\"移除节点：\" + number); } System.out.println(\"最后的节点：\" + head.no); } } class boyNode { int no;//编号 boyNode next;//指针域 public boyNode(int no) { this.no = no; } @Override public String toString() { return \"boyNode{\" + \"no=\" + no + '}'; } } ","date":"2022-07-07","objectID":"/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95java%E7%89%88/:6:3","tags":[],"title":"数据结构与算法（java版）","uri":"/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95java%E7%89%88/"},{"categories":["数据结构与算法"],"content":"四、栈 ","date":"2022-07-07","objectID":"/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95java%E7%89%88/:7:0","tags":[],"title":"数据结构与算法（java版）","uri":"/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95java%E7%89%88/"},{"categories":["数据结构与算法"],"content":"1. 原理 栈是一个先入后出的有序列表 栈是限制线性表中元素的插入和删除只能在线性表的同一端进行的一种特殊线性表。允许插入和删除的一端，为变化的一端，称为栈顶；另一端为固定的一端，成为栈底 最先放入栈的元素在栈底，最后放入的元素在栈顶；而删除元素刚好相反，最后放入的元素最先删除，最先放入的元素最后删除 原理图 ","date":"2022-07-07","objectID":"/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95java%E7%89%88/:7:1","tags":[],"title":"数据结构与算法（java版）","uri":"/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95java%E7%89%88/"},{"categories":["数据结构与算法"],"content":"2. 代码实现（数组模拟栈） package stack; public class stackDemo { public static void main(String[] args) { managerStack m = new managerStack(5); m.push(new node(1)); m.push(new node(2)); m.push(new node(3)); m.list(); System.out.println(m.pop()); System.out.println(m.pop()); m.list(); System.out.println(m.pop()); m.push(new node(4)); m.list(); System.out.println(m.pop()); } //入栈 } class managerStack { int top; int maxSize; node[] nodes; public managerStack(int maxSize) { //初始化栈顶 top = -1; //栈的容量 this.maxSize = maxSize; //初始化栈的大小 nodes = new node[maxSize]; } //入栈 public void push(node node) { if (top \u003c maxSize - 1) { nodes[++top] = node; return; } System.out.println(\"栈溢出\"); } //出栈 public node pop() { if (top == -1) { throw new RuntimeException(\"栈空\"); } return nodes[top--]; } //遍历 public void list() { if (top == -1) { throw new RuntimeException(\"栈空\"); } System.out.println(\"栈元素：\"); for (int i = top; i \u003e= 0; i--) { System.out.printf(\"nodes[%d]=%d\\n\", i, nodes[i].no); } } } class node { int no; public node(int no) { this.no = no; } @Override public String toString() { return \"node{\" + \"no=\" + no + '}'; } } ","date":"2022-07-07","objectID":"/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95java%E7%89%88/:7:2","tags":[],"title":"数据结构与算法（java版）","uri":"/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95java%E7%89%88/"},{"categories":["数据结构与算法"],"content":"3. 波兰计算器 ① 无括号版波兰计算器 思路 针对多位数运算时的逻辑：如果扫描到数字就继续扫描下一位直到不是数字为止，将扫描的数字字符拼接成串然后转化为数字 代码实现 package stack; import java.util.Scanner; public class calculatorByStackDemo { public static void main(String[] args) { Scanner sc = new Scanner(System.in); System.out.print(\"请输入合法表达式：\"); String express = sc.next(); char[] chars = express.toCharArray(); //数字栈 stack stackNumber = new stack(10); //符号栈 stack stackOperation = new stack(10); int num1 = 0; int num2 = 0; int val = 0; int operation = 0; String number = \"\"; for (int i = 0; i \u003c chars.length; ) { //符号 if (stackOperation.isOperation(chars[i])) { //符号栈为空，将符号压入符号栈 if (stackOperation.isEmpty()) stackOperation.push(chars[i]); else if (!stackOperation.isFull()) { //如果符号栈有操作符，进行比较，如果当前操作符的优先级低于或等于栈中的操作符， //就需要从数栈中pop两个数，再从符号栈里pop出一个符号进行运算，将得到的结果压 //入数栈，然后再将当前的操作符压入符号栈中 if (stackOperation.priority(chars[i]) \u003c= stackOperation.priority(stackOperation.showTop())) { num1 = stackNumber.pop(); num2 = stackNumber.pop(); operation = stackOperation.pop(); val = stackNumber.calculate(num2, num1, operation); stackNumber.push(val); stackOperation.push(chars[i]); } else { //当前操作符优先级更高，再压入符号栈中 stackOperation.push(chars[i]); } } i++; } //如果是数字，先别慌入栈，再判断下一位是否是数字,是数字就拼接在一起 // if (stackNumber.isDigital(chars[i])){ // stackNumber.push(Character.getNumericValue(number)); // } while (stackNumber.isDigital(chars[i])) { number = number + chars[i]; i++; if (i == chars.length) break; } System.out.println(number); stackNumber.push(Integer.parseInt(number)); number = \"\";//清空上次数字残余 } //扫描完毕后，顺序从数栈和符号栈pop出两个数字和一个符号，并运算，最后数栈只有一个数字就是结果 //最后的栈中运算符都是同等优先级的运算符 while (!stackOperation.isEmpty()) { num1 = stackNumber.pop(); num2 = stackNumber.pop(); operation = stackOperation.pop(); val = stackOperation.calculate(num2, num1, operation); stackNumber.push(val); } System.out.println(\"计算的结果是：\" + stackNumber.showButton()); } } class stack { int top; int maxSize; int[] nodes; public stack(int maxSize) { top = -1; this.maxSize = maxSize; nodes = new int[maxSize]; } public boolean isEmpty() { return top == -1; } public boolean isFull() { return top == maxSize - 1; } //入栈 public void push(int node) { if (isFull()) throw new RuntimeException(\"栈溢出\"); nodes[++top] = node; } //出栈 public int pop() { if (isEmpty()) throw new RuntimeException(\"栈空\"); return nodes[top--]; } //显示栈顶元素，但不取出 public int showTop() { return nodes[top]; } public int showButton() { return nodes[0]; } //优先级 public int priority(int operation) { if (operation == '*' || operation == '/') return 1; else if (operation == '+' || operation == '-') return 0; return -1; } //判断是不是操作符 public boolean isOperation(int operation) { return operation == '+' || operation == '-' || operation == '*' || operation == '/'; } //判断是不是数字 public boolean isDigital(int number) { return Character.isDigit(number); } //计算方法 public int calculate(int num1, int num2, int operation) { switch (operation) { case '+': return num1 + num2; case '-': return num1 - num2; case '*': return num1 * num2; case '/': return num1 / num2; default: return 0; } } } ② 前缀表达式（波兰表达式） 前缀表达式的运算符位于操作数之前。如(3+4)*5-6这样的中缀表达式对应的前缀表达式-*+3456 前缀表达式的计算机求值：首先从右至左扫描表达式，遇到数字则将数字压入堆栈，遇到运算符则弹出栈顶的两个数字，用运算符对这两数字做运算，并将结果再次入栈，重复上述过程直至表达式最左端，最后数栈的里的最后一个元素就是该表达式的值。 ③ 中缀表达式 中缀表达式是常见的运算表达式，如(3+4)*5-6 中缀表达式的求值是对人类友好的，但是计算机不好理解，在计算机求解过程中往往会将中缀表达式转化为其他表达式来操作（一般往往是后缀表达式，因为中缀表达式的操作符顺序不好确定） ④ 后缀表达式（逆波兰表达式） 与前缀表达式相反，后缀表达式的运算符位于操作数之后，如(3+4)*5-6这样的中缀表达式对应的后缀表达式34+5*6- 后缀表达式的计算机求值，与前缀表达式相反：首先从左至右扫描表达式，遇到数字则将数字压入堆栈，遇到运算符则弹出栈顶的两个数字，用运算符对这两数字做运算，并将结果再次入栈，重复上述过程直至表达式最左端，最后数栈的里的最后一个元素就是该表达式的值。 ","date":"2022-07-07","objectID":"/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95java%E7%89%88/:7:3","tags":[],"title":"数据结构与算法（java版）","uri":"/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95java%E7%89%88/"},{"categories":["数据结构与算法"],"content":"4. 逆波兰计算器 ① 后缀表达式求解 求解思路如上 package stack; import java.util.Arrays; /* * 逆波兰计算器 * 假设输入的字符串没有括号，且都是整型数字 * */ public class poLandNotation { public static void main(String[] args) { //先定义一个逆波兰表达式：（30+4）*5-6 -》30 4 + 5 * 6 - String suffixExpression = \"30 4 + 5 * 6 -\"; char[] chs = suffixExpression.toCharArray(); stackNumber stackNumber = new stackNumber(10); String number = \"\";//数字拼接 int num1 = 0; int num2 = 0; int val = 0; for (int i = 0; i \u003c chs.length; i++) { //扫描数字 while (!isSpace(chs[i])) { if (isDigital(chs[i])) { //数字追加 number += chs[i]; i++; } else if (isOpr(chs[i])) { //扫描到操作符，就退出循环，避免死循环 break; } } if (number != \"\") { stackNumber.push(Integer.parseInt(number)); } //扫描字符串 if (!isSpace(chs[i])) { if (isOpr(chs[i])) { num1 = stackNumber.pop(); num2 = stackNumber.pop(); val = stackNumber.calculate(num2, num1, chs[i]); stackNumber.push(val); } } number = \"\";//清理本次number拼接字符 } // stackNumber.showStack(); System.out.print(\"计算结果：\"); stackNumber.showBottom(); } //是否是操作符 public static boolean isOpr(char ch) { return ch == '+' || ch == '-' || ch == '*' || ch == '/'; } //是否是数字 public static boolean isDigital(char ch) { return Character.isDigit(ch); } //数字结束符 ‘ ’空格 public static boolean isSpace(char ch) { return ch == ' '; } } class stackNumber { int maxSize; int top; int[] arr; public stackNumber(int maxSize) { this.maxSize = maxSize; top = -1; arr = new int[maxSize]; } //是否位空 public boolean isEmpty() { return top == -1; } //是否已满 public boolean isFull() { return top == maxSize - 1; } //入栈 public void push(int num) { if (isFull()) throw new RuntimeException(\"栈溢出\"); arr[++top] = num; } //出栈 public int pop() { if (isEmpty()) throw new RuntimeException(\"栈空\"); return arr[top--]; } //显示队列元素（不取出队列元素） public void showStack() { if (isEmpty()) throw new RuntimeException(\"栈空\"); System.out.println(\"栈元素：\" + Arrays.toString(arr)); } //显示栈底元素 public void showBottom() { System.out.println(arr[0]); } //计算方法 public int calculate(int num1, int num2, char opr) { switch (opr) { case '+': return num1 + num2; case '-': return num1 - num2; case '*': return num1 * num2; case '/': return num1 / num2; default: System.out.println(\"无效的操作符\"); return -1; } } } 该实例演示了对后缀表达式进行计算求解。但是用户往往是输入中缀表达式。以下代码，实现利用中缀表达式转化为后缀表达式，然后再通过以上方法再求解后缀表达式。 ② 中缀表达式转化为后缀表达式 思路（注意各个步骤顺序）： 初始化两个栈：运算符栈s1和储存中间结果栈s2 从左至右扫描中缀表达式 遇到操作数时，将其压入栈s2 遇到括号时：如果是左括号“（”，则直接入栈；如果是右括号“）”，则依次弹出s1栈顶的运算符，并压入到s2中，直到遇到左括号为止，此时将这一对括号丢弃 遇到运算符时，比较其与s1栈顶运算符的优先级：如果s1为空或栈顶运算符为左括号“（”，再或者优先级比栈顶的运算符的高，就将运算符压入s1栈中；否则（不满足前面三个条件之一），即优先级比栈顶运算符的低或相等，将s1栈顶运算符弹出并压入到s2中，再次转到第四步开始重新比较当前运算符 重复步骤2-5，直至表达式地最右边 将s1剩余地运算符依次弹出并压入s2 依次弹s2中的元素并输出结果，结果的逆序极为中缀表达式对应的后缀表达式。 样例分析 中缀表达式：1+((2+3)*4)-5 后缀表达式结果：1 2 3 + 4* + 5 - 代码展示 package stack; public class midfixToSuffixDemo { public static void main(String[] args) { //中缀表达式转化为后缀表达式：`10 + ( ( 2 + 3 ) * 4 ) - 5+1` =》`10 2 3 + 4 * + 5 -` //10, 2, 3, 43, 4, 42, 5, 45, 40, 40, 43, stack2 s1 = new stack2(30);//符号栈(运算符+小括号) stack2 s2 = new stack2(30);//中间结果 String expression = \"10+((2+3)*4)-5\"; String number = \"\"; char[] chs = expression.toCharArray(); for (int i = 0; i \u003c chs.length; i++) { while (isDigital(chs[i])) { System.out.println(\"chs[\" + i + \"]:\" + chs[i]); number += chs[i++];//叠加相邻数字字符 if (i == chs.length) break; } System.out.println(\"number:\" + number); if (i \u003c chs.length) System.out.println(\"chs[\" + i + \"]:\" + chs[i]); if (number != \"\") { //拆解数字,放进栈 for (char v : number.toCharArray()) { s2.push(v); } //放入数字和符号隔离符 '|' s2.push('|'); } number = \"\"; if (i \u003c chs.length) { if (isLeft(chs[i])) { s1.push(chs[i]); } else if (isRight(chs[i])) { while (!isLeft(s1.getTop())) { s2.push(s1.pop()); s2.push('|'); } if (isLeft(s1.getTop())) s1.pop();//pop掉左括号 } } if (i \u003c chs.length) { if (isOpr(chs[i])) { if (s1.isEmpty() || isLeft(s1.getTop()) || s1.priority(chs[i]) \u003e s1.priority(s1.getTop())) { System.out.println(\"ss \" + chs[i]); s1.push(chs[i]); } else if (s1.priority(chs[i]) \u003c= s1.priority(s1.getTop())) { s2.push(s1.pop()); //放入数字和符号隔离符 '|' s2.push('|'); i--;//重新比较 } } } // s1.showStack(); s2.showSt","date":"2022-07-07","objectID":"/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95java%E7%89%88/:7:4","tags":[],"title":"数据结构与算法（java版）","uri":"/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95java%E7%89%88/"},{"categories":["数据结构与算法"],"content":"5. 递归 ① 原理 递归，就是在运行的过程中调用自己。 构成递归需具备的条件 构成递归需具备的条件 不能无限制地调用本身，须有个出口，化简为非递归状况处理 ② 递归机制 进栈：每递归调用一次函数或方法，就需要进栈一次，最多的进栈元素个数称为递归深度，递归次数越多，递归深度越大，开辟的栈空间越大（应当避免过多的递归使得栈溢出） 出栈：每当遇到递归出口或**完成本次执行（完成执行的意思：执行函数或方法到达底部）**时，需出栈一次并恢复参量值，当全部执行完毕时，栈应为空。 为了完成一次典型的递归调用，系统需要分配一些空间来保存三部分重要的信息 函数的返回地址：一旦函数调用完成，程序应该知道返回到哪里，即函数调用之前的位置 函数传递的参数 函数的局部变量 每次进行递归调用时，都会在栈中有自己的形参和局部变量的拷贝，这个由系统自动完成，即将所有的栈变量压入系统的堆栈。（如果是浅拷贝形参则每个方法或函数压入栈的变量都是相互独立的；但是如果是深拷贝，则会互相影响） 由于有了a)的机制，在递归返回的时候才能将前面压入栈的临时变量又恢复到现场 ④ 递归示例图 ⑤ 递归应用 各种数学问题：八皇后、汉诺塔、阶乘、迷宫等 各种算法也会使用递归，比如快排、归并排序、二分查找、分治算法等 利用栈解决的问题-》递归代码比较简洁 迷宫回溯 如下图，红色方块代表墙，红色圆形代表小球，小球在指定起点位置需要寻找一条到达指定目的地的最短路径 思路 初始化地图，假设有墙的地方值为1；没有墙可以走的地方值为0；走过点设置为2；已经探测过没有后路的点设置为3 定制策略，有上下左右四个方向，因此策略有24种策略。只需要找到哪种策略的2的点数最少到达目的地即可 代码实现 package recursion; /* * 迷宫回溯 * */ public class miGong { public static void main(String[] args) { //初始化地图，假设有墙的点值为1；没有墙，可以走的点的值为0；走过的点的值设置为2；已经探测过的点设置为3; //走迷宫的策略方法：下-》右-》上-》左；如果该点走不通则回溯; //当然策略也可以更换其他 int[][] map = new int[8][7]; //上下两行赋值为墙 for (int j = 0; j \u003c map[0].length; j++) { map[0][j] = 1; map[7][j] = 1; } //中间 for (int i = 1; i \u003c map.length - 1; i++) { map[i][0] = 1; map[i][6] = 1; } //内部墙 map[3][1] = 1; map[3][2] = 1; //地图 for (int[] arr : map) { for (int e : arr) { System.out.print(e + \" \"); } System.out.println(); } setWay(map, 1, 1); System.out.println(); //地图 for (int[] arr : map) { for (int e : arr) { System.out.print(e + \" \"); } System.out.println(); } } //i,j表示从地图哪个位置出发，如果能到map[6][5],则说明通路找到 public static boolean setWay(int[][] map, int i, int j) { if (map[6][5] == 2) {//通路找到 return true; } else { if (map[i][j] == 0) {//当前这个点还没有走过 //假定可以走通，如果走不通会更改其值 map[i][j] = 2; //按照策略走下一步:下（1）-》右（2）-》上（3）-》左（4） if (setWay(map, i + 1, j)) {//先走下 return true; } else if (setWay(map, i, j + 1)) {//下路走不通再走右路 return true; } else if (setWay(map, i - 1, j)) {//右路走不通再走上路 return true; } else if (setWay(map, i, j - 1)) {//上路走不通再走左路 return true; } else {//都走不通，回溯 map[i][j] = 3; return false; } } else {//如果该点不为0，为1 2 3时，表明该点可能为墙，或者已经走过，或者此点已经被探测过以后的路走不通 return false; } } } } 八皇后（回溯算法） 在8*8格的国际象棋上拜访八个皇后，使其不能相互攻击，即：任意两个皇后都不能处于同一行、同一列或同一斜线上，问有多少种摆法？（答案：92种） 思路（暴力法） 第一个皇后先放第一列 第二个皇后放在第二行第一列，然后判断是否ok，如果不ok，继续放在第二列、第三列，依次把所有的列放完，找到一个合适的位置 继续第三个皇后，还是第一列、第二列…直到第八个皇后也能放在一个不冲突的位置，于是就找到了一个正确解 当得到一个正确解时，在栈回退到上一个栈时，就会开始回溯，即将第一个皇后放到第一列的所有正确解全部得到 然后回头继续第一个皇后放到第二列，后面依次循环执行1、2、3、4的步骤 说明：理论上应该创建一个二维数组来表示棋盘地图，但是实际上可以通过算法用一个一维数组解决问题，如int[] arr = new int[]{0,4,7,5,2,6,1,3} arr数组下标表示第几行，即第几个皇后，值表示列的位置：arr[i]表示第i+1个皇后，放在第i+1行的第arr[i]+1列。当然也可以使用二维数组实现，但是 代码实现 package recursion; /* * 八皇后 * */ public class eightQueens { //皇后数 static int max = 8; //保存皇后放置位置结果 static int[] arr = new int[max]; //结果数 static int num = 0; public static void main(String[] args) { placeQueen(0); System.out.println(\"总共：\" + num + \"种结果\"); } //放置第n个皇后 public static void placeQueen(int n) { if (n == max) { num++; print(); return; } //放置皇后，并判断是否冲突 for (int i = 0; i \u003c max; i++) { //第n个皇后放到第i列 arr[n] = i; //判断此时放置的位置与前面放好的八皇后是否冲突 if (judge(n)) { //如果不冲突就放置下一个皇后 placeQueen(n + 1); } //如果冲突，把皇后放到下一列 } } //查看当我们放置第n个皇后是否与前面的皇后冲突 public static boolean judge(int n) { for (int i = 0; i \u003c n; i++) { //arr[i]==arr[n] -》表示同列 //Math.abs(n-i)==Math.abs(arr[i]-arr[n]) -》表示同斜线 //是否在同一行，没有必要没有必要 if (arr[i] == arr[n] || Math.abs(n - i) == Math.abs(arr[i] - arr[n])) { return false; } } return true; } //打印八皇后位置 public static void print() { System.out.print(\"八皇后位置:\"); for (int e : arr) { System.out.print(e + \" \"); } System.out.println(); } } ","date":"2022-07-07","objectID":"/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95java%E7%89%88/:7:5","tags":[],"title":"数据结构与算法（java版）","uri":"/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95java%E7%89%88/"},{"categories":["数据结构与算法"],"content":"五、排序算法 排序也称排序算法，排序是将一组数据依指定顺序进行排列的过程。 排序的分类： 内部排序 指将需要处理的所有数据都加载到内部存储器中进行排序。 外部排序 数据量过大，无法全部加载到内存中，需要借助外部存储进行排序 常见的排序算法分类 相关术语： 稳定：如果a原本在b前面，而a=b，排序之后a仍然在b的前面； 不稳定：如果a原本在b的前面，而a=b，排序之后a可能会出现在b的后面； 内排序：所有排序操作都在内存中完成； 外排序：由于数据太大，因此把数据放在磁盘中，而排序通过磁盘和内存的数据传输才能进行； 时间复杂度： 一个算法执行所耗费的时间。 空间复杂度：运行完一个程序所需内存的大小。 n: 数据规模 k: “桶”的个数，不做特殊的处理的话，一般为10 In-place: 不占用额外内存 Out-place: 占用额外内存 ","date":"2022-07-07","objectID":"/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95java%E7%89%88/:8:0","tags":[],"title":"数据结构与算法（java版）","uri":"/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95java%E7%89%88/"},{"categories":["数据结构与算法"],"content":"1. 冒泡排序 原理 通过对待排序序列从前往后（从下标较小的元素开始），依次比较相邻元素的值，若发现逆序则交换，使值较大。 算法复杂度：O() 算法描述 比较相邻的元素。如果第一个比第二个大，就交换它们两个 对每一对相邻元素作同样的工作，从开始第一对到结尾的最后一对，这样在最后的元素应该会是最大的数，同时这个元素也是排好序的元素 针对所有的元素重复以上的步骤，除了最后一个 重复步骤1~3，直到排序完成 优化：因为排序的过程中，各元素不断接近自己的位置，如果一趟比较下 来没有进行过交换，就说明序列已经有序，无需再比较下去 代码实现 package sort; import java.util.Arrays; /* * 冒泡排序 * */ public class bubbleSort { public static void main(String[] args) { int[] arr = new int[]{1, 2, 1, 5, 4, 6, 4, 1}; System.out.println(Arrays.toString(arr)); bubbleSort(arr); System.out.println(Arrays.toString(arr)); } public static void bubbleSort(int[] arr) { int temp = 0; boolean flag = false; for (int i = 0; i \u003c arr.length - 1; i++) {//需要多少轮两两比较，每一轮都会选出一个排好序的数字 flag = false; for (int j = 0; j \u003c arr.length - 1 - i; j++) { // 从第一个元素到第i个元素 if (arr[j] \u003e arr[j + 1]) { //交换值 temp = arr[j]; arr[j] = arr[j + 1]; arr[j + 1] = temp; flag = true; } } if (!flag) { break; } } } } ","date":"2022-07-07","objectID":"/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95java%E7%89%88/:8:1","tags":[],"title":"数据结构与算法（java版）","uri":"/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95java%E7%89%88/"},{"categories":["数据结构与算法"],"content":"2. 选择排序 原理 选择排序是一种简单直观的排序算法，它也是一种交换排序算法，和冒泡排序有一定的相似度，可以认为选择排序是冒泡排序的一种改进 算法描述 在未排序序列中找到最小（大）元素，存放到排序序列的起始位置 从剩余未排序元素中继续寻找最小（大）元素，然后放到已排序序列的末尾 重复第二步，直到所有元素均排序完毕 代码实现 package sort; import java.util.Arrays; /* * 选择排序 * */ public class selectionSort { public static void main(String[] args) { int[] arr = new int[]{1, 2, 1, 5, 4, 6, 4, 1}; System.out.println(Arrays.toString(arr)); selectionSort(arr); System.out.println(Arrays.toString(arr)); } public static void selectionSort(int[] arr) { int temp, min, index = 0; for (int i = 0; i \u003c arr.length - 1; i++) { min = arr[i];//最小值 for (int j = i + 1; j \u003c arr.length; j++) {//从i的下一个元素开始到最后一个元素进行比较 if (min \u003e arr[j]) { min = arr[j]; index = j;//记录最小值角标 } } if (min != arr[i]) {//min值改变，说明找到更小的值，需要交换 temp = arr[i]; arr[i] = arr[index]; arr[index] = temp; } } } } ","date":"2022-07-07","objectID":"/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95java%E7%89%88/:8:2","tags":[],"title":"数据结构与算法（java版）","uri":"/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95java%E7%89%88/"},{"categories":["数据结构与算法"],"content":"3. 插入排序 原理 插入排序是一种简单直观的排序算法。它的工作原理是通过构建有序序列，对于未排序数据，在已排序序列中从后向前扫描，找到相应位置并插入 算法描述 把待排序的数组分成已排序和未排序两部分，初始的时候把第一个元素认为是已排好序的 从第二个元素开始，在已排好序的子数组中寻找到该元素合适的位置并插入该位置 重复上述过程直到最后一个元素被插入有序子数组中 问题 我们看简单的插入排序可能存在的问题. 数组 arr = {2,3,4,5,6,1} 这时需要插入的数 1(最小), 这样的过程是： {2,3,4,5,6,6} {2,3,4,5,5,6} {2,3,4,4,5,6} {2,3,3,4,5,6} {2,2,3,4,5,6} {1,2,3,4,5,6} 结论: 当需要插入的数是较小的数时，后移的次数明显增多，对效率有影响 代码实现 package sort; import java.util.Arrays; /* * 插入排序 */ public class insertionSort { public static void main(String[] args) { int[] arr = new int[]{1, 2, 1, 5, 4, 6, 4, 1}; System.out.println(Arrays.toString(arr)); insertionSort(arr); System.out.println(Arrays.toString(arr)); } public static void insertionSort(int[] arr) { // for (int i = 1; i \u003c arr.length; ++i) { // int value = arr[i];//记录当前需要比较的数据 // int position = i;//记录当前数据角标 // while (position \u003e 0 \u0026\u0026 arr[position - 1] \u003e value) {//大于value的素数组元素往后挪一位，直到不大于 // arr[position] = arr[position - 1]; // position--; // } // arr[position] = value; // } for (int i = 1; i \u003c arr.length; i++) { int val = arr[i]; int index = i; for (; index \u003e 0; index--) { if (arr[index - 1] \u003e val) { arr[index] = arr[index - 1]; } else { break; } } arr[index] = val; } } } ","date":"2022-07-07","objectID":"/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95java%E7%89%88/:8:3","tags":[],"title":"数据结构与算法（java版）","uri":"/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95java%E7%89%88/"},{"categories":["数据结构与算法"],"content":"4. 希尔排序 原理 移位法思路： 希尔排序也是一种插入排序，它是简单插入排序经过改进之后的一个更高效的版本，也称为缩小增量排序。希尔排序是记录下标的一定增量分组，对每组使用直接插入排序算法排序；随着增量逐渐减少，每组包含的关键词越来越多，当增量减至1时，整个文件恰被分成一组，算法便终止。增量序列的选取：希尔增量、Hibbard增量、Sedgewick增量 算法描述 先将整个待排序的记录序列分割成为若干子序列分别进行直接插入排序，具体算法描述： 选择一个增量序列t1，t2，…，tk，其中ti\u003etj，tk=1 按增量序列个数k，对序列进行 k 趟排序 每趟排序，根据对应的增量ti，将待排序列分割成若干长度为m 的子序列，分别对各子表进行直接插入排序。仅增量因子为1 时，整个序列作为一个表来处理，表长度即为整个序列的长度 代码实现（选用希尔增量） 交换法 package sort; import java.util.Arrays; /* * 希尔排序（交换法） */ public class shellSort { public static void main(String[] args) { int[] arr = new int[]{1, 2, 1, 5, 4, 6, 4, 1}; System.out.println(Arrays.toString(arr)); shellSort(arr); System.out.println(Arrays.toString(arr)); } public static void shellSort(int[] arr) { int temp; for (int gap = arr.length / 2; gap \u003e 0; gap /= 2) {//增量序列,gap为步长,分组 for (int i = 0; i \u003c arr.length - gap; i++) {//遍历有多少组：(arr.length-gap)组 for (int j = 0; j \u003c arr.length - gap; j += gap) {//遍历交换组内元素 if (arr[j] \u003e arr[j + gap]) { temp = arr[j]; arr[j] = arr[j + gap]; arr[j + gap] = temp; } } } } } } 注意：交换法对数据进行排序，在数据量大的时候，变得很慢。采用移动法进行优化 移位法 package sort; import java.util.Arrays; public class shellSort { public static void main(String[] args) { int[] arr = new int[]{1, 2, 1, 5, 4, 6, 4, 1}; System.out.println(\"排序前：\" + Arrays.toString(arr)); shellSortByMove(arr); System.out.println(\"移位法排序后：\" + Arrays.toString(arr)); } public static void shellSortByMove(int[] arr) { for (int gap = arr.length / 2; gap \u003e 0; gap /= 2) {//分组 //对每组所有元素进行直接插入排序 for (int i = gap; i \u003c arr.length; i++) { int position = i; int value = arr[i]; if (value \u003c arr[position - gap]) { while (position - gap \u003e= 0 \u0026\u0026 value \u003c arr[position - gap]) { arr[position] = arr[position - gap]; position -= gap; } arr[position] = value; } } } } } ","date":"2022-07-07","objectID":"/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95java%E7%89%88/:8:4","tags":[],"title":"数据结构与算法（java版）","uri":"/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95java%E7%89%88/"},{"categories":["数据结构与算法"],"content":"5. 快速排序 原理 快速排序（Quicksort）是对冒泡排序的一种改进。基本思想是：通过一趟排序将要排序的数据分割成独立的两部分，其中一部分的所有数据都比另外一部分的所有数据都要小，然后再按此方法对这两部分数据分别进行快速排序，整个排序过程可以递归进行，以此达到整个数据变成有序序列 算法描述 从数列中挑出一个元素，称为\"基准\"（pivot） 重新排序数列，所有比基准值小的元素摆放在基准前面，所有比基准值大的元素摆在基准后面（相同的数可以到任何一边）。在这个分区结束之后，该基准就处于数列的中间位置。这个称为分区（partition）操作 递归地（recursively）把小于基准值元素的子数列和大于基准值元素的子数列排序 代码实现 package sort; import java.util.Arrays; public class quickSort { public static void main(String[] args) { int[] arr = new int[]{1, 2, 1, 5, 4, 6, 4, 1}; // int[] arr = new int[]{-9,78,0,23,-567,70}; System.out.println(\"排序前：\" + Arrays.toString(arr)); quickSort(arr); System.out.println(\"排序后：\" + Arrays.toString(arr)); } public static void quickSort(int[] arr) { qsort(arr, 0, arr.length - 1); } private static void qsort(int[] arr, int low, int high) { if (low \u003e= high) return; int pivotIndex = partition(arr, low, high); //将数组分为两部分 qsort(arr, low, pivotIndex - 1); //递归排序左子数组 qsort(arr, pivotIndex + 1, high); //递归排序右子数组 } private static int partition(int[] arr, int low, int high) { int pivot = arr[low]; //记录基准 while (low \u003c high) { while (low \u003c high \u0026\u0026 arr[high] \u003e= pivot) --high; arr[low] = arr[high]; //交换比基准小的记录到左端low角标处 while (low \u003c high \u0026\u0026 arr[low] \u003c= pivot) ++low; arr[high] = arr[low]; //交换比基准大的记录到右端high角标处 } //扫描完成，基准移到新的基准 arr[low] = pivot; //返回的是新基准的位置 return low; } } ","date":"2022-07-07","objectID":"/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95java%E7%89%88/:8:5","tags":[],"title":"数据结构与算法（java版）","uri":"/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95java%E7%89%88/"},{"categories":["数据结构与算法"],"content":"6. 归并排序 原理 归并排序是利用归并的思想实现的排序方法，该算法采用经典的分治策略（分治法将问题分成一些小的问题然后递归求解，而治的阶段则将分的阶段得到的各答案“修补”在一起，即分而治之） 说明：可以看到这种结构很像一棵完全二叉树，本文的归并排序我们采用递归去实现（也可采用迭代的方式去实现）。分阶段可以理解为就是递归拆分子序列的过程。 再来看看治阶段，我们需要将两个已经有序的子序列合并成一个有序序列，比如上图中的最后一次合并，要将[4,5,7,8]和[1,2,3,6]两个已经有序的子序列，合并为最终序列[1,2,3,4,5,6,7,8]，来看下实现步骤(双指针法) 代码实现 package sort; import java.util.Arrays; public class mergeSort { public static void main(String[] args) { int[] arr = {8, 4, 5, 7, 1, 3, 6, 2}; // int[] arr = {4, 5, 7, 8, 1, 2, 3, 6}; int[] temp = new int[arr.length]; System.out.println(\"排序前：\" + Arrays.toString(arr)); mergeSort(arr, 0, arr.length - 1, temp); System.out.println(\"排序后：\" + Arrays.toString(arr)); } //分+和 public static void mergeSort(int[] arr, int left, int right, int[] temp) { if (left \u003c right) { int mid = (left + right) / 2;//中间索引 //向左递归分解 mergeSort(arr, left, mid, temp); //向右递归分解 mergeSort(arr, mid + 1, right, temp); //合并 merge(arr, left, mid, right, temp); } } //mid指左子序列最后一个元素 public static void merge(int[] arr, int left, int mid, int right, int[] temp) { int i = left;//指左子序列起点处 int j = mid + 1;//指右子序列起点处 int t = 0;//temp数组索引 //先将左右两边有序数据填充到temp数组，直到又一边填充完毕 while (i \u003c= mid \u0026\u0026 j \u003c= right) { if (arr[i] \u003c= arr[j]) { temp[t] = arr[i]; t++; i++; } else if (arr[i] \u003e arr[j]) { temp[t] = arr[j]; t++; j++; } } //再将一边剩下的有序数据移到temp后面 while (i \u003c= mid) { temp[t++] = arr[i++]; } while (j \u003c= right) { temp[t++] = arr[j++]; } //拷贝指定数量至arr数组,注意左右拷贝边界 t = 0; int tLeft = left; while (tLeft \u003c= right) { arr[tLeft++] = temp[t++]; } } } ","date":"2022-07-07","objectID":"/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95java%E7%89%88/:8:6","tags":[],"title":"数据结构与算法（java版）","uri":"/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95java%E7%89%88/"},{"categories":["数据结构与算法"],"content":"7. 基数排序 原理 基数排序（radix sort）属于“分配式排序”，又称“桶子法”，顾名思义，它是通过键值的各个位的值，将要排序的元素分配至某些“桶”中，达到排序的作用。基数排序法是属于稳定性的排序，基数排序法的是效率高的稳定性排序法。基数排序是桶排序的扩展。基数排序是这样实现的：将整数按位数切割成不同的数字，然后按每个位数分别比较。 注:假定在待排序的记录序列中，存在多个具有相同的关键字的记录，若经过排序，这些记录的相对次序保持不变，即在原序列中，r[i]=r[j]，且r[i]在r[j]之前，而在排序后的序列中，r[i]仍在r[j]之前，则称这种排序算法是稳定的；否则称为不稳定的 **有负数的数组，我们不用基数排序来进行排序, 如果要支持负数，参考: https://code.i-harness.com/zh-CN/q/e98fa9 ** 算法实现 将所有待比较数值统一为同样的数位长度，数位较短的数前面补零。然后，从最低位开始，依次进行一次排序，相同的数放到同一个“桶”（数组），然后再依次从“桶”中取出元素，形成的序列再比较各个数字下一位，这样从最低位排序一直到最高位排序完成以后, 数列就变成一个有序序列。 代码实现 package sort; import java.util.Arrays; /* * 基数排序：这个版本的实现只适用于非负数的数组排序 * */ public class radixSort { public static void main(String[] args) { int[] arr = new int[]{1, 210, 10, 5, 41, 6, 14, 1, 21, 32, 56, 689, 9923}; System.out.println(\"排序前：\" + Arrays.toString(arr)); radixSort(arr); System.out.println(\"排序后：\" + Arrays.toString(arr)); } public static void radixSort(int[] arr) { //每个“桶”就是一个一维数组，定义10个“桶”，角标表示”桶“能装下的对应某位数字。空间换时间 int[][] buckets = new int[10][arr.length]; //bucketElementCount[0]记录的是buckets[0]中的数据个数，其他依次类推 int[] bucketElementCount = new int[10]; //最大数字 int maxDigits = 0; //最大数字的位数 int numberOfMaxDigits = 0; //取出arr中最大元素 for (int v : arr) { if (v \u003e maxDigits) maxDigits = v; } //计算其位数 while (maxDigits != 0) { numberOfMaxDigits++; maxDigits /= 10; } //进行numberOfMaxDigits轮基数排序 for (int i = 0; i \u003c numberOfMaxDigits; i++) { //放数据入桶 for (int j = 0; j \u003c arr.length; j++) { //依次取出某位数 int digitOfElement = arr[j] / (int) Math.pow(10, i) % 10; //将这位数放到指定的桶 buckets[digitOfElement][bucketElementCount[digitOfElement]] = arr[j]; //记录桶数据个数，这里采用一维数组记录桶的有效数据个数， //也可以采用队列数组的方式来做桶排序，这样就不需要提供这样额外的数组了 bucketElementCount[digitOfElement]++; } int index = 0; //从桶中把数据依次取出，并放入原数组arr中，供下次排序使用 for (int j = 0; j \u003c bucketElementCount.length; j++) { if (bucketElementCount[j] != 0) { //取出对应桶中的数据 for (int l = 0; l \u003c bucketElementCount[j]; l++) { arr[index++] = buckets[j][l]; buckets[j][l] = 0;//清理桶 } bucketElementCount[j] = 0;//清理桶 } } } } } ","date":"2022-07-07","objectID":"/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95java%E7%89%88/:8:7","tags":[],"title":"数据结构与算法（java版）","uri":"/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95java%E7%89%88/"},{"categories":["数据结构与算法"],"content":"8. 堆排序 堆排序如下（本处文字用于本文锚点跳转，请勿删除） 原理 堆是一种特殊的完全二叉树（complete binary tree）。完全二叉树的一个“优秀”的性质是，除了最底层之外，每一层都是满的，这使得堆可以利用数组来表示（普通的一般的二叉树通常用链表作为基本容器表示），每一个结点对应数组中的一个元素。 如下图，是一个堆和数组的相互关系： 数组与完全二叉树的对应关系 二叉堆一般分为两种：最大堆和最小堆。 最大堆： 最大堆中的最大元素值出现在根结点（堆顶），堆中每个父节点的元素值都大于等于其孩子结点（如果存在） 最小堆： 最小堆中的最小元素值出现在根结点（堆顶），堆中每个父节点的元素值都小于等于其孩子结点（如果存在） 算法实现 以数组升序排列为例 将待排序序列构造成一个大顶堆。常规方法是从最后一个非叶子结点从左至右、从上至下的筛选，直到根元素筛选完毕。这个方法叫“筛选法” 此时，整个序列的最大值就是堆顶的根节点 将其与末尾元素进行交换，此时末尾就为最大值 然后将剩余n-1个元素重新构造成一个堆，这样就会得到n个元素的次小值。如此反复执行，便得到一个有序序列了 代码 package sort; import java.util.Arrays; /* * 堆排序：升序排列 * */ public class heapSort { public static void main(String[] args) { int[] arr = {4, 13, 11, 0, 1, 2, 3, 4, 3, 2, 1, 6, 8, 5, 9, 14, 12, 9, 0}; System.out.println(Arrays.toString(sortArray(arr))); } //堆排序 public static int[] sortArray(int[] nums) { //建立初始大根堆 buildMaxHeap(nums); //调整大根堆 for (int i = nums.length - 1; i \u003e 0; i--) { //将大根堆顶与最后一个元素交换，通过不断交换，最后得到一个升序数组（每次交换从堆中移出已排好序的元素，堆逐渐变小） swap(nums, 0, i); //调整剩余数组，使其满足大顶堆 maxHeapify(nums, 0, i); } return nums; } //建立初始大根堆 public static void buildMaxHeap(int[] nums) { //最后一个非叶子节点开始，这样不断地从左至右，从下至上的调整，每次调整只是调整最多三个节点的小堆，方便处理 for (int i = nums.length / 2 - 1; i \u003e= 0; i--) { //调整每一个子树为大根堆 maxHeapify(nums, i, nums.length); } } //调整大根堆，第二个参数为堆顶，第三个参数为，堆的大小 public static void maxHeapify(int[] nums, int i, int heapSize) { //左子树 int l = 2 * i + 1; //右子树 int r = l + 1; //记录根结点、左子树结点、右子树结点三者中的最大值下标 int largest = i; //与左子树结点比较 if (l \u003c heapSize \u0026\u0026 nums[l] \u003e nums[largest]) { largest = l; } //与右子树结点比较 if (r \u003c heapSize \u0026\u0026 nums[r] \u003e nums[largest]) { largest = r; } //如果当前节点的左子树或右子树比它大，则赋值 if (largest != i) { //将最大值交换为根结点 swap(nums, i, largest); //再次调整交换数字后的大顶堆 //当树的深度够大时，如果没有下面这条语句， //也就是说，每次交换只会停留在每一个小堆（最多三个节点构成的小堆） //还需要递归看看其左右子树是否存在比当前节点大的数，有则交换上来，无则退出递归 maxHeapify(nums, largest, heapSize); } } //交换 public static void swap(int[] nums, int i, int j) { int temp = nums[i]; nums[i] = nums[j]; nums[j] = temp; } } ","date":"2022-07-07","objectID":"/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95java%E7%89%88/:8:8","tags":[],"title":"数据结构与算法（java版）","uri":"/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95java%E7%89%88/"},{"categories":["数据结构与算法"],"content":"六、查找算法 查找是在大量的信息中寻找一个特定的信息元素，在计算机应用中，查找是常用的基本运算，例如编译程序中符号表的查找。本文简单概括性的介绍了常见的七种查找算法，说是七种，其实二分查找、插值查找以及斐波那契查找都可以归为一类——插值查找。插值查找和斐波那契查找是在二分查找的基础上的优化查找算法。 查找算法分类： 静态查找和动态查找； 注：静态或者动态都是针对查找表而言的。动态表指查找表中有删除和插入操作的表。 无序查找和有序查找。 无序查找：被查找数列有序无序均可，如顺序查找 有序查找：被查找数列必须为有序数列，如：二分查找，插值查找，斐波那契查找 二分查找，插值查找，斐波那契查找的原理类似，只是分割点的原理不同，二分查找是无脑二分，插值查找是自适应地趋向目标值（采用拉格朗日插值法），斐波那契查找分割是玄学黄金分割。 平均查找长度（Average Search Length，ASL）：需和指定key进行比较的关键字的个数的期望值，称为查找算法在查找成功时的平均查找长度 ​ 对于含有n个数据元素的查找表，查找成功的平均查找长度为：ASL = Pi*Ci的和 Pi：查找表中第i个数据元素的概率 Ci：找到第i个数据元素时已经比较过的次数 ","date":"2022-07-07","objectID":"/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95java%E7%89%88/:9:0","tags":[],"title":"数据结构与算法（java版）","uri":"/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95java%E7%89%88/"},{"categories":["数据结构与算法"],"content":"1. 顺序查找 原理 说明：顺序查找适合于存储结构为顺序存储或链接存储的线性表。 基本思想：顺序查找也称为线性查找，属于无序查找算法。从数据结构线性表的一端开始，顺序扫描，依次将扫描到的结点关键字与给定值k相比较，若相等则表示查找成功；若扫描结束仍没有找到关键字等于k的结点，表示查找失败。 复杂度分析： 查找成功时的平均查找长度为：（假设每个数据元素的概率相等） ASL = 1/n(1+2+3+…+n) = (n+1)/2 ; 当查找不成功时，需要n+1次比较，时间复杂度为O(n);所以，顺序查找的时间复杂度为O(n)。 代码实现 package search; public class seqSearch { public static void main(String[] args) { int[] arr = {1, 4, -1, 2, -4, 8, 90}; int index = seqSearch(arr, 90); System.out.println(\"查找的元素在索引为\" + index + \"的位置\"); } public static int seqSearch(int[] arr, int value) { for (int i = 0; i \u003c arr.length; i++) { if (arr[i] == value) { return i; } } System.out.println(\"没有该数据\"); return 0; } } ","date":"2022-07-07","objectID":"/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95java%E7%89%88/:9:1","tags":[],"title":"数据结构与算法（java版）","uri":"/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95java%E7%89%88/"},{"categories":["数据结构与算法"],"content":"2. 二分查找 二分查找如下（本处文字用于本文锚点跳转，请勿删除） 原理 说明：元素必须是有序的，如果是无序的则要先进行排序操作。 **基本思想：**也称为是折半查找，属于有序查找算法。用给定值k先与中间结点的关键字比较，中间结点把线形表分成两个子表，若相等则查找成功；若不相等，再根据k与该中间结点关键字的比较结果确定下一步查找哪个子表，这样递归进行，直到查找到或查找结束发现表中没有这样的结点。 复杂度分析：最坏情况下，关键词比较次数为log2(n+1)，且期望时间复杂度为O(log2n)； 注：折半查找的前提条件是需要有序表顺序存储，对于静态查找表，一次排序后不再变化，折半查找能得到不错的效率。但对于需要频繁执行插入或删除操作的数据集来说，维护有序的排序会带来不小的工作量，那就不建议使用 代码实现 package search; /* * 二分查找有序序列 * */ public class binarySearch { public static void main(String[] args) { int[] arr = {1, 2, 3, 5, 6, 10, 11}; int index = binarySearch1(arr,5); System.out.println(\"查找的元素在索引为\" + index + \"的位置\"); int index1 = binarySearch2(arr, 4, 0, arr.length - 1); System.out.println(\"查找的元素在索引为\" + index1 + \"的位置\"); } //折半查找:数组已经有序，从小到大的顺序 public static int binarySearch1(int[] arr, int value) { int left = 0, right = arr.length - 1; int mid = 0; while (left \u003c= right) { mid = (left + right) / 2;//边界条件的移动也促使mid的移动 if (arr[mid] \u003e value) {//value在arr[mid]之前 right = mid - 1;//将边界条件缩小至mid-1 } else if (arr[mid] \u003c value) {//value在arr[mid]之后 left = mid + 1;//将边界条件扩大至mid+1 } else { return mid; } } System.out.println(\"数据不存在\"); return -1; } //递归查找 public static int binarySearch2(int[] arr, int value, int left, int right) { int mid = left + (right - left) / 2;//避免数据过大 if (left \u003e right) { System.out.println(\"数据不存在\"); return -1; } if (arr[mid] \u003c value) {//向右子序列递归 return binarySearch2(arr, value, mid + 1, right); } else if (arr[mid] \u003e value) {//向左子序列递归 return binarySearch2(arr, value, left, mid - 1); } else {//递归结束条件 return mid; } } } ","date":"2022-07-07","objectID":"/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95java%E7%89%88/:9:2","tags":[],"title":"数据结构与算法（java版）","uri":"/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95java%E7%89%88/"},{"categories":["数据结构与算法"],"content":"3. 插值查找 原理 在介绍插值查找之前，首先考虑一个新问题，为什么上述算法一定要是折半，而不是折四分之一或者折更多呢？打个比方，在英文字典里面查“apple”，你下意识翻开字典是翻前面的书页还是后面的书页呢？如果再让你查“zoo”，你又怎么查？很显然，这里你绝对不会是从中间开始查起，而是有一定目的的往前或往后翻。同样的，比如要在取值范围1 ~ 10000 之间 100 个元素从小到大均匀分布的数组中查找5， 我们自然会考虑从数组下标较小的开始查找。 经过以上分析，折半查找这种查找方式，不是自适应的（也就是说是傻瓜式的）。二分查找中查找点计算如下： mid=(low+high)/2, 即mid=low+1/2*(high-low)　通过类比，我们可以将查找的点改进为如下： mid=low+(key-a[low])/(a[high]-a[low])*(high-low) 也就是将上述的比例参数1/2改进为自适应的，根据关键字在整个有序表中所处的位置，让mid值的变化更靠近关键字key，这样也就间接地减少了比较次数。 **基本思想：**基于二分查找算法，将查找点的选择改进为自适应选择，可以提高查找效率。当然，差值查找也属于有序查找。 注：对于表长较大，而关键字分布又比较均匀的查找表来说，插值查找算法的平均性能比折半查找要好的多。反之，数组中如果分布非常不均匀，那么插值查找未必是很合适的选择。 复杂度分析：查找成功或者失败的时间复杂度均为O(log2(log2n)) 代码实现 package search; /* * 插值查找，改进版二分查找，使用自适应的mid来靠近value，以避免盲目的分区，节省比较次数 * */ public class insertionSearch { public static void main(String[] args) { int[] arr = {1, 2, 3, 5, 6, 10, 11}; int index1 = insertionSearch(arr, 3, 0, arr.length - 1); System.out.println(\"查找的元素在索引为\" + index1 + \"的位置\"); } public static int insertionSearch(int[] arr, int value, int left, int right) { // int mid = left + (right - left) / 2;//避免数据过大 int mid = left + (value - arr[left]) / (arr[right] - arr[left]) * (right - left);//自适应mid if (left \u003e right) { System.out.println(\"数据不存在\"); return -1; } if (arr[mid] \u003c value) {//向右子序列递归 return insertionSearch(arr, value, mid + 1, right); } else if (arr[mid] \u003e value) {//向左子序列递归 return insertionSearch(arr, value, left, mid - 1); } else {//递归结束条件 return mid; } } } ","date":"2022-07-07","objectID":"/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95java%E7%89%88/:9:3","tags":[],"title":"数据结构与算法（java版）","uri":"/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95java%E7%89%88/"},{"categories":["数据结构与算法"],"content":"4. 斐波那契查找 原理 **基本思想：**斐波那契查找原理与前两种相似，仅仅改变了中间结点（mid）的位置，mid不再是中间或插值得到，而是位于黄金分割点附近，即mid=low+F(k-1)-1（F代表斐波那契数列） 对F(k-1)-1的理解： 由斐波那契数列 F[k]=F[k-1]+F[k-2] 的性质，可以得到 （F[k]-1）=（F[k-1]-1）+（F[k-2]-1）+1 。该式说明：只要顺序表的长度为F[k]-1，则可以将该表分成长度为F[k-1]-1和F[k-2]-1的两段，即如上图所示。从而中间位置为mid=low+F(k-1)-1。类似的，每一子段也可以用相同的方式分割。但顺序表长度n不一定刚好等于F[k]-1，所以需要将原来的顺序表长度n增加至F[k]-1。这里的k值只要能使得F[k]-1恰好大于或等于n即可，由以下代码得到,顺序表长度增加后，新增的位置（从n+1到F[k]-1位置），都赋为n位置的值即可。 while(n\u003efib(k)-1) k++; 开始将k值与第F(k-1)位置的记录进行比较(及mid=low+F(k-1)-1),比较结果也分为三种 相等，mid位置的元素即为所求。注意扩容后的数组长度带来的影响 大于，low=mid+1,k-=2 说明：low=mid+1说明待查找的元素在[mid+1,high]范围内，k-=2 说明范围[mid+1,high]内的元素个数为 n-(F(k-1))= F(k)-1-F(k-1)=F(k)-F(k-1)-1=F(k-2)-1个，所以可以递归地应用斐波那契查找 小于，high=mid-1,k-=1 说明：low=mid+1说明待查找的元素在[low,mid-1]范围内，k-=1 说明范围[low,mid-1]内的元素个数为 F(k-1)-1个，所以可以递归地应用斐波那契查找。 复杂度分析：最坏情况下，时间复杂度为O(log2n)，且其期望复杂度也为O(log2n)。 代码实现 package search; import java.util.Arrays; /* * 斐波那契查找 * */ public class fibonacciSearch { public static int maxSize = 20; public static void main(String[] args) { int[] arr = {1, 8, 10, 89, 1000, 1234}; int index = fibonacciSearch(arr, 1000); System.out.println(index); } //斐波那契查找算法 public static int fibonacciSearch(int[] arr, int val) { int low = 0; int high = arr.length - 1;//代表原数组最高的下标4 int k = 0;//斐波那契分割数值的下标 int mid; int[] f = getFib(); //获取到斐波那契分割数值的下标 1 1 2 3 5 8 while (arr.length \u003e f[k] - 1) {//6 2 k++;//4 } //因为f[k]值可能大于a的长度，因此我们需要使用Arrays类，构造一个新的数组，并指向a[] int[] temp = Arrays.copyOf(arr, f[k]); //填充新的数组，因为新数组长度如果比原数组大，多的部分会被填充0，因此需要使用最后面的最大值来填充，防止数组顺序被破坏 for (int i = high + 1; i \u003c temp.length; i++) { temp[i] = arr[high]; } //找到中间值mid后开始查找 while (low \u003c= high) {//只要这个条件满足，就可以继续查找 mid = low + f[k - 1] - 1;// if (val \u003c temp[mid]) {//向左查找 high = mid - 1; //为什么是k-- //说明 //1.全部元素 = 前面的元素 + 后边元素 //2.f[k] = f[k-1]+f[k-2] //因为前面有f[k-1]个元素，所以可以继续拆分f[k-1] = f[k-2] + f[k-3] //即在f[k-1]的前面继续查找k-- //即下次循环mid = f[k-1-1]-1 k--; } else if (val \u003e temp[mid]) { low = mid + 1; //为什么是k-=2 //说明 //继续拆分f[k-2] = f[k-3] + f[k-4] //即在f[k-2]的前面继续查找k-=2 //即下次循环mid = f[k-1-1-1]-1 k -= 2; } else { if (mid \u003c= high) { return mid; } else { //数组扩容过，此时返回mid就会超出原数组的长度 return high; } } } return -1; } //生成一个斐波那契数列 public static int[] getFib() { int[] f = new int[maxSize]; f[0] = 0; f[1] = 1; for (int i = 2; i \u003c maxSize; i++) { f[i] = f[i - 1] + f[i - 2]; } return f; } } ","date":"2022-07-07","objectID":"/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95java%E7%89%88/:9:4","tags":[],"title":"数据结构与算法（java版）","uri":"/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95java%E7%89%88/"},{"categories":["数据结构与算法"],"content":"5. 哈希表数据结构 原理 散列表（Hash table，也叫哈希表），是根据关键码值(Key value)而直接进行访问的数据结构。也就是说，它通过把关键码值映射到表中一个位置来访问记录，以加快查找的速度。这个映射函数叫做散列函数，存放记录的数组叫做散列表。 可以使用数组+链表实现，或者使用数组+二叉数/二叉排序树实现（性能更高）。在业务里的应用可以是：使用哈希表构建自己项目的缓存中间层放在数据库和服务器之间（造个轮子） 哈希表业务应用 题目： 有一个公司,当有新的员工来报道时,要求将该员工的信息加入\u000b(id,性别,年龄,名字,住址..),当输入该员工的id时,要求查找到该员工的 所有信息. 要求: 不使用数据库,,速度越快越好=\u003e哈希表(散列)、添加时，保证按照id从低到高插入 代码实现： package hashTable; /* * 哈希表 * */ public class hashTable { public static void main(String[] args) { hashTableManager hashTableManager = new hashTableManager(10); hashTableManager.add(new node(114, \"cold bin\", \"重庆\")); hashTableManager.add(new node(224, \"wtl\", \"重庆\")); hashTableManager.add(new node(334, \"sss\", \"ss\")); hashTableManager.add(new node(3, \"bwj\", \"重庆\")); hashTableManager.show(); hashTableManager.delete(224); hashTableManager.show(); hashTableManager.update(new node(334, \"aaa\", \"\")); hashTableManager.show(); node node = hashTableManager.findNode(224); if (node != null) { System.out.println(\"找到节点\" + node); } } } class hashTableManager { int size;//哈希表的数组长度 nodeManager[] hashTable;//哈希表的内存结构 public hashTableManager(int size) { this.size = size; hashTable = new nodeManager[size]; //实例化数组里的对象 for (int i = 0; i \u003c size; i++) { nodeManager nodeManager = new nodeManager(20); hashTable[i] = nodeManager; } } //散列函数 public int hashFunc(int id) { return id % this.size; } //添加 public void add(node node) { int nodeManagerNo = hashFunc(node.id); this.hashTable[nodeManagerNo].add(node); } public void delete(int id) { int nodeManagerNo = hashFunc(id); this.hashTable[nodeManagerNo].delete(id); } public void update(node node) { int nodeManagerNo = hashFunc(node.id); this.hashTable[nodeManagerNo].update(node); } //查询id对应的员工信息 public node findNode(int id) { int nodeManagerNo = hashFunc(id); return this.hashTable[nodeManagerNo].findNode(id); } //遍历 public void show() { for (int i = 0; i \u003c this.size; i++) { if (!this.hashTable[i].isEmpty()) { System.out.print(\"哈希表第\" + (i + 1) + \"行:\"); this.hashTable[i].show(); } else { System.out.println(\"哈希表第\" + (i + 1) + \"行为空\"); } } } } class nodeManager { int maxSize;//节点最大容量 node head;//带数据的头节点 //初始化空的带头节点链表 public nodeManager(int maxSize) { this.head = new node(-1); this.maxSize = maxSize; } public boolean isEmpty() { return head.next == null; } public boolean isFull() { int num = 0; node temp = head; while (temp.next != null) { num++; temp = temp.next; } return num == maxSize - 1; } //增：按照id递增插入 public void add(node node) { if (isFull()) { System.out.println(\"链表已满\"); return; } node temp = head; while (temp != null) { node beforeNode = temp; if (temp.next == null) { temp.next = node; return; } temp = temp.next; if (temp.id \u003e node.id) { //记录对应编号节点的前一个节点位置，将新节点插入这个位置，意即beforeNode之后，temp之前； beforeNode.next = node; node.next = temp; System.out.println(\"插入成功\"); return; } } System.out.println(\"没有找到编号\"); } //删 public void delete(int id) { if (isEmpty()) { System.out.println(\"链表已空\"); return; } node temp = head.next; while (temp != null) { node preNode = temp; temp = temp.next; if (temp.id == id) { preNode.next = temp.next; System.out.println(\"删除成功\"); return; } } System.out.println(\"删除失败，没有该id\"); } //改 public void update(node node) { if (isEmpty()) { System.out.println(\"链表已空\"); return; } node temp = head.next; while (temp != null) { temp = temp.next; if (temp.id == node.id) { temp.name = node.name; temp.address = node.address; System.out.println(\"更新成功\"); return; } } System.out.println(\"更新失败，没有该id\"); } //查找节点 public node findNode(int id) { if (isEmpty()) { System.out.println(\"链表为空\"); return null; } node temp = head; while (temp != null \u0026\u0026 temp.id != id) { temp = temp.next; } if (temp != null) { return temp; } else { System.out.println(\"没有该id\"); return null; } } //查 public void show() { if (isEmpty()) { System.out.println(\"链表为空\"); return; } node temp = head.next; System.out.print(\"链表：\"); while (temp != null) { System.out.print(temp); temp = temp.next; } System.out.println(); } } class node { int id; String name; String address; node next; public node(int id, String name,","date":"2022-07-07","objectID":"/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95java%E7%89%88/:9:5","tags":[],"title":"数据结构与算法（java版）","uri":"/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95java%E7%89%88/"},{"categories":["数据结构与算法"],"content":"七、树 为什么需要树这种数据结构 数组存储方式的分析 优点：通过下标方式访问元素，速度快。对于有序数组，还可使用二分查找提高检索速度 缺点：如果要检索具体某个值，或者插入值(按一定顺序)会整体移动，效率较低 链式存储方式的分析 优点：在一定程度上对数组存储方式有优化(比如：插入一个数值节点，只需要将插入节点，链接到链表中即可， 删除效率也很好)。 缺点：在进行检索时，效率仍然较低，比如(检索某个值，需要从头节点开始遍历) 树存储方式的分析 优点：能提高数据存储，读取的效率, 比如利用 二叉排序树(Binary Sort Tree)，既可以保证数据的检索速度，同时也可以保证数据的插入，删除，修改的速度。案例: [7, 3, 10, 1, 5, 9, 12] ","date":"2022-07-07","objectID":"/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95java%E7%89%88/:10:0","tags":[],"title":"数据结构与算法（java版）","uri":"/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95java%E7%89%88/"},{"categories":["数据结构与算法"],"content":"1. 二叉树 ① 二叉树概念 下图有些错误：H、E、F、G才是叶子节点 树的常用术语 节点 根节点 父节点 子节点 叶子节点 (没有子节点的节点) 节点的权(节点值) 路径(从root节点找到该节点的路线) 层 子树 树的高度(最大层数) 森林 :多颗子树构成森林 概念 树有很多种，每个节点最多只能有两个子节点的一种形式称为二叉树。二叉树的子节点分为左节点和右节点。 如果该二叉树的所有叶子节点都在最后一层，并且结点总数= 2^n -1（等比数列前n项和） ,n 为层数，则我们称为满二叉树。 完全二叉树是由满二叉树而引出来的，若设二叉树的深度为h，除第 h 层外，其它各层 (1～h-1) 的结点数都达到最大个数(即1~h-1层为一个满二叉树)，第 h 层所有的结点都连续集中在最左边，这就是完全二叉树。 注意：完全二叉树 ， 如果把 (61)节点删除，就不是完全二叉树了，因为叶子节点不连续了 ② 二叉树遍历 tips: 看输出父节点的顺序，就确定是前序，中序还是后序 示例一 节点 class treeNode { int no; String name; treeNode leftChildNode; treeNode rightChildNode; public treeNode(int no, String name) { this.no = no; this.name = name; } @Override public String toString() { return \"treeNode{\" + \"no=\" + no + \", name='\" + name + '\\'' + '}'; } //前序遍历public void preShow() //中序遍历public void infixShow() //后序遍历public void postShow() } 前序遍历: 先输出父节点，再假递归遍历左子树和右子树 前序就是 父节点-\u003e左子树-\u003e右子树 ABDHIEJCFG public void preShow(){ System.out.println(this); if (this.leftChildNode!=null){ this.leftChildNode.preShow();//此处还不叫递归，只是调用另一个对象的这个方法 } if (this.rightChildNode!=null){ this.rightChildNode.preShow(); } } 中序遍历: 先假递归遍历左子树，再输出父节点，再假递归遍历右子树 中序就是左子树-\u003e父节点-\u003e右子树 则图所示二叉树的前序遍历输出为： HDIBJEAFCG public void infixShow(){ if (this.leftChildNode!=null){ this.leftChildNode.infixShow(); } System.out.println(this); if (this.rightChildNode!=null){ this.rightChildNode.infixShow(); } } 后序遍历: 先递归遍历左子树，再递归遍历右子树，最后输出父节点 后序是 左子树 -\u003e 右子树 -\u003e父节点 HIDJEBFGCA public void postShow(){ if (this.leftChildNode!=null){ this.leftChildNode.postShow(); } if (this.rightChildNode!=null){ this.rightChildNode.postShow(); } System.out.println(this); } 示例二 前上图的 3号节点 “卢俊” , 增加一个左子节点 [5, 关胜] 代码实现 package binaryTree; public class binaryTree { public static void main(String[] args) { treeManager treeManager = new treeManager(new treeNode(1, \"宋江\")); treeNode n2 = new treeNode(2, \"吴用\"); treeNode n3 = new treeNode(3, \"卢俊义\"); treeNode n4 = new treeNode(4, \"林冲\"); treeManager.root.leftChildNode = n2; treeManager.root.rightChildNode = n3; n3.rightChildNode = n4; System.out.println(\"前序遍历：\"); treeManager.preShow(); System.out.println(); System.out.println(\"后序遍历：\"); treeManager.postShow(); System.out.println(); System.out.println(\"中序遍历：\"); treeManager.infixShow(); System.out.println(\"插入节点后\"); treeNode n5 = new treeNode(5, \"关胜\"); treeManager.add(n5); System.out.println(\"前序遍历：\"); treeManager.preShow(); System.out.println(); System.out.println(\"后序遍历：\"); treeManager.postShow(); System.out.println(); System.out.println(\"中序遍历：\"); treeManager.infixShow(); } } class treeManager { treeNode root; public treeManager(treeNode root) { this.root = root; } //添加节点 public void add(treeNode node) { root.add(node); } public boolean isEmpty() { return this.root == null; } public void preShow() { if (isEmpty()) { System.out.println(\"树为空\"); return; } root.preShow(); } public void infixShow() { if (isEmpty()) { System.out.println(\"树为空\"); return; } root.infixShow(); } public void postShow() { if (isEmpty()) { System.out.println(\"树为空\"); return; } root.postShow(); } } class treeNode { int no; String name; treeNode leftChildNode; treeNode rightChildNode; public treeNode(int no, String name) { this.no = no; this.name = name; } @Override public String toString() { return \"treeNode{\" + \"no=\" + no + \", name='\" + name + '\\'' + '}'; } //前序遍历 public void add(treeNode node) { if (this.no == 3) { this.leftChildNode = node; } if (this.leftChildNode != null) { this.leftChildNode.add(node); } if (this.rightChildNode != null) { this.rightChildNode.add(node); } } public void preShow() { System.out.println(this); if (this.leftChildNode != null) { this.leftChildNode.preShow(); } if (this.rightChildNode != null) { this.rightChildNode.preShow(); } } //中序遍历 public void infixShow() { if (this.leftChildNode != null) { this.leftChildNode.infixShow(); } System.out.println(this); if (this.rightChildNode != null) { this.rightChildNode.infixShow(); } } //后序遍历 public void postShow() { if (this.leftChildNode != null) { this.leftChildNode.postShow(); } if (this.rightChildNode != null) { this.rightChildNod","date":"2022-07-07","objectID":"/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95java%E7%89%88/:10:1","tags":[],"title":"数据结构与算法（java版）","uri":"/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95java%E7%89%88/"},{"categories":["数据结构与算法"],"content":"2. 树结构的实际应用 ① 堆排序 如右：锚点连接 ② 赫夫曼树 给定n个权值作为n个叶子结点，构造一棵二叉树，若该树的带权路径长度(wpl)达到最小，称这样的二叉树为最优二叉树，也称为哈夫曼树(Huffman Tree), 还有的书翻译为霍夫曼树。赫夫曼树是带权路径长度最短的树，权值较大的结点离根较近。 路径和路径的长度：在一棵树中，从一个结点往下可以达到的孩子或孙子结点之间的通路，称为路径。通路中分支节点的数目称为路径长度。若规定根结点的层数为1，则从根结点到第L层结点的路径长度为L-1 结点的权及带权路径长度：若将树中结点赋给一个有着某种含义的数值，则这个数值称为该结点的权。结点的带权路径长度为：从根结点到该结点之间的路径长度与该结点的权的乘积 树的带权路径长度：树的带权路径长度规定为所有叶子结点的带权路径长度之和，记为WPL(weighted path length) ,权值越大的结点离根结点越近的二叉树才是最优二叉树。WPL最小的就是赫夫曼树 构成赫夫曼树的步骤： 从小到大进行排序, 将每一个数据，每个数据都是一个节点，每个节点可以看成是一颗最简单的二叉树 取出根节点权值最小的两颗二叉树 组成一颗新的二叉树, 该新的二叉树的根节点的权值是前面两颗二叉树根节点权值的和 再将这颗新的二叉树，以根节点的权值大小 再次排序,不断重复1-2-3-4的步骤，直到数列中，所有的数据都被处理，就得到一颗赫夫曼树 package binaryTree; import java.util.ArrayList; import java.util.Collections; /* * 赫夫曼树创建 * */ public class huffmanTree { public static void main(String[] args) { int[] arr = {13, 7, 8, 3, 29, 6, 1}; node tree = createHuffmanTree(arr); tree.preShow(tree); } public static node createHuffmanTree(int[] arr) { //创建节点,并塞入集合之中 ArrayList\u003cnode\u003e nodes = new ArrayList\u003c\u003e(); for (int val : arr) { nodes.add(new node(val)); } while (nodes.size() \u003e 1) { //排序 Collections.sort(nodes); //去除权值最小的两个node组成新的二叉树 node left = nodes.get(0); node right = nodes.get(1); node parent = new node(left.val + right.val); parent.left = left; parent.right = right; nodes.remove(0); nodes.remove(0);//这里的索引应该还是0，前面已经移除了一个元素 //将新的节点加进去 nodes.add(parent); } return nodes.get(0); } } class node implements Comparable\u003cnode\u003e { int val;//哈夫曼树的权值 node left; node right; public node(int val) { this.val = val; } @Override public String toString() { return \"node{\" + \"val=\" + val + '}'; } @Override public int compareTo(node o) { return this.val - o.val; } public void preShow(node root) { node n = root; if (n == null) return; System.out.println(n); //左右子节点分别递归 if (n.left != null) preShow(n.left); if (n.right != null) preShow(n.right); } } ③ 赫夫曼编码 赫夫曼编码也翻译为哈夫曼编码(Huffman Coding)，又称霍夫曼编码，是一种编码方式, 属于一种程序算法。赫夫曼编码是赫哈夫曼树在电讯通信中的经典的应用之一。赫夫曼编码广泛地用于数据文件压缩。其压缩率通常在20%～90%之间赫夫曼码是可变字长编码(VLC)的一种。Huffman于1952年提出一种编码方法，称之为最佳编码 变长编码（❌） i like like like java do you like a java// 共40个字符(包括空格) d:1 y:1 u:1 j:2 v:2 o:2 l:4 k:4 e:4 i:5 a:5 :9 // 各个字符对应的个数，9对应的是空格 0= , 1=a, 10=i, 11=e, 100=k, 101=l, 110=o, 111=v, 1000=j, 1001=u, 1010=y, 1011=d 说明：按照各个字符出现的次数进行编码，原则是出现次数越多的，则编码越小，比如 空格出现了9次，编码为0，其它依次类推。按照上面给各个字符规定的编码，则我们在传输\"i like like like java do you like a java\"数据时，编码就是10010110100... 字符的编码都不能是其他字符编码的前缀，符合此要求的编码叫做前缀编码， 即不能匹配到重复的编码。意思是：当计算机在扫描编码数据时，首先得到1，匹配a字符，但是后面还有个0,可以匹配空格，也可以10匹配i字符，这样的编码方式就导致某个字符对应的编码是另一个字符对应编码的前缀（即不符合前缀编码），这样就存在不好解码的问题 赫夫曼编码 赫夫曼编码原理 i like like like java do you like a java // 共40个字符(包括空格) d:1 y:1 u:1 j:2 v:2 o:2 l:4 k:4 e:4 i:5 a:5 :9 各个字符对应的个数 按照上面字符出现的次数构建一颗赫夫曼树, 次数作为权值(如图)： 根据赫夫曼树，给各个字符规定编码，向左的路径为0，向右的路径为1。（一定是前缀编码）编码如下： o: 1000 u: 10010 d: 100110 y: 100111 i: 101a : 110 k: 1110 e: 1111 j: 0000 v: 0001l: 001 : 01//空格 按照上面的赫夫曼编码，我们的i like like like java do you like a java字符串对应的编码为 (注意这里我们使用的无损压缩)： 1010100110111101111010011011110111101001101111011110100001100001110011001111000011001111000100100100110111101111011100100001100001110 长度为：133 说明:原来长度是359，压缩了(359-133)/359 = 62.9%。此编码满足前缀编码, 即字符的编码都不能是其他字符编码的前缀，不会造成匹配的多义性 注意：这个赫夫曼树根据排序方法不同，也可能不太一样，这样对应的赫夫曼编码也不完全一样，但是wpl是一样的，都是最小的, 比如：如果我们让每次生成的新的二叉树总是排在权值相同的二叉树的最后一个，则生成的二叉树为： 最佳实践–数据压缩与解压 将给出的一段文本，比如 “i like like like java do you like a java” ， 根据前面的讲的赫夫曼编码原理，对其进行数据压缩处理 ，形式如下 1010100110111101111010011011110111101001101111011110100001100001110011001111000011001111000100100100110111101111011100100001100001110 步骤1：根据赫夫曼编码压缩数据的原理，需要创建 “i like like like java do you like a java” 对应的赫夫曼树 步骤2：生成赫夫曼树对应的赫夫曼编码表，如下表 :=01 a=100 d=11000 u=11001 e=1110 v=11011 i=101 y=11010 j=0010 k=1111 l=000 o=0011 步骤3：使用赫夫曼编码来生成赫夫曼编码数据 ,即按照上面的赫夫曼编码表，将\"i like like like java do you like a java\"字符串生成对应的编码数据, 形式如下（目前只是编码好了，还未压缩） 1010100010111111110010001011111111001000101111111100100101001101110001110000011011101000111100101000101111111100110001001010011011100 步骤4：将编码好的数据进行压缩，每8位二进制字符串为组成一个十进制或16进制数据，这样可以减少二进制字符串的长度，从而达到压缩的目的，值得注意的是最后8位的特殊处理，因为可能赫夫曼编码过后不满足8位，此时就需要记录这个有多","date":"2022-07-07","objectID":"/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95java%E7%89%88/:10:2","tags":[],"title":"数据结构与算法（java版）","uri":"/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95java%E7%89%88/"},{"categories":["数据结构与算法"],"content":"3. B树 B-tree树即B树，B即Balanced，平衡的意思。有人把B-tree翻译成B-树，容易让人产生误解。会以为B-树是一种树，而B树又是另一种树。实际上，B-tree就是指的B树。 ① 二叉树的问题分析 二叉树的操作效率较高，但是也存在问题, 请看下面的二叉树 二叉树需要加载到内存的，如果二叉树的节点少，没有什么问题，但是如果二叉树的节点很多(比如1亿)，就存在如下问题: 问题1：在构建二叉树时，需要多次进行i/o操作(海量数据存在数据库或文件中)，节点海量，构建二叉树时，速度有影响 问题2：节点海量，也会造成二叉树的高度很大，会降低操作速度 ② 多叉树 在二叉树中，每个节点有数据项，最多有两个子节点。如果允许每个节点可以有更多的数据项和更多的子节点，就是多叉树（multiway tree） 后面我们讲解的2-3树，2-3-4树就是多叉树，多叉树通过重新组织节点，减少树的高度，能对二叉树进行优化。 举例说明(下面2-3树就是一颗多叉树) ③ B树 B树通过重新组织节点，降低树的高度，并且减少i/o读写次数来提升效率。 如图B树通过重新组织节点，降低了树的高度。 文件系统及数据库系统的设计者利用了磁盘预读原理，将一个节点的大小设为等于一个页(页得大小通常为4k)，这样每个节点只需要一次I/O就可以完全载入 将树的度M设置为1024，在600亿个元素中最多只需要4次I/O操作就可以读取到想要的元素, B树(B+)广泛应用于文件存储系统以及数据库系统中 B树的说明： B树的阶：节点的最多子节点个数。比如2-3树的阶是3，2-3-4树的阶是4 B树的搜索：从根结点开始，对结点内的关键字（有序）序列进行二分查找，如果命中则结束，否则进入查询关键字所属范围的儿子结点；重复，直到所对应的儿子指针为空，或已经是叶子结点 关键字集合分布在整颗树中, 即叶子节点和非叶子节点都存放数据 搜索有可能在非叶子结点结束 其搜索性能等价于在关键字全集内做一次二分查找 2-3树基本介绍 2-3树是最简单的B树结构, 具有如下特点： 2-3树的所有叶子节点都在同一层(只要是B树都满足这个条件) 有两个子节点的节点叫二节点，二节点要么没有子节点，要么有两个子节点 有三个子节点的节点叫三节点，三节点要么没有子节点，要么有三个子节点 2-3树是由二节点和三节点构成的树。 2-3树应用案例 将数列{16, 24, 12, 32, 14, 26, 34, 10, 8, 28, 38, 20}构建成2-3树，并保证数据插入的大小顺序。 2-3树插入规则: 2-3树的所有叶子节点都在同一层(只要是B树都满足这个条件) 有两个子节点的节点叫二节点，二节点要么没有子节点，要么有两个子节点 有三个子节点的节点叫三节点，三节点要么没有子节点，要么有三个子节点 当按照规则插入一个数到某个节点时，不能满足上面三个要求，就需要拆，先向上拆，如果上层满，则拆本层，拆后仍然需要满足上面3个条件。 对于三节点的子树的值大小仍然遵守(BST二叉排序树)的规则 除了23树，还有234树等，概念和23树类似，也是一种B树。 ⑤ B+树 前面已经介绍了2-3树和2-3-4树，他们就是B树(英语：B-tree 也写成B-树)，这里我们再做一个说明，我们在学习Mysql时，经常听到说某种类型的索引是基于B树或者B+树的，如图: B+树的说明: B+树的搜索与B树也基本相同，区别是B+树只有达到叶子结点才命中（B树可以在非叶子结点命中），其性能也等价于在关键字全集做一次二分查找 所有关键字都出现在叶子结点的链表中（即数据只能在叶子节点【也叫稠密索引】），且链表中的关键字(数据)恰好是有序的。 不可能在非叶子结点命中 非叶子结点相当于是叶子结点的索引（稀疏索引），叶子结点相当于是存储（关键字）数据的数据层 更适合文件索引系统 B树和B+树各有自己的应用场景，不能说B+树完全比B树好，反之亦然. ⑥ B*树 B*树是B+树的变体，在B+树的非根和非叶子结点再增加指向兄弟的指针。 B树的说明: B树定义了非叶子结点关键字个数至少为(2/3)*M，即块的最低使用率为2/3，而B+树的块的最低使用率为B+树的1/2。 从第1个特点我们可以看出，B*树分配新结点的概率比B+树要低，空间使用率更高 ","date":"2022-07-07","objectID":"/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95java%E7%89%88/:10:3","tags":[],"title":"数据结构与算法（java版）","uri":"/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95java%E7%89%88/"},{"categories":["数据结构与算法"],"content":"八、图 ","date":"2022-07-07","objectID":"/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95java%E7%89%88/:11:0","tags":[],"title":"数据结构与算法（java版）","uri":"/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95java%E7%89%88/"},{"categories":["数据结构与算法"],"content":"1. 图的基本介绍 为什么要有图 线性表局限于一个直接前驱和一个直接后继的关系 树也只能有一个直接前驱也就是父节点 当我们需要表示多对多的关系时， 这里我们就用到了图 图的举例说明 图是一种数据结构，其中结点可以具有零个或多个相邻元素。两个结点之间的连接称为边。 结点也可以称为顶点。如图： 图的常用概念 图的表示方式 图的表示方式有两种：二维数组表示（邻接矩阵）；链表表示（邻接表）。 邻接矩阵 邻接矩阵是表示图形中顶点之间相邻关系的矩阵，对于n个顶点的图而言，矩阵是的row和col表示的是1~n个点。1表示直接连接，0表示不能直接连接。 邻接表 邻接矩阵需要为每个顶点都分配n个边的空间，其实有很多边都是不存在,会造成空间的一定损失。邻接表的实现只关心存在的边，不关心不存在的边。因此没有空间浪费，邻接表由数组+链表组成。 创建图 package graph; import java.util.ArrayList; import java.util.Arrays; /* * 图 * */ public class graph { private ArrayList\u003cString\u003e vertexList;//存储顶点集合 private int[][] edges;//表示图对应的邻接矩阵 private int numOfEdge;//表示边的数目 public static void main(String[] args) { int n = 5; String[] vertexes = {\"A\", \"B\", \"C\", \"D\", \"E\"}; graph g = new graph(n); //添加顶点 for (String v : vertexes) { g.insertVertex(v); } //添加边 g.insertEdge(0, 1, 1); g.insertEdge(0, 2, 1); g.insertEdge(1, 2, 1); g.insertEdge(1, 3, 1); g.insertEdge(1, 4, 1); g.show(); } public graph(int n) { this.edges = new int[n][n]; this.numOfEdge = 0; this.vertexList = new ArrayList\u003c\u003e(n); } //返回节点的个数 public int getNumOfVertex() { return this.vertexList.size(); } //返回边的个数 public int getNumOfEdge() { return this.numOfEdge; } //返回节点对应的数据 public String getValueByIndex(int index) { return this.vertexList.get(index); } //返回权值 public int getWeight(int index1, int index2) { return this.edges[index1][index2]; } //插入节点 public void insertVertex(String vertex) { this.vertexList.add(vertex); } //添加边 public void insertEdge(int index1, int index2, int weight) { this.edges[index1][index2] = weight; this.edges[index2][index1] = weight; } //显示图对应的矩阵 public void show() { for (int[] v1 : this.edges) { System.out.println(Arrays.toString(v1)); } } } ","date":"2022-07-07","objectID":"/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95java%E7%89%88/:11:1","tags":[],"title":"数据结构与算法（java版）","uri":"/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95java%E7%89%88/"},{"categories":["数据结构与算法"],"content":"2. 图的深度优先算法（DFS） 深度优先搜索类似于树的先序遍历。如其名称中所暗含的意思一样，这种搜索算法所遵循的搜索策略是尽可能“深”地搜索一个图。它的基本思想如下:首先访问图中某一起始顶点v，然后由v出发，访问与v邻接且未被访问的任一顶点w，再访问与w邻接且未被访问的任一顶点…重复上述过程。当不能再继续向下访问时，依次退回（回溯）到最近被访问的顶点，若它还有邻接顶点未被访问过，则从该点开始继续上述搜索过程，直至图中所有顶点均被访问过为止。 算法过程 访问初始结点v，并标记结点v为已访问 查找结点v的第一个邻接结点w 若w存在，则继续执行第4步；如果w不存在，则回到第1步，将从v的下一个结点继续 若w未被访问，对w进行深度优先遍历递归（即把w当做另一个v，然后进行步骤123）；若w已被访问，查找结点v的w邻接结点的下一个未访问的邻接结点，转到第3步 代码实现 bool visited[MAX_VERTEX_NUM]; //访问标记数组 /*从顶点出发，深度优先遍历图G*/ void DFS(Graph G, int v){ int w; visit(v); //访问顶点 visited[v] = TRUE; //设已访问标记 //FirstNeighbor(G,v):求图G中顶点v的第一个邻接点，若有则返回顶点号，否则返回-1。 //NextNeighbor(G,v,w):假设图G中顶点w是顶点v的一个邻接点，返回除w外顶点v for(w = FirstNeighbor(G, v); w\u003e=0; w=NextNeighor(G, v, w)){ if(!visited[w]){ //w为u的尚未访问的邻接顶点 DFS(G, w); } } } /*对图进行深度优先遍历*/ void DFSTraverse(MGraph G){ int v; for(v=0; v\u003cG.vexnum; ++v){ visited[v] = FALSE; //初始化已访问标记数据 } for(v=0; v\u003cG.vexnum; ++v){ //从v=0开始遍历 if(!visited[v]){ DFS(G, v); } } } ","date":"2022-07-07","objectID":"/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95java%E7%89%88/:11:2","tags":[],"title":"数据结构与算法（java版）","uri":"/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95java%E7%89%88/"},{"categories":["数据结构与算法"],"content":"3. 图的广度优先算法（BFS） 如果说图的深度优先遍历类似树的前序遍历，那么图的广度优先遍历就类似于树的层序遍历了。 广度优先搜索是一种分层的查找过程，每向前走一步可能访问一批顶点，不像深度优先搜索那样有往回退（回溯）的情况，因此它不是一个递归的算法。为了实现逐层的访问，算法必须借助一个辅助队列，以记忆正在访问的顶点（只有知道了上层顶点，才可以访问邻接的下层顶点，所以需要记录正在访问的顶点。使用一个队列以保持访问过的结点的顺序，以便按这个顺序来访问这些结点的邻接结点）。 算法过程 访问初始结点v并标记结点v为已访问 结点v入队列 当队列非空时，继续执行，否则算法结束 出队列，取得队头结点u 查找结点u的第一个邻接结点w 若结点u的邻接结点w不存在，则转到步骤3；否则循环执行以下三个步骤： 若结点w尚未被访问，则访问结点w并标记为已访问 结点w入队列 查找结点u的继w邻接结点后的下一个邻接结点w，转到步骤6 代码实现 /*邻接矩阵的广度遍历算法*/ void BFSTraverse(MGraph G){ int i, j; Queue Q; for(i = 0; i\u003cG,numVertexes; i++){ visited[i] = FALSE; } InitQueue(\u0026Q); //初始化一辅助用的队列 for(i=0; i\u003cG.numVertexes; i++){ //若是未访问过就处理 if(!visited[i]){ vivited[i] = TRUE; //设置当前访问过 visit(i); //访问顶点 EnQueue(\u0026Q, i); //将此顶点入队列 //若当前队列不为空 while(!QueueEmpty(Q)){ DeQueue(\u0026Q, \u0026i); //顶点i出队列 //FirstNeighbor(G,v):求图G中顶点v的第一个邻接点，若有则返回顶点号，否则返回-1。 //NextNeighbor(G,v,w):假设图G中顶点w是顶点v的一个邻接点，返回除w外顶点v for(j=FirstNeighbor(G, i); j\u003e=0; j=NextNeighbor(G, i, j)){ //检验i的所有邻接点 if(!visited[j]){ visit(j); //访问顶点j visited[j] = TRUE; //访问标记 EnQueue(Q, j); //顶点j入队列 } } } } } } ","date":"2022-07-07","objectID":"/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95java%E7%89%88/:11:3","tags":[],"title":"数据结构与算法（java版）","uri":"/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95java%E7%89%88/"},{"categories":["数据结构与算法"],"content":"九、十大算法 ","date":"2022-07-07","objectID":"/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95java%E7%89%88/:12:0","tags":[],"title":"数据结构与算法（java版）","uri":"/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95java%E7%89%88/"},{"categories":["数据结构与算法"],"content":"1. 查找算法 锚点连接 ","date":"2022-07-07","objectID":"/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95java%E7%89%88/:12:1","tags":[],"title":"数据结构与算法（java版）","uri":"/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95java%E7%89%88/"},{"categories":["数据结构与算法"],"content":"2. 分治算法 ① 基本概念 在计算机科学中，分治法是一种很重要的算法。字面上的解释是“分而治之”，就是把一个复杂的问题分成两个或更多的相同或相似的子问题，再把子问题分成更小的子问题……直到最后子问题可以简单的直接求解，原问题的解即子问题的解的合并。这个技巧是很多高效算法的基础，如排序算法(快速排序，归并排序)，傅立叶变换(快速傅立叶变换)…… 任何一个可以用计算机求解的问题所需的计算时间都与其规模有关。问题的规模越小，越容易直接求解，解题所需的计算时间也越少。例如，对于n个元素的排序问题，当n=1时，不需任何计算。n=2时，只要作一次比较即可排好序。n=3时只要作3次比较即可。而当n较大时，问题就不那么容易处理了。要想直接解决一个规模较大的问题，有时是相当困难的 ② 基本思想及策略 分治法的设计思想是：将一个难以直接解决的大问题，分割成一些规模较小的相同问题，以便各个击破，分而治之。 分治策略是：对于一个规模为n的问题，若该问题可以容易地解决（比如说规模n较小）则直接解决，否则将其分解为k个规模较小的子问题，这些子问题互相独立且与原问题形式相同，递归地解这些子问题，然后将各子问题的解合并得到原问题的解。这种算法设计策略叫做分治法。 如果原问题可分割成k个子问题，1\u003ck≤n，且这些子问题都可解并可利用这些子问题的解求出原问题的解，那么这种分治法就是可行的。由分治法产生的子问题往往是原问题的较小模式，这就为使用递归技术提供了方便。在这种情况下，反复应用分治手段，可以使子问题与原问题类型一致而其规模却不断缩小，最终使子问题缩小到很容易直接求出其解。这自然导致递归过程的产生。分治与递归像一对孪生兄弟，经常同时应用在算法设计之中，并由此产生许多高效算法。 ③ 适用的情况 分治法所能解决的问题一般具有以下几个特征： 该问题的规模缩小到一定的程度就可以容易地解决 该问题可以分解为若干个规模较小的相同问题，即该问题具有最优子结构性质 利用该问题分解出的子问题的解可以合并为该问题的解 该问题所分解出的各个子问题是相互独立的，即子问题之间不包含公共的子子问题 第一条特征是绝大多数问题都可以满足的，因为问题的计算复杂性一般是随着问题规模的增加而增加 第二条特征是应用分治法的前提，它也是大多数问题可以满足的，此特征反映了递归思想的应用 第三条特征是关键，能否利用分治法完全取决于问题是否具有第三条特征，如果具备了第一条和第二条特征，而不具备第三条特征，则可以考虑用贪心法或动态规划法 第四条特征涉及到分治法的效率，如果各子问题是不独立的则分治法要做许多不必要的工作，重复分解公共的子问题，此时虽然可用分治法，但一般用动态规划法较好 ④ 基本步骤 分治法在每一层递归上都有三个步骤： 分解：将原问题分解为若干个规模较小，相互独立，与原问题形式相同的子问题； 解决：若子问题规模较小而容易被解决则直接解决，否则递归地解决各个子问题 合并：将各个子问题的解合并为原问题的解。 它的一般的算法设计模式如下： Divide-and-Conquer(P)： if |P|\u003c=n~0~ then return(ADHOC(P)) 将P分解为较小的子问题 P~1~ ,P~2~ ,…,P~k~ for i ← 1 to k do y~i~ ← Divide-and-Conquer(P~i~) 递归解决P~i~ T ← MERGE(y~1~,y~2~,…,y~k~) 合并子问题 return(T) 其中|P|表示问题P的规模；n~0~为一阈值，表示当问题P的规模不超过n~0~时，问题已容易直接解出，不必再继续分解。ADHOC(P)是该分治法中的基本子算法，用于直接解小规模的问题P。因此，当P的规模不超过n0时直接用算法ADHOC(P)求解。算法MERGE(y~1~,y~2~,…,y~k~)是该分治法中的合并子算法，用于将P的子问题P~1~ ,P~2~ ,…,P~k~的相应的解y~1~,y~2~,…,y~k~合并为P的解。 ⑤ 复杂性分析 一个分治法将规模为n的问题分成k个规模为n／m的子问题去解。设分解阀值n0=1，且ADHOC(基本子算法)解规模为1的问题耗费1个单位时间。再设将原问题分解为k个子问题以及用merge将k个子问题的解合并为原问题的解需用f(n)个单位时间。用T(n)表示该分治法解规模为|P| = n的问题所需的计算时间，则有： T(n)= k T(n/m)+f(n) 通过迭代法求得方程的解： 递归方程及其解只给出n等于m的方幂时T(n)的值，但是如果认为T(n)足够平滑，那么由n等于m的方幂时T(n)的值可以估计T(n)的增长速度。通常假定T(n)是单调上升的，从而当mi≤n\u003cmi+1时，T(mi)≤T(n)\u003cT(mi+1)。 ⑥ 分治法求解的一些经典问题 二分搜索 大整数乘法 Strassen矩阵乘法 棋盘覆盖 合并排序 快速排序 线性时间选择 最接近点对问题 循环赛日程表 汉诺塔 ⑦ 依据分治法设计程序时的思维过程 实际上就是类似于数学归纳法，找到解决本问题的求解方程公式，然后根据方程公式设计递归程序。 一定是先找到最小问题规模时的求解方法 然后考虑随着问题规模增大时的求解方法 找到求解的递归函数式后（各种规模或因子），设计递归程序即可。 ⑧ 案例 汉诺塔 题目 汉诺塔：汉诺塔（又称河内塔）问题是源于印度一个古老传说的益智玩具。大梵天创造世界的时候做了三根金刚石柱子，在一根柱子上从下往上按照大小顺序摞着64片黄金圆盘。大梵天命令婆罗门把圆盘从下面开始按大小顺序重新摆放在另一根柱子上。并且规定，在小圆盘上不能放大圆盘，在三根柱子之间一次只能移动一个圆盘。 代码 package algorithm.dac; public class hanoitower { public static void main(String[] args) { move(5, 'A', 'B', 'C'); } //将第num个盘从a移到c处，中间借助b public static void move(int num, char a, char b, char c) { if (num == 1) { System.out.println(\"第\" + num + \"个盘从 \" + a + \"-\u003e\" + c); return; } //先将最上面的所有盘，从a移动b move(num - 1, a, c, b); //把最下边的盘，从a移动到c System.out.println(\"第\" + num + \"个盘从 \" + a + \"-\u003e\" + c); // move(1,a,b,c); //再把b塔所有盘，从b移动至c，借助a塔 move(num - 1, b, a, c); } } ","date":"2022-07-07","objectID":"/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95java%E7%89%88/:12:2","tags":[],"title":"数据结构与算法（java版）","uri":"/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95java%E7%89%88/"},{"categories":["数据结构与算法"],"content":"3. 动态规划 ① 算法的核心 理解一个算法就要理解一个算法的核心，动态规划算法的核心是一个小故事。 A \"1+1+1+1+1+1+1+1 =？\" A : \"上面等式的值是多少\" B : 计算 \"8\" A : 在上面等式的左边写上 \"1+\" A : \"此时等式的值为多少\" B : quickly* \"9\" A : \"你怎么这么快就知道答案了\" A : \"只要在8的基础上加1就行了\" A : \"所以你不用重新计算因为你记住了第一个等式的值为8!动态规划算法也可以说是 '记住求过的解来节省时间'\" 由上面的小故事可以知道动态规划算法的核心就是记住已经解决过的子问题的解。 ② 算法的两种形式 上面已经知道动态规划算法的核心是记住已经求过的解，记住求解的方式有两种：①自顶向下的备忘录法 ②自底向上。 为了说明动态规划的这两种方法，举一个最简单的例子：求斐波拉契数列**Fibonacci **。先看一下这个问题： Fibonacci (n) = 1; n = 0 Fibonacci (n) = 1; n = 1 Fibonacci (n) = Fibonacci(n-1) + Fibonacci(n-2) 以前学c语言的时候写过这个算法使用递归十分的简单。先使用递归版本来实现这个算法： public int fib(int n){ if(n\u003c=0) return 0; if(n==1) return 1; return fib( n-1)+fib(n-2); } //输入6 //输出：8 先来分析一下递归算法的执行流程，假如输入6，那么执行的递归树如下： 上面的递归树中的每一个子节点都会执行一次，很多重复的节点被执行，fib(2)被重复执行了5次。由于调用每一个函数的时候都要保留上下文，所以空间上开销也不小。这么多的子节点被重复执行，如果在执行的时候把执行过的子节点保存起来，后面要用到的时候直接查表调用的话可以节约大量的时间。下面就看看动态规划的两种方法怎样来解决斐波拉契数列**Fibonacci **数列问题。 自顶向下的备忘录法 public static int Fibonacci(int n){ if(n\u003c=0) return n; int []Memo=new int[n+1]; for(int i=0;i\u003c=n;i++) Memo[i]=-1; return fib(n, Memo); } public static int fib(int n,int []Memo){ if(Memo[n]!=-1) return Memo[n]; //如果已经求出了fib（n）的值直接返回，否则将求出的值保存在Memo备忘录中。 if(n\u003c=2) Memo[n]=1; else Memo[n]=fib( n-1,Memo)+fib(n-2,Memo); return Memo[n]; } 备忘录法也是比较好理解的，创建了一个n+1大小的数组来保存求出的斐波拉契数列中的每一个值，在递归的时候如果发现前面fib（n）的值计算出来了就不再计算，如果未计算出来，则计算出来后保存在Memo数组中，下次在调用fib（n）的时候就不会重新递归了。比如上面的递归树中在计算fib（6）的时候先计算fib（5），调用fib（5）算出了fib（4）后，fib（6）再调用fib（4）就不会在递归fib（4）的子树了，因为fib（4）的值已经保存在Memo[4]中。 暴力递归之中有很多节点需要重复计算，为了减少重复计算，需要使用一个缓存表（一般是一位或二位数组）记录当前递归节点的结果，以后需要再次使用时，无需递归，直接从缓存里拿结果，从而提升速度。即记忆化搜索 自底向上的动态规划 备忘录法还是利用了递归，上面算法不管怎样，计算fib（6）的时候最后还是要计算出fib（1），fib（2），fib（3）……,那么何不先计算出fib（1），fib（2），fib（3）……呢？这也就是动态规划的核心，先计算子问题，再由子问题计算父问题。 动态规划算法与分治算法类似，其基本思想也是将待求解问题分解成若干个子问题，先求解子问题，然后从这些子问题的解得到原问题的解。 与分治法不同的是，适合于用动态规划求解的问题，经分解得到子问题往往不是互相独立的。 ( 即下一个子阶段的求解是建立在上一个子阶段的解的基础上，进行进一步的求解 ) 动态规划可以通过填表的方式来逐步推进，得到最优解 public static int fib(int n){ if(n\u003c=0) return n; int []Memo=new int[n+1]; Memo[0]=0; Memo[1]=1; for(int i=2;i\u003c=n;i++) { Memo[i]=Memo[i-1]+Memo[i-2]; } return Memo[n]; } 自底向上方法也是利用数组保存了先计算的值，为后面的调用服务。观察参与循环的只有 i，i-1 , i-2三项，因此该方法的空间可以进一步的压缩如下。 public static int fib(int n){ if(n\u003c=1) return n; int Memo_i_2=0; int Memo_i_1=1; int Memo_i=1; for(int i=2;i\u003c=n;i++) { Memo_i=Memo_i_2+Memo_i_1; Memo_i_2=Memo_i_1; Memo_i_1=Memo_i; } return Memo_i; } 一般来说由于备忘录方式的动态规划方法使用了递归，递归的时候会产生额外的开销，使用自底向上的动态规划方法要比备忘录方法好。 ③ 案例 01背包问题 题目 背包问题：有一个背包，容量为4磅，现有如下物品。要求达到的目标为装入的背包的总价值最大，并且重量不超出要求装入的物品不能重复 物品 重量 价格 吉他(G) 1 1500 音响(S) 4 3000 电脑(L) 3 2000 思路 背包问题主要是指一个给定容量的背包、若干具有一定价值和重量的物品，如何选择物品放入背包使物品的价值最大。其中又分01背包和完全背包(完全背包指的是：每种物品都有无限件可用)这里的问题属于01背包，即每个物品最多放一个。而无限背包可以转化为01背包。 算法的主要思想，利用动态规划来解决。每次遍历到的第i个物品，根据w[i]和v[i]来确定是否需要将该物品放入背包中。即对于给定的n个物品，设v[i]、w[i]分别为第i个物品的价值和重量，C为背包的容量。再令v[i][j]表示在前i个物品中能够装入容量为j的背包中的最大价值。则我们有下面的结果： 物品 0 磅 1磅 2磅 3磅 4磅 0 0 0 0 0 吉他(G) 0 1500(G) 1500(G) 1500(G) 1500(G) 音响(S) 0 1500(G) 1500(G) 1500(G) 3000(S) 电脑(L) 0 1500(G) 1500(G) 2000(L) 3500(L+G) (1) v[i][0]=v[0][j]=0; //表示 填入表 第一行和第一列是0 (2) 当w[i]\u003e j 时：v[i][j]=v[i-1][j] //当准备加入新增的商品的容量大于当前背包的容量时，就直接使用上一个单元格的装入策略 (3) 当j\u003e=w[i]时： v[i][j]=max{v[i-1][j], v[i]+v[i-1][j-w[i]]} //当准备加入的新增的商品的容量小于等于当前背包的容量, // 装入的方式: v[i-1][j] ： 就是上一个单元格的装入的最大值 v[i] : 表示当前商品的价值 v[i-1][j-w[i]] ： 装入i-1商品，到剩余空间j-w[i]的最大值 当j\u003e=w[i]时 ： v[i][j]=max{v[i-1][j], v[i]+v[i-1][j-w[i]]} 代码 package algorithm.DP; import java.util.Arrays; /* * 01背包问题 * */ public class backpackProblem { public static void main(String[] args) { //创建表 int[][] v = new int[4][5];//前i个物品中能够装入容量为j的背包中的最大价值 int[] w = new int[4];//第i个物品重量 w[1] = 1; w[2] = 4; w[3] = 3; int[] a = new int[4];//物品第i个物品单价 a[1] = 1500; a[2] = 3000; a[3] = 2000; //开始决策填表 //填充第一列 int rowNum = v.length; for (int i = 0; i \u003c rowNum; i++) { v[i][0] = 0; } //填充第一行 int colNum = v[0].length; for (int j = 0; j \u003c colNum; j++) { v[0][j] = 0; } //填充第二行：只能放第一个商品 for (int j = 0; j \u003c colNum; j++) { if (w[1] \u003c= j) v[1][j] = a[1]; } //决策其他单元格 for (int i = 2; i \u003c rowNum; i++) { for (int j = 1; j \u003c colNum; j++) { //当前物品放不下，只有依赖上次决策的结果 if (w[i] \u003e j) v[i][j] = v[i - 1][j]; //当前物品放得下，","date":"2022-07-07","objectID":"/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95java%E7%89%88/:12:3","tags":[],"title":"数据结构与算法（java版）","uri":"/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95java%E7%89%88/"},{"categories":["数据结构与算法"],"content":"4. KMP算法 很详尽KMP算法（厉害） ① 暴力匹配算法 假设现在我们面临这样一个问题：有一个文本串S，和一个模式串P，现在要查找P在S中的位置，怎么查找呢？ 如果用暴力匹配的思路，并假设现在文本串S匹配到 i 位置，模式串P匹配到 j 位置，则有： 如果当前字符匹配成功（即S[i] == P[j]），则i++，j++，继续匹配下一个字符； 如果失配（即S[i]! = P[j]），令i = i - (j - 1)，j = 0。相当于每次匹配失败时，i 回溯，j 被置为0。 理清楚了暴力匹配算法的流程及内在的逻辑，咱们可以写出暴力匹配的代码，如下： int ViolentMatch(char* s, char* p) { int sLen = strlen(s); int pLen = strlen(p); int i = 0; int j = 0; while (i \u003c sLen \u0026\u0026 j \u003c pLen) { if (s[i] == p[j]) { //①如果当前字符匹配成功（即S[i] == P[j]），则i++，j++ i++; j++; } else { //②如果失配（即S[i]! = P[j]），令i = i - (j - 1)，j = 0 i = i - j + 1; j = 0; } } //匹配成功，返回模式串p在文本串s中的位置，否则返回-1 if (j == pLen) return i - j; else return -1; } 举个例子，如果给定文本串S:“BBC ABCDAB ABCDABCDABDE”，和模式串P:“ABCDABD”，现在要拿模式串P去跟文本串S匹配，整个过程如下所示： S[0]为B，P[0]为A，不匹配，执行第②条指令：“如果失配（即S[i]! = P[j]），令i = i - (j - 1)，j = 0\"。S[1]跟P[0]匹配，相当于模式串要往右移动一位（i=1，j=0） S[1]跟P[0]还是不匹配，继续执行第②条指令：“如果失配（即S[i]! = P[j]），令i = i - (j - 1)，j = 0”，S[2]跟P[0]匹配（i=2，j=0），从而模式串不断的向右移动一位（不断的执行“令i = i - (j - 1)，j = 0”，i从2变到4，j一直为0） 直到S[4]跟P[0]匹配成功（i=4，j=0），此时按照上面的暴力匹配算法的思路，转而执行第①条指令：“如果当前字符匹配成功（即S[i] == P[j]），则i++，j++”，可得S[i]为S[5]，P[j]为P[1]，即接下来S[5]跟P[1]匹配（i=5，j=1） S[5]跟P[1]匹配成功，继续执行第①条指令：“如果当前字符匹配成功（即S[i] == P[j]），则i++，j++”，得到S[6]跟P[2]匹配（i=6，j=2），如此进行下去 直到S[10]为空格字符，P[6]为字符D（i=10，j=6），因为不匹配，重新执行第②条指令：“如果失配（即S[i]! = P[j]），令i = i - (j - 1)，j = 0”，相当于S[5]跟P[0]匹配（i=5，j=0） 至此，我们可以看到，如果按照暴力匹配算法的思路，尽管之前文本串和模式串已经分别匹配到了S[9]、P[5]，但因为S[10]跟P[6]不匹配，所以文本串回溯到S[5]，模式串回溯到P[0]，从而让S[5]跟P[0]匹配。 而S[5]肯定跟P[0]失配。为什么呢？因为在之前第4步匹配中，我们已经得知S[5] = P[1] = B，而P[0] = A，即P[1] != P[0]，故S[5]必定不等于P[0]，所以回溯过去必然会导致失配。那有没有一种算法，让i不往回退，只需要移动j即可呢？ 答案是肯定的。这种算法就是本文的主旨KMP算法，它利用之前已经部分匹配这个有效信息，保持i不回溯，通过修改j的位置，让模式串尽量地移动到有效的位置。 ② KMP算法 定义 下面先直接给出KMP的算法流程 假设现在文本串S匹配到 i 位置，模式串P匹配到j位置 如果j = -1，或者当前字符匹配成功（即S[i] == P[j]），都令i++，j++，继续匹配下一个字符； 如果j != -1，且当前字符匹配失败（即S[i] != P[j]），则令 i 不变，j = next[j]。此举意味着失配时，模式串P相对于文本串S向右移动了j - next [j] 位。（注意：是相对，文本串没有移动，只是将模式串的j指针往前（向左）移动到next[j]位置，因此相对文本串来说：模式串S向右移动了j-next[j]位） 换言之，当匹配失败时，模式串向右移动的位数为：失配字符所在位置 - 失配字符对应的next值），即移动的实际位数为：j - next[j]，且此值大于等于1。 很快，你也会意识到next 数组各值的含义：代表当前字符之前的字符串中，有多大长度的相同前缀后缀。例如如果next [j] = k，代表j 之前的字符串中有最大长度为k 的相同前缀后缀。 此也意味着在某个字符失配时，该字符对应的next 值会告诉你下一步匹配中，模式串应该跳到哪个位置（跳到next [j] 的位置）。如果next [j] 等于0或-1，则跳到模式串的开头字符，若next [j] = k 且 k \u003e 0，代表下次匹配跳到j 之前的某个字符，而不是跳到开头，且具体跳过了k 个字符。 步骤 寻找前缀后缀最长公共元素长度 对于P= p~0~,p~1~…p~j-1~，p~j~，寻找模式串P中长度最大且相等的前缀和后缀。如果存在p~0~,p~1~ …p~k-1~,p~k~ = p~j-k~,p~j-k+1~…p~j-1~,p~j~，那么在包含p~j~的模式串中有最大长度为k+1的相同前缀后缀。举个例子，如果给定的模式串为“abab”，那么它的各个子串的前缀后缀的公共元素的最大长度如下表格所示： 比如对于字符串aba来说，它有长度为1的相同前缀后缀a；而对于字符串abab来说，它有长度为2的相同前缀后缀ab（相同前缀后缀的长度为k + 1，k + 1 = 2)。 根据前缀后缀最长公共元素长度，求next数组（也可以不求，直接根据最大长度表得出结果） 将第①步骤中求得的值整体右移一位，然后初值赋为-1，即结果 根据next数组进行匹配 匹配失配，j = next [j]，模式串向右移动的位数为：j - next[j]。换言之，当模式串的后缀p~j-k~,p~j-k+1~, …, p~j-1~ 跟文本串 s~i-k~,s~i-k+1~, …, s~i-1~匹配成功，但 p~j~ 跟 s~i~ 匹配失败时，因为next[j] = k，相当于在不包含 p~j~ 的模式串中有最大长度为 k 的相同前缀后缀，即p~0~,p~1~ …p~k-1~ = p~j-k~,p~j-k+1~…p~j-1~，故令j = next[j]，从而让模式串右移j - next[j] 位，使得模式串的前缀p~0~,p~1~…p~k-1~对应着文本串 s~i-k~,s~i-k+1~…s~i-1~，而后让p~k~跟s~i~继续匹配。如下图所示： 综上，KMP的next 数组相当于告诉我们：当模式串中的某个字符跟文本串中的某个字符匹配失配时，模式串下一步应该跳到哪个位置。如模式串中在j 处的字符跟文本串在i 处的字符匹配失配时，下一步用next [j] 处的字符继续跟文本串i 处的字符匹配，相当于模式串向右移动 j - next[j] 位。 代码 package algorithm.kmp; import java.util.Arrays; public class kmp { public static void main(String[] args) { String str1 = \"BBC ABCDAB ABCDABCDABDE\"; String str2 = \"ABCDABD\"; // String str2 = \"ABCDABDABCE\"; // String str2 = \"agctagcagctagct\"; // System.out.println(Arrays.toString(kmpNext(str2))); System.out.println(\"第一次出现的位置：\" + kmpSearch(str1, str2, kmpNext(str2))); } //kmp搜索 public static int kmpSearch(String str1, String str2, int[] next) { for (int i = 0, j = 0; i \u003c str1.length(); i++) { //在部分匹配表往前递归，知道找到相等字符值才停止， while (j \u003e 0 \u0026\u0026 str1.charAt(i) != str2.charAt(j)) { j = next[j - 1]; } if (str1.charAt(i) == str2.charAt(j)) { j++; } if (j == str2.length()) { return i - j + 1; } } return -1; } //获取子串的部分匹配值表 public static int[] kmpNext(String dest) { int[] next = new int[dest.length()]; for (int i = 1; i \u003c dest.length(); i++) { int j = next[i - 1];//j指针记录移动的位置，也是","date":"2022-07-07","objectID":"/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95java%E7%89%88/:12:4","tags":[],"title":"数据结构与算法（java版）","uri":"/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95java%E7%89%88/"},{"categories":["数据结构与算法"],"content":"5. 贪心算法 ① 基本概念 所谓贪心算法是指，在对问题求解时，总是做出在当前看来是最好的选择。也就是说，不从整体最优上加以考虑，它所做出的仅仅是在某种意义上的局部最优解。 贪心算法没有固定的算法框架，算法设计的关键是贪心策略的选择。必须注意的是，贪心算法不是对所有问题都能得到整体最优解，选择的贪心策略必须具备无后效性（即某个状态以后的过程不会影响以前的状态，只与当前状态有关。） 所以，对所采用的贪心策略一定要仔细分析其是否满足无后效性。 ② 贪心算法的基本思路 建立数学模型来描述问题 把求解的问题分成若干个子问题 对每个子问题求解，得到子问题的局部最优解 把子问题的解局部最优解合成原来问题的一个解 ③ 该算法存在的问题 不能保证求得的最后解是最佳的 不能用来求最大值或最小值的问题 只能求满足某些约束条件的可行解的范围 ④ 贪心算法适用的问题 贪心策略适用的前提是：局部最优策略能导致产生全局最优解。 实际上，贪心算法适用的情况很少。一般对一个问题分析是否适用于贪心算法，可以先选择该问题下的几个实际数据进行分析，就可以做出判断。 ⑤ 贪心选择性质 所谓贪心选择性质是指所求问题的整体最优解可以通过一系列局部最优的选择，换句话说，当考虑做何种选择的时候，我们只考虑对当前问题最佳的选择而不考虑子问题的结果。这是贪心算法可行的第一个基本要素。贪心算法以迭代的方式作出相继的贪心选择，每作一次贪心选择就将所求问题简化为规模更小的子问题。对于一个具体问题，要确定它是否具有贪心选择性质，必须证明每一步所作的贪心选择最终导致问题的整体最优解。 当一个问题的最优解包含其子问题的最优解时，称此问题具有最优子结构性质。问题的最优子结构性质是该问题可用贪心算法求解的关键特征。 ⑥ 贪心算法的实现框架 从问题的某一初始解出发： while (朝给定总目标前进一步) { 利用可行的决策，求出可行解的一个解元素。 } 由所有解元素组合成问题的一个可行解； ⑦ 例题分析 【背包问题】有一个背包，容量是M=150，有7个物品，物品可以分割成任意大小。要求尽可能让装入背包中的物品总价值最大，但不能超过总容量。 物品： 重量：35 30 60 50 40 10 25 价值：10 40 30 50 35 40 30 分析： 目标函数： ∑p~i~最大 约束条件是装入的物品总质量不超过背包容量：∑w~i~\u003c=M( M=150) （1）根据贪心的策略，每次挑选价值最大的物品装入背包，得到的结果是否最优？ （2）每次挑选所占重量最小的物品装入是否能得到最优解？ （3）每次选取单位重量价值最大的物品，成为解本题的策略 值得注意的是，贪心算法并不是完全不可以使用，贪心策略一旦经过证明成立后，它就是一种高效的算法。比如，求最小生成树的Prim算法和Kruskal算法都是漂亮的贪心算法。 贪心算法还是很常见的算法之一，这是由于它简单易行，构造贪心策略不是很困难。 可惜的是，它需要证明后才能真正运用到题目的算法中。 一般来说，贪心算法的证明围绕着：整个问题的最优解一定由在贪心策略中存在的子问题的最优解得来的。 对于例题中的3种贪心策略，都是无法成立（无法被证明）的，解释如下： 贪心策略：选取价值最大者。 反例： W=30 物品：A B C 重量：28 12 12 价值：30 20 20 根据策略，首先选取物品A，接下来就无法再选取了，可是，选取B、C则更好。 （2）贪心策略：选取重量最小。它的反例与第一种策略的反例差不多。 （3）贪心策略：选取单位重量价值最大的物品。反例： W=30 物品：A B C 重量：28 20 10 价值：28 20 10 根据策略，三种物品单位重量价值一样，程序无法依据现有策略作出判断，如果选择A，则答案错误。但是果在条件中加一句当遇见单位价值相同的时候,优先装重量小的,这样的问题就可以解决. 所以需要说明的是，贪心算法可以与随机化算法一起使用，具体的例子就不再多举了。（因为这一类算法普及性不高，而且技术含量是非常高的，需要通过一些反例确定随机的对象是什么，随机程度如何，但也是不能保证完全正确，只能是极大的几率正确） ","date":"2022-07-07","objectID":"/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95java%E7%89%88/:12:5","tags":[],"title":"数据结构与算法（java版）","uri":"/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95java%E7%89%88/"},{"categories":["数据结构与算法"],"content":"6. 最小生成树（MST） 定义： 在给定一张无向图，如果在它的子图中，任意两个顶点都是互相连通，并且是一个树结构，那么这棵树叫做生成树。当连接顶点之间的图有权重时，权重之和最小的树结构为最小生成树！ 性质： 给定一个带权的无向连通图,如何选取一棵生成树,使树上所有边上权的总和为最小,这叫最小生成树 N个顶点，一定有N-1条边 包含全部顶点N-1条边都在图中 举例说明(如图:) 求最小生成树的算法主要是普里姆算法和克鲁斯卡尔算法 ① Prim算法（普里姆算法） Prim算法是另一种贪心算法，和Kuskral算法的贪心策略不同，Kuskral算法主要对边进行操作，而Prim算法则是对节点进行操作，每次遍历添加一个点，这时候我们就不需要使用并查集了。具体步骤为： 设G=(V,E)是连通网，T=(U,D)是最小生成树，V,U是顶点集合，E,D是边的集合 若从顶点u开始构造最小生成树，则从集合V中取出顶点u放入集合U中，标记顶点v的visited[u]=1 若集合U中顶点u~i~与集合V-U中的顶点v~j~之间存在边，则寻找这些边中权值最小的边，但不能构成回路，将顶点v~j~加入集合U中，将边（u~i~,v~j~）加入集合D中，标记visited[v~j~]=1 重复步骤②，直到U与V相等，即所有顶点都被标记为访问过，此时D中有n-1条边 注意：对于单连通，从一个节点出发就可以直接找到完整的最小生成树，因此最外的for循环也可以为一个顺序结构，但多连通，就需要从不同的节点出发了！！！ 代码实现： package algorithm.prim; import java.util.Arrays; /* * prim算法解决最小生成树问题 * */ public class prim { public static void main(String[] args) { char[] vertexes = new char[]{'A', 'B', 'C', 'D', 'E', 'F', 'G'}; int vertexNum = vertexes.length; int max = Integer.MAX_VALUE; int[][] weight = new int[][]{ {max, 5, 7, max, max, max, 2}, {5, max, max, 9, max, max, 3}, {7, max, max, max, 8, max, max}, {max, 9, max, max, max, 4, max}, {max, max, 8, max, max, 5, 4}, {max, max, max, 4, 5, max, 6}, {2, 3, max, max, 4, 6, max}, }; MGraph graph = new MGraph(vertexNum); MST mst = new MST(); mst.createGraph(graph, vertexNum, vertexes, weight); // mst.showGraph(graph); mst.prim(graph, 1); } } //创建最小生成树 class MST { //构建图 public void createGraph(MGraph graph, int vertexNum, char[] vertexes, int[][] weight) { graph.vertexNum = vertexNum; //浅拷贝 for (int i = 0; i \u003c vertexNum; i++) { graph.vertexes[i] = vertexes[i]; for (int j = 0; j \u003c vertexNum; j++) { graph.weight[i][j] = weight[i][j]; } } } public void showGraph(MGraph graph) { for (int[] row : graph.weight) { System.out.println(Arrays.toString(row)); } } //使用prim算法，从第v个节点开始进行访问 public void prim(MGraph graph, int v) { boolean[] isVisited = new boolean[graph.vertexNum]; //标记第一个节点已被访问 isVisited[v] = true; //记录最小边的to节点下标 int x = -1; int y = -1; int min = Integer.MAX_VALUE; //需要循环 graph.vertexNum-1 次，因为有 graph.vertexNum-1 条边 for (int k = 1; k \u003c graph.vertexNum; k++) { //这个是确定每一次生成的子图 ，和哪个结点的距离最近，就找到一个边：所有被访问节点与未访问节点中，寻找最短边 for (int i = 0; i \u003c graph.vertexNum; i++) { for (int j = 0; j \u003c graph.vertexNum; j++) { //这里i表示已访问节点，j表示未访问节点 if (isVisited[i] \u0026\u0026 !isVisited[j] \u0026\u0026 graph.weight[i][j] \u003c min) { min = graph.weight[i][j]; x = i; y = j; } } } System.out.println(\"边\u003c\" + graph.vertexes[x] + \"--\" + graph.vertexes[y] + \"\u003e\" + \" 权值：\" + graph.weight[x][y]); //还原 isVisited[y] = true; min = Integer.MAX_VALUE; } } } class MGraph { int vertexNum;//图节点的个数 char[] vertexes;//存放节点 int[][] weight;//存放边，邻接矩阵 public MGraph(int nodeNum) { this.vertexNum = nodeNum; this.vertexes = new char[nodeNum]; this.weight = new int[nodeNum][nodeNum]; } } ② Kruskal算法（克鲁斯卡尔算法） 在含有n个顶点的连通图中选择n-1条边，构成一棵极小连通子图，并使该连通子图中n-1条边上权值之和达到最小，则称其为连通网的最小生成树。 例如，对于如上图G4所示的连通网可以有多棵权值总和不相同的生成树。 克鲁斯卡尔算法图解 以上图G4为例，来对克鲁斯卡尔进行演示(假设，用数组R保存最小生成树结果)。 第1步：将边\u003cE,F\u003e加入R中。 边\u003cE,F\u003e的权值最小，因此将它加入到最小生成树结果R中。 第2步：将边\u003cC,D\u003e加入R中。 上一步操作之后，边\u003cC,D\u003e的权值最小，因此将它加入到最小生成树结果R中。 第3步：将边\u003cD,E\u003e加入R中。 上一步操作之后，边\u003cD,E\u003e的权值最小，因此将它加入到最小生成树结果R中。 第4步：将边\u003cB,F\u003e加入R中。 上一步操作之后，边\u003cC,E\u003e的权值最小，但\u003cC,E\u003e会和已有的边构成回路；因此，跳过边\u003cC,E\u003e。同理，跳过边\u003cC,F\u003e。将边\u003cB,F\u003e加入到最小生成树结果R中。 第5步：将边\u003cE,G\u003e加入R中。 上一步操作之后，边\u003cE,G\u003e的权值最小，因此将它加入到最小生成树结果R中。 第6步：将边\u003cA,B\u003e加入R中。 上一步操作之后，边\u003cF,G\u003e的权值最小，但\u003cF,G\u003e会和已有的边构成回路；因此，跳过边\u003cF,G\u003e。同理，跳过边\u003cB,C\u003e。将边\u003cA,B\u003e加入到最小生成树结果R中。 此时，最小生成树构造完成！它包括的边依次是：\u003cE,F\u003e \u003cC,D\u003e \u003cD,E\u003e \u003cB,F\u003e \u003cE,G\u003e \u003cA,B\u003e。 克鲁斯卡尔算法分析 根据前面介绍的克鲁斯卡尔算法的基本思想和做法，我们能够了解到，克鲁斯卡尔算法重点需要解决的以下两个问题： 问题一 对图的所有边按照权值大小进行排序。 问题二 将边添加到最小生成树中时，怎么样判断是否形成了回路。 问题一很好解决，采用排序算法进行排序即可。 问题二，实现克鲁斯卡尔算法的难点在于“如何判断一个新边是否会和已选择的边构成环路”，有一种判断的方法： 初始状态下，为连通网中的各个顶点配置不同的标记。对于一个新边，如果它两端顶点的标记不同，就不会构成环路，可以组成最小生成树。一旦新边被选择，需要将它的两个顶点以及和它直接相连的所有已选边两端的顶点改为相同的标记；反之，如果新边两端顶点的标记相同，就表示会构成环路。 代码： package algorithm.kruskal; import java.util.ArrayList; import java.util.Arrays; import java.util.Collections; /* * 克鲁斯卡尔算法求最小生成树 * */ public class kruskal { private int edgeNum = 0; private char[] vertexes; private int[][] matrix; private static final int INF = Integer.MAX_VALUE;//两个顶点不能联通 public stat","date":"2022-07-07","objectID":"/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95java%E7%89%88/:12:6","tags":[],"title":"数据结构与算法（java版）","uri":"/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95java%E7%89%88/"},{"categories":["数据结构与算法"],"content":"7. 最短路径 ① Dijkstra算法（迪杰斯特拉算法） 看一个应用场景和问题： 战争时期，胜利乡有7个村庄(A, B, C, D, E, F, G) ，现在有六个邮差，从G点出发，需要分别把邮件分别送到 A, B, C , D, E, F 六个村庄各个村庄的距离用边线表示(权) ，比如 A – B 距离 5公里问：如何计算出G村庄到 其它各个村庄的最短距离? 如果从其它点出发到各个点的最短距离又是多少? **介绍: ** 迪杰斯特拉(Dijkstra)算法是典型最短路径算法，用于计算一个结点到其他结点的最短路径。 它的主要特点是以起始点为中心向外层层扩展(广度优先搜索思想)，直到扩展到终点为止。 算法过程: 设置出发顶点为v，顶点集合V{v1,v2,vi…}，v到V中各顶点的距离构成距离集合Dis，Dis{d1,d2,di…}，Dis集合记录着v到图中各顶点的距离(到自身可以看作0，v到vi距离对应为di) 从Dis中选择值最小的di并移出Dis集合，同时移出V集合中对应的顶点vi，此时的v到vi即为最短路径 更新Dis集合，更新规则为：比较v到V集合中顶点的距离值，与v通过vi到V集合中顶点的距离值，保留值较小的一个(同时也应该更新顶点的前驱节点为vi，表明是通过vi到达的) 重复执行两步骤，直到最短路径顶点为目标顶点即可结束 代码: package algorithm.dijkstra; import java.util.Arrays; /* * 迪杰斯特拉算法求解最短路径问题 * */ public class dijkstra { public static void main(String[] args) { char[] vertex = {'A', 'B', 'C', 'D', 'E', 'F', 'G'}; //邻接矩阵 int[][] matrix = new int[vertex.length][vertex.length]; final int N = 65535;// 表示不可以连接 matrix[0] = new int[]{N, 5, 7, N, N, N, 2}; matrix[1] = new int[]{5, N, N, 9, N, N, 3}; matrix[2] = new int[]{7, N, N, N, 8, N, N}; matrix[3] = new int[]{N, 9, N, N, N, 4, N}; matrix[4] = new int[]{N, N, 8, N, N, 5, 4}; matrix[5] = new int[]{N, N, N, 4, 5, N, 6}; matrix[6] = new int[]{2, 3, N, N, 4, 6, N}; //创建 Graph对象 Graph graph = new Graph(vertex, matrix); //测试, 看看图的邻接矩阵是否ok graph.showGraph(); //测试迪杰斯特拉算法 graph.dsj(0);//A graph.showDijkstra(); } } class Graph { private char[] vertex; // 顶点数组 private int[][] matrix; // 邻接矩阵 private VisitedVertex vv; //已经访问的顶点的集合 // 构造器 public Graph(char[] vertex, int[][] matrix) { this.vertex = vertex; this.matrix = matrix; } //显示结果 public void showDijkstra() { vv.show(); } // 显示图 public void showGraph() { for (int[] link : matrix) { System.out.println(Arrays.toString(link)); } } //迪杰斯特拉算法实现 /** * @param index 表示出发顶点对应的下标 */ public void dsj(int index) { vv = new VisitedVertex(vertex.length, index); update(index);//更新index顶点到周围顶点的距离和前驱顶点 for (int j = 1; j \u003c vertex.length; j++) { index = vv.updateArr();// 选择并返回新的访问顶点 update(index); // 更新index顶点到周围顶点的距离和前驱顶点 } } //更新index下标顶点到周围顶点的距离和周围顶点的前驱顶点, private void update(int index) { int len = 0; //根据遍历我们的邻接矩阵的 matrix[index]行 for (int j = 0; j \u003c matrix[index].length; j++) { // len 含义是 : 出发顶点到index顶点的距离 + 从index顶点到j顶点的距离的和 len = vv.getDis(index) + matrix[index][j]; // 如果j顶点没有被访问过，并且 len 小于出发顶点到j顶点的距离，就需要更新 if (!vv.in(j) \u0026\u0026 len \u003c vv.getDis(j)) { vv.updatePre(j, index); //更新j顶点的前驱为index顶点 vv.updateDis(j, len); //更新出发顶点到j顶点的距离 } } } } // 已访问顶点集合 class VisitedVertex { // 记录各个顶点是否访问过 1表示访问过,0未访问,会动态更新 public int[] already_arr; // 每个下标对应的值为前一个顶点下标, 会动态更新 public int[] pre_visited; // 记录出发顶点到其他所有顶点的距离,比如G为出发顶点，就会记录G到其它顶点的距离，会动态更新，求的最短距离就会存放到dis public int[] dis; //构造器 /** * @param length :表示顶点的个数 * @param index: 出发顶点对应的下标, 比如G顶点，下标就是6 */ public VisitedVertex(int length, int index) { this.already_arr = new int[length]; this.pre_visited = new int[length]; this.dis = new int[length]; //初始化 dis数组 Arrays.fill(dis, 65535); this.already_arr[index] = 1; //设置出发顶点被访问过 this.dis[index] = 0;//设置出发顶点的访问距离为0 } /** * 功能: 判断index顶点是否被访问过 * * @param index * @return 如果访问过，就返回true, 否则访问false */ public boolean in(int index) { return already_arr[index] == 1; } /** * 功能: 更新出发顶点到index顶点的距离 * * @param index * @param len */ public void updateDis(int index, int len) { dis[index] = len; } /** * 功能: 更新pre这个顶点的前驱顶点为index顶点 * * @param pre * @param index */ public void updatePre(int pre, int index) { pre_visited[pre] = index; } /** * 功能:返回出发顶点到index顶点的距离 * * @param index */ public int getDis(int index) { return dis[index]; } /** * 继续选择并返回新的访问顶点， 比如这里的G 完后，就是 A点作为新的访问顶点(注意不是出发顶点) * * @return */ public int updateArr() { int min = 65535, index = 0; for (int i = 0; i \u003c already_arr.length; i++) { if (already_arr[i] == 0 \u0026\u0026 dis[i] \u003c min) { min = dis[i]; index = i; } } //更新 index 顶点被访问过 already_arr[index] = 1; return index; } //显示最后的结果 //即将三个数组的情况输出 public void show() { System.out.println(\"==========================\"); //输出already_arr for (int i : already_arr) { System.out.print(i + \" \"); } System.out.println(); //输出pre_visited for (int i : pre","date":"2022-07-07","objectID":"/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95java%E7%89%88/:12:7","tags":[],"title":"数据结构与算法（java版）","uri":"/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95java%E7%89%88/"},{"categories":["数据结构与算法"],"content":"8. 马踏棋盘算法 需求 将马随机放在国际象棋的Board[0～7][0～7]的某个方格中，马按走棋规则进行移动。，走遍棋盘上全部64个方格。编制非递归程序，求出马的行走路线，并按求出的行走路线，将数字1，2，…，64依次填入一个8×8的方阵，输出之。 分析 最基本的应该是深度优先搜索，但是对于一个8×8的棋盘，如果采取暴力搜索，将会耗费很长时间而得不到一个结果，如果采用贪心算法，对路径有目的地筛选，尽量选择出口少的路先走，也就是对当前点的下一个落脚点（可能是8个）进行排序，优先走可走的路最少的那个点，使得走法较好。通俗来讲，就是先预判下一个可能落脚点的出口数，出口数最少的先走掉。 贪心算法 贪心算法（又称贪婪算法）是指，在对问题求解时，总是做出在当前看来是最好的选择。也就是说，不从整体最优上加以考虑，他所做出的是在某种意义上的局部最优解。 贪心算法不是对所有问题都能得到整体最优解，关键是贪心策略的选择，选择的贪心策略必须具备无后效性，即某个状态以前的过程不会影响以后的状态，只与当前状态有关。 ——百度百科 代码 package algorithm.horse; import java.awt.Point; import java.util.ArrayList; import java.util.Comparator; public class horse { private static int X; // 棋盘的列数 private static int Y; // 棋盘的行数 //创建一个数组，标记棋盘的各个位置是否被访问过 private static boolean[] visited; //使用一个属性，标记是否棋盘的所有位置都被访问 private static boolean finished; // 如果为true,表示成功 public static void main(String[] args) { System.out.println(\"骑士周游算法，开始运行~~\"); //测试骑士周游算法是否正确 X = 8; Y = 8; int row = 1; //马儿初始位置的行，从1开始编号 int column = 1; //马儿初始位置的列，从1开始编号 //创建棋盘 int[][] chessboard = new int[X][Y]; visited = new boolean[X * Y];//初始值都是false //测试一下耗时 long start = System.currentTimeMillis(); traversalChessboard(chessboard, row - 1, column - 1, 1); long end = System.currentTimeMillis(); System.out.println(\"共耗时: \" + (end - start) + \" 毫秒\"); //输出棋盘的最后情况 for (int[] rows : chessboard) { for (int step : rows) { System.out.print(step + \"\\t\"); } System.out.println(); } } /** * 完成骑士周游问题的算法 * * @param chessboard 棋盘 * @param row 马儿当前的位置的行 从0开始 * @param column 马儿当前的位置的列 从0开始 * @param step 是第几步 ,初始位置就是第1步 */ public static void traversalChessboard(int[][] chessboard, int row, int column, int step) { chessboard[row][column] = step; //row = 4 X = 8 column = 4 = 4 * 8 + 4 = 36 visited[row * X + column] = true; //标记该位置已经访问 //获取当前位置可以走的下一个位置的集合 ArrayList\u003cPoint\u003e ps = next(new Point(row, column)); //贪心策略优化：对ps进行排序,排序的规则就是对ps的所有的Point对象的下一步的位置的数目，进行非递减排序 sort(ps); //遍历 ps while (!ps.isEmpty()) { Point p = ps.remove(0);//取出下一个可以走的位置 //判断该点是否已经访问过 if (!visited[p.x * X + p.y]) {//说明还没有访问过 traversalChessboard(chessboard, p.x, p.y, step + 1); } } //判断马儿是否完成了任务，使用 step 和应该走的步数比较 ， //如果没有达到数量，则表示没有完成任务，将整个棋盘置0 //说明: step \u003c X * Y 成立的情况有两种 //1. 棋盘到目前位置,仍然没有走完 //2. 棋盘处于一个回溯过程 if (step \u003c X * Y \u0026\u0026 !finished) { chessboard[row][column] = 0; visited[row * X + column] = false; } else { finished = true; } } /** * 功能： 根据当前位置(Point对象)，计算马儿还能走哪些位置(Point)，并放入到一个集合中(ArrayList), 最多有8个位置 * * @param curPoint * @return */ public static ArrayList\u003cPoint\u003e next(Point curPoint) { //创建一个ArrayList ArrayList\u003cPoint\u003e ps = new ArrayList\u003c\u003e(); //创建一个Point Point p1 = new Point(); //表示马儿可以走5这个位置 if ((p1.x = curPoint.x - 2) \u003e= 0 \u0026\u0026 (p1.y = curPoint.y - 1) \u003e= 0) { ps.add(new Point(p1)); } //判断马儿可以走6这个位置 if ((p1.x = curPoint.x - 1) \u003e= 0 \u0026\u0026 (p1.y = curPoint.y - 2) \u003e= 0) { ps.add(new Point(p1)); } //判断马儿可以走7这个位置 if ((p1.x = curPoint.x + 1) \u003c X \u0026\u0026 (p1.y = curPoint.y - 2) \u003e= 0) { ps.add(new Point(p1)); } //判断马儿可以走0这个位置 if ((p1.x = curPoint.x + 2) \u003c X \u0026\u0026 (p1.y = curPoint.y - 1) \u003e= 0) { ps.add(new Point(p1)); } //判断马儿可以走1这个位置 if ((p1.x = curPoint.x + 2) \u003c X \u0026\u0026 (p1.y = curPoint.y + 1) \u003c Y) { ps.add(new Point(p1)); } //判断马儿可以走2这个位置 if ((p1.x = curPoint.x + 1) \u003c X \u0026\u0026 (p1.y = curPoint.y + 2) \u003c Y) { ps.add(new Point(p1)); } //判断马儿可以走3这个位置 if ((p1.x = curPoint.x - 1) \u003e= 0 \u0026\u0026 (p1.y = curPoint.y + 2) \u003c Y) { ps.add(new Point(p1)); } //判断马儿可以走4这个位置 if ((p1.x = curPoint.x - 2) \u003e= 0 \u0026\u0026 (p1.y = curPoint.y + 1) \u003c Y) { ps.add(new Point(p1)); } return ps; } //根据当前这个一步的所有的下一步的选择位置，进行非递减排序, 减少回溯的次数 public static void sort(ArrayList\u003cPoint\u003e ps) { ps.sort(new Comparator\u003cPoint\u003e() { @Override public int compare(Point o1, Point o2) { // TODO Auto-generated method stub //获取到o1的下一步的所有位置个数 int count1 = next(o1).size(); //获取到o2的下一步的所有位置个数 int count2 = next(o2).size(); if (count1 \u003c count2) { return -1; } else if (count1 == count2) { return 0; } else { return 1; } } }); } } ","date":"2022-07-07","objectID":"/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95java%E7%89%88/:12:8","tags":[],"title":"数据结构与算法（java版）","uri":"/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95java%E7%89%88/"}]